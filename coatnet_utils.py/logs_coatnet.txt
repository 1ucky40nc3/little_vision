[2022-05-30 21:27:10 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 21:27:10 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cub-200
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 21:27:10 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 21:27:11 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2022-05-30 21:27:11 MetaFG_0] (main.py 110): INFO number of params: 28170955
[2022-05-30 21:27:11 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 21:27:11 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 21:27:11 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 21:27:12 MetaFG_0] (main.py 141): INFO no checkpoint found in output/MetaFG_0/cub-200_v1, ignoring auto resume
[2022-05-30 21:27:12 MetaFG_0] (main.py 159): INFO Start training
[2022-05-30 21:27:18 MetaFG_0] (main.py 265): INFO Train: [0/300][0/187]	eta 0:21:09 lr 0.000000	time 6.7905 (6.7905)	loss 2.6949 (2.6949)	grad_norm 5.5637 (5.5637)	mem 13710MB
[2022-05-30 21:27:28 MetaFG_0] (main.py 265): INFO Train: [0/300][10/187]	eta 0:04:18 lr 0.000000	time 0.9206 (1.4627)	loss 2.6612 (2.6723)	grad_norm 6.0101 (inf)	mem 13710MB
[2022-05-30 21:27:37 MetaFG_0] (main.py 265): INFO Train: [0/300][20/187]	eta 0:03:22 lr 0.000000	time 0.9295 (1.2109)	loss 2.7018 (2.6764)	grad_norm 7.2567 (inf)	mem 13710MB
[2022-05-30 21:27:46 MetaFG_0] (main.py 265): INFO Train: [0/300][30/187]	eta 0:02:56 lr 0.000000	time 0.9224 (1.1259)	loss 2.6840 (2.6797)	grad_norm 5.0166 (inf)	mem 13710MB
[2022-05-30 21:27:56 MetaFG_0] (main.py 265): INFO Train: [0/300][40/187]	eta 0:02:38 lr 0.000000	time 0.9244 (1.0777)	loss 2.6776 (2.6786)	grad_norm 6.8069 (inf)	mem 13710MB
[2022-05-30 21:28:05 MetaFG_0] (main.py 265): INFO Train: [0/300][50/187]	eta 0:02:24 lr 0.000000	time 0.9241 (1.0516)	loss 2.6671 (2.6792)	grad_norm 3.5341 (inf)	mem 13710MB
[2022-05-30 21:28:24 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 21:28:24 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cub-200
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 21:28:24 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 21:28:25 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2022-05-30 21:28:25 MetaFG_0] (main.py 110): INFO number of params: 28170955
[2022-05-30 21:28:25 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 21:28:25 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 21:28:25 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 21:28:25 MetaFG_0] (main.py 141): INFO no checkpoint found in output/MetaFG_0/cub-200_v1, ignoring auto resume
[2022-05-30 21:28:25 MetaFG_0] (main.py 159): INFO Start training
[2022-05-30 21:28:31 MetaFG_0] (main.py 265): INFO Train: [0/300][0/187]	eta 0:18:59 lr 0.000000	time 6.0913 (6.0913)	loss 2.6949 (2.6949)	grad_norm 5.5622 (5.5622)	mem 13711MB
[2022-05-30 21:28:40 MetaFG_0] (main.py 265): INFO Train: [0/300][10/187]	eta 0:04:08 lr 0.000000	time 0.9216 (1.4017)	loss 2.6612 (2.6723)	grad_norm 6.0146 (inf)	mem 13711MB
[2022-05-30 21:28:49 MetaFG_0] (main.py 265): INFO Train: [0/300][20/187]	eta 0:03:16 lr 0.000000	time 0.9663 (1.1792)	loss 2.7019 (2.6764)	grad_norm 7.2635 (inf)	mem 13711MB
[2022-05-30 21:28:59 MetaFG_0] (main.py 265): INFO Train: [0/300][30/187]	eta 0:02:52 lr 0.000000	time 0.9208 (1.0990)	loss 2.6839 (2.6797)	grad_norm 5.0172 (inf)	mem 13711MB
[2022-05-30 21:29:08 MetaFG_0] (main.py 265): INFO Train: [0/300][40/187]	eta 0:02:35 lr 0.000000	time 0.9208 (1.0571)	loss 2.6775 (2.6786)	grad_norm 6.8331 (inf)	mem 13711MB
[2022-05-30 21:29:17 MetaFG_0] (main.py 265): INFO Train: [0/300][50/187]	eta 0:02:21 lr 0.000000	time 0.9265 (1.0324)	loss 2.6671 (2.6792)	grad_norm 3.5461 (inf)	mem 13711MB
[2022-05-30 21:29:27 MetaFG_0] (main.py 265): INFO Train: [0/300][60/187]	eta 0:02:09 lr 0.000000	time 0.9341 (1.0161)	loss 2.6468 (2.6783)	grad_norm 4.9177 (inf)	mem 13711MB
[2022-05-30 21:29:36 MetaFG_0] (main.py 265): INFO Train: [0/300][70/187]	eta 0:01:57 lr 0.000000	time 0.9216 (1.0044)	loss 2.7217 (2.6794)	grad_norm 4.9800 (inf)	mem 13711MB
[2022-05-30 21:29:45 MetaFG_0] (main.py 265): INFO Train: [0/300][80/187]	eta 0:01:46 lr 0.000000	time 0.9192 (0.9952)	loss 2.7209 (2.6814)	grad_norm 9.5378 (inf)	mem 13711MB
[2022-05-30 21:29:55 MetaFG_0] (main.py 265): INFO Train: [0/300][90/187]	eta 0:01:35 lr 0.000000	time 0.9217 (0.9880)	loss 2.6791 (2.6803)	grad_norm 6.2042 (inf)	mem 13711MB
[2022-05-30 21:30:04 MetaFG_0] (main.py 265): INFO Train: [0/300][100/187]	eta 0:01:25 lr 0.000000	time 0.9242 (0.9824)	loss 2.6980 (2.6802)	grad_norm 4.4378 (inf)	mem 13711MB
[2022-05-30 21:30:13 MetaFG_0] (main.py 265): INFO Train: [0/300][110/187]	eta 0:01:15 lr 0.000000	time 0.9216 (0.9777)	loss 2.6989 (2.6813)	grad_norm 5.3630 (inf)	mem 13711MB
[2022-05-30 21:30:23 MetaFG_0] (main.py 265): INFO Train: [0/300][120/187]	eta 0:01:05 lr 0.000000	time 0.9232 (0.9738)	loss 2.6533 (2.6798)	grad_norm 5.0245 (inf)	mem 13711MB
[2022-05-30 21:30:32 MetaFG_0] (main.py 265): INFO Train: [0/300][130/187]	eta 0:00:55 lr 0.000000	time 0.9221 (0.9706)	loss 2.7159 (2.6812)	grad_norm 7.7093 (inf)	mem 13711MB
[2022-05-30 21:30:41 MetaFG_0] (main.py 265): INFO Train: [0/300][140/187]	eta 0:00:45 lr 0.000000	time 0.9222 (0.9678)	loss 2.6441 (2.6814)	grad_norm 4.5542 (inf)	mem 13711MB
[2022-05-30 21:30:50 MetaFG_0] (main.py 265): INFO Train: [0/300][150/187]	eta 0:00:35 lr 0.000000	time 0.9243 (0.9654)	loss 2.6843 (2.6813)	grad_norm 5.2015 (inf)	mem 13711MB
[2022-05-30 21:31:00 MetaFG_0] (main.py 265): INFO Train: [0/300][160/187]	eta 0:00:26 lr 0.000000	time 0.9222 (0.9631)	loss 2.6888 (2.6814)	grad_norm 7.9665 (nan)	mem 13711MB
[2022-05-30 21:31:09 MetaFG_0] (main.py 265): INFO Train: [0/300][170/187]	eta 0:00:16 lr 0.000000	time 0.9232 (0.9612)	loss 2.7036 (2.6816)	grad_norm 4.1889 (nan)	mem 13711MB
[2022-05-30 21:31:18 MetaFG_0] (main.py 265): INFO Train: [0/300][180/187]	eta 0:00:06 lr 0.000000	time 0.9049 (0.9593)	loss 2.6860 (2.6822)	grad_norm 7.8138 (nan)	mem 13711MB
[2022-05-30 21:31:24 MetaFG_0] (main.py 272): INFO EPOCH 0 training takes 0:02:59
[2022-05-30 21:31:24 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_0.pth saving......
[2022-05-30 21:31:25 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_0.pth saved !!!
[2022-05-30 21:31:25 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:31:26 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:31:26 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:31:28 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.313 (2.313)	Loss 5.3518 (5.3518)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 13711MB
[2022-05-30 21:31:32 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.276 (0.541)	Loss 5.3444 (5.3407)	Acc@1 0.000 (0.568)	Acc@5 0.000 (2.557)	Mem 13711MB
[2022-05-30 21:31:36 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.586 (0.464)	Loss 5.3997 (5.3633)	Acc@1 0.000 (0.298)	Acc@5 0.000 (1.935)	Mem 13711MB
[2022-05-30 21:31:39 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.276 (0.428)	Loss 5.4062 (5.3694)	Acc@1 0.000 (0.403)	Acc@5 3.125 (1.815)	Mem 13711MB
[2022-05-30 21:31:43 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.554 (0.421)	Loss 5.3580 (5.3636)	Acc@1 0.000 (0.381)	Acc@5 0.000 (2.058)	Mem 13711MB
[2022-05-30 21:31:47 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.281 (0.410)	Loss 5.3661 (5.3626)	Acc@1 0.000 (0.490)	Acc@5 0.000 (1.961)	Mem 13711MB
[2022-05-30 21:31:51 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.621 (0.407)	Loss 5.2667 (5.3585)	Acc@1 0.000 (0.512)	Acc@5 0.000 (2.049)	Mem 13711MB
[2022-05-30 21:31:55 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.277 (0.404)	Loss 5.2736 (5.3599)	Acc@1 0.000 (0.528)	Acc@5 0.000 (1.981)	Mem 13711MB
[2022-05-30 21:31:59 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.743 (0.406)	Loss 5.4238 (5.3598)	Acc@1 0.000 (0.617)	Acc@5 0.000 (2.045)	Mem 13711MB
[2022-05-30 21:32:03 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.282 (0.401)	Loss 5.3719 (5.3586)	Acc@1 0.000 (0.652)	Acc@5 0.000 (2.163)	Mem 13711MB
[2022-05-30 21:32:06 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.640 (0.398)	Loss 5.3680 (5.3577)	Acc@1 0.000 (0.619)	Acc@5 0.000 (2.166)	Mem 13711MB
[2022-05-30 21:32:10 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.285 (0.394)	Loss 5.3403 (5.3557)	Acc@1 0.000 (0.648)	Acc@5 3.125 (2.140)	Mem 13711MB
[2022-05-30 21:32:14 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.683 (0.394)	Loss 5.2934 (5.3538)	Acc@1 0.000 (0.671)	Acc@5 3.125 (2.169)	Mem 13711MB
[2022-05-30 21:32:17 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.311 (0.391)	Loss 5.4165 (5.3525)	Acc@1 0.000 (0.716)	Acc@5 3.125 (2.171)	Mem 13711MB
[2022-05-30 21:32:21 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.616 (0.391)	Loss 5.3925 (5.3524)	Acc@1 0.000 (0.665)	Acc@5 0.000 (2.194)	Mem 13711MB
[2022-05-30 21:32:25 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.341 (0.390)	Loss 5.2868 (5.3521)	Acc@1 0.000 (0.662)	Acc@5 0.000 (2.194)	Mem 13711MB
[2022-05-30 21:32:29 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.556 (0.389)	Loss 5.4248 (5.3519)	Acc@1 0.000 (0.621)	Acc@5 0.000 (2.174)	Mem 13711MB
[2022-05-30 21:32:32 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.326 (0.388)	Loss 5.4447 (5.3525)	Acc@1 0.000 (0.621)	Acc@5 3.125 (2.175)	Mem 13711MB
[2022-05-30 21:32:36 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.385)	Loss 5.3820 (5.3527)	Acc@1 0.000 (0.622)	Acc@5 0.000 (2.158)	Mem 13711MB
[2022-05-30 21:32:36 MetaFG_0] (main.py 330): INFO  * Acc@1 0.621 Acc@5 2.157
[2022-05-30 21:32:36 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.6%
[2022-05-30 21:32:36 MetaFG_0] (main.py 171): INFO Max accuracy: 0.62%
[2022-05-30 21:32:39 MetaFG_0] (main.py 265): INFO Train: [1/300][0/187]	eta 0:08:34 lr 0.000000	time 2.7498 (2.7498)	loss 2.6476 (2.6476)	grad_norm 3.1734 (3.1734)	mem 13711MB
[2022-05-30 21:32:49 MetaFG_0] (main.py 265): INFO Train: [1/300][10/187]	eta 0:03:16 lr 0.000000	time 0.9222 (1.1122)	loss 2.6412 (2.6782)	grad_norm 4.6808 (5.2469)	mem 13711MB
[2022-05-30 21:32:58 MetaFG_0] (main.py 265): INFO Train: [1/300][20/187]	eta 0:02:51 lr 0.000000	time 0.9211 (1.0265)	loss 2.6879 (2.6753)	grad_norm 5.8501 (5.7869)	mem 13711MB
[2022-05-30 21:33:07 MetaFG_0] (main.py 265): INFO Train: [1/300][30/187]	eta 0:02:36 lr 0.000000	time 0.9209 (0.9956)	loss 2.6606 (2.6768)	grad_norm 5.6897 (6.3209)	mem 13711MB
[2022-05-30 21:33:17 MetaFG_0] (main.py 265): INFO Train: [1/300][40/187]	eta 0:02:23 lr 0.000000	time 0.9214 (0.9795)	loss 2.6710 (2.6764)	grad_norm 6.2650 (6.4144)	mem 13711MB
[2022-05-30 21:33:26 MetaFG_0] (main.py 265): INFO Train: [1/300][50/187]	eta 0:02:12 lr 0.000000	time 0.9214 (0.9700)	loss 2.6941 (2.6778)	grad_norm 5.9694 (6.4792)	mem 13711MB
[2022-05-30 21:33:35 MetaFG_0] (main.py 265): INFO Train: [1/300][60/187]	eta 0:02:02 lr 0.000000	time 0.9223 (0.9635)	loss 2.7099 (2.6800)	grad_norm 7.3150 (6.6536)	mem 13711MB
[2022-05-30 21:33:45 MetaFG_0] (main.py 265): INFO Train: [1/300][70/187]	eta 0:01:52 lr 0.000000	time 0.9221 (0.9588)	loss 2.6795 (2.6783)	grad_norm 4.2417 (6.6002)	mem 13711MB
[2022-05-30 21:33:54 MetaFG_0] (main.py 265): INFO Train: [1/300][80/187]	eta 0:01:42 lr 0.000000	time 0.9220 (0.9553)	loss 2.6939 (2.6790)	grad_norm inf (inf)	mem 13711MB
[2022-05-30 21:34:03 MetaFG_0] (main.py 265): INFO Train: [1/300][90/187]	eta 0:01:32 lr 0.000000	time 0.9212 (0.9524)	loss 2.6905 (2.6786)	grad_norm 4.8807 (nan)	mem 13711MB
[2022-05-30 21:34:12 MetaFG_0] (main.py 265): INFO Train: [1/300][100/187]	eta 0:01:22 lr 0.000000	time 0.9230 (0.9503)	loss 2.7033 (2.6782)	grad_norm 4.9353 (nan)	mem 13711MB
[2022-05-30 21:34:22 MetaFG_0] (main.py 265): INFO Train: [1/300][110/187]	eta 0:01:13 lr 0.000001	time 0.9210 (0.9485)	loss 2.6911 (2.6771)	grad_norm 10.5786 (nan)	mem 13711MB
[2022-05-30 21:34:31 MetaFG_0] (main.py 265): INFO Train: [1/300][120/187]	eta 0:01:03 lr 0.000001	time 0.9218 (0.9470)	loss 2.6637 (2.6775)	grad_norm 7.2317 (nan)	mem 13711MB
[2022-05-30 21:34:40 MetaFG_0] (main.py 265): INFO Train: [1/300][130/187]	eta 0:00:53 lr 0.000001	time 0.9218 (0.9457)	loss 2.6784 (2.6776)	grad_norm 5.0192 (nan)	mem 13711MB
[2022-05-30 21:34:50 MetaFG_0] (main.py 265): INFO Train: [1/300][140/187]	eta 0:00:44 lr 0.000001	time 0.9217 (0.9446)	loss 2.6796 (2.6777)	grad_norm 4.5137 (nan)	mem 13711MB
[2022-05-30 21:34:59 MetaFG_0] (main.py 265): INFO Train: [1/300][150/187]	eta 0:00:34 lr 0.000001	time 0.9216 (0.9436)	loss 2.7015 (2.6783)	grad_norm 4.6400 (nan)	mem 13711MB
[2022-05-30 21:35:08 MetaFG_0] (main.py 265): INFO Train: [1/300][160/187]	eta 0:00:25 lr 0.000001	time 0.9221 (0.9428)	loss 2.6913 (2.6787)	grad_norm 5.1758 (nan)	mem 13711MB
[2022-05-30 21:35:18 MetaFG_0] (main.py 265): INFO Train: [1/300][170/187]	eta 0:00:16 lr 0.000001	time 0.9213 (0.9421)	loss 2.7162 (2.6784)	grad_norm 6.4998 (nan)	mem 13711MB
[2022-05-30 21:35:27 MetaFG_0] (main.py 265): INFO Train: [1/300][180/187]	eta 0:00:06 lr 0.000001	time 0.9047 (0.9412)	loss 2.6739 (2.6784)	grad_norm 4.5888 (nan)	mem 13711MB
[2022-05-30 21:35:32 MetaFG_0] (main.py 272): INFO EPOCH 1 training takes 0:02:55
[2022-05-30 21:35:32 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_1.pth saving......
[2022-05-30 21:35:34 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_1.pth saved !!!
[2022-05-30 21:35:34 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:35:35 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:35:35 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:35:37 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.274 (2.274)	Loss 5.3525 (5.3525)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 13711MB
[2022-05-30 21:35:41 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.286 (0.537)	Loss 5.3777 (5.3585)	Acc@1 0.000 (0.568)	Acc@5 0.000 (1.989)	Mem 13711MB
[2022-05-30 21:35:45 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.708 (0.476)	Loss 5.3649 (5.3608)	Acc@1 0.000 (0.446)	Acc@5 0.000 (2.827)	Mem 13711MB
[2022-05-30 21:35:48 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.280 (0.440)	Loss 5.3808 (5.3623)	Acc@1 0.000 (0.302)	Acc@5 3.125 (2.722)	Mem 13711MB
[2022-05-30 21:35:52 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.608 (0.428)	Loss 5.3324 (5.3581)	Acc@1 0.000 (0.305)	Acc@5 0.000 (2.210)	Mem 13711MB
[2022-05-30 21:35:56 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.279 (0.415)	Loss 5.4079 (5.3550)	Acc@1 0.000 (0.306)	Acc@5 3.125 (2.267)	Mem 13711MB
[2022-05-30 21:36:00 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.733 (0.413)	Loss 5.3520 (5.3546)	Acc@1 0.000 (0.256)	Acc@5 6.250 (2.254)	Mem 13711MB
[2022-05-30 21:36:03 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.276 (0.404)	Loss 5.2965 (5.3507)	Acc@1 0.000 (0.396)	Acc@5 3.125 (2.421)	Mem 13711MB
[2022-05-30 21:36:08 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.757 (0.405)	Loss 5.2929 (5.3479)	Acc@1 3.125 (0.502)	Acc@5 6.250 (2.431)	Mem 13711MB
[2022-05-30 21:36:11 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.280 (0.400)	Loss 5.3843 (5.3506)	Acc@1 0.000 (0.549)	Acc@5 0.000 (2.438)	Mem 13711MB
[2022-05-30 21:36:15 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.729 (0.400)	Loss 5.2798 (5.3488)	Acc@1 0.000 (0.619)	Acc@5 3.125 (2.506)	Mem 13711MB
[2022-05-30 21:36:19 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.281 (0.398)	Loss 5.4142 (5.3499)	Acc@1 0.000 (0.591)	Acc@5 0.000 (2.449)	Mem 13711MB
[2022-05-30 21:36:23 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.616 (0.396)	Loss 5.2967 (5.3485)	Acc@1 0.000 (0.594)	Acc@5 0.000 (2.402)	Mem 13711MB
[2022-05-30 21:36:26 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.280 (0.393)	Loss 5.3461 (5.3494)	Acc@1 0.000 (0.549)	Acc@5 0.000 (2.219)	Mem 13711MB
[2022-05-30 21:36:30 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.638 (0.393)	Loss 5.2600 (5.3484)	Acc@1 0.000 (0.532)	Acc@5 3.125 (2.283)	Mem 13711MB
[2022-05-30 21:36:34 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.284 (0.391)	Loss 5.4077 (5.3483)	Acc@1 0.000 (0.497)	Acc@5 3.125 (2.256)	Mem 13711MB
[2022-05-30 21:36:38 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.705 (0.392)	Loss 5.3581 (5.3465)	Acc@1 3.125 (0.524)	Acc@5 3.125 (2.271)	Mem 13711MB
[2022-05-30 21:36:41 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.285 (0.390)	Loss 5.3351 (5.3470)	Acc@1 0.000 (0.512)	Acc@5 0.000 (2.211)	Mem 13711MB
[2022-05-30 21:36:45 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.387)	Loss 5.3243 (5.3447)	Acc@1 0.000 (0.535)	Acc@5 6.250 (2.331)	Mem 13711MB
[2022-05-30 21:36:45 MetaFG_0] (main.py 330): INFO  * Acc@1 0.535 Acc@5 2.330
[2022-05-30 21:36:45 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.5%
[2022-05-30 21:36:45 MetaFG_0] (main.py 171): INFO Max accuracy: 0.62%
[2022-05-30 21:36:48 MetaFG_0] (main.py 265): INFO Train: [2/300][0/187]	eta 0:08:40 lr 0.000001	time 2.7809 (2.7809)	loss 2.6567 (2.6567)	grad_norm 4.2747 (4.2747)	mem 13711MB
[2022-05-30 21:36:57 MetaFG_0] (main.py 265): INFO Train: [2/300][10/187]	eta 0:03:18 lr 0.000001	time 0.9236 (1.1230)	loss 2.7021 (2.6844)	grad_norm 5.8479 (6.4174)	mem 13711MB
[2022-05-30 21:37:07 MetaFG_0] (main.py 265): INFO Train: [2/300][20/187]	eta 0:02:52 lr 0.000001	time 0.9216 (1.0316)	loss 2.6758 (2.6764)	grad_norm 3.9565 (6.2586)	mem 13711MB
[2022-05-30 21:37:16 MetaFG_0] (main.py 265): INFO Train: [2/300][30/187]	eta 0:02:36 lr 0.000001	time 0.9220 (0.9997)	loss 2.6466 (2.6789)	grad_norm 7.0672 (6.1672)	mem 13711MB
[2022-05-30 21:37:25 MetaFG_0] (main.py 265): INFO Train: [2/300][40/187]	eta 0:02:24 lr 0.000001	time 0.9218 (0.9830)	loss 2.6861 (2.6788)	grad_norm 4.0067 (6.3236)	mem 13711MB
[2022-05-30 21:37:35 MetaFG_0] (main.py 265): INFO Train: [2/300][50/187]	eta 0:02:13 lr 0.000001	time 0.9224 (0.9729)	loss 2.6687 (2.6809)	grad_norm 5.4363 (6.2337)	mem 13711MB
[2022-05-30 21:37:44 MetaFG_0] (main.py 265): INFO Train: [2/300][60/187]	eta 0:02:02 lr 0.000001	time 0.9238 (0.9661)	loss 2.6794 (2.6800)	grad_norm 3.3008 (6.0681)	mem 13711MB
[2022-05-30 21:37:53 MetaFG_0] (main.py 265): INFO Train: [2/300][70/187]	eta 0:01:52 lr 0.000001	time 0.9261 (0.9612)	loss 2.6701 (2.6806)	grad_norm 4.9757 (6.0072)	mem 13711MB
[2022-05-30 21:38:03 MetaFG_0] (main.py 265): INFO Train: [2/300][80/187]	eta 0:01:42 lr 0.000001	time 0.9229 (0.9575)	loss 2.6669 (2.6801)	grad_norm 4.5877 (6.0406)	mem 13711MB
[2022-05-30 21:38:12 MetaFG_0] (main.py 265): INFO Train: [2/300][90/187]	eta 0:01:32 lr 0.000001	time 0.9349 (0.9547)	loss 2.6540 (2.6800)	grad_norm 3.5435 (6.1318)	mem 13711MB
[2022-05-30 21:38:21 MetaFG_0] (main.py 265): INFO Train: [2/300][100/187]	eta 0:01:22 lr 0.000001	time 0.9235 (0.9524)	loss 2.6837 (2.6803)	grad_norm 3.8911 (6.2049)	mem 13711MB
[2022-05-30 21:38:30 MetaFG_0] (main.py 265): INFO Train: [2/300][110/187]	eta 0:01:13 lr 0.000001	time 0.9224 (0.9504)	loss 2.6796 (2.6803)	grad_norm 3.9758 (6.1597)	mem 13711MB
[2022-05-30 21:38:40 MetaFG_0] (main.py 265): INFO Train: [2/300][120/187]	eta 0:01:03 lr 0.000001	time 0.9230 (0.9489)	loss 2.6609 (2.6792)	grad_norm 5.3972 (6.1300)	mem 13711MB
[2022-05-30 21:38:49 MetaFG_0] (main.py 265): INFO Train: [2/300][130/187]	eta 0:00:54 lr 0.000001	time 0.9238 (0.9475)	loss 2.6714 (2.6796)	grad_norm 4.4166 (6.1515)	mem 13711MB
[2022-05-30 21:38:58 MetaFG_0] (main.py 265): INFO Train: [2/300][140/187]	eta 0:00:44 lr 0.000001	time 0.9232 (0.9464)	loss 2.6402 (2.6790)	grad_norm 6.6724 (6.1223)	mem 13711MB
[2022-05-30 21:39:08 MetaFG_0] (main.py 265): INFO Train: [2/300][150/187]	eta 0:00:34 lr 0.000001	time 0.9230 (0.9454)	loss 2.6409 (2.6788)	grad_norm 4.2177 (6.0880)	mem 13711MB
[2022-05-30 21:39:17 MetaFG_0] (main.py 265): INFO Train: [2/300][160/187]	eta 0:00:25 lr 0.000001	time 0.9226 (0.9445)	loss 2.6468 (2.6784)	grad_norm 4.4276 (6.0994)	mem 13711MB
[2022-05-30 21:39:26 MetaFG_0] (main.py 265): INFO Train: [2/300][170/187]	eta 0:00:16 lr 0.000001	time 0.9227 (0.9437)	loss 2.6858 (2.6787)	grad_norm 4.5933 (6.1245)	mem 13711MB
[2022-05-30 21:39:36 MetaFG_0] (main.py 265): INFO Train: [2/300][180/187]	eta 0:00:06 lr 0.000001	time 0.9058 (0.9428)	loss 2.6780 (2.6785)	grad_norm 5.3879 (6.1302)	mem 13711MB
[2022-05-30 21:39:41 MetaFG_0] (main.py 272): INFO EPOCH 2 training takes 0:02:56
[2022-05-30 21:39:41 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_2.pth saving......
[2022-05-30 21:39:42 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_2.pth saved !!!
[2022-05-30 21:39:42 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:39:44 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:39:44 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:39:46 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.205 (2.205)	Loss 5.3104 (5.3104)	Acc@1 0.000 (0.000)	Acc@5 3.125 (3.125)	Mem 13711MB
[2022-05-30 21:39:49 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.285 (0.517)	Loss 5.3087 (5.3520)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.852)	Mem 13711MB
[2022-05-30 21:39:53 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.474 (0.450)	Loss 5.3425 (5.3523)	Acc@1 0.000 (0.298)	Acc@5 6.250 (1.637)	Mem 13711MB
[2022-05-30 21:39:57 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.285 (0.427)	Loss 5.2748 (5.3486)	Acc@1 0.000 (0.302)	Acc@5 0.000 (1.512)	Mem 13711MB
[2022-05-30 21:40:00 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.284 (0.414)	Loss 5.3583 (5.3417)	Acc@1 0.000 (0.305)	Acc@5 0.000 (1.829)	Mem 13711MB
[2022-05-30 21:40:04 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.278 (0.407)	Loss 5.3566 (5.3390)	Acc@1 0.000 (0.429)	Acc@5 0.000 (2.206)	Mem 13711MB
[2022-05-30 21:40:08 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.569 (0.400)	Loss 5.2833 (5.3350)	Acc@1 0.000 (0.461)	Acc@5 0.000 (2.305)	Mem 13711MB
[2022-05-30 21:40:12 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.288 (0.396)	Loss 5.2885 (5.3336)	Acc@1 3.125 (0.572)	Acc@5 6.250 (2.509)	Mem 13711MB
[2022-05-30 21:40:16 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.619 (0.396)	Loss 5.3535 (5.3319)	Acc@1 0.000 (0.502)	Acc@5 0.000 (2.392)	Mem 13711MB
[2022-05-30 21:40:19 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.282 (0.393)	Loss 5.3456 (5.3327)	Acc@1 0.000 (0.515)	Acc@5 0.000 (2.404)	Mem 13711MB
[2022-05-30 21:40:23 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.563 (0.392)	Loss 5.3030 (5.3329)	Acc@1 0.000 (0.557)	Acc@5 3.125 (2.382)	Mem 13711MB
[2022-05-30 21:40:27 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.287 (0.389)	Loss 5.2919 (5.3324)	Acc@1 0.000 (0.535)	Acc@5 6.250 (2.309)	Mem 13711MB
[2022-05-30 21:40:30 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.598 (0.388)	Loss 5.3226 (5.3335)	Acc@1 0.000 (0.568)	Acc@5 0.000 (2.273)	Mem 13711MB
[2022-05-30 21:40:34 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.276 (0.385)	Loss 5.3597 (5.3322)	Acc@1 0.000 (0.549)	Acc@5 3.125 (2.242)	Mem 13711MB
[2022-05-30 21:40:38 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.549 (0.385)	Loss 5.3281 (5.3322)	Acc@1 0.000 (0.510)	Acc@5 3.125 (2.194)	Mem 13711MB
[2022-05-30 21:40:41 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.279 (0.383)	Loss 5.2938 (5.3329)	Acc@1 0.000 (0.538)	Acc@5 3.125 (2.214)	Mem 13711MB
[2022-05-30 21:40:45 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.407 (0.383)	Loss 5.3699 (5.3346)	Acc@1 3.125 (0.524)	Acc@5 3.125 (2.213)	Mem 13711MB
[2022-05-30 21:40:49 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.276 (0.383)	Loss 5.3373 (5.3341)	Acc@1 0.000 (0.585)	Acc@5 3.125 (2.266)	Mem 13711MB
[2022-05-30 21:40:52 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.380)	Loss 5.3223 (5.3329)	Acc@1 0.000 (0.604)	Acc@5 3.125 (2.296)	Mem 13711MB
[2022-05-30 21:40:52 MetaFG_0] (main.py 330): INFO  * Acc@1 0.604 Acc@5 2.295
[2022-05-30 21:40:52 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.6%
[2022-05-30 21:40:52 MetaFG_0] (main.py 171): INFO Max accuracy: 0.62%
[2022-05-30 21:40:55 MetaFG_0] (main.py 265): INFO Train: [3/300][0/187]	eta 0:09:18 lr 0.000001	time 2.9886 (2.9886)	loss 2.6717 (2.6717)	grad_norm 4.5724 (4.5724)	mem 13711MB
[2022-05-30 21:41:05 MetaFG_0] (main.py 265): INFO Train: [3/300][10/187]	eta 0:03:18 lr 0.000001	time 0.9209 (1.1214)	loss 2.6353 (2.6661)	grad_norm 5.7703 (5.9782)	mem 13711MB
[2022-05-30 21:41:14 MetaFG_0] (main.py 265): INFO Train: [3/300][20/187]	eta 0:02:52 lr 0.000001	time 0.9215 (1.0303)	loss 2.7172 (2.6685)	grad_norm 5.5156 (6.3988)	mem 13711MB
[2022-05-30 21:41:23 MetaFG_0] (main.py 265): INFO Train: [3/300][30/187]	eta 0:02:36 lr 0.000001	time 0.9207 (0.9983)	loss 2.6957 (2.6690)	grad_norm 4.8313 (6.1623)	mem 13711MB
[2022-05-30 21:41:33 MetaFG_0] (main.py 265): INFO Train: [3/300][40/187]	eta 0:02:24 lr 0.000001	time 0.9200 (0.9817)	loss 2.6683 (2.6697)	grad_norm 3.1710 (6.1848)	mem 13711MB
[2022-05-30 21:41:42 MetaFG_0] (main.py 265): INFO Train: [3/300][50/187]	eta 0:02:13 lr 0.000001	time 0.9214 (0.9716)	loss 2.6672 (2.6706)	grad_norm 6.4963 (6.2886)	mem 13711MB
[2022-05-30 21:41:51 MetaFG_0] (main.py 265): INFO Train: [3/300][60/187]	eta 0:02:02 lr 0.000001	time 0.9209 (0.9648)	loss 2.6818 (2.6713)	grad_norm 3.2966 (6.1991)	mem 13711MB
[2022-05-30 21:42:01 MetaFG_0] (main.py 265): INFO Train: [3/300][70/187]	eta 0:01:52 lr 0.000001	time 0.9213 (0.9606)	loss 2.6519 (2.6716)	grad_norm 4.1132 (6.1677)	mem 13711MB
[2022-05-30 21:42:10 MetaFG_0] (main.py 265): INFO Train: [3/300][80/187]	eta 0:01:42 lr 0.000001	time 0.9227 (0.9568)	loss 2.6795 (2.6719)	grad_norm 4.9220 (6.1968)	mem 13711MB
[2022-05-30 21:42:19 MetaFG_0] (main.py 265): INFO Train: [3/300][90/187]	eta 0:01:32 lr 0.000001	time 0.9216 (0.9539)	loss 2.6887 (2.6721)	grad_norm 4.4591 (6.1783)	mem 13711MB
[2022-05-30 21:42:29 MetaFG_0] (main.py 265): INFO Train: [3/300][100/187]	eta 0:01:22 lr 0.000001	time 0.9213 (0.9516)	loss 2.6361 (2.6722)	grad_norm 7.9522 (6.1564)	mem 13711MB
[2022-05-30 21:42:38 MetaFG_0] (main.py 265): INFO Train: [3/300][110/187]	eta 0:01:13 lr 0.000001	time 0.9218 (0.9497)	loss 2.6536 (2.6722)	grad_norm 5.9566 (6.1033)	mem 13711MB
[2022-05-30 21:42:47 MetaFG_0] (main.py 265): INFO Train: [3/300][120/187]	eta 0:01:03 lr 0.000001	time 0.9220 (0.9480)	loss 2.6976 (2.6729)	grad_norm 8.0695 (6.0889)	mem 13711MB
[2022-05-30 21:42:56 MetaFG_0] (main.py 265): INFO Train: [3/300][130/187]	eta 0:00:53 lr 0.000001	time 0.9244 (0.9469)	loss 2.6463 (2.6722)	grad_norm 3.4660 (6.0695)	mem 13711MB
[2022-05-30 21:43:06 MetaFG_0] (main.py 265): INFO Train: [3/300][140/187]	eta 0:00:44 lr 0.000001	time 0.9223 (0.9457)	loss 2.6698 (2.6711)	grad_norm 6.8829 (6.0268)	mem 13711MB
[2022-05-30 21:43:15 MetaFG_0] (main.py 265): INFO Train: [3/300][150/187]	eta 0:00:34 lr 0.000001	time 0.9230 (0.9447)	loss 2.6544 (2.6704)	grad_norm 10.1545 (6.0654)	mem 13711MB
[2022-05-30 21:43:24 MetaFG_0] (main.py 265): INFO Train: [3/300][160/187]	eta 0:00:25 lr 0.000001	time 0.9254 (0.9439)	loss 2.6725 (2.6702)	grad_norm 3.8472 (6.0197)	mem 13711MB
[2022-05-30 21:43:34 MetaFG_0] (main.py 265): INFO Train: [3/300][170/187]	eta 0:00:16 lr 0.000001	time 0.9203 (0.9430)	loss 2.6271 (2.6705)	grad_norm 4.5207 (6.0082)	mem 13711MB
[2022-05-30 21:43:43 MetaFG_0] (main.py 265): INFO Train: [3/300][180/187]	eta 0:00:06 lr 0.000001	time 0.9057 (0.9421)	loss 2.6815 (2.6703)	grad_norm 5.0468 (5.9632)	mem 13711MB
[2022-05-30 21:43:49 MetaFG_0] (main.py 272): INFO EPOCH 3 training takes 0:02:56
[2022-05-30 21:43:49 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_3.pth saving......
[2022-05-30 21:43:49 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_3.pth saved !!!
[2022-05-30 21:43:49 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:43:51 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:43:51 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:43:53 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.132 (2.132)	Loss 5.3223 (5.3223)	Acc@1 0.000 (0.000)	Acc@5 6.250 (6.250)	Mem 13711MB
[2022-05-30 21:43:56 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.276 (0.506)	Loss 5.3001 (5.3069)	Acc@1 0.000 (0.852)	Acc@5 3.125 (3.409)	Mem 13711MB
[2022-05-30 21:44:00 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.613 (0.447)	Loss 5.3041 (5.3080)	Acc@1 3.125 (0.744)	Acc@5 3.125 (2.381)	Mem 13711MB
[2022-05-30 21:44:04 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.276 (0.413)	Loss 5.3573 (5.3133)	Acc@1 3.125 (0.706)	Acc@5 3.125 (2.520)	Mem 13711MB
[2022-05-30 21:44:07 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.544 (0.402)	Loss 5.2457 (5.3173)	Acc@1 0.000 (0.762)	Acc@5 6.250 (2.591)	Mem 13711MB
[2022-05-30 21:44:11 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.308 (0.392)	Loss 5.2842 (5.3175)	Acc@1 0.000 (0.797)	Acc@5 3.125 (2.819)	Mem 13711MB
[2022-05-30 21:44:15 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.387 (0.391)	Loss 5.2601 (5.3170)	Acc@1 0.000 (0.717)	Acc@5 0.000 (2.459)	Mem 13711MB
[2022-05-30 21:44:18 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.318 (0.388)	Loss 5.3055 (5.3166)	Acc@1 0.000 (0.704)	Acc@5 0.000 (2.333)	Mem 13711MB
[2022-05-30 21:44:22 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.529 (0.388)	Loss 5.3162 (5.3159)	Acc@1 0.000 (0.694)	Acc@5 0.000 (2.353)	Mem 13711MB
[2022-05-30 21:44:26 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.287 (0.389)	Loss 5.2827 (5.3165)	Acc@1 3.125 (0.790)	Acc@5 6.250 (2.438)	Mem 13711MB
[2022-05-30 21:44:30 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.314 (0.387)	Loss 5.3398 (5.3171)	Acc@1 0.000 (0.712)	Acc@5 0.000 (2.475)	Mem 13711MB
[2022-05-30 21:44:34 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.287 (0.387)	Loss 5.3425 (5.3179)	Acc@1 0.000 (0.760)	Acc@5 3.125 (2.449)	Mem 13711MB
[2022-05-30 21:44:37 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.284 (0.385)	Loss 5.3847 (5.3186)	Acc@1 0.000 (0.697)	Acc@5 3.125 (2.350)	Mem 13711MB
[2022-05-30 21:44:41 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.285 (0.384)	Loss 5.3149 (5.3181)	Acc@1 0.000 (0.716)	Acc@5 6.250 (2.529)	Mem 13711MB
[2022-05-30 21:44:45 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.331 (0.382)	Loss 5.2938 (5.3176)	Acc@1 3.125 (0.731)	Acc@5 6.250 (2.527)	Mem 13711MB
[2022-05-30 21:44:49 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.447 (0.382)	Loss 5.2941 (5.3172)	Acc@1 0.000 (0.724)	Acc@5 3.125 (2.587)	Mem 13711MB
[2022-05-30 21:44:52 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.313 (0.381)	Loss 5.3141 (5.3178)	Acc@1 0.000 (0.718)	Acc@5 0.000 (2.601)	Mem 13711MB
[2022-05-30 21:44:56 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.522 (0.381)	Loss 5.3293 (5.3181)	Acc@1 0.000 (0.694)	Acc@5 0.000 (2.577)	Mem 13711MB
[2022-05-30 21:44:59 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.378)	Loss 5.3611 (5.3182)	Acc@1 0.000 (0.656)	Acc@5 0.000 (2.555)	Mem 13711MB
[2022-05-30 21:44:59 MetaFG_0] (main.py 330): INFO  * Acc@1 0.656 Acc@5 2.572
[2022-05-30 21:44:59 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.7%
[2022-05-30 21:44:59 MetaFG_0] (main.py 171): INFO Max accuracy: 0.66%
[2022-05-30 21:45:02 MetaFG_0] (main.py 265): INFO Train: [4/300][0/187]	eta 0:08:49 lr 0.000001	time 2.8310 (2.8310)	loss 2.6568 (2.6568)	grad_norm 12.7393 (12.7393)	mem 13711MB
[2022-05-30 21:45:12 MetaFG_0] (main.py 265): INFO Train: [4/300][10/187]	eta 0:03:17 lr 0.000001	time 0.9239 (1.1145)	loss 2.6698 (2.6669)	grad_norm 8.1229 (7.4605)	mem 13711MB
[2022-05-30 21:45:21 MetaFG_0] (main.py 265): INFO Train: [4/300][20/187]	eta 0:02:51 lr 0.000001	time 0.9235 (1.0274)	loss 2.6444 (2.6680)	grad_norm 3.4333 (6.1077)	mem 13711MB
[2022-05-30 21:45:30 MetaFG_0] (main.py 265): INFO Train: [4/300][30/187]	eta 0:02:36 lr 0.000001	time 0.9228 (0.9966)	loss 2.6884 (2.6680)	grad_norm 4.3411 (6.0349)	mem 13711MB
[2022-05-30 21:45:40 MetaFG_0] (main.py 265): INFO Train: [4/300][40/187]	eta 0:02:24 lr 0.000001	time 0.9244 (0.9800)	loss 2.6570 (2.6687)	grad_norm 3.7133 (5.9932)	mem 13711MB
[2022-05-30 21:45:49 MetaFG_0] (main.py 265): INFO Train: [4/300][50/187]	eta 0:02:12 lr 0.000001	time 0.9222 (0.9707)	loss 2.6783 (2.6671)	grad_norm 5.8168 (6.0480)	mem 13711MB
[2022-05-30 21:45:58 MetaFG_0] (main.py 265): INFO Train: [4/300][60/187]	eta 0:02:02 lr 0.000001	time 0.9238 (0.9643)	loss 2.6428 (2.6650)	grad_norm 4.8261 (6.1031)	mem 13711MB
[2022-05-30 21:46:08 MetaFG_0] (main.py 265): INFO Train: [4/300][70/187]	eta 0:01:52 lr 0.000001	time 0.9248 (0.9597)	loss 2.6766 (2.6652)	grad_norm 5.7132 (6.0486)	mem 13711MB
[2022-05-30 21:46:17 MetaFG_0] (main.py 265): INFO Train: [4/300][80/187]	eta 0:01:42 lr 0.000001	time 0.9249 (0.9563)	loss 2.6476 (2.6652)	grad_norm 4.3586 (6.0161)	mem 13711MB
[2022-05-30 21:46:26 MetaFG_0] (main.py 265): INFO Train: [4/300][90/187]	eta 0:01:32 lr 0.000001	time 0.9239 (0.9536)	loss 2.6670 (2.6643)	grad_norm 4.2388 (5.9809)	mem 13711MB
[2022-05-30 21:46:36 MetaFG_0] (main.py 265): INFO Train: [4/300][100/187]	eta 0:01:22 lr 0.000001	time 0.9230 (0.9515)	loss 2.6525 (2.6650)	grad_norm 5.2244 (5.9695)	mem 13711MB
[2022-05-30 21:46:45 MetaFG_0] (main.py 265): INFO Train: [4/300][110/187]	eta 0:01:13 lr 0.000001	time 0.9230 (0.9498)	loss 2.6609 (2.6644)	grad_norm 4.3405 (5.9800)	mem 13711MB
[2022-05-30 21:46:54 MetaFG_0] (main.py 265): INFO Train: [4/300][120/187]	eta 0:01:03 lr 0.000001	time 0.9222 (0.9481)	loss 2.6515 (2.6644)	grad_norm 3.2855 (5.9438)	mem 13711MB
[2022-05-30 21:47:04 MetaFG_0] (main.py 265): INFO Train: [4/300][130/187]	eta 0:00:53 lr 0.000001	time 0.9244 (0.9469)	loss 2.6555 (2.6643)	grad_norm 4.7641 (5.9024)	mem 13711MB
[2022-05-30 21:47:13 MetaFG_0] (main.py 265): INFO Train: [4/300][140/187]	eta 0:00:44 lr 0.000001	time 0.9250 (0.9458)	loss 2.6426 (2.6648)	grad_norm 6.1994 (5.9204)	mem 13711MB
[2022-05-30 21:47:22 MetaFG_0] (main.py 265): INFO Train: [4/300][150/187]	eta 0:00:34 lr 0.000002	time 0.9238 (0.9449)	loss 2.6479 (2.6652)	grad_norm 6.3078 (5.9529)	mem 13711MB
[2022-05-30 21:47:31 MetaFG_0] (main.py 265): INFO Train: [4/300][160/187]	eta 0:00:25 lr 0.000002	time 0.9305 (0.9441)	loss 2.6493 (2.6652)	grad_norm 3.9965 (6.2808)	mem 13711MB
[2022-05-30 21:47:41 MetaFG_0] (main.py 265): INFO Train: [4/300][170/187]	eta 0:00:16 lr 0.000002	time 0.9226 (0.9434)	loss 2.6668 (2.6653)	grad_norm 5.1954 (6.2666)	mem 13711MB
[2022-05-30 21:47:50 MetaFG_0] (main.py 265): INFO Train: [4/300][180/187]	eta 0:00:06 lr 0.000002	time 0.9065 (0.9426)	loss 2.6539 (2.6654)	grad_norm 4.7465 (6.2277)	mem 13711MB
[2022-05-30 21:47:56 MetaFG_0] (main.py 272): INFO EPOCH 4 training takes 0:02:56
[2022-05-30 21:47:56 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_4.pth saving......
[2022-05-30 21:47:56 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_4.pth saved !!!
[2022-05-30 21:47:56 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:47:58 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:47:58 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:48:00 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.086 (2.086)	Loss 5.3238 (5.3238)	Acc@1 3.125 (3.125)	Acc@5 3.125 (3.125)	Mem 13711MB
[2022-05-30 21:48:04 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.280 (0.518)	Loss 5.3143 (5.3024)	Acc@1 0.000 (1.136)	Acc@5 0.000 (3.125)	Mem 13711MB
[2022-05-30 21:48:08 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.650 (0.456)	Loss 5.3385 (5.3070)	Acc@1 0.000 (1.042)	Acc@5 0.000 (3.274)	Mem 13711MB
[2022-05-30 21:48:11 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.286 (0.423)	Loss 5.3119 (5.3101)	Acc@1 0.000 (0.907)	Acc@5 3.125 (2.722)	Mem 13711MB
[2022-05-30 21:48:15 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.701 (0.412)	Loss 5.3329 (5.3081)	Acc@1 0.000 (0.762)	Acc@5 3.125 (2.896)	Mem 13711MB
[2022-05-30 21:48:18 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.278 (0.401)	Loss 5.3435 (5.3073)	Acc@1 0.000 (0.797)	Acc@5 3.125 (3.002)	Mem 13711MB
[2022-05-30 21:48:22 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.609 (0.400)	Loss 5.2657 (5.3050)	Acc@1 0.000 (0.820)	Acc@5 6.250 (2.971)	Mem 13711MB
[2022-05-30 21:48:26 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.276 (0.393)	Loss 5.3129 (5.3003)	Acc@1 0.000 (0.880)	Acc@5 3.125 (3.301)	Mem 13711MB
[2022-05-30 21:48:30 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.726 (0.393)	Loss 5.3832 (5.2986)	Acc@1 0.000 (0.887)	Acc@5 0.000 (3.202)	Mem 13711MB
[2022-05-30 21:48:33 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.281 (0.388)	Loss 5.3126 (5.2993)	Acc@1 3.125 (0.962)	Acc@5 3.125 (3.091)	Mem 13711MB
[2022-05-30 21:48:37 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.678 (0.389)	Loss 5.2565 (5.2984)	Acc@1 0.000 (0.897)	Acc@5 9.375 (3.187)	Mem 13711MB
[2022-05-30 21:48:41 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.284 (0.386)	Loss 5.3358 (5.3009)	Acc@1 0.000 (0.845)	Acc@5 0.000 (3.125)	Mem 13711MB
[2022-05-30 21:48:45 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.582 (0.385)	Loss 5.2556 (5.3012)	Acc@1 0.000 (0.775)	Acc@5 0.000 (3.048)	Mem 13711MB
[2022-05-30 21:48:48 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.282 (0.383)	Loss 5.2481 (5.3005)	Acc@1 0.000 (0.787)	Acc@5 6.250 (3.053)	Mem 13711MB
[2022-05-30 21:48:52 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.639 (0.384)	Loss 5.3037 (5.3005)	Acc@1 0.000 (0.754)	Acc@5 9.375 (3.125)	Mem 13711MB
[2022-05-30 21:48:56 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.363 (0.382)	Loss 5.2949 (5.3012)	Acc@1 0.000 (0.704)	Acc@5 6.250 (3.084)	Mem 13711MB
[2022-05-30 21:49:00 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.678 (0.383)	Loss 5.2765 (5.3020)	Acc@1 6.250 (0.699)	Acc@5 6.250 (3.009)	Mem 13711MB
[2022-05-30 21:49:03 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.276 (0.381)	Loss 5.2626 (5.3025)	Acc@1 6.250 (0.749)	Acc@5 6.250 (3.070)	Mem 13711MB
[2022-05-30 21:49:06 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.378)	Loss 5.3635 (5.3034)	Acc@1 0.000 (0.742)	Acc@5 3.125 (3.090)	Mem 13711MB
[2022-05-30 21:49:06 MetaFG_0] (main.py 330): INFO  * Acc@1 0.742 Acc@5 3.089
[2022-05-30 21:49:06 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.7%
[2022-05-30 21:49:06 MetaFG_0] (main.py 171): INFO Max accuracy: 0.74%
[2022-05-30 21:49:09 MetaFG_0] (main.py 265): INFO Train: [5/300][0/187]	eta 0:08:07 lr 0.000002	time 2.6094 (2.6094)	loss 2.6787 (2.6787)	grad_norm 4.6031 (4.6031)	mem 13711MB
[2022-05-30 21:49:18 MetaFG_0] (main.py 265): INFO Train: [5/300][10/187]	eta 0:03:12 lr 0.000002	time 0.9252 (1.0900)	loss 2.6269 (2.6565)	grad_norm 5.0537 (5.6834)	mem 13711MB
[2022-05-30 21:49:28 MetaFG_0] (main.py 265): INFO Train: [5/300][20/187]	eta 0:02:49 lr 0.000002	time 0.9217 (1.0141)	loss 2.6695 (2.6670)	grad_norm 4.5869 (5.9377)	mem 13711MB
[2022-05-30 21:49:37 MetaFG_0] (main.py 265): INFO Train: [5/300][30/187]	eta 0:02:34 lr 0.000002	time 0.9224 (0.9872)	loss 2.6509 (2.6638)	grad_norm 5.9540 (5.5443)	mem 13711MB
[2022-05-30 21:49:46 MetaFG_0] (main.py 265): INFO Train: [5/300][40/187]	eta 0:02:23 lr 0.000002	time 0.9224 (0.9733)	loss 2.6381 (2.6650)	grad_norm 3.9151 (5.5778)	mem 13711MB
[2022-05-30 21:49:56 MetaFG_0] (main.py 265): INFO Train: [5/300][50/187]	eta 0:02:12 lr 0.000002	time 0.9238 (0.9650)	loss 2.6387 (2.6652)	grad_norm 5.4603 (5.5484)	mem 13711MB
[2022-05-30 21:50:05 MetaFG_0] (main.py 265): INFO Train: [5/300][60/187]	eta 0:02:01 lr 0.000002	time 0.9210 (0.9593)	loss 2.6477 (2.6648)	grad_norm 4.1999 (5.5110)	mem 13711MB
[2022-05-30 21:50:14 MetaFG_0] (main.py 265): INFO Train: [5/300][70/187]	eta 0:01:51 lr 0.000002	time 0.9229 (0.9554)	loss 2.6647 (2.6639)	grad_norm 4.9655 (5.4679)	mem 13711MB
[2022-05-30 21:50:24 MetaFG_0] (main.py 265): INFO Train: [5/300][80/187]	eta 0:01:41 lr 0.000002	time 0.9228 (0.9523)	loss 2.6580 (2.6643)	grad_norm 3.8166 (5.5675)	mem 13711MB
[2022-05-30 21:50:33 MetaFG_0] (main.py 265): INFO Train: [5/300][90/187]	eta 0:01:32 lr 0.000002	time 0.9205 (0.9499)	loss 2.6507 (2.6630)	grad_norm 3.2149 (5.5316)	mem 13711MB
[2022-05-30 21:50:42 MetaFG_0] (main.py 265): INFO Train: [5/300][100/187]	eta 0:01:22 lr 0.000002	time 0.9222 (0.9481)	loss 2.6732 (2.6632)	grad_norm 4.0832 (5.5166)	mem 13711MB
[2022-05-30 21:50:52 MetaFG_0] (main.py 265): INFO Train: [5/300][110/187]	eta 0:01:12 lr 0.000002	time 0.9220 (0.9465)	loss 2.6921 (2.6622)	grad_norm 4.7312 (5.5650)	mem 13711MB
[2022-05-30 21:51:01 MetaFG_0] (main.py 265): INFO Train: [5/300][120/187]	eta 0:01:03 lr 0.000002	time 0.9222 (0.9451)	loss 2.6685 (2.6607)	grad_norm 3.3909 (5.5216)	mem 13711MB
[2022-05-30 21:51:10 MetaFG_0] (main.py 265): INFO Train: [5/300][130/187]	eta 0:00:53 lr 0.000002	time 0.9212 (0.9440)	loss 2.6723 (2.6604)	grad_norm 4.1877 (5.5396)	mem 13711MB
[2022-05-30 21:51:19 MetaFG_0] (main.py 265): INFO Train: [5/300][140/187]	eta 0:00:44 lr 0.000002	time 0.9257 (0.9431)	loss 2.6616 (2.6608)	grad_norm 4.2955 (5.4732)	mem 13711MB
[2022-05-30 21:51:29 MetaFG_0] (main.py 265): INFO Train: [5/300][150/187]	eta 0:00:34 lr 0.000002	time 0.9210 (0.9421)	loss 2.6458 (2.6607)	grad_norm 4.0755 (5.4512)	mem 13711MB
[2022-05-30 21:51:38 MetaFG_0] (main.py 265): INFO Train: [5/300][160/187]	eta 0:00:25 lr 0.000002	time 0.9208 (0.9414)	loss 2.6696 (2.6606)	grad_norm 5.1310 (5.4301)	mem 13711MB
[2022-05-30 21:51:47 MetaFG_0] (main.py 265): INFO Train: [5/300][170/187]	eta 0:00:15 lr 0.000002	time 0.9204 (0.9407)	loss 2.6702 (2.6603)	grad_norm 3.9111 (5.3936)	mem 13711MB
[2022-05-30 21:51:57 MetaFG_0] (main.py 265): INFO Train: [5/300][180/187]	eta 0:00:06 lr 0.000002	time 0.9044 (0.9400)	loss 2.6581 (2.6604)	grad_norm 3.4359 (5.4153)	mem 13711MB
[2022-05-30 21:52:02 MetaFG_0] (main.py 272): INFO EPOCH 5 training takes 0:02:55
[2022-05-30 21:52:02 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_5.pth saving......
[2022-05-30 21:52:03 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_5.pth saved !!!
[2022-05-30 21:52:03 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:52:05 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:52:05 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:52:07 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.028 (2.028)	Loss 5.3227 (5.3227)	Acc@1 0.000 (0.000)	Acc@5 6.250 (6.250)	Mem 13711MB
[2022-05-30 21:52:10 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.278 (0.518)	Loss 5.2789 (5.3077)	Acc@1 0.000 (1.136)	Acc@5 3.125 (4.261)	Mem 13711MB
[2022-05-30 21:52:14 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.625 (0.457)	Loss 5.3140 (5.3074)	Acc@1 0.000 (0.893)	Acc@5 0.000 (3.571)	Mem 13711MB
[2022-05-30 21:52:18 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.276 (0.423)	Loss 5.3276 (5.2973)	Acc@1 0.000 (0.806)	Acc@5 3.125 (3.629)	Mem 13711MB
[2022-05-30 21:52:22 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.588 (0.414)	Loss 5.2731 (5.2934)	Acc@1 0.000 (0.915)	Acc@5 3.125 (3.811)	Mem 13711MB
[2022-05-30 21:52:25 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.275 (0.401)	Loss 5.2878 (5.2912)	Acc@1 3.125 (1.042)	Acc@5 3.125 (3.738)	Mem 13711MB
[2022-05-30 21:52:29 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.611 (0.398)	Loss 5.3247 (5.2885)	Acc@1 0.000 (1.025)	Acc@5 6.250 (3.637)	Mem 13711MB
[2022-05-30 21:52:32 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.276 (0.392)	Loss 5.2863 (5.2885)	Acc@1 0.000 (0.924)	Acc@5 0.000 (3.389)	Mem 13711MB
[2022-05-30 21:52:36 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.720 (0.394)	Loss 5.3300 (5.2870)	Acc@1 0.000 (0.849)	Acc@5 0.000 (3.202)	Mem 13711MB
[2022-05-30 21:52:40 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.283 (0.392)	Loss 5.2813 (5.2865)	Acc@1 3.125 (0.859)	Acc@5 3.125 (3.297)	Mem 13711MB
[2022-05-30 21:52:44 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.598 (0.391)	Loss 5.2421 (5.2851)	Acc@1 0.000 (0.959)	Acc@5 3.125 (3.496)	Mem 13711MB
[2022-05-30 21:52:48 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.284 (0.389)	Loss 5.2243 (5.2831)	Acc@1 0.000 (0.873)	Acc@5 0.000 (3.463)	Mem 13711MB
[2022-05-30 21:52:52 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.635 (0.388)	Loss 5.3014 (5.2840)	Acc@1 3.125 (0.852)	Acc@5 6.250 (3.487)	Mem 13711MB
[2022-05-30 21:52:55 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.276 (0.386)	Loss 5.3001 (5.2843)	Acc@1 3.125 (0.906)	Acc@5 6.250 (3.459)	Mem 13711MB
[2022-05-30 21:52:59 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.634 (0.387)	Loss 5.3369 (5.2851)	Acc@1 0.000 (0.842)	Acc@5 0.000 (3.435)	Mem 13711MB
[2022-05-30 21:53:03 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.275 (0.385)	Loss 5.3063 (5.2854)	Acc@1 0.000 (0.828)	Acc@5 0.000 (3.477)	Mem 13711MB
[2022-05-30 21:53:06 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.527 (0.384)	Loss 5.2559 (5.2854)	Acc@1 0.000 (0.796)	Acc@5 3.125 (3.436)	Mem 13711MB
[2022-05-30 21:53:10 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.290 (0.383)	Loss 5.3253 (5.2853)	Acc@1 0.000 (0.786)	Acc@5 3.125 (3.454)	Mem 13711MB
[2022-05-30 21:53:13 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.275 (0.380)	Loss 5.3264 (5.2860)	Acc@1 0.000 (0.777)	Acc@5 3.125 (3.419)	Mem 13711MB
[2022-05-30 21:53:13 MetaFG_0] (main.py 330): INFO  * Acc@1 0.777 Acc@5 3.417
[2022-05-30 21:53:13 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 0.8%
[2022-05-30 21:53:13 MetaFG_0] (main.py 171): INFO Max accuracy: 0.78%
[2022-05-30 21:53:16 MetaFG_0] (main.py 265): INFO Train: [6/300][0/187]	eta 0:08:33 lr 0.000002	time 2.7467 (2.7467)	loss 2.6736 (2.6736)	grad_norm 4.4192 (4.4192)	mem 13711MB
[2022-05-30 21:53:26 MetaFG_0] (main.py 265): INFO Train: [6/300][10/187]	eta 0:03:16 lr 0.000002	time 0.9233 (1.1102)	loss 2.6235 (2.6530)	grad_norm 3.0913 (4.4724)	mem 13711MB
[2022-05-30 21:53:35 MetaFG_0] (main.py 265): INFO Train: [6/300][20/187]	eta 0:02:51 lr 0.000002	time 0.9215 (1.0244)	loss 2.6506 (2.6523)	grad_norm 4.2209 (4.8443)	mem 13711MB
[2022-05-30 21:53:44 MetaFG_0] (main.py 265): INFO Train: [6/300][30/187]	eta 0:02:36 lr 0.000002	time 0.9232 (0.9943)	loss 2.6490 (2.6539)	grad_norm 3.8007 (4.9030)	mem 13711MB
[2022-05-30 21:53:54 MetaFG_0] (main.py 265): INFO Train: [6/300][40/187]	eta 0:02:23 lr 0.000002	time 0.9251 (0.9787)	loss 2.6361 (2.6542)	grad_norm 3.3165 (4.8907)	mem 13711MB
[2022-05-30 21:54:03 MetaFG_0] (main.py 265): INFO Train: [6/300][50/187]	eta 0:02:12 lr 0.000002	time 0.9232 (0.9692)	loss 2.6561 (2.6556)	grad_norm 3.3498 (4.9220)	mem 13711MB
[2022-05-30 21:54:12 MetaFG_0] (main.py 265): INFO Train: [6/300][60/187]	eta 0:02:02 lr 0.000002	time 0.9215 (0.9634)	loss 2.6272 (2.6562)	grad_norm 2.7348 (4.8759)	mem 13711MB
[2022-05-30 21:54:22 MetaFG_0] (main.py 265): INFO Train: [6/300][70/187]	eta 0:01:52 lr 0.000002	time 0.9219 (0.9587)	loss 2.6503 (2.6559)	grad_norm 8.1471 (4.8962)	mem 13711MB
[2022-05-30 21:54:31 MetaFG_0] (main.py 265): INFO Train: [6/300][80/187]	eta 0:01:42 lr 0.000002	time 0.9223 (0.9552)	loss 2.6335 (2.6564)	grad_norm 3.6450 (4.8835)	mem 13711MB
[2022-05-30 21:54:40 MetaFG_0] (main.py 265): INFO Train: [6/300][90/187]	eta 0:01:32 lr 0.000002	time 0.9214 (0.9525)	loss 2.6588 (2.6558)	grad_norm 3.1732 (4.8546)	mem 13711MB
[2022-05-30 21:54:49 MetaFG_0] (main.py 265): INFO Train: [6/300][100/187]	eta 0:01:22 lr 0.000002	time 0.9209 (0.9502)	loss 2.6271 (2.6553)	grad_norm 3.5626 (4.8591)	mem 13711MB
[2022-05-30 21:54:59 MetaFG_0] (main.py 265): INFO Train: [6/300][110/187]	eta 0:01:13 lr 0.000002	time 0.9221 (0.9484)	loss 2.6144 (2.6548)	grad_norm 3.6268 (4.8935)	mem 13711MB
[2022-05-30 21:55:08 MetaFG_0] (main.py 265): INFO Train: [6/300][120/187]	eta 0:01:03 lr 0.000002	time 0.9214 (0.9469)	loss 2.6408 (2.6545)	grad_norm 2.5673 (4.7947)	mem 13711MB
[2022-05-30 21:55:17 MetaFG_0] (main.py 265): INFO Train: [6/300][130/187]	eta 0:00:53 lr 0.000002	time 0.9211 (0.9457)	loss 2.6581 (2.6545)	grad_norm 3.2874 (4.7944)	mem 13711MB
[2022-05-30 21:55:27 MetaFG_0] (main.py 265): INFO Train: [6/300][140/187]	eta 0:00:44 lr 0.000002	time 0.9212 (0.9447)	loss 2.6498 (2.6540)	grad_norm 6.3437 (4.8092)	mem 13711MB
[2022-05-30 21:55:36 MetaFG_0] (main.py 265): INFO Train: [6/300][150/187]	eta 0:00:34 lr 0.000002	time 0.9215 (0.9439)	loss 2.6354 (2.6534)	grad_norm 3.7819 (4.8132)	mem 13711MB
[2022-05-30 21:55:45 MetaFG_0] (main.py 265): INFO Train: [6/300][160/187]	eta 0:00:25 lr 0.000002	time 0.9248 (0.9431)	loss 2.6618 (2.6529)	grad_norm 2.8460 (4.9294)	mem 13711MB
[2022-05-30 21:55:55 MetaFG_0] (main.py 265): INFO Train: [6/300][170/187]	eta 0:00:16 lr 0.000002	time 0.9233 (0.9423)	loss 2.6647 (2.6530)	grad_norm 6.7830 (4.9633)	mem 13711MB
[2022-05-30 21:56:04 MetaFG_0] (main.py 265): INFO Train: [6/300][180/187]	eta 0:00:06 lr 0.000002	time 0.9051 (0.9415)	loss 2.6620 (2.6527)	grad_norm 6.7835 (4.9551)	mem 13711MB
[2022-05-30 21:56:10 MetaFG_0] (main.py 272): INFO EPOCH 6 training takes 0:02:56
[2022-05-30 21:56:10 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_6.pth saving......
[2022-05-30 21:56:10 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_6.pth saved !!!
[2022-05-30 21:56:10 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 21:56:12 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 21:56:12 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 21:56:14 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.164 (2.164)	Loss 5.2380 (5.2380)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 13711MB
[2022-05-30 21:56:18 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.279 (0.517)	Loss 5.3055 (5.2626)	Acc@1 0.000 (1.136)	Acc@5 0.000 (3.977)	Mem 13711MB
[2022-05-30 21:56:21 MetaFG_0] (main.py 324): INFO Test: [20/182]	Time 0.592 (0.453)	Loss 5.2777 (5.2662)	Acc@1 0.000 (0.893)	Acc@5 0.000 (4.018)	Mem 13711MB
[2022-05-30 21:56:25 MetaFG_0] (main.py 324): INFO Test: [30/182]	Time 0.282 (0.422)	Loss 5.2934 (5.2724)	Acc@1 0.000 (1.109)	Acc@5 0.000 (3.931)	Mem 13711MB
[2022-05-30 21:56:29 MetaFG_0] (main.py 324): INFO Test: [40/182]	Time 0.629 (0.416)	Loss 5.3321 (5.2692)	Acc@1 0.000 (1.067)	Acc@5 3.125 (4.116)	Mem 13711MB
[2022-05-30 21:56:32 MetaFG_0] (main.py 324): INFO Test: [50/182]	Time 0.278 (0.403)	Loss 5.2923 (5.2668)	Acc@1 0.000 (0.980)	Acc@5 0.000 (4.105)	Mem 13711MB
[2022-05-30 21:56:36 MetaFG_0] (main.py 324): INFO Test: [60/182]	Time 0.751 (0.403)	Loss 5.2751 (5.2656)	Acc@1 0.000 (1.076)	Acc@5 3.125 (4.252)	Mem 13711MB
[2022-05-30 21:56:40 MetaFG_0] (main.py 324): INFO Test: [70/182]	Time 0.287 (0.400)	Loss 5.3339 (5.2663)	Acc@1 0.000 (1.056)	Acc@5 0.000 (4.005)	Mem 13711MB
[2022-05-30 21:56:44 MetaFG_0] (main.py 324): INFO Test: [80/182]	Time 0.657 (0.399)	Loss 5.2274 (5.2644)	Acc@1 0.000 (1.157)	Acc@5 0.000 (4.282)	Mem 13711MB
[2022-05-30 21:56:48 MetaFG_0] (main.py 324): INFO Test: [90/182]	Time 0.276 (0.392)	Loss 5.2941 (5.2670)	Acc@1 3.125 (1.133)	Acc@5 6.250 (4.052)	Mem 13711MB
[2022-05-30 21:56:51 MetaFG_0] (main.py 324): INFO Test: [100/182]	Time 0.705 (0.392)	Loss 5.1977 (5.2659)	Acc@1 0.000 (1.114)	Acc@5 6.250 (4.084)	Mem 13711MB
[2022-05-30 21:56:55 MetaFG_0] (main.py 324): INFO Test: [110/182]	Time 0.282 (0.389)	Loss 5.2917 (5.2663)	Acc@1 0.000 (1.070)	Acc@5 6.250 (4.139)	Mem 13711MB
[2022-05-30 21:56:59 MetaFG_0] (main.py 324): INFO Test: [120/182]	Time 0.651 (0.389)	Loss 5.2901 (5.2665)	Acc@1 0.000 (1.085)	Acc@5 3.125 (4.158)	Mem 13711MB
[2022-05-30 21:57:03 MetaFG_0] (main.py 324): INFO Test: [130/182]	Time 0.282 (0.387)	Loss 5.2591 (5.2665)	Acc@1 0.000 (1.002)	Acc@5 0.000 (4.151)	Mem 13711MB
[2022-05-30 21:57:06 MetaFG_0] (main.py 324): INFO Test: [140/182]	Time 0.596 (0.387)	Loss 5.1739 (5.2663)	Acc@1 0.000 (0.975)	Acc@5 9.375 (4.255)	Mem 13711MB
[2022-05-30 21:57:10 MetaFG_0] (main.py 324): INFO Test: [150/182]	Time 0.293 (0.385)	Loss 5.3091 (5.2663)	Acc@1 3.125 (0.973)	Acc@5 3.125 (4.284)	Mem 13711MB
[2022-05-30 21:57:14 MetaFG_0] (main.py 324): INFO Test: [160/182]	Time 0.663 (0.388)	Loss 5.2640 (5.2661)	Acc@1 0.000 (0.970)	Acc@5 3.125 (4.212)	Mem 13711MB
[2022-05-30 21:57:18 MetaFG_0] (main.py 324): INFO Test: [170/182]	Time 0.278 (0.386)	Loss 5.2766 (5.2651)	Acc@1 0.000 (1.078)	Acc@5 3.125 (4.349)	Mem 13711MB
[2022-05-30 21:57:22 MetaFG_0] (main.py 324): INFO Test: [180/182]	Time 0.290 (0.387)	Loss 5.2806 (5.2664)	Acc@1 0.000 (1.088)	Acc@5 3.125 (4.316)	Mem 13711MB
[2022-05-30 21:57:22 MetaFG_0] (main.py 330): INFO  * Acc@1 1.087 Acc@5 4.315
[2022-05-30 21:57:22 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 5794 test images: 1.1%
[2022-05-30 21:57:22 MetaFG_0] (main.py 171): INFO Max accuracy: 1.09%
[2022-05-30 21:57:26 MetaFG_0] (main.py 265): INFO Train: [7/300][0/187]	eta 0:11:28 lr 0.000002	time 3.6794 (3.6794)	loss 2.6613 (2.6613)	grad_norm 9.4804 (9.4804)	mem 13711MB
[2022-05-30 21:57:36 MetaFG_0] (main.py 265): INFO Train: [7/300][10/187]	eta 0:03:43 lr 0.000002	time 0.9330 (1.2654)	loss 2.6754 (2.6471)	grad_norm 3.3417 (6.1828)	mem 13711MB
[2022-05-30 21:57:46 MetaFG_0] (main.py 265): INFO Train: [7/300][20/187]	eta 0:03:07 lr 0.000002	time 0.9398 (1.1201)	loss 2.6516 (2.6477)	grad_norm 3.0884 (6.5419)	mem 13711MB
[2022-05-30 21:57:55 MetaFG_0] (main.py 265): INFO Train: [7/300][30/187]	eta 0:02:48 lr 0.000002	time 0.9688 (1.0701)	loss 2.6477 (2.6491)	grad_norm 3.1517 (6.2679)	mem 13711MB
[2022-05-30 21:58:05 MetaFG_0] (main.py 265): INFO Train: [7/300][40/187]	eta 0:02:34 lr 0.000002	time 0.9458 (1.0480)	loss 2.6437 (2.6491)	grad_norm 4.6984 (6.1450)	mem 13711MB
[2022-05-30 21:58:15 MetaFG_0] (main.py 265): INFO Train: [7/300][50/187]	eta 0:02:21 lr 0.000002	time 0.9285 (1.0302)	loss 2.6575 (2.6481)	grad_norm 4.0580 (6.0474)	mem 13711MB
[2022-05-30 21:58:25 MetaFG_0] (main.py 265): INFO Train: [7/300][60/187]	eta 0:02:10 lr 0.000002	time 1.0165 (1.0302)	loss 2.6362 (2.6480)	grad_norm 3.8924 (5.9882)	mem 13711MB
[2022-05-30 21:58:36 MetaFG_0] (main.py 265): INFO Train: [7/300][70/187]	eta 0:02:01 lr 0.000002	time 1.0151 (1.0349)	loss 2.6405 (2.6478)	grad_norm 4.2156 (5.9406)	mem 13711MB
[2022-05-30 21:58:46 MetaFG_0] (main.py 265): INFO Train: [7/300][80/187]	eta 0:01:50 lr 0.000002	time 0.9564 (1.0315)	loss 2.6386 (2.6465)	grad_norm 5.5851 (5.8648)	mem 13711MB
[2022-05-30 21:58:55 MetaFG_0] (main.py 265): INFO Train: [7/300][90/187]	eta 0:01:39 lr 0.000002	time 0.9788 (1.0257)	loss 2.6358 (2.6459)	grad_norm 3.8390 (5.7234)	mem 13711MB
[2022-05-30 21:59:05 MetaFG_0] (main.py 265): INFO Train: [7/300][100/187]	eta 0:01:28 lr 0.000002	time 0.9584 (1.0207)	loss 2.6251 (2.6466)	grad_norm 2.9793 (5.6937)	mem 13711MB
[2022-05-30 21:59:15 MetaFG_0] (main.py 265): INFO Train: [7/300][110/187]	eta 0:01:18 lr 0.000002	time 0.9710 (1.0159)	loss 2.6528 (2.6474)	grad_norm 3.3264 (5.5701)	mem 13711MB
[2022-05-30 21:59:25 MetaFG_0] (main.py 265): INFO Train: [7/300][120/187]	eta 0:01:07 lr 0.000002	time 0.9824 (1.0117)	loss 2.6864 (2.6482)	grad_norm 3.3240 (5.5307)	mem 13711MB
[2022-05-30 21:59:34 MetaFG_0] (main.py 265): INFO Train: [7/300][130/187]	eta 0:00:57 lr 0.000002	time 0.9183 (1.0077)	loss 2.6273 (2.6479)	grad_norm 4.3435 (5.4383)	mem 13711MB
[2022-05-30 21:59:44 MetaFG_0] (main.py 265): INFO Train: [7/300][140/187]	eta 0:00:47 lr 0.000002	time 0.9573 (1.0049)	loss 2.6612 (2.6480)	grad_norm 3.7896 (5.3892)	mem 13711MB
[2022-05-30 21:59:53 MetaFG_0] (main.py 265): INFO Train: [7/300][150/187]	eta 0:00:37 lr 0.000002	time 0.9418 (1.0021)	loss 2.6167 (2.6471)	grad_norm 3.1465 (5.5404)	mem 13711MB
[2022-05-30 22:00:03 MetaFG_0] (main.py 265): INFO Train: [7/300][160/187]	eta 0:00:26 lr 0.000002	time 0.9547 (0.9999)	loss 2.6495 (2.6473)	grad_norm 6.4347 (5.5029)	mem 13711MB
[2022-05-30 22:00:13 MetaFG_0] (main.py 265): INFO Train: [7/300][170/187]	eta 0:00:16 lr 0.000002	time 0.9341 (0.9977)	loss 2.6538 (2.6473)	grad_norm 3.2529 (5.4638)	mem 13711MB
[2022-05-30 22:00:23 MetaFG_0] (main.py 265): INFO Train: [7/300][180/187]	eta 0:00:06 lr 0.000002	time 0.9460 (0.9976)	loss 2.6196 (2.6472)	grad_norm 3.0620 (5.4751)	mem 13711MB
[2022-05-30 22:00:29 MetaFG_0] (main.py 272): INFO EPOCH 7 training takes 0:03:06
[2022-05-30 22:00:29 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_7.pth saving......
[2022-05-30 22:00:30 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_7.pth saved !!!
[2022-05-30 22:00:30 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:00:31 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:00:31 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:00:34 MetaFG_0] (main.py 324): INFO Test: [0/182]	Time 2.549 (2.549)	Loss 5.2567 (5.2567)	Acc@1 0.000 (0.000)	Acc@5 3.125 (3.125)	Mem 13711MB
[2022-05-30 22:00:38 MetaFG_0] (main.py 324): INFO Test: [10/182]	Time 0.280 (0.624)	Loss 5.2998 (5.2442)	Acc@1 0.000 (1.136)	Acc@5 3.125 (5.398)	Mem 13711MB
[2022-05-30 22:01:36 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:01:36 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:01:43 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:01:44 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2022-05-30 22:01:44 MetaFG_0] (main.py 110): INFO number of params: 28094055
[2022-05-30 22:01:44 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:01:44 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:01:44 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:01:44 MetaFG_0] (main.py 139): INFO auto resuming from output/MetaFG_0/cub-200_v1/latest.pth
[2022-05-30 22:01:44 MetaFG_0] (main.py 144): INFO **********normal test***********
[2022-05-30 22:01:44 MetaFG_0] (utils.py 79): INFO ==============> Resuming form output/MetaFG_0/cub-200_v1/latest.pth....................
[2022-05-30 22:03:33 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:03:33 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:03:35 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:03:35 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2022-05-30 22:03:35 MetaFG_0] (main.py 110): INFO number of params: 28094055
[2022-05-30 22:03:35 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:03:35 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:03:35 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:03:35 MetaFG_0] (main.py 139): INFO auto resuming from output/MetaFG_0/cub-200_v1/latest.pth
[2022-05-30 22:03:35 MetaFG_0] (main.py 144): INFO **********normal test***********
[2022-05-30 22:03:35 MetaFG_0] (utils.py 79): INFO ==============> Resuming form output/MetaFG_0/cub-200_v1/latest.pth....................
[2022-05-30 22:05:14 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:05:14 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:05:15 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:05:16 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2022-05-30 22:05:16 MetaFG_0] (main.py 110): INFO number of params: 28170955
[2022-05-30 22:05:16 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:05:16 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:05:16 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:05:16 MetaFG_0] (main.py 139): INFO auto resuming from output/MetaFG_0/cub-200_v1/latest.pth
[2022-05-30 22:05:16 MetaFG_0] (main.py 144): INFO **********normal test***********
[2022-05-30 22:05:16 MetaFG_0] (utils.py 79): INFO ==============> Resuming form output/MetaFG_0/cub-200_v1/latest.pth....................
[2022-05-30 22:05:16 MetaFG_0] (utils.py 91): INFO <All keys matched successfully>
[2022-05-30 22:05:16 MetaFG_0] (utils.py 101): INFO => loaded successfully 'output/MetaFG_0/cub-200_v1/latest.pth' (epoch 7)
[2022-05-30 22:05:20 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 3.214 (3.214)	Loss 5.3679 (5.3679)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 7931MB
[2022-05-30 22:05:23 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.294 (0.551)	Loss 5.4472 (5.3543)	Acc@1 0.000 (0.284)	Acc@5 0.000 (1.136)	Mem 7931MB
[2022-05-30 22:05:25 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.275 (0.421)	Loss 5.4198 (5.3676)	Acc@1 0.000 (0.298)	Acc@5 0.000 (1.339)	Mem 7931MB
[2022-05-30 22:05:28 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.276 (0.375)	Loss 5.3204 (5.3592)	Acc@1 0.000 (0.202)	Acc@5 0.000 (1.512)	Mem 7931MB
[2022-05-30 22:05:31 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.276 (0.352)	Loss 5.3735 (5.3709)	Acc@1 0.000 (0.152)	Acc@5 0.000 (1.448)	Mem 7931MB
[2022-05-30 22:05:34 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.278 (0.337)	Loss 5.3594 (5.3696)	Acc@1 0.000 (0.123)	Acc@5 0.000 (1.654)	Mem 7931MB
[2022-05-30 22:05:36 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.276 (0.328)	Loss 5.3638 (5.3676)	Acc@1 0.000 (0.102)	Acc@5 0.000 (1.639)	Mem 7931MB
[2022-05-30 22:05:39 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.275 (0.321)	Loss 5.4196 (5.3693)	Acc@1 0.000 (0.088)	Acc@5 0.000 (1.585)	Mem 7931MB
[2022-05-30 22:05:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.283 (0.316)	Loss 5.3511 (5.3727)	Acc@1 3.125 (0.116)	Acc@5 3.125 (1.543)	Mem 7931MB
[2022-05-30 22:05:45 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.291 (0.312)	Loss 5.4637 (5.3710)	Acc@1 0.000 (0.103)	Acc@5 0.000 (1.545)	Mem 7931MB
[2022-05-30 22:05:48 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.280 (0.309)	Loss 5.4478 (5.3706)	Acc@1 0.000 (0.093)	Acc@5 0.000 (1.578)	Mem 7931MB
[2022-05-30 22:05:50 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.276 (0.306)	Loss 5.3243 (5.3683)	Acc@1 0.000 (0.141)	Acc@5 0.000 (1.661)	Mem 7931MB
[2022-05-30 22:05:53 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.283 (0.304)	Loss 5.2960 (5.3687)	Acc@1 0.000 (0.155)	Acc@5 3.125 (1.653)	Mem 7931MB
[2022-05-30 22:05:56 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.275 (0.302)	Loss 5.2758 (5.3680)	Acc@1 0.000 (0.143)	Acc@5 3.125 (1.718)	Mem 7931MB
[2022-05-30 22:05:59 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.277 (0.301)	Loss 5.4357 (5.3686)	Acc@1 0.000 (0.133)	Acc@5 0.000 (1.795)	Mem 7931MB
[2022-05-30 22:06:02 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.276 (0.299)	Loss 5.3804 (5.3708)	Acc@1 3.125 (0.166)	Acc@5 3.125 (1.780)	Mem 7931MB
[2022-05-30 22:06:04 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.286 (0.298)	Loss 5.3390 (5.3691)	Acc@1 0.000 (0.155)	Acc@5 3.125 (1.825)	Mem 7931MB
[2022-05-30 22:06:07 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.276 (0.297)	Loss 5.4288 (5.3703)	Acc@1 0.000 (0.146)	Acc@5 3.125 (1.864)	Mem 7931MB
[2022-05-30 22:06:10 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.277 (0.296)	Loss 5.2820 (5.3677)	Acc@1 0.000 (0.155)	Acc@5 3.125 (1.865)	Mem 7931MB
[2022-05-30 22:06:13 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.275 (0.295)	Loss 5.3695 (5.3676)	Acc@1 0.000 (0.147)	Acc@5 6.250 (1.898)	Mem 7931MB
[2022-05-30 22:06:16 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.276 (0.294)	Loss 5.3515 (5.3663)	Acc@1 0.000 (0.140)	Acc@5 0.000 (1.835)	Mem 7931MB
[2022-05-30 22:06:18 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.276 (0.293)	Loss 5.3389 (5.3669)	Acc@1 0.000 (0.148)	Acc@5 3.125 (1.836)	Mem 7931MB
[2022-05-30 22:06:21 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.276 (0.293)	Loss 5.3426 (5.3680)	Acc@1 0.000 (0.141)	Acc@5 6.250 (1.782)	Mem 7931MB
[2022-05-30 22:06:24 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.282 (0.292)	Loss 5.3667 (5.3681)	Acc@1 0.000 (0.149)	Acc@5 0.000 (1.786)	Mem 7931MB
[2022-05-30 22:06:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.278 (0.292)	Loss 5.4640 (5.3693)	Acc@1 0.000 (0.156)	Acc@5 0.000 (1.738)	Mem 7931MB
[2022-05-30 22:06:30 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.292 (0.292)	Loss 5.3398 (5.3691)	Acc@1 0.000 (0.149)	Acc@5 0.000 (1.706)	Mem 7931MB
[2022-05-30 22:06:32 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.279 (0.291)	Loss 5.4420 (5.3700)	Acc@1 0.000 (0.156)	Acc@5 0.000 (1.676)	Mem 7931MB
[2022-05-30 22:06:35 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.286 (0.291)	Loss 5.3529 (5.3701)	Acc@1 0.000 (0.161)	Acc@5 0.000 (1.684)	Mem 7931MB
[2022-05-30 22:06:38 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.276 (0.290)	Loss 5.4561 (5.3701)	Acc@1 0.000 (0.156)	Acc@5 0.000 (1.668)	Mem 7931MB
[2022-05-30 22:06:41 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.287 (0.290)	Loss 5.4072 (5.3704)	Acc@1 0.000 (0.172)	Acc@5 6.250 (1.740)	Mem 7931MB
[2022-05-30 22:06:44 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.276 (0.290)	Loss 5.3906 (5.3715)	Acc@1 0.000 (0.176)	Acc@5 3.125 (1.734)	Mem 7931MB
[2022-05-30 22:06:46 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.275 (0.289)	Loss 5.4316 (5.3713)	Acc@1 0.000 (0.181)	Acc@5 3.125 (1.768)	Mem 7931MB
[2022-05-30 22:06:48 MetaFG_0] (main.py 330): INFO  * Acc@1 0.180 Acc@5 1.760
[2022-05-30 22:06:48 MetaFG_0] (main.py 147): INFO Accuracy of the network on the 10000 test images: 0.2%
[2022-05-30 22:06:48 MetaFG_0] (main.py 159): INFO Start training
[2022-05-30 22:06:53 MetaFG_0] (main.py 265): INFO Train: [8/300][0/1562]	eta 2:27:08 lr 0.000003	time 5.6519 (5.6519)	loss 2.7107 (2.7107)	grad_norm 28.9198 (28.9198)	mem 13929MB
[2022-05-30 22:07:03 MetaFG_0] (main.py 265): INFO Train: [8/300][10/1562]	eta 0:35:41 lr 0.000006	time 0.9213 (1.3797)	loss 2.6813 (2.6820)	grad_norm 18.9299 (22.6067)	mem 13929MB
[2022-05-30 22:07:12 MetaFG_0] (main.py 265): INFO Train: [8/300][20/1562]	eta 0:29:57 lr 0.000006	time 0.9227 (1.1657)	loss 2.7247 (2.6817)	grad_norm 23.3503 (20.8708)	mem 13929MB
[2022-05-30 22:07:22 MetaFG_0] (main.py 265): INFO Train: [8/300][30/1562]	eta 0:27:49 lr 0.000006	time 0.9215 (1.0901)	loss 2.6556 (2.6783)	grad_norm 19.9662 (21.1403)	mem 13929MB
[2022-05-30 22:07:31 MetaFG_0] (main.py 265): INFO Train: [8/300][40/1562]	eta 0:26:39 lr 0.000006	time 0.9225 (1.0510)	loss 2.7073 (2.6774)	grad_norm 35.6207 (20.8305)	mem 13929MB
[2022-05-30 22:07:40 MetaFG_0] (main.py 265): INFO Train: [8/300][50/1562]	eta 0:25:52 lr 0.000006	time 0.9225 (1.0269)	loss 2.6804 (2.6774)	grad_norm 12.1593 (20.2308)	mem 13929MB
[2022-05-30 22:07:49 MetaFG_0] (main.py 265): INFO Train: [8/300][60/1562]	eta 0:25:19 lr 0.000006	time 0.9238 (1.0114)	loss 2.6810 (2.6744)	grad_norm 12.5280 (19.7297)	mem 13929MB
[2022-05-30 22:07:59 MetaFG_0] (main.py 265): INFO Train: [8/300][70/1562]	eta 0:24:51 lr 0.000006	time 0.9217 (0.9997)	loss 2.6415 (2.6722)	grad_norm 17.2779 (19.2947)	mem 13929MB
[2022-05-30 22:08:08 MetaFG_0] (main.py 265): INFO Train: [8/300][80/1562]	eta 0:24:28 lr 0.000006	time 0.9227 (0.9910)	loss 2.6567 (2.6709)	grad_norm 24.0867 (19.5337)	mem 13929MB
[2022-05-30 22:08:17 MetaFG_0] (main.py 265): INFO Train: [8/300][90/1562]	eta 0:24:09 lr 0.000006	time 0.9216 (0.9844)	loss 2.6332 (2.6695)	grad_norm 12.2301 (19.1189)	mem 13929MB
[2022-05-30 22:08:27 MetaFG_0] (main.py 265): INFO Train: [8/300][100/1562]	eta 0:23:51 lr 0.000006	time 0.9220 (0.9790)	loss 2.6543 (2.6672)	grad_norm 11.9481 (18.7595)	mem 13929MB
[2022-05-30 22:08:36 MetaFG_0] (main.py 265): INFO Train: [8/300][110/1562]	eta 0:23:35 lr 0.000006	time 0.9222 (0.9747)	loss 2.6177 (2.6650)	grad_norm 16.1494 (18.5722)	mem 13929MB
[2022-05-30 22:09:00 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:09:00 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: ''
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:09:02 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:09:02 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2022-05-30 22:09:03 MetaFG_0] (main.py 110): INFO number of params: 28094055
[2022-05-30 22:09:03 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:09:03 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:09:03 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:09:03 MetaFG_0] (main.py 139): INFO auto resuming from output/MetaFG_0/cub-200_v1/latest.pth
[2022-05-30 22:09:03 MetaFG_0] (main.py 144): INFO **********normal test***********
[2022-05-30 22:09:03 MetaFG_0] (utils.py 79): INFO ==============> Resuming form output/MetaFG_0/cub-200_v1/latest.pth....................
[2022-05-30 22:15:33 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:15:33 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 384
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: false
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: false
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:15:35 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:15:36 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2022-05-30 22:15:36 MetaFG_0] (main.py 110): INFO number of params: 28094055
[2022-05-30 22:15:36 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:15:36 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:15:36 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:15:36 MetaFG_0] (main.py 159): INFO Start training
[2022-05-30 22:15:41 MetaFG_0] (main.py 265): INFO Train: [0/300][0/1562]	eta 2:26:49 lr 0.000000	time 5.6401 (5.6401)	loss 2.3252 (2.3252)	grad_norm inf (inf)	mem 13709MB
[2022-05-30 22:15:51 MetaFG_0] (main.py 265): INFO Train: [0/300][10/1562]	eta 0:35:06 lr 0.000000	time 0.9226 (1.3574)	loss 2.3306 (2.3387)	grad_norm 31.6562 (nan)	mem 13709MB
[2022-05-30 22:16:00 MetaFG_0] (main.py 265): INFO Train: [0/300][20/1562]	eta 0:29:38 lr 0.000000	time 0.9215 (1.1536)	loss 2.3374 (2.3429)	grad_norm inf (nan)	mem 13709MB
[2022-05-30 22:16:09 MetaFG_0] (main.py 265): INFO Train: [0/300][30/1562]	eta 0:27:38 lr 0.000000	time 0.9250 (1.0825)	loss 2.3364 (2.3416)	grad_norm 19.9451 (nan)	mem 13709MB
[2022-05-30 22:16:18 MetaFG_0] (main.py 265): INFO Train: [0/300][40/1562]	eta 0:26:31 lr 0.000000	time 0.9264 (1.0456)	loss 2.3562 (2.3414)	grad_norm 35.4240 (nan)	mem 13709MB
[2022-05-30 22:16:28 MetaFG_0] (main.py 265): INFO Train: [0/300][50/1562]	eta 0:25:46 lr 0.000000	time 0.9219 (1.0231)	loss 2.3544 (2.3436)	grad_norm 21.7951 (nan)	mem 13709MB
[2022-05-30 22:16:37 MetaFG_0] (main.py 265): INFO Train: [0/300][60/1562]	eta 0:25:14 lr 0.000000	time 0.9248 (1.0080)	loss 2.3876 (2.3440)	grad_norm 17.8752 (nan)	mem 13709MB
[2022-05-30 22:16:46 MetaFG_0] (main.py 265): INFO Train: [0/300][70/1562]	eta 0:24:47 lr 0.000000	time 0.9130 (0.9970)	loss 2.3336 (2.3432)	grad_norm 31.5881 (nan)	mem 13709MB
[2022-05-30 22:16:56 MetaFG_0] (main.py 265): INFO Train: [0/300][80/1562]	eta 0:24:25 lr 0.000000	time 0.9237 (0.9887)	loss 2.3452 (2.3417)	grad_norm 28.1965 (nan)	mem 13709MB
[2022-05-30 22:17:05 MetaFG_0] (main.py 265): INFO Train: [0/300][90/1562]	eta 0:24:06 lr 0.000000	time 0.9224 (0.9824)	loss 2.3454 (2.3443)	grad_norm 14.9523 (nan)	mem 13709MB
[2022-05-30 22:17:14 MetaFG_0] (main.py 265): INFO Train: [0/300][100/1562]	eta 0:23:48 lr 0.000000	time 0.9232 (0.9774)	loss 2.3073 (2.3438)	grad_norm 15.5448 (nan)	mem 13709MB
[2022-05-30 22:17:24 MetaFG_0] (main.py 265): INFO Train: [0/300][110/1562]	eta 0:23:33 lr 0.000000	time 0.9230 (0.9732)	loss 2.3613 (2.3428)	grad_norm 15.4464 (nan)	mem 13709MB
[2022-05-30 22:17:33 MetaFG_0] (main.py 265): INFO Train: [0/300][120/1562]	eta 0:23:18 lr 0.000000	time 0.9218 (0.9696)	loss 2.3636 (2.3439)	grad_norm 21.9623 (nan)	mem 13709MB
[2022-05-30 22:17:50 MetaFG_0] (main.py 398): INFO Full config saved to output/MetaFG_0/cub-200_v1/config.json
[2022-05-30 22:17:50 MetaFG_0] (main.py 401): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  ADD_META: false
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: ./imagenet
  FUSION: early
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  LATE_FUSION_LAYER: -1
  MASK_PROB: 0.0
  MASK_TYPE: constant
  NUM_READERS: 4
  NUM_WORKERS: 4
  PIN_MEMORY: true
  TRAIN_INTERPOLATION: bicubic
  TRAIN_PATH: null
  VAL_PATH: null
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DORP_HEAD: true
  DORP_META: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  EXTRA_TOKEN_NUM: 1
  LABEL_SMOOTHING: 0.1
  META_DIMS: []
  NAME: MetaFG_0
  NUM_CLASSES: 1000
  ONLY_LAST_CLS: false
  PRETRAINED: ./pretrained_model/metafg_0_1k_224.pth
  RESUME: false
  TYPE: MetaFG
OUTPUT: output/MetaFG_0/cub-200_v1
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: cub-200_v1
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: false
  BASE_LR: 6.25e-06
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 6.25e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 6.25e-09
  WEIGHT_DECAY: 0.05

[2022-05-30 22:17:52 MetaFG_0] (main.py 99): INFO Creating model:MetaFG/MetaFG_0
[2022-05-30 22:17:52 MetaFG_0] (main.py 102): INFO MetaFG(
  (stage_0): Sequential(
    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage_1): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (_bn1): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_2): ModuleList(
    (0): MBConvBlock(
      (_expand_conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
      (_bn1): BatchNorm2d(384, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (1): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
    (2): MBConvBlock(
      (_expand_conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn0): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_depthwise_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
      (_bn1): BatchNorm2d(768, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_se_reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))
      (_se_expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))
      (_project_conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (_bn2): BatchNorm2d(192, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)
      (_swish): MemoryEfficientSwish()
    )
  )
  (stage_3): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): MHSABlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (stage_4): ModuleList(
    (0): MHSABlock(
      (patch_embed): OverlapPatchEmbed(
        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): MHSABlock(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Relative_Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (softmax): Softmax(dim=-1)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (cl_1_fc): Sequential(
    (0): Mlp(
      (fc1): Linear(in_features=384, out_features=384, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=384, out_features=768, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))
  (norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2022-05-30 22:17:52 MetaFG_0] (main.py 110): INFO number of params: 28029095
[2022-05-30 22:17:52 MetaFG_0] (utils.py 43): INFO ==============> pretrain form ./pretrained_model/metafg_0_1k_224.pth....................
[2022-05-30 22:17:52 MetaFG_0] (utils.py 53): INFO ==============> drop head....................
[2022-05-30 22:17:52 MetaFG_0] (utils.py 63): INFO ==============> drop meta head....................
[2022-05-30 22:17:52 MetaFG_0] (main.py 159): INFO Start training
[2022-05-30 22:17:54 MetaFG_0] (main.py 265): INFO Train: [0/300][0/1562]	eta 0:55:05 lr 0.000000	time 2.1159 (2.1159)	loss 2.3407 (2.3407)	grad_norm inf (inf)	mem 4879MB
[2022-05-30 22:17:58 MetaFG_0] (main.py 265): INFO Train: [0/300][10/1562]	eta 0:12:46 lr 0.000000	time 0.3364 (0.4938)	loss 2.3662 (2.3441)	grad_norm 19.2023 (nan)	mem 4879MB
[2022-05-30 22:18:01 MetaFG_0] (main.py 265): INFO Train: [0/300][20/1562]	eta 0:10:53 lr 0.000000	time 0.3357 (0.4239)	loss 2.3435 (2.3358)	grad_norm 15.6053 (nan)	mem 4879MB
[2022-05-30 22:18:05 MetaFG_0] (main.py 265): INFO Train: [0/300][30/1562]	eta 0:10:08 lr 0.000000	time 0.3043 (0.3974)	loss 2.3304 (2.3355)	grad_norm 18.7409 (nan)	mem 4879MB
[2022-05-30 22:18:08 MetaFG_0] (main.py 265): INFO Train: [0/300][40/1562]	eta 0:09:30 lr 0.000000	time 0.2922 (0.3745)	loss 2.3267 (2.3362)	grad_norm 18.6518 (nan)	mem 4879MB
[2022-05-30 22:18:11 MetaFG_0] (main.py 265): INFO Train: [0/300][50/1562]	eta 0:09:05 lr 0.000000	time 0.2987 (0.3607)	loss 2.2980 (2.3363)	grad_norm 14.3522 (nan)	mem 4879MB
[2022-05-30 22:18:14 MetaFG_0] (main.py 265): INFO Train: [0/300][60/1562]	eta 0:08:47 lr 0.000000	time 0.2934 (0.3514)	loss 2.3061 (2.3360)	grad_norm 16.6624 (nan)	mem 4879MB
[2022-05-30 22:18:17 MetaFG_0] (main.py 265): INFO Train: [0/300][70/1562]	eta 0:08:34 lr 0.000000	time 0.2981 (0.3448)	loss 2.3420 (2.3352)	grad_norm 21.8437 (nan)	mem 4879MB
[2022-05-30 22:18:20 MetaFG_0] (main.py 265): INFO Train: [0/300][80/1562]	eta 0:08:23 lr 0.000000	time 0.2931 (0.3396)	loss 2.3037 (2.3346)	grad_norm 22.3895 (nan)	mem 4879MB
[2022-05-30 22:18:23 MetaFG_0] (main.py 265): INFO Train: [0/300][90/1562]	eta 0:08:14 lr 0.000000	time 0.2979 (0.3357)	loss 2.3445 (2.3349)	grad_norm 30.8735 (nan)	mem 4879MB
[2022-05-30 22:18:26 MetaFG_0] (main.py 265): INFO Train: [0/300][100/1562]	eta 0:08:06 lr 0.000000	time 0.2929 (0.3327)	loss 2.3282 (2.3339)	grad_norm 13.6469 (nan)	mem 4879MB
[2022-05-30 22:18:29 MetaFG_0] (main.py 265): INFO Train: [0/300][110/1562]	eta 0:07:59 lr 0.000000	time 0.2979 (0.3303)	loss 2.3321 (2.3343)	grad_norm 13.9712 (nan)	mem 4879MB
[2022-05-30 22:18:32 MetaFG_0] (main.py 265): INFO Train: [0/300][120/1562]	eta 0:07:53 lr 0.000000	time 0.3148 (0.3284)	loss 2.3659 (2.3347)	grad_norm 24.9631 (nan)	mem 4879MB
[2022-05-30 22:18:35 MetaFG_0] (main.py 265): INFO Train: [0/300][130/1562]	eta 0:07:48 lr 0.000000	time 0.2985 (0.3269)	loss 2.3583 (2.3349)	grad_norm 10.8822 (nan)	mem 4879MB
[2022-05-30 22:18:38 MetaFG_0] (main.py 265): INFO Train: [0/300][140/1562]	eta 0:07:42 lr 0.000000	time 0.2930 (0.3253)	loss 2.3243 (2.3354)	grad_norm 12.1277 (nan)	mem 4879MB
[2022-05-30 22:18:41 MetaFG_0] (main.py 265): INFO Train: [0/300][150/1562]	eta 0:07:37 lr 0.000000	time 0.2966 (0.3239)	loss 2.3398 (2.3357)	grad_norm 12.8757 (nan)	mem 4879MB
[2022-05-30 22:18:44 MetaFG_0] (main.py 265): INFO Train: [0/300][160/1562]	eta 0:07:32 lr 0.000000	time 0.2918 (0.3226)	loss 2.3271 (2.3351)	grad_norm 19.0207 (nan)	mem 4879MB
[2022-05-30 22:18:47 MetaFG_0] (main.py 265): INFO Train: [0/300][170/1562]	eta 0:07:27 lr 0.000000	time 0.2922 (0.3214)	loss 2.3754 (2.3363)	grad_norm 15.5998 (nan)	mem 4879MB
[2022-05-30 22:18:50 MetaFG_0] (main.py 265): INFO Train: [0/300][180/1562]	eta 0:07:22 lr 0.000000	time 0.2982 (0.3205)	loss 2.3480 (2.3364)	grad_norm 12.9035 (nan)	mem 4879MB
[2022-05-30 22:18:53 MetaFG_0] (main.py 265): INFO Train: [0/300][190/1562]	eta 0:07:18 lr 0.000000	time 0.2978 (0.3196)	loss 2.3055 (2.3364)	grad_norm 17.4505 (nan)	mem 4879MB
[2022-05-30 22:18:56 MetaFG_0] (main.py 265): INFO Train: [0/300][200/1562]	eta 0:07:14 lr 0.000000	time 0.2982 (0.3190)	loss 2.3355 (2.3368)	grad_norm 12.7725 (nan)	mem 4879MB
[2022-05-30 22:18:59 MetaFG_0] (main.py 265): INFO Train: [0/300][210/1562]	eta 0:07:10 lr 0.000000	time 0.2926 (0.3183)	loss 2.3493 (2.3378)	grad_norm 10.9237 (nan)	mem 4879MB
[2022-05-30 22:19:02 MetaFG_0] (main.py 265): INFO Train: [0/300][220/1562]	eta 0:07:06 lr 0.000000	time 0.2929 (0.3176)	loss 2.3765 (2.3380)	grad_norm 12.4978 (nan)	mem 4879MB
[2022-05-30 22:19:06 MetaFG_0] (main.py 265): INFO Train: [0/300][230/1562]	eta 0:07:02 lr 0.000000	time 0.2983 (0.3170)	loss 2.3336 (2.3379)	grad_norm 18.3301 (nan)	mem 4879MB
[2022-05-30 22:19:09 MetaFG_0] (main.py 265): INFO Train: [0/300][240/1562]	eta 0:06:58 lr 0.000000	time 0.2924 (0.3164)	loss 2.3505 (2.3380)	grad_norm 19.4440 (nan)	mem 4879MB
[2022-05-30 22:19:12 MetaFG_0] (main.py 265): INFO Train: [0/300][250/1562]	eta 0:06:54 lr 0.000000	time 0.2940 (0.3160)	loss 2.3151 (2.3385)	grad_norm 17.1109 (nan)	mem 4879MB
[2022-05-30 22:19:15 MetaFG_0] (main.py 265): INFO Train: [0/300][260/1562]	eta 0:06:50 lr 0.000000	time 0.2919 (0.3154)	loss 2.3365 (2.3388)	grad_norm 9.9125 (nan)	mem 4879MB
[2022-05-30 22:19:18 MetaFG_0] (main.py 265): INFO Train: [0/300][270/1562]	eta 0:06:46 lr 0.000000	time 0.2997 (0.3150)	loss 2.3481 (2.3389)	grad_norm 12.9104 (nan)	mem 4879MB
[2022-05-30 22:19:21 MetaFG_0] (main.py 265): INFO Train: [0/300][280/1562]	eta 0:06:43 lr 0.000000	time 0.2971 (0.3146)	loss 2.3180 (2.3391)	grad_norm 17.5637 (nan)	mem 4879MB
[2022-05-30 22:19:24 MetaFG_0] (main.py 265): INFO Train: [0/300][290/1562]	eta 0:06:39 lr 0.000000	time 0.2991 (0.3142)	loss 2.3469 (2.3395)	grad_norm 14.2003 (nan)	mem 4879MB
[2022-05-30 22:19:27 MetaFG_0] (main.py 265): INFO Train: [0/300][300/1562]	eta 0:06:37 lr 0.000000	time 0.2998 (0.3147)	loss 2.3170 (2.3394)	grad_norm 13.0825 (nan)	mem 4879MB
[2022-05-30 22:19:31 MetaFG_0] (main.py 265): INFO Train: [0/300][310/1562]	eta 0:06:35 lr 0.000000	time 0.3672 (0.3163)	loss 2.3630 (2.3399)	grad_norm 19.6308 (nan)	mem 4879MB
[2022-05-30 22:19:34 MetaFG_0] (main.py 265): INFO Train: [0/300][320/1562]	eta 0:06:34 lr 0.000000	time 0.3732 (0.3176)	loss 2.2857 (2.3392)	grad_norm 14.9461 (nan)	mem 4879MB
[2022-05-30 22:19:38 MetaFG_0] (main.py 265): INFO Train: [0/300][330/1562]	eta 0:06:32 lr 0.000000	time 0.3280 (0.3186)	loss 2.3172 (2.3396)	grad_norm 14.6683 (nan)	mem 4879MB
[2022-05-30 22:19:41 MetaFG_0] (main.py 265): INFO Train: [0/300][340/1562]	eta 0:06:29 lr 0.000000	time 0.2980 (0.3188)	loss 2.2949 (2.3391)	grad_norm 18.4788 (nan)	mem 4879MB
[2022-05-30 22:19:44 MetaFG_0] (main.py 265): INFO Train: [0/300][350/1562]	eta 0:06:25 lr 0.000000	time 0.2984 (0.3184)	loss 2.3051 (2.3391)	grad_norm 13.3032 (nan)	mem 4879MB
[2022-05-30 22:19:47 MetaFG_0] (main.py 265): INFO Train: [0/300][360/1562]	eta 0:06:22 lr 0.000000	time 0.2984 (0.3180)	loss 2.3366 (2.3388)	grad_norm 19.9575 (nan)	mem 4879MB
[2022-05-30 22:19:50 MetaFG_0] (main.py 265): INFO Train: [0/300][370/1562]	eta 0:06:18 lr 0.000000	time 0.2942 (0.3176)	loss 2.3462 (2.3387)	grad_norm 19.2966 (nan)	mem 4879MB
[2022-05-30 22:19:53 MetaFG_0] (main.py 265): INFO Train: [0/300][380/1562]	eta 0:06:15 lr 0.000000	time 0.3210 (0.3175)	loss 2.3044 (2.3383)	grad_norm 28.7177 (nan)	mem 4879MB
[2022-05-30 22:19:57 MetaFG_0] (main.py 265): INFO Train: [0/300][390/1562]	eta 0:06:12 lr 0.000000	time 0.3432 (0.3181)	loss 2.3625 (2.3384)	grad_norm 14.3366 (nan)	mem 4879MB
[2022-05-30 22:20:00 MetaFG_0] (main.py 265): INFO Train: [0/300][400/1562]	eta 0:06:10 lr 0.000000	time 0.3311 (0.3189)	loss 2.3249 (2.3385)	grad_norm 17.2441 (nan)	mem 4879MB
[2022-05-30 22:20:03 MetaFG_0] (main.py 265): INFO Train: [0/300][410/1562]	eta 0:06:07 lr 0.000000	time 0.2977 (0.3188)	loss 2.3039 (2.3386)	grad_norm 15.5421 (nan)	mem 4879MB
[2022-05-30 22:20:06 MetaFG_0] (main.py 265): INFO Train: [0/300][420/1562]	eta 0:06:03 lr 0.000000	time 0.2924 (0.3184)	loss 2.3115 (2.3388)	grad_norm 17.9894 (nan)	mem 4879MB
[2022-05-30 22:20:09 MetaFG_0] (main.py 265): INFO Train: [0/300][430/1562]	eta 0:06:00 lr 0.000000	time 0.2919 (0.3181)	loss 2.3418 (2.3387)	grad_norm 14.6377 (nan)	mem 4879MB
[2022-05-30 22:20:12 MetaFG_0] (main.py 265): INFO Train: [0/300][440/1562]	eta 0:05:56 lr 0.000000	time 0.2923 (0.3177)	loss 2.3539 (2.3385)	grad_norm 19.1955 (nan)	mem 4879MB
[2022-05-30 22:20:15 MetaFG_0] (main.py 265): INFO Train: [0/300][450/1562]	eta 0:05:52 lr 0.000000	time 0.2918 (0.3174)	loss 2.3354 (2.3387)	grad_norm 14.5343 (nan)	mem 4879MB
[2022-05-30 22:20:18 MetaFG_0] (main.py 265): INFO Train: [0/300][460/1562]	eta 0:05:49 lr 0.000000	time 0.3002 (0.3172)	loss 2.3460 (2.3386)	grad_norm 14.3095 (nan)	mem 4879MB
[2022-05-30 22:20:22 MetaFG_0] (main.py 265): INFO Train: [0/300][470/1562]	eta 0:05:46 lr 0.000000	time 0.2960 (0.3169)	loss 2.3542 (2.3388)	grad_norm 16.0983 (nan)	mem 4879MB
[2022-05-30 22:20:25 MetaFG_0] (main.py 265): INFO Train: [0/300][480/1562]	eta 0:05:42 lr 0.000000	time 0.2930 (0.3165)	loss 2.3229 (2.3387)	grad_norm 12.8384 (nan)	mem 4879MB
[2022-05-30 22:20:28 MetaFG_0] (main.py 265): INFO Train: [0/300][490/1562]	eta 0:05:39 lr 0.000000	time 0.2977 (0.3162)	loss 2.3423 (2.3387)	grad_norm 20.7223 (nan)	mem 4879MB
[2022-05-30 22:20:31 MetaFG_0] (main.py 265): INFO Train: [0/300][500/1562]	eta 0:05:35 lr 0.000000	time 0.2932 (0.3160)	loss 2.3198 (2.3387)	grad_norm 14.0040 (nan)	mem 4879MB
[2022-05-30 22:20:34 MetaFG_0] (main.py 265): INFO Train: [0/300][510/1562]	eta 0:05:32 lr 0.000000	time 0.2933 (0.3157)	loss 2.3100 (2.3385)	grad_norm 20.5642 (nan)	mem 4879MB
[2022-05-30 22:20:37 MetaFG_0] (main.py 265): INFO Train: [0/300][520/1562]	eta 0:05:28 lr 0.000000	time 0.2940 (0.3155)	loss 2.3401 (2.3385)	grad_norm 30.4683 (nan)	mem 4879MB
[2022-05-30 22:20:40 MetaFG_0] (main.py 265): INFO Train: [0/300][530/1562]	eta 0:05:25 lr 0.000000	time 0.2933 (0.3153)	loss 2.3210 (2.3386)	grad_norm 28.3733 (nan)	mem 4879MB
[2022-05-30 22:20:43 MetaFG_0] (main.py 265): INFO Train: [0/300][540/1562]	eta 0:05:22 lr 0.000000	time 0.2993 (0.3151)	loss 2.3186 (2.3385)	grad_norm 15.1763 (nan)	mem 4879MB
[2022-05-30 22:20:46 MetaFG_0] (main.py 265): INFO Train: [0/300][550/1562]	eta 0:05:18 lr 0.000000	time 0.2975 (0.3149)	loss 2.3268 (2.3384)	grad_norm 12.0907 (nan)	mem 4879MB
[2022-05-30 22:20:49 MetaFG_0] (main.py 265): INFO Train: [0/300][560/1562]	eta 0:05:15 lr 0.000000	time 0.2951 (0.3146)	loss 2.3332 (2.3384)	grad_norm 16.4605 (nan)	mem 4879MB
[2022-05-30 22:20:52 MetaFG_0] (main.py 265): INFO Train: [0/300][570/1562]	eta 0:05:11 lr 0.000000	time 0.2921 (0.3145)	loss 2.3083 (2.3382)	grad_norm 11.2330 (nan)	mem 4879MB
[2022-05-30 22:20:55 MetaFG_0] (main.py 265): INFO Train: [0/300][580/1562]	eta 0:05:08 lr 0.000000	time 0.2985 (0.3143)	loss 2.3023 (2.3385)	grad_norm 11.5818 (nan)	mem 4879MB
[2022-05-30 22:20:58 MetaFG_0] (main.py 265): INFO Train: [0/300][590/1562]	eta 0:05:05 lr 0.000000	time 0.2921 (0.3143)	loss 2.3283 (2.3383)	grad_norm 13.2553 (nan)	mem 4879MB
[2022-05-30 22:21:01 MetaFG_0] (main.py 265): INFO Train: [0/300][600/1562]	eta 0:05:02 lr 0.000000	time 0.2930 (0.3141)	loss 2.3160 (2.3383)	grad_norm 14.4028 (nan)	mem 4879MB
[2022-05-30 22:21:04 MetaFG_0] (main.py 265): INFO Train: [0/300][610/1562]	eta 0:04:58 lr 0.000000	time 0.2978 (0.3139)	loss 2.3446 (2.3384)	grad_norm 10.8623 (nan)	mem 4879MB
[2022-05-30 22:21:07 MetaFG_0] (main.py 265): INFO Train: [0/300][620/1562]	eta 0:04:55 lr 0.000000	time 0.2926 (0.3138)	loss 2.3599 (2.3384)	grad_norm 18.2446 (nan)	mem 4879MB
[2022-05-30 22:21:10 MetaFG_0] (main.py 265): INFO Train: [0/300][630/1562]	eta 0:04:52 lr 0.000000	time 0.2946 (0.3136)	loss 2.3369 (2.3383)	grad_norm 19.0052 (nan)	mem 4879MB
[2022-05-30 22:21:13 MetaFG_0] (main.py 265): INFO Train: [0/300][640/1562]	eta 0:04:49 lr 0.000000	time 0.2984 (0.3135)	loss 2.3603 (2.3382)	grad_norm 22.1228 (nan)	mem 4879MB
[2022-05-30 22:21:16 MetaFG_0] (main.py 265): INFO Train: [0/300][650/1562]	eta 0:04:45 lr 0.000000	time 0.2994 (0.3134)	loss 2.3335 (2.3381)	grad_norm 40.1291 (nan)	mem 4879MB
[2022-05-30 22:21:19 MetaFG_0] (main.py 265): INFO Train: [0/300][660/1562]	eta 0:04:42 lr 0.000000	time 0.2942 (0.3132)	loss 2.3475 (2.3383)	grad_norm 22.6280 (nan)	mem 4879MB
[2022-05-30 22:21:22 MetaFG_0] (main.py 265): INFO Train: [0/300][670/1562]	eta 0:04:39 lr 0.000000	time 0.3303 (0.3131)	loss 2.3052 (2.3383)	grad_norm 55.8316 (nan)	mem 4879MB
[2022-05-30 22:21:26 MetaFG_0] (main.py 265): INFO Train: [0/300][680/1562]	eta 0:04:36 lr 0.000000	time 0.2936 (0.3132)	loss 2.3549 (2.3383)	grad_norm 14.8740 (nan)	mem 4879MB
[2022-05-30 22:21:29 MetaFG_0] (main.py 265): INFO Train: [0/300][690/1562]	eta 0:04:32 lr 0.000000	time 0.2996 (0.3130)	loss 2.3096 (2.3382)	grad_norm 22.5592 (nan)	mem 4879MB
[2022-05-30 22:21:32 MetaFG_0] (main.py 265): INFO Train: [0/300][700/1562]	eta 0:04:29 lr 0.000000	time 0.2993 (0.3129)	loss 2.3332 (2.3381)	grad_norm 11.7014 (nan)	mem 4879MB
[2022-05-30 22:21:35 MetaFG_0] (main.py 265): INFO Train: [0/300][710/1562]	eta 0:04:26 lr 0.000000	time 0.2986 (0.3128)	loss 2.3486 (2.3380)	grad_norm 16.4008 (nan)	mem 4879MB
[2022-05-30 22:21:38 MetaFG_0] (main.py 265): INFO Train: [0/300][720/1562]	eta 0:04:23 lr 0.000000	time 0.2915 (0.3127)	loss 2.3591 (2.3381)	grad_norm 16.9871 (nan)	mem 4879MB
[2022-05-30 22:21:41 MetaFG_0] (main.py 265): INFO Train: [0/300][730/1562]	eta 0:04:20 lr 0.000000	time 0.2979 (0.3126)	loss 2.2874 (2.3379)	grad_norm 19.5118 (nan)	mem 4879MB
[2022-05-30 22:21:44 MetaFG_0] (main.py 265): INFO Train: [0/300][740/1562]	eta 0:04:16 lr 0.000000	time 0.2923 (0.3124)	loss 2.3581 (2.3377)	grad_norm 24.5230 (nan)	mem 4879MB
[2022-05-30 22:21:47 MetaFG_0] (main.py 265): INFO Train: [0/300][750/1562]	eta 0:04:13 lr 0.000000	time 0.2987 (0.3123)	loss 2.3275 (2.3377)	grad_norm 15.5324 (nan)	mem 4879MB
[2022-05-30 22:21:50 MetaFG_0] (main.py 265): INFO Train: [0/300][760/1562]	eta 0:04:10 lr 0.000000	time 0.2984 (0.3122)	loss 2.3103 (2.3378)	grad_norm 11.0562 (nan)	mem 4879MB
[2022-05-30 22:21:53 MetaFG_0] (main.py 265): INFO Train: [0/300][770/1562]	eta 0:04:07 lr 0.000000	time 0.2995 (0.3121)	loss 2.3234 (2.3378)	grad_norm 16.0145 (nan)	mem 4879MB
[2022-05-30 22:21:56 MetaFG_0] (main.py 265): INFO Train: [0/300][780/1562]	eta 0:04:03 lr 0.000000	time 0.2919 (0.3120)	loss 2.3106 (2.3376)	grad_norm 12.6505 (nan)	mem 4879MB
[2022-05-30 22:21:59 MetaFG_0] (main.py 265): INFO Train: [0/300][790/1562]	eta 0:04:00 lr 0.000000	time 0.2978 (0.3119)	loss 2.3310 (2.3375)	grad_norm 21.9020 (nan)	mem 4879MB
[2022-05-30 22:22:02 MetaFG_0] (main.py 265): INFO Train: [0/300][800/1562]	eta 0:03:57 lr 0.000000	time 0.2925 (0.3118)	loss 2.3326 (2.3377)	grad_norm 14.3955 (nan)	mem 4879MB
[2022-05-30 22:22:05 MetaFG_0] (main.py 265): INFO Train: [0/300][810/1562]	eta 0:03:54 lr 0.000000	time 0.2987 (0.3117)	loss 2.3201 (2.3375)	grad_norm 21.5636 (nan)	mem 4879MB
[2022-05-30 22:22:08 MetaFG_0] (main.py 265): INFO Train: [0/300][820/1562]	eta 0:03:51 lr 0.000000	time 0.2978 (0.3116)	loss 2.3182 (2.3374)	grad_norm 14.1738 (nan)	mem 4879MB
[2022-05-30 22:22:11 MetaFG_0] (main.py 265): INFO Train: [0/300][830/1562]	eta 0:03:48 lr 0.000000	time 0.2917 (0.3115)	loss 2.2885 (2.3373)	grad_norm 14.9191 (nan)	mem 4879MB
[2022-05-30 22:22:14 MetaFG_0] (main.py 265): INFO Train: [0/300][840/1562]	eta 0:03:44 lr 0.000000	time 0.2922 (0.3114)	loss 2.3277 (2.3374)	grad_norm 19.6856 (nan)	mem 4879MB
[2022-05-30 22:22:17 MetaFG_0] (main.py 265): INFO Train: [0/300][850/1562]	eta 0:03:41 lr 0.000000	time 0.2993 (0.3113)	loss 2.3514 (2.3374)	grad_norm 12.3508 (nan)	mem 4879MB
[2022-05-30 22:22:20 MetaFG_0] (main.py 265): INFO Train: [0/300][860/1562]	eta 0:03:38 lr 0.000000	time 0.2989 (0.3112)	loss 2.3456 (2.3372)	grad_norm 16.5785 (nan)	mem 4879MB
[2022-05-30 22:22:23 MetaFG_0] (main.py 265): INFO Train: [0/300][870/1562]	eta 0:03:35 lr 0.000000	time 0.2945 (0.3111)	loss 2.3469 (2.3372)	grad_norm 18.2992 (nan)	mem 4879MB
[2022-05-30 22:22:26 MetaFG_0] (main.py 265): INFO Train: [0/300][880/1562]	eta 0:03:32 lr 0.000000	time 0.2928 (0.3110)	loss 2.3653 (2.3373)	grad_norm 14.9136 (nan)	mem 4879MB
[2022-05-30 22:22:29 MetaFG_0] (main.py 265): INFO Train: [0/300][890/1562]	eta 0:03:28 lr 0.000000	time 0.2924 (0.3109)	loss 2.3301 (2.3372)	grad_norm 11.7686 (nan)	mem 4879MB
[2022-05-30 22:22:32 MetaFG_0] (main.py 265): INFO Train: [0/300][900/1562]	eta 0:03:25 lr 0.000000	time 0.2923 (0.3108)	loss 2.3514 (2.3372)	grad_norm 15.4888 (nan)	mem 4879MB
[2022-05-30 22:22:35 MetaFG_0] (main.py 265): INFO Train: [0/300][910/1562]	eta 0:03:22 lr 0.000000	time 0.2930 (0.3108)	loss 2.2680 (2.3373)	grad_norm 13.3447 (nan)	mem 4879MB
[2022-05-30 22:22:38 MetaFG_0] (main.py 265): INFO Train: [0/300][920/1562]	eta 0:03:19 lr 0.000000	time 0.2957 (0.3107)	loss 2.3762 (2.3373)	grad_norm 12.6806 (nan)	mem 4879MB
[2022-05-30 22:22:41 MetaFG_0] (main.py 265): INFO Train: [0/300][930/1562]	eta 0:03:16 lr 0.000000	time 0.2999 (0.3106)	loss 2.3181 (2.3375)	grad_norm 15.0018 (nan)	mem 4879MB
[2022-05-30 22:22:45 MetaFG_0] (main.py 265): INFO Train: [0/300][940/1562]	eta 0:03:13 lr 0.000000	time 0.3044 (0.3106)	loss 2.3659 (2.3374)	grad_norm 31.7313 (nan)	mem 4879MB
[2022-05-30 22:22:48 MetaFG_0] (main.py 265): INFO Train: [0/300][950/1562]	eta 0:03:10 lr 0.000000	time 0.2988 (0.3105)	loss 2.3125 (2.3374)	grad_norm 19.7488 (nan)	mem 4879MB
[2022-05-30 22:22:51 MetaFG_0] (main.py 265): INFO Train: [0/300][960/1562]	eta 0:03:06 lr 0.000000	time 0.2952 (0.3104)	loss 2.3736 (2.3374)	grad_norm 16.2550 (nan)	mem 4879MB
[2022-05-30 22:22:54 MetaFG_0] (main.py 265): INFO Train: [0/300][970/1562]	eta 0:03:03 lr 0.000000	time 0.2991 (0.3104)	loss 2.3531 (2.3374)	grad_norm 14.8186 (nan)	mem 4879MB
[2022-05-30 22:22:57 MetaFG_0] (main.py 265): INFO Train: [0/300][980/1562]	eta 0:03:00 lr 0.000000	time 0.2987 (0.3103)	loss 2.3640 (2.3374)	grad_norm 12.1637 (nan)	mem 4879MB
[2022-05-30 22:23:00 MetaFG_0] (main.py 265): INFO Train: [0/300][990/1562]	eta 0:02:57 lr 0.000000	time 0.2975 (0.3103)	loss 2.3538 (2.3374)	grad_norm 16.3437 (nan)	mem 4879MB
[2022-05-30 22:23:03 MetaFG_0] (main.py 265): INFO Train: [0/300][1000/1562]	eta 0:02:54 lr 0.000000	time 0.2922 (0.3102)	loss 2.3610 (2.3373)	grad_norm 11.4581 (nan)	mem 4879MB
[2022-05-30 22:23:06 MetaFG_0] (main.py 265): INFO Train: [0/300][1010/1562]	eta 0:02:51 lr 0.000000	time 0.2983 (0.3101)	loss 2.3659 (2.3373)	grad_norm 11.9394 (nan)	mem 4879MB
[2022-05-30 22:23:09 MetaFG_0] (main.py 265): INFO Train: [0/300][1020/1562]	eta 0:02:48 lr 0.000000	time 0.2933 (0.3101)	loss 2.3527 (2.3374)	grad_norm 11.6545 (nan)	mem 4879MB
[2022-05-30 22:23:12 MetaFG_0] (main.py 265): INFO Train: [0/300][1030/1562]	eta 0:02:44 lr 0.000000	time 0.2924 (0.3100)	loss 2.3333 (2.3375)	grad_norm 17.3687 (nan)	mem 4879MB
[2022-05-30 22:23:15 MetaFG_0] (main.py 265): INFO Train: [0/300][1040/1562]	eta 0:02:41 lr 0.000000	time 0.2921 (0.3099)	loss 2.3110 (2.3375)	grad_norm 27.1040 (nan)	mem 4879MB
[2022-05-30 22:23:18 MetaFG_0] (main.py 265): INFO Train: [0/300][1050/1562]	eta 0:02:38 lr 0.000000	time 0.2937 (0.3099)	loss 2.3478 (2.3374)	grad_norm 18.8301 (nan)	mem 4879MB
[2022-05-30 22:23:21 MetaFG_0] (main.py 265): INFO Train: [0/300][1060/1562]	eta 0:02:35 lr 0.000000	time 0.2922 (0.3099)	loss 2.3337 (2.3375)	grad_norm 23.6458 (nan)	mem 4879MB
[2022-05-30 22:23:24 MetaFG_0] (main.py 265): INFO Train: [0/300][1070/1562]	eta 0:02:32 lr 0.000000	time 0.2988 (0.3098)	loss 2.3196 (2.3373)	grad_norm 12.7724 (nan)	mem 4879MB
[2022-05-30 22:23:27 MetaFG_0] (main.py 265): INFO Train: [0/300][1080/1562]	eta 0:02:29 lr 0.000000	time 0.2919 (0.3098)	loss 2.3353 (2.3373)	grad_norm 18.4042 (nan)	mem 4879MB
[2022-05-30 22:23:30 MetaFG_0] (main.py 265): INFO Train: [0/300][1090/1562]	eta 0:02:26 lr 0.000000	time 0.2984 (0.3097)	loss 2.3278 (2.3372)	grad_norm 18.0182 (nan)	mem 4879MB
[2022-05-30 22:23:33 MetaFG_0] (main.py 265): INFO Train: [0/300][1100/1562]	eta 0:02:23 lr 0.000000	time 0.2926 (0.3097)	loss 2.3728 (2.3374)	grad_norm 12.4742 (nan)	mem 4879MB
[2022-05-30 22:23:36 MetaFG_0] (main.py 265): INFO Train: [0/300][1110/1562]	eta 0:02:19 lr 0.000000	time 0.2932 (0.3096)	loss 2.3321 (2.3374)	grad_norm 14.7917 (nan)	mem 4879MB
[2022-05-30 22:23:39 MetaFG_0] (main.py 265): INFO Train: [0/300][1120/1562]	eta 0:02:16 lr 0.000000	time 0.2921 (0.3096)	loss 2.3479 (2.3375)	grad_norm 16.0076 (nan)	mem 4879MB
[2022-05-30 22:23:42 MetaFG_0] (main.py 265): INFO Train: [0/300][1130/1562]	eta 0:02:13 lr 0.000000	time 0.2926 (0.3095)	loss 2.3517 (2.3374)	grad_norm 16.2532 (nan)	mem 4879MB
[2022-05-30 22:23:45 MetaFG_0] (main.py 265): INFO Train: [0/300][1140/1562]	eta 0:02:10 lr 0.000000	time 0.2992 (0.3094)	loss 2.3197 (2.3373)	grad_norm 11.7531 (nan)	mem 4879MB
[2022-05-30 22:23:48 MetaFG_0] (main.py 265): INFO Train: [0/300][1150/1562]	eta 0:02:07 lr 0.000000	time 0.2935 (0.3094)	loss 2.2867 (2.3373)	grad_norm 13.6172 (nan)	mem 4879MB
[2022-05-30 22:23:51 MetaFG_0] (main.py 265): INFO Train: [0/300][1160/1562]	eta 0:02:04 lr 0.000000	time 0.2931 (0.3094)	loss 2.3385 (2.3373)	grad_norm 11.4720 (nan)	mem 4879MB
[2022-05-30 22:23:54 MetaFG_0] (main.py 265): INFO Train: [0/300][1170/1562]	eta 0:02:01 lr 0.000000	time 0.3000 (0.3093)	loss 2.3383 (2.3372)	grad_norm 24.1704 (nan)	mem 4879MB
[2022-05-30 22:23:58 MetaFG_0] (main.py 265): INFO Train: [0/300][1180/1562]	eta 0:01:58 lr 0.000000	time 0.2925 (0.3092)	loss 2.3472 (2.3372)	grad_norm 16.2691 (nan)	mem 4879MB
[2022-05-30 22:24:01 MetaFG_0] (main.py 265): INFO Train: [0/300][1190/1562]	eta 0:01:55 lr 0.000000	time 0.2920 (0.3092)	loss 2.3459 (2.3372)	grad_norm 17.4608 (nan)	mem 4879MB
[2022-05-30 22:24:04 MetaFG_0] (main.py 265): INFO Train: [0/300][1200/1562]	eta 0:01:51 lr 0.000000	time 0.2935 (0.3092)	loss 2.3531 (2.3371)	grad_norm 11.4046 (nan)	mem 4879MB
[2022-05-30 22:24:07 MetaFG_0] (main.py 265): INFO Train: [0/300][1210/1562]	eta 0:01:48 lr 0.000000	time 0.2928 (0.3091)	loss 2.3514 (2.3371)	grad_norm 18.0342 (nan)	mem 4879MB
[2022-05-30 22:24:10 MetaFG_0] (main.py 265): INFO Train: [0/300][1220/1562]	eta 0:01:45 lr 0.000000	time 0.2993 (0.3091)	loss 2.3485 (2.3370)	grad_norm 19.1135 (nan)	mem 4879MB
[2022-05-30 22:24:13 MetaFG_0] (main.py 265): INFO Train: [0/300][1230/1562]	eta 0:01:42 lr 0.000000	time 0.2923 (0.3090)	loss 2.3437 (2.3371)	grad_norm 15.3889 (nan)	mem 4879MB
[2022-05-30 22:24:16 MetaFG_0] (main.py 265): INFO Train: [0/300][1240/1562]	eta 0:01:39 lr 0.000000	time 0.2980 (0.3090)	loss 2.3148 (2.3369)	grad_norm 14.0480 (nan)	mem 4879MB
[2022-05-30 22:24:19 MetaFG_0] (main.py 265): INFO Train: [0/300][1250/1562]	eta 0:01:36 lr 0.000000	time 0.2922 (0.3089)	loss 2.3715 (2.3369)	grad_norm 12.7980 (nan)	mem 4879MB
[2022-05-30 22:24:22 MetaFG_0] (main.py 265): INFO Train: [0/300][1260/1562]	eta 0:01:33 lr 0.000000	time 0.2933 (0.3089)	loss 2.3410 (2.3368)	grad_norm 28.3076 (nan)	mem 4879MB
[2022-05-30 22:24:25 MetaFG_0] (main.py 265): INFO Train: [0/300][1270/1562]	eta 0:01:30 lr 0.000000	time 0.2937 (0.3089)	loss 2.3135 (2.3366)	grad_norm 31.4998 (nan)	mem 4879MB
[2022-05-30 22:24:28 MetaFG_0] (main.py 265): INFO Train: [0/300][1280/1562]	eta 0:01:27 lr 0.000000	time 0.2922 (0.3088)	loss 2.3654 (2.3366)	grad_norm 13.2985 (nan)	mem 4879MB
[2022-05-30 22:24:31 MetaFG_0] (main.py 265): INFO Train: [0/300][1290/1562]	eta 0:01:23 lr 0.000000	time 0.2988 (0.3088)	loss 2.3600 (2.3365)	grad_norm 21.3204 (nan)	mem 4879MB
[2022-05-30 22:24:34 MetaFG_0] (main.py 265): INFO Train: [0/300][1300/1562]	eta 0:01:20 lr 0.000000	time 0.2921 (0.3087)	loss 2.3127 (2.3365)	grad_norm 16.0903 (nan)	mem 4879MB
[2022-05-30 22:24:37 MetaFG_0] (main.py 265): INFO Train: [0/300][1310/1562]	eta 0:01:17 lr 0.000000	time 0.2933 (0.3087)	loss 2.3618 (2.3365)	grad_norm 30.3396 (nan)	mem 4879MB
[2022-05-30 22:24:40 MetaFG_0] (main.py 265): INFO Train: [0/300][1320/1562]	eta 0:01:14 lr 0.000000	time 0.2991 (0.3086)	loss 2.2962 (2.3366)	grad_norm 14.2223 (nan)	mem 4879MB
[2022-05-30 22:24:43 MetaFG_0] (main.py 265): INFO Train: [0/300][1330/1562]	eta 0:01:11 lr 0.000000	time 0.2972 (0.3086)	loss 2.3180 (2.3366)	grad_norm 13.6711 (nan)	mem 4879MB
[2022-05-30 22:24:46 MetaFG_0] (main.py 265): INFO Train: [0/300][1340/1562]	eta 0:01:08 lr 0.000000	time 0.2979 (0.3086)	loss 2.3094 (2.3365)	grad_norm 9.1603 (nan)	mem 4879MB
[2022-05-30 22:24:49 MetaFG_0] (main.py 265): INFO Train: [0/300][1350/1562]	eta 0:01:05 lr 0.000000	time 0.2962 (0.3085)	loss 2.3758 (2.3365)	grad_norm 16.4064 (nan)	mem 4879MB
[2022-05-30 22:24:52 MetaFG_0] (main.py 265): INFO Train: [0/300][1360/1562]	eta 0:01:02 lr 0.000000	time 0.2993 (0.3085)	loss 2.3423 (2.3365)	grad_norm 10.2452 (nan)	mem 4879MB
[2022-05-30 22:24:55 MetaFG_0] (main.py 265): INFO Train: [0/300][1370/1562]	eta 0:00:59 lr 0.000000	time 0.2939 (0.3085)	loss 2.3249 (2.3365)	grad_norm 27.0599 (nan)	mem 4879MB
[2022-05-30 22:24:58 MetaFG_0] (main.py 265): INFO Train: [0/300][1380/1562]	eta 0:00:56 lr 0.000000	time 0.2922 (0.3084)	loss 2.2826 (2.3364)	grad_norm 20.6211 (nan)	mem 4879MB
[2022-05-30 22:25:01 MetaFG_0] (main.py 265): INFO Train: [0/300][1390/1562]	eta 0:00:53 lr 0.000000	time 0.2982 (0.3084)	loss 2.3107 (2.3363)	grad_norm 20.0604 (nan)	mem 4879MB
[2022-05-30 22:25:04 MetaFG_0] (main.py 265): INFO Train: [0/300][1400/1562]	eta 0:00:49 lr 0.000000	time 0.2937 (0.3084)	loss 2.3264 (2.3362)	grad_norm 9.4528 (nan)	mem 4879MB
[2022-05-30 22:25:07 MetaFG_0] (main.py 265): INFO Train: [0/300][1410/1562]	eta 0:00:46 lr 0.000000	time 0.2958 (0.3083)	loss 2.3775 (2.3362)	grad_norm 11.2111 (nan)	mem 4879MB
[2022-05-30 22:25:10 MetaFG_0] (main.py 265): INFO Train: [0/300][1420/1562]	eta 0:00:43 lr 0.000000	time 0.2915 (0.3083)	loss 2.3249 (2.3363)	grad_norm 21.2566 (nan)	mem 4879MB
[2022-05-30 22:25:13 MetaFG_0] (main.py 265): INFO Train: [0/300][1430/1562]	eta 0:00:40 lr 0.000000	time 0.2922 (0.3082)	loss 2.3497 (2.3363)	grad_norm 19.6098 (nan)	mem 4879MB
[2022-05-30 22:25:16 MetaFG_0] (main.py 265): INFO Train: [0/300][1440/1562]	eta 0:00:37 lr 0.000000	time 0.2956 (0.3082)	loss 2.3004 (2.3362)	grad_norm 11.1523 (nan)	mem 4879MB
[2022-05-30 22:25:19 MetaFG_0] (main.py 265): INFO Train: [0/300][1450/1562]	eta 0:00:34 lr 0.000000	time 0.3003 (0.3082)	loss 2.3568 (2.3361)	grad_norm 10.9398 (nan)	mem 4879MB
[2022-05-30 22:25:23 MetaFG_0] (main.py 265): INFO Train: [0/300][1460/1562]	eta 0:00:31 lr 0.000000	time 0.2981 (0.3082)	loss 2.3585 (2.3360)	grad_norm 15.4532 (nan)	mem 4879MB
[2022-05-30 22:25:26 MetaFG_0] (main.py 265): INFO Train: [0/300][1470/1562]	eta 0:00:28 lr 0.000000	time 0.2921 (0.3081)	loss 2.3566 (2.3359)	grad_norm 22.7256 (nan)	mem 4879MB
[2022-05-30 22:25:29 MetaFG_0] (main.py 265): INFO Train: [0/300][1480/1562]	eta 0:00:25 lr 0.000000	time 0.2952 (0.3081)	loss 2.3668 (2.3360)	grad_norm 7.4951 (nan)	mem 4879MB
[2022-05-30 22:25:32 MetaFG_0] (main.py 265): INFO Train: [0/300][1490/1562]	eta 0:00:22 lr 0.000000	time 0.2988 (0.3081)	loss 2.3466 (2.3361)	grad_norm 12.5014 (nan)	mem 4879MB
[2022-05-30 22:25:35 MetaFG_0] (main.py 265): INFO Train: [0/300][1500/1562]	eta 0:00:19 lr 0.000000	time 0.2920 (0.3081)	loss 2.3174 (2.3360)	grad_norm 19.2294 (nan)	mem 4879MB
[2022-05-30 22:25:38 MetaFG_0] (main.py 265): INFO Train: [0/300][1510/1562]	eta 0:00:16 lr 0.000000	time 0.3231 (0.3081)	loss 2.3275 (2.3360)	grad_norm 14.0325 (nan)	mem 4879MB
[2022-05-30 22:25:41 MetaFG_0] (main.py 265): INFO Train: [0/300][1520/1562]	eta 0:00:12 lr 0.000000	time 0.2933 (0.3080)	loss 2.3364 (2.3360)	grad_norm 19.4707 (nan)	mem 4879MB
[2022-05-30 22:25:44 MetaFG_0] (main.py 265): INFO Train: [0/300][1530/1562]	eta 0:00:09 lr 0.000000	time 0.2989 (0.3080)	loss 2.3137 (2.3359)	grad_norm 19.3396 (nan)	mem 4879MB
[2022-05-30 22:25:47 MetaFG_0] (main.py 265): INFO Train: [0/300][1540/1562]	eta 0:00:06 lr 0.000000	time 0.2999 (0.3080)	loss 2.3169 (2.3359)	grad_norm 16.6553 (nan)	mem 4879MB
[2022-05-30 22:25:50 MetaFG_0] (main.py 265): INFO Train: [0/300][1550/1562]	eta 0:00:03 lr 0.000000	time 0.2920 (0.3080)	loss 2.3176 (2.3358)	grad_norm 17.9816 (nan)	mem 4879MB
[2022-05-30 22:25:53 MetaFG_0] (main.py 265): INFO Train: [0/300][1560/1562]	eta 0:00:00 lr 0.000000	time 0.2914 (0.3079)	loss 2.3427 (2.3358)	grad_norm 24.9898 (nan)	mem 4879MB
[2022-05-30 22:25:53 MetaFG_0] (main.py 272): INFO EPOCH 0 training takes 0:08:01
[2022-05-30 22:25:53 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_0.pth saving......
[2022-05-30 22:25:54 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_0.pth saved !!!
[2022-05-30 22:25:54 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:25:56 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:25:56 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:25:56 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.604 (0.604)	Loss 4.6622 (4.6622)	Acc@1 0.000 (0.000)	Acc@5 6.250 (6.250)	Mem 4879MB
[2022-05-30 22:25:58 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.149)	Loss 4.6233 (4.6550)	Acc@1 0.000 (0.284)	Acc@5 6.250 (4.830)	Mem 4879MB
[2022-05-30 22:25:58 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.098 (0.124)	Loss 4.6201 (4.6643)	Acc@1 0.000 (0.298)	Acc@5 0.000 (3.869)	Mem 4879MB
[2022-05-30 22:25:59 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.114)	Loss 4.6444 (4.6605)	Acc@1 0.000 (0.504)	Acc@5 6.250 (4.032)	Mem 4879MB
[2022-05-30 22:26:00 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.109)	Loss 4.6892 (4.6632)	Acc@1 0.000 (0.534)	Acc@5 0.000 (3.811)	Mem 4879MB
[2022-05-30 22:26:01 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.106)	Loss 4.6228 (4.6609)	Acc@1 0.000 (0.551)	Acc@5 0.000 (3.983)	Mem 4879MB
[2022-05-30 22:26:02 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.104)	Loss 4.6928 (4.6635)	Acc@1 0.000 (0.564)	Acc@5 0.000 (4.047)	Mem 4879MB
[2022-05-30 22:26:03 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.095 (0.103)	Loss 4.5955 (4.6610)	Acc@1 3.125 (0.704)	Acc@5 9.375 (4.269)	Mem 4879MB
[2022-05-30 22:26:04 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.095 (0.102)	Loss 4.6810 (4.6590)	Acc@1 0.000 (0.694)	Acc@5 6.250 (4.398)	Mem 4879MB
[2022-05-30 22:26:05 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.099 (0.101)	Loss 4.6785 (4.6574)	Acc@1 0.000 (0.755)	Acc@5 3.125 (4.602)	Mem 4879MB
[2022-05-30 22:26:06 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.106 (0.100)	Loss 4.7443 (4.6558)	Acc@1 0.000 (0.835)	Acc@5 0.000 (4.641)	Mem 4879MB
[2022-05-30 22:26:07 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.093 (0.099)	Loss 4.6845 (4.6557)	Acc@1 0.000 (0.873)	Acc@5 3.125 (4.786)	Mem 4879MB
[2022-05-30 22:26:08 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.088 (0.099)	Loss 4.7308 (4.6559)	Acc@1 0.000 (0.878)	Acc@5 0.000 (4.778)	Mem 4879MB
[2022-05-30 22:26:09 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.098)	Loss 4.7001 (4.6575)	Acc@1 0.000 (0.835)	Acc@5 3.125 (4.866)	Mem 4879MB
[2022-05-30 22:26:10 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.098)	Loss 4.6591 (4.6576)	Acc@1 0.000 (0.820)	Acc@5 9.375 (4.854)	Mem 4879MB
[2022-05-30 22:26:11 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 4.6407 (4.6580)	Acc@1 3.125 (0.869)	Acc@5 3.125 (4.760)	Mem 4879MB
[2022-05-30 22:26:12 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.095 (0.097)	Loss 4.5588 (4.6592)	Acc@1 0.000 (0.835)	Acc@5 9.375 (4.697)	Mem 4879MB
[2022-05-30 22:26:13 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.097)	Loss 4.7150 (4.6595)	Acc@1 0.000 (0.804)	Acc@5 0.000 (4.532)	Mem 4879MB
[2022-05-30 22:26:13 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.097)	Loss 4.6815 (4.6592)	Acc@1 3.125 (0.829)	Acc@5 3.125 (4.506)	Mem 4879MB
[2022-05-30 22:26:14 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 4.5986 (4.6589)	Acc@1 6.250 (0.867)	Acc@5 9.375 (4.516)	Mem 4879MB
[2022-05-30 22:26:15 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 4.6641 (4.6602)	Acc@1 0.000 (0.840)	Acc@5 6.250 (4.447)	Mem 4879MB
[2022-05-30 22:26:16 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.097)	Loss 4.7075 (4.6605)	Acc@1 0.000 (0.859)	Acc@5 3.125 (4.473)	Mem 4879MB
[2022-05-30 22:26:17 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.099 (0.096)	Loss 4.4983 (4.6597)	Acc@1 3.125 (0.877)	Acc@5 15.625 (4.511)	Mem 4879MB
[2022-05-30 22:26:18 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.096)	Loss 4.7346 (4.6600)	Acc@1 3.125 (0.906)	Acc@5 3.125 (4.532)	Mem 4879MB
[2022-05-30 22:26:19 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.096)	Loss 4.6699 (4.6601)	Acc@1 0.000 (0.895)	Acc@5 3.125 (4.499)	Mem 4879MB
[2022-05-30 22:26:20 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.091 (0.096)	Loss 4.6122 (4.6600)	Acc@1 3.125 (0.909)	Acc@5 3.125 (4.445)	Mem 4879MB
[2022-05-30 22:26:21 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.100 (0.096)	Loss 4.5544 (4.6587)	Acc@1 3.125 (0.898)	Acc@5 3.125 (4.442)	Mem 4879MB
[2022-05-30 22:26:22 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.095 (0.096)	Loss 4.6611 (4.6585)	Acc@1 0.000 (0.899)	Acc@5 3.125 (4.543)	Mem 4879MB
[2022-05-30 22:26:23 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 4.6880 (4.6582)	Acc@1 3.125 (0.890)	Acc@5 3.125 (4.515)	Mem 4879MB
[2022-05-30 22:26:24 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 4.6148 (4.6589)	Acc@1 0.000 (0.870)	Acc@5 0.000 (4.446)	Mem 4879MB
[2022-05-30 22:26:25 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.093 (0.096)	Loss 4.5937 (4.6586)	Acc@1 0.000 (0.872)	Acc@5 6.250 (4.475)	Mem 4879MB
[2022-05-30 22:26:26 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.095)	Loss 4.6819 (4.6582)	Acc@1 0.000 (0.884)	Acc@5 6.250 (4.502)	Mem 4879MB
[2022-05-30 22:26:26 MetaFG_0] (main.py 330): INFO  * Acc@1 0.900 Acc@5 4.510
[2022-05-30 22:26:26 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 0.9%
[2022-05-30 22:26:26 MetaFG_0] (main.py 171): INFO Max accuracy: 0.90%
[2022-05-30 22:26:27 MetaFG_0] (main.py 265): INFO Train: [1/300][0/1562]	eta 0:24:36 lr 0.000000	time 0.9455 (0.9455)	loss 2.3327 (2.3327)	grad_norm 15.5037 (15.5037)	mem 4879MB
[2022-05-30 22:26:30 MetaFG_0] (main.py 265): INFO Train: [1/300][10/1562]	eta 0:09:29 lr 0.000000	time 0.2930 (0.3669)	loss 2.3189 (2.3463)	grad_norm 18.5456 (15.9213)	mem 4879MB
[2022-05-30 22:26:33 MetaFG_0] (main.py 265): INFO Train: [1/300][20/1562]	eta 0:08:39 lr 0.000000	time 0.2980 (0.3367)	loss 2.3043 (2.3437)	grad_norm 27.1562 (16.4548)	mem 4879MB
[2022-05-30 22:26:36 MetaFG_0] (main.py 265): INFO Train: [1/300][30/1562]	eta 0:08:19 lr 0.000000	time 0.3012 (0.3261)	loss 2.3380 (2.3370)	grad_norm 13.0997 (16.0098)	mem 4879MB
[2022-05-30 22:26:39 MetaFG_0] (main.py 265): INFO Train: [1/300][40/1562]	eta 0:08:08 lr 0.000000	time 0.2991 (0.3206)	loss 2.3478 (2.3337)	grad_norm 11.8083 (15.5888)	mem 4879MB
[2022-05-30 22:26:42 MetaFG_0] (main.py 265): INFO Train: [1/300][50/1562]	eta 0:07:59 lr 0.000000	time 0.2935 (0.3169)	loss 2.3075 (2.3322)	grad_norm 13.3996 (15.4633)	mem 4879MB
[2022-05-30 22:26:45 MetaFG_0] (main.py 265): INFO Train: [1/300][60/1562]	eta 0:07:52 lr 0.000000	time 0.2975 (0.3146)	loss 2.3362 (2.3327)	grad_norm 11.5310 (15.5963)	mem 4879MB
[2022-05-30 22:26:48 MetaFG_0] (main.py 265): INFO Train: [1/300][70/1562]	eta 0:07:46 lr 0.000000	time 0.2923 (0.3130)	loss 2.3651 (2.3322)	grad_norm 19.2639 (15.5834)	mem 4879MB
[2022-05-30 22:26:51 MetaFG_0] (main.py 265): INFO Train: [1/300][80/1562]	eta 0:07:42 lr 0.000000	time 0.2936 (0.3118)	loss 2.3399 (2.3333)	grad_norm 18.8085 (15.6656)	mem 4879MB
[2022-05-30 22:26:54 MetaFG_0] (main.py 265): INFO Train: [1/300][90/1562]	eta 0:07:37 lr 0.000000	time 0.2917 (0.3108)	loss 2.3797 (2.3329)	grad_norm 23.0177 (16.3041)	mem 4879MB
[2022-05-30 22:26:57 MetaFG_0] (main.py 265): INFO Train: [1/300][100/1562]	eta 0:07:33 lr 0.000000	time 0.2932 (0.3099)	loss 2.3482 (2.3332)	grad_norm 8.8964 (16.1250)	mem 4879MB
[2022-05-30 22:27:00 MetaFG_0] (main.py 265): INFO Train: [1/300][110/1562]	eta 0:07:29 lr 0.000000	time 0.2935 (0.3093)	loss 2.3138 (2.3333)	grad_norm 17.0301 (16.3904)	mem 4879MB
[2022-05-30 22:27:03 MetaFG_0] (main.py 265): INFO Train: [1/300][120/1562]	eta 0:07:25 lr 0.000000	time 0.2983 (0.3090)	loss 2.3097 (2.3331)	grad_norm 15.4625 (16.3868)	mem 4879MB
[2022-05-30 22:27:06 MetaFG_0] (main.py 265): INFO Train: [1/300][130/1562]	eta 0:07:21 lr 0.000000	time 0.2915 (0.3085)	loss 2.3353 (2.3332)	grad_norm 14.0124 (16.4537)	mem 4879MB
[2022-05-30 22:27:09 MetaFG_0] (main.py 265): INFO Train: [1/300][140/1562]	eta 0:07:18 lr 0.000000	time 0.2942 (0.3080)	loss 2.3113 (2.3328)	grad_norm 14.3971 (16.2139)	mem 4879MB
[2022-05-30 22:27:13 MetaFG_0] (main.py 265): INFO Train: [1/300][150/1562]	eta 0:07:14 lr 0.000000	time 0.2961 (0.3078)	loss 2.3302 (2.3324)	grad_norm 26.0150 (16.3086)	mem 4879MB
[2022-05-30 22:27:16 MetaFG_0] (main.py 265): INFO Train: [1/300][160/1562]	eta 0:07:11 lr 0.000000	time 0.2994 (0.3076)	loss 2.3091 (2.3324)	grad_norm 9.9301 (16.3074)	mem 4879MB
[2022-05-30 22:27:19 MetaFG_0] (main.py 265): INFO Train: [1/300][170/1562]	eta 0:07:08 lr 0.000000	time 0.2927 (0.3075)	loss 2.3076 (2.3319)	grad_norm 11.4981 (16.1331)	mem 4879MB
[2022-05-30 22:27:22 MetaFG_0] (main.py 265): INFO Train: [1/300][180/1562]	eta 0:07:04 lr 0.000000	time 0.2988 (0.3072)	loss 2.2987 (2.3312)	grad_norm 16.2369 (16.1505)	mem 4879MB
[2022-05-30 22:27:25 MetaFG_0] (main.py 265): INFO Train: [1/300][190/1562]	eta 0:07:01 lr 0.000000	time 0.2925 (0.3070)	loss 2.3428 (2.3310)	grad_norm 15.2248 (16.1402)	mem 4879MB
[2022-05-30 22:27:28 MetaFG_0] (main.py 265): INFO Train: [1/300][200/1562]	eta 0:06:57 lr 0.000000	time 0.2923 (0.3068)	loss 2.2978 (2.3302)	grad_norm 15.3134 (16.1610)	mem 4879MB
[2022-05-30 22:27:31 MetaFG_0] (main.py 265): INFO Train: [1/300][210/1562]	eta 0:06:54 lr 0.000000	time 0.2918 (0.3066)	loss 2.3107 (2.3298)	grad_norm 16.3071 (16.1387)	mem 4879MB
[2022-05-30 22:27:34 MetaFG_0] (main.py 265): INFO Train: [1/300][220/1562]	eta 0:06:51 lr 0.000000	time 0.2916 (0.3064)	loss 2.2968 (2.3295)	grad_norm 23.4710 (16.1326)	mem 4879MB
[2022-05-30 22:27:37 MetaFG_0] (main.py 265): INFO Train: [1/300][230/1562]	eta 0:06:47 lr 0.000000	time 0.2921 (0.3063)	loss 2.2890 (2.3299)	grad_norm 12.6600 (16.1283)	mem 4879MB
[2022-05-30 22:27:40 MetaFG_0] (main.py 265): INFO Train: [1/300][240/1562]	eta 0:06:44 lr 0.000000	time 0.2930 (0.3061)	loss 2.3809 (2.3303)	grad_norm 16.7939 (16.0983)	mem 4879MB
[2022-05-30 22:27:43 MetaFG_0] (main.py 265): INFO Train: [1/300][250/1562]	eta 0:06:41 lr 0.000000	time 0.2988 (0.3061)	loss 2.3636 (2.3305)	grad_norm 13.2713 (16.0139)	mem 4879MB
[2022-05-30 22:27:46 MetaFG_0] (main.py 265): INFO Train: [1/300][260/1562]	eta 0:06:38 lr 0.000000	time 0.2922 (0.3060)	loss 2.3132 (2.3306)	grad_norm 12.5092 (16.1079)	mem 4879MB
[2022-05-30 22:27:49 MetaFG_0] (main.py 265): INFO Train: [1/300][270/1562]	eta 0:06:35 lr 0.000000	time 0.2923 (0.3060)	loss 2.3573 (2.3307)	grad_norm 11.6622 (15.9879)	mem 4879MB
[2022-05-30 22:27:52 MetaFG_0] (main.py 265): INFO Train: [1/300][280/1562]	eta 0:06:32 lr 0.000000	time 0.2923 (0.3059)	loss 2.3534 (2.3305)	grad_norm 10.0658 (16.0362)	mem 4879MB
[2022-05-30 22:27:55 MetaFG_0] (main.py 265): INFO Train: [1/300][290/1562]	eta 0:06:29 lr 0.000000	time 0.2926 (0.3059)	loss 2.3386 (2.3306)	grad_norm 12.8655 (16.0220)	mem 4879MB
[2022-05-30 22:27:58 MetaFG_0] (main.py 265): INFO Train: [1/300][300/1562]	eta 0:06:26 lr 0.000000	time 0.2971 (0.3062)	loss 2.3256 (2.3304)	grad_norm 15.7250 (15.9900)	mem 4879MB
[2022-05-30 22:28:01 MetaFG_0] (main.py 265): INFO Train: [1/300][310/1562]	eta 0:06:23 lr 0.000000	time 0.2924 (0.3061)	loss 2.3080 (2.3303)	grad_norm 21.0370 (16.0586)	mem 4879MB
[2022-05-30 22:28:04 MetaFG_0] (main.py 265): INFO Train: [1/300][320/1562]	eta 0:06:20 lr 0.000000	time 0.2986 (0.3060)	loss 2.3513 (2.3303)	grad_norm 18.6113 (16.1479)	mem 4879MB
[2022-05-30 22:28:07 MetaFG_0] (main.py 265): INFO Train: [1/300][330/1562]	eta 0:06:16 lr 0.000000	time 0.2932 (0.3059)	loss 2.3186 (2.3301)	grad_norm 14.8276 (16.1756)	mem 4879MB
[2022-05-30 22:28:10 MetaFG_0] (main.py 265): INFO Train: [1/300][340/1562]	eta 0:06:13 lr 0.000000	time 0.2977 (0.3059)	loss 2.3058 (2.3300)	grad_norm 9.8354 (16.1425)	mem 4879MB
[2022-05-30 22:28:13 MetaFG_0] (main.py 265): INFO Train: [1/300][350/1562]	eta 0:06:10 lr 0.000000	time 0.2977 (0.3058)	loss 2.2977 (2.3298)	grad_norm 14.7339 (16.1446)	mem 4879MB
[2022-05-30 22:28:16 MetaFG_0] (main.py 265): INFO Train: [1/300][360/1562]	eta 0:06:07 lr 0.000000	time 0.2915 (0.3058)	loss 2.3314 (2.3296)	grad_norm 12.9390 (16.1546)	mem 4879MB
[2022-05-30 22:28:19 MetaFG_0] (main.py 265): INFO Train: [1/300][370/1562]	eta 0:06:04 lr 0.000000	time 0.3002 (0.3057)	loss 2.3228 (2.3298)	grad_norm 12.0122 (16.1512)	mem 4879MB
[2022-05-30 22:28:22 MetaFG_0] (main.py 265): INFO Train: [1/300][380/1562]	eta 0:06:01 lr 0.000000	time 0.2970 (0.3056)	loss 2.2861 (2.3294)	grad_norm 11.4551 (16.1920)	mem 4879MB
[2022-05-30 22:28:26 MetaFG_0] (main.py 265): INFO Train: [1/300][390/1562]	eta 0:05:58 lr 0.000000	time 0.2934 (0.3056)	loss 2.3450 (2.3294)	grad_norm 16.9879 (16.1860)	mem 4879MB
[2022-05-30 22:28:29 MetaFG_0] (main.py 265): INFO Train: [1/300][400/1562]	eta 0:05:55 lr 0.000000	time 0.2922 (0.3055)	loss 2.3318 (2.3296)	grad_norm 16.3647 (16.1489)	mem 4879MB
[2022-05-30 22:28:32 MetaFG_0] (main.py 265): INFO Train: [1/300][410/1562]	eta 0:05:51 lr 0.000000	time 0.2923 (0.3055)	loss 2.3247 (2.3294)	grad_norm 20.6513 (16.1732)	mem 4879MB
[2022-05-30 22:28:35 MetaFG_0] (main.py 265): INFO Train: [1/300][420/1562]	eta 0:05:48 lr 0.000000	time 0.2935 (0.3055)	loss 2.3184 (2.3292)	grad_norm 10.1659 (16.1959)	mem 4879MB
[2022-05-30 22:28:38 MetaFG_0] (main.py 265): INFO Train: [1/300][430/1562]	eta 0:05:45 lr 0.000000	time 0.3003 (0.3054)	loss 2.2979 (2.3291)	grad_norm 35.6637 (16.1924)	mem 4879MB
[2022-05-30 22:28:41 MetaFG_0] (main.py 265): INFO Train: [1/300][440/1562]	eta 0:05:42 lr 0.000000	time 0.3047 (0.3054)	loss 2.3164 (2.3288)	grad_norm 12.0007 (16.2057)	mem 4879MB
[2022-05-30 22:28:44 MetaFG_0] (main.py 265): INFO Train: [1/300][450/1562]	eta 0:05:39 lr 0.000000	time 0.2917 (0.3053)	loss 2.3047 (2.3290)	grad_norm 23.4707 (16.1701)	mem 4879MB
[2022-05-30 22:28:47 MetaFG_0] (main.py 265): INFO Train: [1/300][460/1562]	eta 0:05:36 lr 0.000000	time 0.2977 (0.3053)	loss 2.3639 (2.3290)	grad_norm 14.1993 (16.1212)	mem 4879MB
[2022-05-30 22:28:50 MetaFG_0] (main.py 265): INFO Train: [1/300][470/1562]	eta 0:05:33 lr 0.000000	time 0.2982 (0.3052)	loss 2.3556 (2.3291)	grad_norm 21.2734 (16.0782)	mem 4879MB
[2022-05-30 22:28:53 MetaFG_0] (main.py 265): INFO Train: [1/300][480/1562]	eta 0:05:30 lr 0.000000	time 0.2984 (0.3052)	loss 2.3417 (2.3290)	grad_norm 14.7809 (16.0877)	mem 4879MB
[2022-05-30 22:28:56 MetaFG_0] (main.py 265): INFO Train: [1/300][490/1562]	eta 0:05:27 lr 0.000000	time 0.2931 (0.3052)	loss 2.3128 (2.3292)	grad_norm 9.1485 (16.1008)	mem 4879MB
[2022-05-30 22:28:59 MetaFG_0] (main.py 265): INFO Train: [1/300][500/1562]	eta 0:05:24 lr 0.000000	time 0.2975 (0.3052)	loss 2.3240 (2.3293)	grad_norm 13.2317 (16.1165)	mem 4879MB
[2022-05-30 22:29:02 MetaFG_0] (main.py 265): INFO Train: [1/300][510/1562]	eta 0:05:20 lr 0.000000	time 0.2919 (0.3051)	loss 2.3230 (2.3293)	grad_norm 16.2801 (16.0855)	mem 4879MB
[2022-05-30 22:29:05 MetaFG_0] (main.py 265): INFO Train: [1/300][520/1562]	eta 0:05:17 lr 0.000000	time 0.2920 (0.3051)	loss 2.3480 (2.3293)	grad_norm 14.4576 (16.0856)	mem 4879MB
[2022-05-30 22:29:08 MetaFG_0] (main.py 265): INFO Train: [1/300][530/1562]	eta 0:05:14 lr 0.000000	time 0.2928 (0.3050)	loss 2.3237 (2.3293)	grad_norm 18.3573 (16.0720)	mem 4879MB
[2022-05-30 22:29:11 MetaFG_0] (main.py 265): INFO Train: [1/300][540/1562]	eta 0:05:11 lr 0.000000	time 0.2935 (0.3050)	loss 2.3276 (2.3294)	grad_norm 14.0644 (16.0884)	mem 4879MB
[2022-05-30 22:29:14 MetaFG_0] (main.py 265): INFO Train: [1/300][550/1562]	eta 0:05:08 lr 0.000000	time 0.2995 (0.3050)	loss 2.3100 (2.3293)	grad_norm 17.9949 (16.0917)	mem 4879MB
[2022-05-30 22:29:17 MetaFG_0] (main.py 265): INFO Train: [1/300][560/1562]	eta 0:05:05 lr 0.000000	time 0.2983 (0.3050)	loss 2.3036 (2.3292)	grad_norm 17.9610 (16.1128)	mem 4879MB
[2022-05-30 22:29:20 MetaFG_0] (main.py 265): INFO Train: [1/300][570/1562]	eta 0:05:02 lr 0.000000	time 0.2927 (0.3050)	loss 2.3325 (2.3291)	grad_norm 16.3045 (16.0838)	mem 4879MB
[2022-05-30 22:29:23 MetaFG_0] (main.py 265): INFO Train: [1/300][580/1562]	eta 0:04:59 lr 0.000000	time 0.2933 (0.3050)	loss 2.3088 (2.3289)	grad_norm 17.0777 (16.0638)	mem 4879MB
[2022-05-30 22:29:26 MetaFG_0] (main.py 265): INFO Train: [1/300][590/1562]	eta 0:04:56 lr 0.000000	time 0.2934 (0.3050)	loss 2.3272 (2.3289)	grad_norm 13.1754 (16.0466)	mem 4879MB
[2022-05-30 22:29:29 MetaFG_0] (main.py 265): INFO Train: [1/300][600/1562]	eta 0:04:53 lr 0.000000	time 0.2937 (0.3049)	loss 2.2945 (2.3289)	grad_norm 11.8095 (15.9901)	mem 4879MB
[2022-05-30 22:29:32 MetaFG_0] (main.py 265): INFO Train: [1/300][610/1562]	eta 0:04:50 lr 0.000000	time 0.2924 (0.3049)	loss 2.3620 (2.3287)	grad_norm 19.6303 (15.9592)	mem 4879MB
[2022-05-30 22:29:35 MetaFG_0] (main.py 265): INFO Train: [1/300][620/1562]	eta 0:04:47 lr 0.000000	time 0.2951 (0.3049)	loss 2.3499 (2.3288)	grad_norm 16.5372 (15.9274)	mem 4879MB
[2022-05-30 22:29:38 MetaFG_0] (main.py 265): INFO Train: [1/300][630/1562]	eta 0:04:44 lr 0.000000	time 0.2920 (0.3048)	loss 2.3436 (2.3287)	grad_norm 25.4527 (15.8945)	mem 4879MB
[2022-05-30 22:29:41 MetaFG_0] (main.py 265): INFO Train: [1/300][640/1562]	eta 0:04:41 lr 0.000000	time 0.2946 (0.3048)	loss 2.3340 (2.3287)	grad_norm 10.3264 (15.8404)	mem 4879MB
[2022-05-30 22:29:44 MetaFG_0] (main.py 265): INFO Train: [1/300][650/1562]	eta 0:04:37 lr 0.000000	time 0.2922 (0.3047)	loss 2.3337 (2.3288)	grad_norm 14.5553 (15.8448)	mem 4879MB
[2022-05-30 22:29:47 MetaFG_0] (main.py 265): INFO Train: [1/300][660/1562]	eta 0:04:34 lr 0.000000	time 0.2934 (0.3047)	loss 2.3064 (2.3286)	grad_norm 9.1031 (15.8598)	mem 4879MB
[2022-05-30 22:29:51 MetaFG_0] (main.py 265): INFO Train: [1/300][670/1562]	eta 0:04:31 lr 0.000000	time 0.2951 (0.3047)	loss 2.3067 (2.3285)	grad_norm 13.8361 (15.9341)	mem 4879MB
[2022-05-30 22:29:54 MetaFG_0] (main.py 265): INFO Train: [1/300][680/1562]	eta 0:04:28 lr 0.000000	time 0.2931 (0.3047)	loss 2.2987 (2.3283)	grad_norm 15.7404 (15.9043)	mem 4879MB
[2022-05-30 22:29:57 MetaFG_0] (main.py 265): INFO Train: [1/300][690/1562]	eta 0:04:25 lr 0.000000	time 0.2986 (0.3047)	loss 2.3350 (2.3283)	grad_norm 11.6016 (15.9178)	mem 4879MB
[2022-05-30 22:30:00 MetaFG_0] (main.py 265): INFO Train: [1/300][700/1562]	eta 0:04:22 lr 0.000000	time 0.2986 (0.3047)	loss 2.3049 (2.3282)	grad_norm 14.8019 (15.8967)	mem 4879MB
[2022-05-30 22:30:03 MetaFG_0] (main.py 265): INFO Train: [1/300][710/1562]	eta 0:04:19 lr 0.000000	time 0.2946 (0.3047)	loss 2.3357 (2.3281)	grad_norm 14.6758 (15.8740)	mem 4879MB
[2022-05-30 22:30:06 MetaFG_0] (main.py 265): INFO Train: [1/300][720/1562]	eta 0:04:16 lr 0.000000	time 0.2926 (0.3047)	loss 2.3053 (2.3280)	grad_norm 11.9631 (15.8276)	mem 4879MB
[2022-05-30 22:30:09 MetaFG_0] (main.py 265): INFO Train: [1/300][730/1562]	eta 0:04:13 lr 0.000000	time 0.2977 (0.3046)	loss 2.3235 (2.3278)	grad_norm 20.6580 (15.8350)	mem 4879MB
[2022-05-30 22:30:12 MetaFG_0] (main.py 265): INFO Train: [1/300][740/1562]	eta 0:04:10 lr 0.000000	time 0.2924 (0.3046)	loss 2.3025 (2.3277)	grad_norm 11.7199 (15.8638)	mem 4879MB
[2022-05-30 22:30:15 MetaFG_0] (main.py 265): INFO Train: [1/300][750/1562]	eta 0:04:07 lr 0.000000	time 0.2976 (0.3046)	loss 2.3435 (2.3279)	grad_norm 13.0140 (15.8225)	mem 4879MB
[2022-05-30 22:30:18 MetaFG_0] (main.py 265): INFO Train: [1/300][760/1562]	eta 0:04:04 lr 0.000000	time 0.2938 (0.3046)	loss 2.3209 (2.3279)	grad_norm 11.0333 (15.8065)	mem 4879MB
[2022-05-30 22:30:21 MetaFG_0] (main.py 265): INFO Train: [1/300][770/1562]	eta 0:04:01 lr 0.000000	time 0.2934 (0.3047)	loss 2.3055 (2.3277)	grad_norm 10.2947 (15.7886)	mem 4879MB
[2022-05-30 22:30:24 MetaFG_0] (main.py 265): INFO Train: [1/300][780/1562]	eta 0:03:58 lr 0.000000	time 0.2919 (0.3047)	loss 2.3174 (2.3276)	grad_norm 11.3352 (15.7542)	mem 4879MB
[2022-05-30 22:30:27 MetaFG_0] (main.py 265): INFO Train: [1/300][790/1562]	eta 0:03:55 lr 0.000000	time 0.2930 (0.3047)	loss 2.3009 (2.3274)	grad_norm 8.2081 (15.6941)	mem 4879MB
[2022-05-30 22:30:30 MetaFG_0] (main.py 265): INFO Train: [1/300][800/1562]	eta 0:03:52 lr 0.000000	time 0.2952 (0.3047)	loss 2.3176 (2.3273)	grad_norm 17.8832 (15.6859)	mem 4879MB
[2022-05-30 22:30:33 MetaFG_0] (main.py 265): INFO Train: [1/300][810/1562]	eta 0:03:49 lr 0.000000	time 0.2979 (0.3047)	loss 2.3514 (2.3274)	grad_norm 14.2066 (15.6752)	mem 4879MB
[2022-05-30 22:30:36 MetaFG_0] (main.py 265): INFO Train: [1/300][820/1562]	eta 0:03:46 lr 0.000000	time 0.2937 (0.3047)	loss 2.3062 (2.3274)	grad_norm 14.2950 (15.6629)	mem 4879MB
[2022-05-30 22:30:39 MetaFG_0] (main.py 265): INFO Train: [1/300][830/1562]	eta 0:03:43 lr 0.000000	time 0.3013 (0.3047)	loss 2.3226 (2.3274)	grad_norm 7.9575 (15.6463)	mem 4879MB
[2022-05-30 22:30:42 MetaFG_0] (main.py 265): INFO Train: [1/300][840/1562]	eta 0:03:39 lr 0.000000	time 0.2966 (0.3046)	loss 2.3251 (2.3273)	grad_norm 12.8744 (15.6246)	mem 4879MB
[2022-05-30 22:30:45 MetaFG_0] (main.py 265): INFO Train: [1/300][850/1562]	eta 0:03:36 lr 0.000000	time 0.3006 (0.3046)	loss 2.3502 (2.3274)	grad_norm 17.8233 (15.5966)	mem 4879MB
[2022-05-30 22:30:48 MetaFG_0] (main.py 265): INFO Train: [1/300][860/1562]	eta 0:03:33 lr 0.000000	time 0.2941 (0.3046)	loss 2.3112 (2.3272)	grad_norm 14.8014 (15.5882)	mem 4879MB
[2022-05-30 22:30:51 MetaFG_0] (main.py 265): INFO Train: [1/300][870/1562]	eta 0:03:30 lr 0.000000	time 0.2936 (0.3046)	loss 2.3020 (2.3271)	grad_norm 8.1405 (15.5854)	mem 4879MB
[2022-05-30 22:30:54 MetaFG_0] (main.py 265): INFO Train: [1/300][880/1562]	eta 0:03:27 lr 0.000000	time 0.2939 (0.3046)	loss 2.3236 (2.3271)	grad_norm 20.6962 (15.5782)	mem 4879MB
[2022-05-30 22:30:57 MetaFG_0] (main.py 265): INFO Train: [1/300][890/1562]	eta 0:03:24 lr 0.000000	time 0.2937 (0.3046)	loss 2.3269 (2.3271)	grad_norm 12.0720 (15.5537)	mem 4879MB
[2022-05-30 22:31:00 MetaFG_0] (main.py 265): INFO Train: [1/300][900/1562]	eta 0:03:21 lr 0.000000	time 0.2925 (0.3046)	loss 2.3096 (2.3270)	grad_norm 13.6388 (15.5401)	mem 4879MB
[2022-05-30 22:31:04 MetaFG_0] (main.py 265): INFO Train: [1/300][910/1562]	eta 0:03:18 lr 0.000001	time 0.2972 (0.3046)	loss 2.3390 (2.3271)	grad_norm 11.6318 (15.5170)	mem 4879MB
[2022-05-30 22:31:07 MetaFG_0] (main.py 265): INFO Train: [1/300][920/1562]	eta 0:03:15 lr 0.000001	time 0.2940 (0.3046)	loss 2.3152 (2.3270)	grad_norm 10.8954 (15.4835)	mem 4879MB
[2022-05-30 22:31:10 MetaFG_0] (main.py 265): INFO Train: [1/300][930/1562]	eta 0:03:12 lr 0.000001	time 0.2934 (0.3045)	loss 2.3240 (2.3268)	grad_norm 23.9979 (15.4740)	mem 4879MB
[2022-05-30 22:31:13 MetaFG_0] (main.py 265): INFO Train: [1/300][940/1562]	eta 0:03:09 lr 0.000001	time 0.2977 (0.3045)	loss 2.3302 (2.3268)	grad_norm 13.2414 (15.4441)	mem 4879MB
[2022-05-30 22:31:16 MetaFG_0] (main.py 265): INFO Train: [1/300][950/1562]	eta 0:03:06 lr 0.000001	time 0.2928 (0.3045)	loss 2.3075 (2.3267)	grad_norm 15.6170 (15.4359)	mem 4879MB
[2022-05-30 22:31:19 MetaFG_0] (main.py 265): INFO Train: [1/300][960/1562]	eta 0:03:03 lr 0.000001	time 0.2920 (0.3045)	loss 2.3557 (2.3267)	grad_norm 22.4120 (15.4259)	mem 4879MB
[2022-05-30 22:31:22 MetaFG_0] (main.py 265): INFO Train: [1/300][970/1562]	eta 0:03:00 lr 0.000001	time 0.2922 (0.3045)	loss 2.2948 (2.3265)	grad_norm 15.0956 (15.4181)	mem 4879MB
[2022-05-30 22:31:25 MetaFG_0] (main.py 265): INFO Train: [1/300][980/1562]	eta 0:02:57 lr 0.000001	time 0.2917 (0.3045)	loss 2.3645 (2.3265)	grad_norm 15.6679 (15.4074)	mem 4879MB
[2022-05-30 22:31:28 MetaFG_0] (main.py 265): INFO Train: [1/300][990/1562]	eta 0:02:54 lr 0.000001	time 0.2915 (0.3045)	loss 2.3157 (2.3265)	grad_norm 11.9311 (15.3792)	mem 4879MB
[2022-05-30 22:31:31 MetaFG_0] (main.py 265): INFO Train: [1/300][1000/1562]	eta 0:02:51 lr 0.000001	time 0.2943 (0.3045)	loss 2.3270 (2.3264)	grad_norm 12.1995 (15.3641)	mem 4879MB
[2022-05-30 22:31:34 MetaFG_0] (main.py 265): INFO Train: [1/300][1010/1562]	eta 0:02:48 lr 0.000001	time 0.2997 (0.3045)	loss 2.3551 (2.3264)	grad_norm 15.5347 (15.3548)	mem 4879MB
[2022-05-30 22:31:37 MetaFG_0] (main.py 265): INFO Train: [1/300][1020/1562]	eta 0:02:45 lr 0.000001	time 0.2931 (0.3044)	loss 2.3173 (2.3263)	grad_norm 9.9669 (15.3360)	mem 4879MB
[2022-05-30 22:31:40 MetaFG_0] (main.py 265): INFO Train: [1/300][1030/1562]	eta 0:02:41 lr 0.000001	time 0.2929 (0.3044)	loss 2.3532 (2.3263)	grad_norm 10.6612 (15.3250)	mem 4879MB
[2022-05-30 22:31:43 MetaFG_0] (main.py 265): INFO Train: [1/300][1040/1562]	eta 0:02:38 lr 0.000001	time 0.2923 (0.3044)	loss 2.3023 (2.3263)	grad_norm 17.8554 (15.3213)	mem 4879MB
[2022-05-30 22:31:46 MetaFG_0] (main.py 265): INFO Train: [1/300][1050/1562]	eta 0:02:35 lr 0.000001	time 0.2951 (0.3044)	loss 2.3091 (2.3262)	grad_norm 14.1895 (15.3201)	mem 4879MB
[2022-05-30 22:31:49 MetaFG_0] (main.py 265): INFO Train: [1/300][1060/1562]	eta 0:02:32 lr 0.000001	time 0.2922 (0.3044)	loss 2.3139 (2.3261)	grad_norm 9.1440 (15.3048)	mem 4879MB
[2022-05-30 22:31:52 MetaFG_0] (main.py 265): INFO Train: [1/300][1070/1562]	eta 0:02:29 lr 0.000001	time 0.2929 (0.3044)	loss 2.3303 (2.3258)	grad_norm 16.2051 (15.2782)	mem 4879MB
[2022-05-30 22:31:55 MetaFG_0] (main.py 265): INFO Train: [1/300][1080/1562]	eta 0:02:26 lr 0.000001	time 0.2919 (0.3044)	loss 2.3334 (2.3258)	grad_norm 14.9745 (15.2452)	mem 4879MB
[2022-05-30 22:31:58 MetaFG_0] (main.py 265): INFO Train: [1/300][1090/1562]	eta 0:02:23 lr 0.000001	time 0.2987 (0.3044)	loss 2.3421 (2.3257)	grad_norm 21.3753 (15.2716)	mem 4879MB
[2022-05-30 22:32:01 MetaFG_0] (main.py 265): INFO Train: [1/300][1100/1562]	eta 0:02:20 lr 0.000001	time 0.2951 (0.3044)	loss 2.3243 (2.3256)	grad_norm 18.1147 (15.2385)	mem 4879MB
[2022-05-30 22:32:04 MetaFG_0] (main.py 265): INFO Train: [1/300][1110/1562]	eta 0:02:17 lr 0.000001	time 0.2973 (0.3044)	loss 2.3069 (2.3254)	grad_norm 10.6290 (15.2319)	mem 4879MB
[2022-05-30 22:32:07 MetaFG_0] (main.py 265): INFO Train: [1/300][1120/1562]	eta 0:02:14 lr 0.000001	time 0.2917 (0.3044)	loss 2.3428 (2.3254)	grad_norm 12.7579 (15.2190)	mem 4879MB
[2022-05-30 22:32:10 MetaFG_0] (main.py 265): INFO Train: [1/300][1130/1562]	eta 0:02:11 lr 0.000001	time 0.2978 (0.3044)	loss 2.3100 (2.3254)	grad_norm 17.3325 (15.1971)	mem 4879MB
[2022-05-30 22:32:13 MetaFG_0] (main.py 265): INFO Train: [1/300][1140/1562]	eta 0:02:08 lr 0.000001	time 0.2970 (0.3043)	loss 2.3179 (2.3253)	grad_norm 9.6990 (15.1740)	mem 4879MB
[2022-05-30 22:32:16 MetaFG_0] (main.py 265): INFO Train: [1/300][1150/1562]	eta 0:02:05 lr 0.000001	time 0.2928 (0.3043)	loss 2.2733 (2.3252)	grad_norm 10.1146 (15.1808)	mem 4879MB
[2022-05-30 22:32:19 MetaFG_0] (main.py 265): INFO Train: [1/300][1160/1562]	eta 0:02:02 lr 0.000001	time 0.2928 (0.3043)	loss 2.3630 (2.3252)	grad_norm 11.3436 (15.1631)	mem 4879MB
[2022-05-30 22:32:22 MetaFG_0] (main.py 265): INFO Train: [1/300][1170/1562]	eta 0:01:59 lr 0.000001	time 0.2916 (0.3043)	loss 2.3145 (2.3252)	grad_norm 7.7439 (15.1690)	mem 4879MB
[2022-05-30 22:32:25 MetaFG_0] (main.py 265): INFO Train: [1/300][1180/1562]	eta 0:01:56 lr 0.000001	time 0.2982 (0.3043)	loss 2.3474 (2.3252)	grad_norm 13.7149 (15.1522)	mem 4879MB
[2022-05-30 22:32:28 MetaFG_0] (main.py 265): INFO Train: [1/300][1190/1562]	eta 0:01:53 lr 0.000001	time 0.2972 (0.3043)	loss 2.3111 (2.3252)	grad_norm 11.5989 (15.1456)	mem 4879MB
[2022-05-30 22:32:31 MetaFG_0] (main.py 265): INFO Train: [1/300][1200/1562]	eta 0:01:50 lr 0.000001	time 0.2931 (0.3043)	loss 2.3171 (2.3252)	grad_norm 13.0553 (15.1363)	mem 4879MB
[2022-05-30 22:32:35 MetaFG_0] (main.py 265): INFO Train: [1/300][1210/1562]	eta 0:01:47 lr 0.000001	time 0.2961 (0.3043)	loss 2.2953 (2.3252)	grad_norm 9.3332 (15.1162)	mem 4879MB
[2022-05-30 22:32:38 MetaFG_0] (main.py 265): INFO Train: [1/300][1220/1562]	eta 0:01:44 lr 0.000001	time 0.2985 (0.3043)	loss 2.3013 (2.3252)	grad_norm 9.8784 (15.1151)	mem 4879MB
[2022-05-30 22:32:41 MetaFG_0] (main.py 265): INFO Train: [1/300][1230/1562]	eta 0:01:41 lr 0.000001	time 0.3117 (0.3043)	loss 2.3123 (2.3251)	grad_norm 15.4901 (15.1079)	mem 4879MB
[2022-05-30 22:32:44 MetaFG_0] (main.py 265): INFO Train: [1/300][1240/1562]	eta 0:01:37 lr 0.000001	time 0.2988 (0.3043)	loss 2.3027 (2.3250)	grad_norm 10.7060 (15.0985)	mem 4879MB
[2022-05-30 22:32:47 MetaFG_0] (main.py 265): INFO Train: [1/300][1250/1562]	eta 0:01:34 lr 0.000001	time 0.2990 (0.3043)	loss 2.2938 (2.3250)	grad_norm 12.0860 (15.1123)	mem 4879MB
[2022-05-30 22:32:50 MetaFG_0] (main.py 265): INFO Train: [1/300][1260/1562]	eta 0:01:31 lr 0.000001	time 0.2981 (0.3043)	loss 2.3598 (2.3249)	grad_norm 14.3724 (15.0990)	mem 4879MB
[2022-05-30 22:32:53 MetaFG_0] (main.py 265): INFO Train: [1/300][1270/1562]	eta 0:01:28 lr 0.000001	time 0.2929 (0.3043)	loss 2.3377 (2.3249)	grad_norm 11.8170 (15.0887)	mem 4879MB
[2022-05-30 22:32:56 MetaFG_0] (main.py 265): INFO Train: [1/300][1280/1562]	eta 0:01:25 lr 0.000001	time 0.3052 (0.3043)	loss 2.3523 (2.3248)	grad_norm 18.2926 (15.0688)	mem 4879MB
[2022-05-30 22:32:59 MetaFG_0] (main.py 265): INFO Train: [1/300][1290/1562]	eta 0:01:22 lr 0.000001	time 0.2932 (0.3042)	loss 2.3266 (2.3248)	grad_norm 13.3736 (15.0661)	mem 4879MB
[2022-05-30 22:33:02 MetaFG_0] (main.py 265): INFO Train: [1/300][1300/1562]	eta 0:01:19 lr 0.000001	time 0.2917 (0.3042)	loss 2.3251 (2.3247)	grad_norm 26.2314 (15.0819)	mem 4879MB
[2022-05-30 22:33:05 MetaFG_0] (main.py 265): INFO Train: [1/300][1310/1562]	eta 0:01:16 lr 0.000001	time 0.2929 (0.3042)	loss 2.3044 (2.3246)	grad_norm 12.1461 (15.0766)	mem 4879MB
[2022-05-30 22:33:08 MetaFG_0] (main.py 265): INFO Train: [1/300][1320/1562]	eta 0:01:13 lr 0.000001	time 0.2917 (0.3042)	loss 2.3029 (2.3246)	grad_norm 16.8920 (15.0723)	mem 4879MB
[2022-05-30 22:33:11 MetaFG_0] (main.py 265): INFO Train: [1/300][1330/1562]	eta 0:01:10 lr 0.000001	time 0.2920 (0.3042)	loss 2.3017 (2.3245)	grad_norm 9.5868 (15.0678)	mem 4879MB
[2022-05-30 22:33:14 MetaFG_0] (main.py 265): INFO Train: [1/300][1340/1562]	eta 0:01:07 lr 0.000001	time 0.2974 (0.3042)	loss 2.3292 (2.3244)	grad_norm 12.3687 (15.0534)	mem 4879MB
[2022-05-30 22:33:17 MetaFG_0] (main.py 265): INFO Train: [1/300][1350/1562]	eta 0:01:04 lr 0.000001	time 0.2945 (0.3042)	loss 2.3300 (2.3243)	grad_norm 17.1572 (15.0485)	mem 4879MB
[2022-05-30 22:33:20 MetaFG_0] (main.py 265): INFO Train: [1/300][1360/1562]	eta 0:01:01 lr 0.000001	time 0.2922 (0.3042)	loss 2.3317 (2.3243)	grad_norm 9.3811 (15.0491)	mem 4879MB
[2022-05-30 22:33:23 MetaFG_0] (main.py 265): INFO Train: [1/300][1370/1562]	eta 0:00:58 lr 0.000001	time 0.2925 (0.3042)	loss 2.3076 (2.3243)	grad_norm 9.5545 (15.0343)	mem 4879MB
[2022-05-30 22:33:26 MetaFG_0] (main.py 265): INFO Train: [1/300][1380/1562]	eta 0:00:55 lr 0.000001	time 0.2917 (0.3042)	loss 2.3066 (2.3243)	grad_norm 16.6623 (15.0389)	mem 4879MB
[2022-05-30 22:33:29 MetaFG_0] (main.py 265): INFO Train: [1/300][1390/1562]	eta 0:00:52 lr 0.000001	time 0.3002 (0.3041)	loss 2.2706 (2.3241)	grad_norm 17.1845 (15.0394)	mem 4879MB
[2022-05-30 22:33:32 MetaFG_0] (main.py 265): INFO Train: [1/300][1400/1562]	eta 0:00:49 lr 0.000001	time 0.2934 (0.3041)	loss 2.3038 (2.3240)	grad_norm 9.8694 (15.0304)	mem 4879MB
[2022-05-30 22:33:35 MetaFG_0] (main.py 265): INFO Train: [1/300][1410/1562]	eta 0:00:46 lr 0.000001	time 0.2943 (0.3041)	loss 2.2994 (2.3239)	grad_norm 10.1100 (15.0090)	mem 4879MB
[2022-05-30 22:33:38 MetaFG_0] (main.py 265): INFO Train: [1/300][1420/1562]	eta 0:00:43 lr 0.000001	time 0.2932 (0.3041)	loss 2.3112 (2.3238)	grad_norm 14.5490 (15.0036)	mem 4879MB
[2022-05-30 22:33:41 MetaFG_0] (main.py 265): INFO Train: [1/300][1430/1562]	eta 0:00:40 lr 0.000001	time 0.2948 (0.3041)	loss 2.3257 (2.3238)	grad_norm 7.9106 (14.9809)	mem 4879MB
[2022-05-30 22:33:44 MetaFG_0] (main.py 265): INFO Train: [1/300][1440/1562]	eta 0:00:37 lr 0.000001	time 0.2935 (0.3041)	loss 2.3513 (2.3238)	grad_norm 15.0004 (14.9831)	mem 4879MB
[2022-05-30 22:33:47 MetaFG_0] (main.py 265): INFO Train: [1/300][1450/1562]	eta 0:00:34 lr 0.000001	time 0.2984 (0.3041)	loss 2.2960 (2.3238)	grad_norm 6.5225 (14.9744)	mem 4879MB
[2022-05-30 22:33:50 MetaFG_0] (main.py 265): INFO Train: [1/300][1460/1562]	eta 0:00:31 lr 0.000001	time 0.2933 (0.3041)	loss 2.3365 (2.3237)	grad_norm 11.3948 (14.9737)	mem 4879MB
[2022-05-30 22:33:53 MetaFG_0] (main.py 265): INFO Train: [1/300][1470/1562]	eta 0:00:27 lr 0.000001	time 0.2930 (0.3041)	loss 2.2866 (2.3236)	grad_norm 9.4440 (14.9498)	mem 4879MB
[2022-05-30 22:33:56 MetaFG_0] (main.py 265): INFO Train: [1/300][1480/1562]	eta 0:00:24 lr 0.000001	time 0.2916 (0.3041)	loss 2.3135 (2.3235)	grad_norm 16.0114 (14.9438)	mem 4879MB
[2022-05-30 22:33:59 MetaFG_0] (main.py 265): INFO Train: [1/300][1490/1562]	eta 0:00:21 lr 0.000001	time 0.2920 (0.3041)	loss 2.3167 (2.3234)	grad_norm 9.9820 (14.9180)	mem 4879MB
[2022-05-30 22:34:02 MetaFG_0] (main.py 265): INFO Train: [1/300][1500/1562]	eta 0:00:18 lr 0.000001	time 0.2922 (0.3041)	loss 2.3081 (2.3232)	grad_norm 12.6779 (14.9184)	mem 4879MB
[2022-05-30 22:34:06 MetaFG_0] (main.py 265): INFO Train: [1/300][1510/1562]	eta 0:00:15 lr 0.000001	time 0.2922 (0.3041)	loss 2.3254 (2.3232)	grad_norm 12.5373 (14.9064)	mem 4879MB
[2022-05-30 22:34:09 MetaFG_0] (main.py 265): INFO Train: [1/300][1520/1562]	eta 0:00:12 lr 0.000001	time 0.2977 (0.3041)	loss 2.3204 (2.3232)	grad_norm 22.5698 (14.9036)	mem 4879MB
[2022-05-30 22:34:12 MetaFG_0] (main.py 265): INFO Train: [1/300][1530/1562]	eta 0:00:09 lr 0.000001	time 0.2922 (0.3041)	loss 2.2992 (2.3231)	grad_norm 9.0007 (14.8974)	mem 4879MB
[2022-05-30 22:34:15 MetaFG_0] (main.py 265): INFO Train: [1/300][1540/1562]	eta 0:00:06 lr 0.000001	time 0.2978 (0.3041)	loss 2.3088 (2.3231)	grad_norm 11.7400 (14.8789)	mem 4879MB
[2022-05-30 22:34:18 MetaFG_0] (main.py 265): INFO Train: [1/300][1550/1562]	eta 0:00:03 lr 0.000001	time 0.2978 (0.3041)	loss 2.3198 (2.3231)	grad_norm 11.7757 (14.8661)	mem 4879MB
[2022-05-30 22:34:21 MetaFG_0] (main.py 265): INFO Train: [1/300][1560/1562]	eta 0:00:00 lr 0.000001	time 0.2915 (0.3041)	loss 2.2823 (2.3230)	grad_norm 7.9613 (14.8529)	mem 4879MB
[2022-05-30 22:34:21 MetaFG_0] (main.py 272): INFO EPOCH 1 training takes 0:07:55
[2022-05-30 22:34:21 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_1.pth saving......
[2022-05-30 22:34:22 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_1.pth saved !!!
[2022-05-30 22:34:22 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:34:23 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:34:23 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:34:24 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.495 (0.495)	Loss 4.6777 (4.6777)	Acc@1 0.000 (0.000)	Acc@5 6.250 (6.250)	Mem 4879MB
[2022-05-30 22:34:25 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.092 (0.140)	Loss 4.5515 (4.6141)	Acc@1 3.125 (0.284)	Acc@5 9.375 (4.261)	Mem 4879MB
[2022-05-30 22:34:26 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.118)	Loss 4.6492 (4.6203)	Acc@1 0.000 (0.893)	Acc@5 3.125 (5.060)	Mem 4879MB
[2022-05-30 22:34:27 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.088 (0.110)	Loss 4.7143 (4.6161)	Acc@1 0.000 (0.907)	Acc@5 3.125 (4.738)	Mem 4879MB
[2022-05-30 22:34:28 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.106)	Loss 4.6497 (4.6159)	Acc@1 0.000 (0.915)	Acc@5 6.250 (5.183)	Mem 4879MB
[2022-05-30 22:34:29 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.104)	Loss 4.6028 (4.6187)	Acc@1 0.000 (0.980)	Acc@5 6.250 (5.576)	Mem 4879MB
[2022-05-30 22:34:30 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.107 (0.102)	Loss 4.6225 (4.6209)	Acc@1 0.000 (1.076)	Acc@5 12.500 (6.148)	Mem 4879MB
[2022-05-30 22:34:31 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.092 (0.101)	Loss 4.6570 (4.6214)	Acc@1 0.000 (1.012)	Acc@5 3.125 (5.942)	Mem 4879MB
[2022-05-30 22:34:32 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.099 (0.100)	Loss 4.6459 (4.6189)	Acc@1 3.125 (1.157)	Acc@5 6.250 (6.134)	Mem 4879MB
[2022-05-30 22:34:33 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.091 (0.099)	Loss 4.5785 (4.6177)	Acc@1 9.375 (1.305)	Acc@5 18.750 (6.353)	Mem 4879MB
[2022-05-30 22:34:33 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.099)	Loss 4.6077 (4.6138)	Acc@1 6.250 (1.423)	Acc@5 9.375 (6.559)	Mem 4879MB
[2022-05-30 22:34:34 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.098)	Loss 4.6625 (4.6160)	Acc@1 0.000 (1.351)	Acc@5 3.125 (6.447)	Mem 4879MB
[2022-05-30 22:34:35 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.098)	Loss 4.6847 (4.6166)	Acc@1 0.000 (1.265)	Acc@5 0.000 (6.327)	Mem 4879MB
[2022-05-30 22:34:36 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.093 (0.097)	Loss 4.5417 (4.6162)	Acc@1 3.125 (1.288)	Acc@5 6.250 (6.369)	Mem 4879MB
[2022-05-30 22:34:37 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.097 (0.097)	Loss 4.5449 (4.6147)	Acc@1 9.375 (1.330)	Acc@5 12.500 (6.383)	Mem 4879MB
[2022-05-30 22:34:38 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.097)	Loss 4.6277 (4.6137)	Acc@1 0.000 (1.304)	Acc@5 9.375 (6.271)	Mem 4879MB
[2022-05-30 22:34:39 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.097)	Loss 4.5985 (4.6147)	Acc@1 0.000 (1.281)	Acc@5 3.125 (6.172)	Mem 4879MB
[2022-05-30 22:34:40 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.096)	Loss 4.6184 (4.6149)	Acc@1 3.125 (1.298)	Acc@5 6.250 (6.122)	Mem 4879MB
[2022-05-30 22:34:41 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.096)	Loss 4.6052 (4.6127)	Acc@1 0.000 (1.295)	Acc@5 3.125 (6.233)	Mem 4879MB
[2022-05-30 22:34:42 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.095 (0.096)	Loss 4.5603 (4.6109)	Acc@1 0.000 (1.309)	Acc@5 12.500 (6.332)	Mem 4879MB
[2022-05-30 22:34:43 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.096)	Loss 4.5234 (4.6101)	Acc@1 3.125 (1.306)	Acc@5 6.250 (6.343)	Mem 4879MB
[2022-05-30 22:34:44 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.108 (0.096)	Loss 4.6360 (4.6093)	Acc@1 0.000 (1.303)	Acc@5 3.125 (6.294)	Mem 4879MB
[2022-05-30 22:34:45 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.101 (0.096)	Loss 4.5415 (4.6094)	Acc@1 9.375 (1.315)	Acc@5 15.625 (6.264)	Mem 4879MB
[2022-05-30 22:34:46 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.096)	Loss 4.5880 (4.6099)	Acc@1 0.000 (1.272)	Acc@5 12.500 (6.291)	Mem 4879MB
[2022-05-30 22:34:47 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.092 (0.096)	Loss 4.5674 (4.6103)	Acc@1 0.000 (1.297)	Acc@5 0.000 (6.302)	Mem 4879MB
[2022-05-30 22:34:48 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.091 (0.096)	Loss 4.5905 (4.6102)	Acc@1 0.000 (1.282)	Acc@5 9.375 (6.250)	Mem 4879MB
[2022-05-30 22:34:48 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 4.6067 (4.6108)	Acc@1 3.125 (1.281)	Acc@5 9.375 (6.214)	Mem 4879MB
[2022-05-30 22:34:49 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 4.6071 (4.6094)	Acc@1 6.250 (1.280)	Acc@5 15.625 (6.215)	Mem 4879MB
[2022-05-30 22:34:50 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.095 (0.095)	Loss 4.5097 (4.6082)	Acc@1 0.000 (1.279)	Acc@5 15.625 (6.250)	Mem 4879MB
[2022-05-30 22:34:51 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.095)	Loss 4.6074 (4.6087)	Acc@1 0.000 (1.256)	Acc@5 6.250 (6.271)	Mem 4879MB
[2022-05-30 22:34:52 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.096)	Loss 4.6901 (4.6091)	Acc@1 0.000 (1.246)	Acc@5 6.250 (6.188)	Mem 4879MB
[2022-05-30 22:34:53 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 4.6410 (4.6090)	Acc@1 0.000 (1.206)	Acc@5 3.125 (6.180)	Mem 4879MB
[2022-05-30 22:34:53 MetaFG_0] (main.py 330): INFO  * Acc@1 1.230 Acc@5 6.200
[2022-05-30 22:34:53 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 1.2%
[2022-05-30 22:34:53 MetaFG_0] (main.py 171): INFO Max accuracy: 1.23%
[2022-05-30 22:34:54 MetaFG_0] (main.py 265): INFO Train: [2/300][0/1562]	eta 0:26:50 lr 0.000001	time 1.0311 (1.0311)	loss 2.3197 (2.3197)	grad_norm 15.4196 (15.4196)	mem 4879MB
[2022-05-30 22:34:58 MetaFG_0] (main.py 265): INFO Train: [2/300][10/1562]	eta 0:09:47 lr 0.000001	time 0.2935 (0.3783)	loss 2.3472 (2.3204)	grad_norm 16.0968 (13.2524)	mem 4879MB
[2022-05-30 22:35:01 MetaFG_0] (main.py 265): INFO Train: [2/300][20/1562]	eta 0:08:47 lr 0.000001	time 0.2920 (0.3422)	loss 2.3185 (2.3213)	grad_norm 8.8645 (13.4728)	mem 4879MB
[2022-05-30 22:35:04 MetaFG_0] (main.py 265): INFO Train: [2/300][30/1562]	eta 0:08:28 lr 0.000001	time 0.3048 (0.3317)	loss 2.3232 (2.3191)	grad_norm 15.6956 (13.5743)	mem 4879MB
[2022-05-30 22:35:07 MetaFG_0] (main.py 265): INFO Train: [2/300][40/1562]	eta 0:08:14 lr 0.000001	time 0.2924 (0.3250)	loss 2.2880 (2.3160)	grad_norm 6.8679 (13.5500)	mem 4879MB
[2022-05-30 22:35:10 MetaFG_0] (main.py 265): INFO Train: [2/300][50/1562]	eta 0:08:05 lr 0.000001	time 0.2999 (0.3210)	loss 2.2726 (2.3165)	grad_norm 10.7659 (13.5421)	mem 4879MB
[2022-05-30 22:35:13 MetaFG_0] (main.py 265): INFO Train: [2/300][60/1562]	eta 0:07:58 lr 0.000001	time 0.2971 (0.3184)	loss 2.3328 (2.3165)	grad_norm 10.8475 (13.3939)	mem 4879MB
[2022-05-30 22:35:16 MetaFG_0] (main.py 265): INFO Train: [2/300][70/1562]	eta 0:07:52 lr 0.000001	time 0.2987 (0.3164)	loss 2.3040 (2.3155)	grad_norm 7.4799 (13.1925)	mem 4879MB
[2022-05-30 22:35:19 MetaFG_0] (main.py 265): INFO Train: [2/300][80/1562]	eta 0:07:46 lr 0.000001	time 0.2974 (0.3149)	loss 2.3222 (2.3158)	grad_norm 10.7393 (12.9835)	mem 4879MB
[2022-05-30 22:35:22 MetaFG_0] (main.py 265): INFO Train: [2/300][90/1562]	eta 0:07:41 lr 0.000001	time 0.2920 (0.3136)	loss 2.3068 (2.3159)	grad_norm 17.6926 (13.0492)	mem 4879MB
[2022-05-30 22:35:25 MetaFG_0] (main.py 265): INFO Train: [2/300][100/1562]	eta 0:07:36 lr 0.000001	time 0.2932 (0.3125)	loss 2.3069 (2.3159)	grad_norm 8.2089 (13.0229)	mem 4879MB
[2022-05-30 22:35:28 MetaFG_0] (main.py 265): INFO Train: [2/300][110/1562]	eta 0:07:32 lr 0.000001	time 0.2924 (0.3118)	loss 2.3132 (2.3150)	grad_norm 17.9940 (13.0167)	mem 4879MB
[2022-05-30 22:35:31 MetaFG_0] (main.py 265): INFO Train: [2/300][120/1562]	eta 0:07:28 lr 0.000001	time 0.2920 (0.3110)	loss 2.3040 (2.3144)	grad_norm 12.2535 (12.8674)	mem 4879MB
[2022-05-30 22:35:34 MetaFG_0] (main.py 265): INFO Train: [2/300][130/1562]	eta 0:07:24 lr 0.000001	time 0.2922 (0.3106)	loss 2.2887 (2.3140)	grad_norm 8.5136 (12.9260)	mem 4879MB
[2022-05-30 22:35:37 MetaFG_0] (main.py 265): INFO Train: [2/300][140/1562]	eta 0:07:21 lr 0.000001	time 0.2922 (0.3102)	loss 2.3199 (2.3137)	grad_norm 8.7798 (13.0060)	mem 4879MB
[2022-05-30 22:35:40 MetaFG_0] (main.py 265): INFO Train: [2/300][150/1562]	eta 0:07:17 lr 0.000001	time 0.2923 (0.3098)	loss 2.2971 (2.3134)	grad_norm 8.0346 (12.9720)	mem 4879MB
[2022-05-30 22:35:43 MetaFG_0] (main.py 265): INFO Train: [2/300][160/1562]	eta 0:07:13 lr 0.000001	time 0.2935 (0.3094)	loss 2.3157 (2.3134)	grad_norm 11.4302 (13.0851)	mem 4879MB
[2022-05-30 22:35:46 MetaFG_0] (main.py 265): INFO Train: [2/300][170/1562]	eta 0:07:10 lr 0.000001	time 0.2933 (0.3090)	loss 2.2952 (2.3135)	grad_norm 12.3613 (13.1254)	mem 4879MB
[2022-05-30 22:35:49 MetaFG_0] (main.py 265): INFO Train: [2/300][180/1562]	eta 0:07:06 lr 0.000001	time 0.2986 (0.3087)	loss 2.3042 (2.3137)	grad_norm 14.3860 (13.1744)	mem 4879MB
[2022-05-30 22:35:52 MetaFG_0] (main.py 265): INFO Train: [2/300][190/1562]	eta 0:07:03 lr 0.000001	time 0.2922 (0.3084)	loss 2.3098 (2.3134)	grad_norm 7.5472 (13.0538)	mem 4879MB
[2022-05-30 22:35:55 MetaFG_0] (main.py 265): INFO Train: [2/300][200/1562]	eta 0:06:59 lr 0.000001	time 0.2982 (0.3082)	loss 2.2849 (2.3129)	grad_norm 5.8566 (13.0167)	mem 4879MB
[2022-05-30 22:35:58 MetaFG_0] (main.py 265): INFO Train: [2/300][210/1562]	eta 0:06:56 lr 0.000001	time 0.2928 (0.3081)	loss 2.2965 (2.3129)	grad_norm 10.9487 (12.9876)	mem 4879MB
[2022-05-30 22:36:01 MetaFG_0] (main.py 265): INFO Train: [2/300][220/1562]	eta 0:06:53 lr 0.000001	time 0.2936 (0.3078)	loss 2.3205 (2.3130)	grad_norm 8.5443 (12.9919)	mem 4879MB
[2022-05-30 22:36:04 MetaFG_0] (main.py 265): INFO Train: [2/300][230/1562]	eta 0:06:49 lr 0.000001	time 0.2924 (0.3076)	loss 2.3255 (2.3127)	grad_norm 12.8295 (12.8952)	mem 4879MB
[2022-05-30 22:36:07 MetaFG_0] (main.py 265): INFO Train: [2/300][240/1562]	eta 0:06:46 lr 0.000001	time 0.2921 (0.3075)	loss 2.3124 (2.3124)	grad_norm 13.0370 (12.8163)	mem 4879MB
[2022-05-30 22:36:11 MetaFG_0] (main.py 265): INFO Train: [2/300][250/1562]	eta 0:06:43 lr 0.000001	time 0.2974 (0.3073)	loss 2.3258 (2.3125)	grad_norm 14.2568 (12.8315)	mem 4879MB
[2022-05-30 22:36:14 MetaFG_0] (main.py 265): INFO Train: [2/300][260/1562]	eta 0:06:39 lr 0.000001	time 0.2954 (0.3072)	loss 2.2888 (2.3121)	grad_norm 9.8291 (12.8180)	mem 4879MB
[2022-05-30 22:36:17 MetaFG_0] (main.py 265): INFO Train: [2/300][270/1562]	eta 0:06:36 lr 0.000001	time 0.2934 (0.3071)	loss 2.2815 (2.3123)	grad_norm 8.4909 (12.7557)	mem 4879MB
[2022-05-30 22:36:20 MetaFG_0] (main.py 265): INFO Train: [2/300][280/1562]	eta 0:06:33 lr 0.000001	time 0.2946 (0.3069)	loss 2.3220 (2.3122)	grad_norm 11.1239 (12.7085)	mem 4879MB
[2022-05-30 22:36:23 MetaFG_0] (main.py 265): INFO Train: [2/300][290/1562]	eta 0:06:30 lr 0.000001	time 0.2930 (0.3068)	loss 2.3034 (2.3118)	grad_norm 10.0028 (12.6945)	mem 4879MB
[2022-05-30 22:36:26 MetaFG_0] (main.py 265): INFO Train: [2/300][300/1562]	eta 0:06:27 lr 0.000001	time 0.2930 (0.3067)	loss 2.3501 (2.3119)	grad_norm 12.2634 (12.6759)	mem 4879MB
[2022-05-30 22:36:29 MetaFG_0] (main.py 265): INFO Train: [2/300][310/1562]	eta 0:06:23 lr 0.000001	time 0.2939 (0.3066)	loss 2.2988 (2.3117)	grad_norm 12.8190 (12.6751)	mem 4879MB
[2022-05-30 22:36:32 MetaFG_0] (main.py 265): INFO Train: [2/300][320/1562]	eta 0:06:20 lr 0.000001	time 0.2920 (0.3065)	loss 2.2871 (2.3115)	grad_norm 12.7158 (12.6299)	mem 4879MB
[2022-05-30 22:36:35 MetaFG_0] (main.py 265): INFO Train: [2/300][330/1562]	eta 0:06:17 lr 0.000001	time 0.2985 (0.3064)	loss 2.3529 (2.3116)	grad_norm 9.4066 (12.6177)	mem 4879MB
[2022-05-30 22:36:38 MetaFG_0] (main.py 265): INFO Train: [2/300][340/1562]	eta 0:06:14 lr 0.000001	time 0.2921 (0.3063)	loss 2.3042 (2.3117)	grad_norm 18.2477 (12.6389)	mem 4879MB
[2022-05-30 22:36:41 MetaFG_0] (main.py 265): INFO Train: [2/300][350/1562]	eta 0:06:11 lr 0.000001	time 0.2919 (0.3063)	loss 2.3054 (2.3116)	grad_norm 13.9124 (12.6135)	mem 4879MB
[2022-05-30 22:36:44 MetaFG_0] (main.py 265): INFO Train: [2/300][360/1562]	eta 0:06:08 lr 0.000001	time 0.2924 (0.3062)	loss 2.3168 (2.3117)	grad_norm 11.2664 (12.6506)	mem 4879MB
[2022-05-30 22:36:47 MetaFG_0] (main.py 265): INFO Train: [2/300][370/1562]	eta 0:06:04 lr 0.000001	time 0.2950 (0.3061)	loss 2.3292 (2.3117)	grad_norm 10.0634 (12.5992)	mem 4879MB
[2022-05-30 22:36:50 MetaFG_0] (main.py 265): INFO Train: [2/300][380/1562]	eta 0:06:01 lr 0.000001	time 0.2982 (0.3060)	loss 2.3066 (2.3117)	grad_norm 9.6648 (12.5471)	mem 4879MB
[2022-05-30 22:36:53 MetaFG_0] (main.py 265): INFO Train: [2/300][390/1562]	eta 0:05:58 lr 0.000001	time 0.2922 (0.3060)	loss 2.3104 (2.3116)	grad_norm 10.4308 (12.5299)	mem 4879MB
[2022-05-30 22:36:56 MetaFG_0] (main.py 265): INFO Train: [2/300][400/1562]	eta 0:05:55 lr 0.000001	time 0.2919 (0.3059)	loss 2.3065 (2.3116)	grad_norm 10.3660 (12.5537)	mem 4879MB
[2022-05-30 22:36:59 MetaFG_0] (main.py 265): INFO Train: [2/300][410/1562]	eta 0:05:52 lr 0.000001	time 0.2943 (0.3058)	loss 2.2857 (2.3114)	grad_norm 13.9863 (12.5318)	mem 4879MB
[2022-05-30 22:37:02 MetaFG_0] (main.py 265): INFO Train: [2/300][420/1562]	eta 0:05:49 lr 0.000001	time 0.2941 (0.3058)	loss 2.2865 (2.3112)	grad_norm 10.6298 (12.4857)	mem 4879MB
[2022-05-30 22:37:05 MetaFG_0] (main.py 265): INFO Train: [2/300][430/1562]	eta 0:05:46 lr 0.000001	time 0.2979 (0.3057)	loss 2.2903 (2.3111)	grad_norm 7.3764 (12.4484)	mem 4879MB
[2022-05-30 22:37:08 MetaFG_0] (main.py 265): INFO Train: [2/300][440/1562]	eta 0:05:42 lr 0.000001	time 0.2919 (0.3057)	loss 2.3227 (2.3108)	grad_norm 6.7781 (12.4267)	mem 4879MB
[2022-05-30 22:37:11 MetaFG_0] (main.py 265): INFO Train: [2/300][450/1562]	eta 0:05:39 lr 0.000001	time 0.2941 (0.3056)	loss 2.3134 (2.3108)	grad_norm 10.1639 (12.3982)	mem 4879MB
[2022-05-30 22:37:14 MetaFG_0] (main.py 265): INFO Train: [2/300][460/1562]	eta 0:05:36 lr 0.000001	time 0.2926 (0.3056)	loss 2.3055 (2.3109)	grad_norm 9.9844 (12.3829)	mem 4879MB
[2022-05-30 22:37:17 MetaFG_0] (main.py 265): INFO Train: [2/300][470/1562]	eta 0:05:33 lr 0.000001	time 0.2923 (0.3055)	loss 2.2753 (2.3108)	grad_norm 8.1887 (12.3657)	mem 4879MB
[2022-05-30 22:37:20 MetaFG_0] (main.py 265): INFO Train: [2/300][480/1562]	eta 0:05:30 lr 0.000001	time 0.2976 (0.3055)	loss 2.3104 (2.3108)	grad_norm 9.7456 (12.3508)	mem 4879MB
[2022-05-30 22:37:24 MetaFG_0] (main.py 265): INFO Train: [2/300][490/1562]	eta 0:05:27 lr 0.000001	time 0.3129 (0.3058)	loss 2.3198 (2.3106)	grad_norm 8.7046 (12.3304)	mem 4879MB
[2022-05-30 22:37:27 MetaFG_0] (main.py 265): INFO Train: [2/300][500/1562]	eta 0:05:24 lr 0.000001	time 0.2996 (0.3058)	loss 2.3173 (2.3106)	grad_norm 10.1538 (12.3346)	mem 4879MB
[2022-05-30 22:37:30 MetaFG_0] (main.py 265): INFO Train: [2/300][510/1562]	eta 0:05:21 lr 0.000001	time 0.2914 (0.3057)	loss 2.3062 (2.3107)	grad_norm 8.0786 (12.3043)	mem 4879MB
[2022-05-30 22:37:33 MetaFG_0] (main.py 265): INFO Train: [2/300][520/1562]	eta 0:05:18 lr 0.000001	time 0.2985 (0.3057)	loss 2.3119 (2.3106)	grad_norm 43.5863 (12.3551)	mem 4879MB
[2022-05-30 22:37:36 MetaFG_0] (main.py 265): INFO Train: [2/300][530/1562]	eta 0:05:15 lr 0.000001	time 0.2978 (0.3057)	loss 2.3188 (2.3106)	grad_norm 8.6798 (12.3016)	mem 4879MB
[2022-05-30 22:37:39 MetaFG_0] (main.py 265): INFO Train: [2/300][540/1562]	eta 0:05:12 lr 0.000001	time 0.2976 (0.3056)	loss 2.3079 (2.3106)	grad_norm 7.2683 (12.2689)	mem 4879MB
[2022-05-30 22:37:42 MetaFG_0] (main.py 265): INFO Train: [2/300][550/1562]	eta 0:05:09 lr 0.000001	time 0.2978 (0.3056)	loss 2.3333 (2.3107)	grad_norm 9.6988 (12.2103)	mem 4879MB
[2022-05-30 22:37:45 MetaFG_0] (main.py 265): INFO Train: [2/300][560/1562]	eta 0:05:06 lr 0.000001	time 0.2985 (0.3056)	loss 2.3212 (2.3107)	grad_norm 7.4820 (12.1971)	mem 4879MB
[2022-05-30 22:37:48 MetaFG_0] (main.py 265): INFO Train: [2/300][570/1562]	eta 0:05:03 lr 0.000001	time 0.3036 (0.3056)	loss 2.3265 (2.3107)	grad_norm 10.1355 (12.1740)	mem 4879MB
[2022-05-30 22:37:51 MetaFG_0] (main.py 265): INFO Train: [2/300][580/1562]	eta 0:05:00 lr 0.000001	time 0.2988 (0.3056)	loss 2.2901 (2.3106)	grad_norm 13.3137 (12.1626)	mem 4879MB
[2022-05-30 22:37:54 MetaFG_0] (main.py 265): INFO Train: [2/300][590/1562]	eta 0:04:56 lr 0.000001	time 0.2924 (0.3055)	loss 2.2920 (2.3106)	grad_norm 14.9722 (12.1487)	mem 4879MB
[2022-05-30 22:37:57 MetaFG_0] (main.py 265): INFO Train: [2/300][600/1562]	eta 0:04:53 lr 0.000001	time 0.2918 (0.3055)	loss 2.3016 (2.3106)	grad_norm 6.9945 (12.1547)	mem 4879MB
[2022-05-30 22:38:00 MetaFG_0] (main.py 265): INFO Train: [2/300][610/1562]	eta 0:04:50 lr 0.000001	time 0.2999 (0.3055)	loss 2.3043 (2.3105)	grad_norm 13.3635 (12.1238)	mem 4879MB
[2022-05-30 22:38:03 MetaFG_0] (main.py 265): INFO Train: [2/300][620/1562]	eta 0:04:47 lr 0.000001	time 0.2931 (0.3055)	loss 2.2994 (2.3104)	grad_norm 7.2703 (12.1107)	mem 4879MB
[2022-05-30 22:38:06 MetaFG_0] (main.py 265): INFO Train: [2/300][630/1562]	eta 0:04:44 lr 0.000001	time 0.2999 (0.3055)	loss 2.2827 (2.3103)	grad_norm 8.6201 (12.0901)	mem 4879MB
[2022-05-30 22:38:09 MetaFG_0] (main.py 265): INFO Train: [2/300][640/1562]	eta 0:04:41 lr 0.000001	time 0.2921 (0.3055)	loss 2.3047 (2.3104)	grad_norm 11.9124 (12.0500)	mem 4879MB
[2022-05-30 22:38:12 MetaFG_0] (main.py 265): INFO Train: [2/300][650/1562]	eta 0:04:38 lr 0.000001	time 0.2992 (0.3055)	loss 2.3007 (2.3104)	grad_norm 9.8537 (12.0289)	mem 4879MB
[2022-05-30 22:38:15 MetaFG_0] (main.py 265): INFO Train: [2/300][660/1562]	eta 0:04:35 lr 0.000001	time 0.2987 (0.3055)	loss 2.3142 (2.3103)	grad_norm 9.1956 (12.0159)	mem 4879MB
[2022-05-30 22:38:18 MetaFG_0] (main.py 265): INFO Train: [2/300][670/1562]	eta 0:04:32 lr 0.000001	time 0.2963 (0.3054)	loss 2.2776 (2.3101)	grad_norm 8.9256 (11.9892)	mem 4879MB
[2022-05-30 22:38:21 MetaFG_0] (main.py 265): INFO Train: [2/300][680/1562]	eta 0:04:29 lr 0.000001	time 0.3006 (0.3054)	loss 2.2827 (2.3101)	grad_norm 9.4544 (11.9675)	mem 4879MB
[2022-05-30 22:38:24 MetaFG_0] (main.py 265): INFO Train: [2/300][690/1562]	eta 0:04:26 lr 0.000001	time 0.2921 (0.3054)	loss 2.2854 (2.3100)	grad_norm 9.8756 (11.9677)	mem 4879MB
[2022-05-30 22:38:27 MetaFG_0] (main.py 265): INFO Train: [2/300][700/1562]	eta 0:04:23 lr 0.000001	time 0.2925 (0.3054)	loss 2.3022 (2.3100)	grad_norm 5.4542 (11.9382)	mem 4879MB
[2022-05-30 22:38:30 MetaFG_0] (main.py 265): INFO Train: [2/300][710/1562]	eta 0:04:20 lr 0.000001	time 0.2939 (0.3053)	loss 2.3124 (2.3100)	grad_norm 8.6106 (11.9472)	mem 4879MB
[2022-05-30 22:38:34 MetaFG_0] (main.py 265): INFO Train: [2/300][720/1562]	eta 0:04:17 lr 0.000001	time 0.2916 (0.3053)	loss 2.3143 (2.3099)	grad_norm 10.1912 (11.9313)	mem 4879MB
[2022-05-30 22:38:37 MetaFG_0] (main.py 265): INFO Train: [2/300][730/1562]	eta 0:04:14 lr 0.000001	time 0.2984 (0.3053)	loss 2.2872 (2.3099)	grad_norm 11.6968 (11.9129)	mem 4879MB
[2022-05-30 22:38:40 MetaFG_0] (main.py 265): INFO Train: [2/300][740/1562]	eta 0:04:10 lr 0.000001	time 0.2984 (0.3053)	loss 2.3252 (2.3099)	grad_norm 13.9564 (11.9046)	mem 4879MB
[2022-05-30 22:38:43 MetaFG_0] (main.py 265): INFO Train: [2/300][750/1562]	eta 0:04:07 lr 0.000001	time 0.2987 (0.3053)	loss 2.2895 (2.3099)	grad_norm 8.8661 (11.9004)	mem 4879MB
[2022-05-30 22:38:46 MetaFG_0] (main.py 265): INFO Train: [2/300][760/1562]	eta 0:04:04 lr 0.000001	time 0.2922 (0.3053)	loss 2.3169 (2.3099)	grad_norm 8.7413 (11.8833)	mem 4879MB
[2022-05-30 22:38:49 MetaFG_0] (main.py 265): INFO Train: [2/300][770/1562]	eta 0:04:01 lr 0.000001	time 0.2916 (0.3052)	loss 2.2972 (2.3099)	grad_norm 13.8054 (11.8805)	mem 4879MB
[2022-05-30 22:38:52 MetaFG_0] (main.py 265): INFO Train: [2/300][780/1562]	eta 0:03:58 lr 0.000001	time 0.3000 (0.3052)	loss 2.3280 (2.3099)	grad_norm 12.2981 (11.8706)	mem 4879MB
[2022-05-30 22:38:55 MetaFG_0] (main.py 265): INFO Train: [2/300][790/1562]	eta 0:03:55 lr 0.000001	time 0.2919 (0.3052)	loss 2.2896 (2.3097)	grad_norm 7.8537 (11.8464)	mem 4879MB
[2022-05-30 22:38:58 MetaFG_0] (main.py 265): INFO Train: [2/300][800/1562]	eta 0:03:52 lr 0.000001	time 0.2987 (0.3052)	loss 2.2728 (2.3097)	grad_norm 11.9471 (11.8526)	mem 4879MB
[2022-05-30 22:39:01 MetaFG_0] (main.py 265): INFO Train: [2/300][810/1562]	eta 0:03:49 lr 0.000001	time 0.2938 (0.3052)	loss 2.3115 (2.3097)	grad_norm 10.5317 (11.8377)	mem 4879MB
[2022-05-30 22:39:04 MetaFG_0] (main.py 265): INFO Train: [2/300][820/1562]	eta 0:03:46 lr 0.000001	time 0.2922 (0.3052)	loss 2.3150 (2.3098)	grad_norm 6.3779 (11.8012)	mem 4879MB
[2022-05-30 22:39:07 MetaFG_0] (main.py 265): INFO Train: [2/300][830/1562]	eta 0:03:43 lr 0.000001	time 0.2984 (0.3052)	loss 2.2987 (2.3097)	grad_norm 14.7105 (11.7884)	mem 4879MB
[2022-05-30 22:39:10 MetaFG_0] (main.py 265): INFO Train: [2/300][840/1562]	eta 0:03:40 lr 0.000001	time 0.2920 (0.3051)	loss 2.2947 (2.3095)	grad_norm 11.4442 (11.7654)	mem 4879MB
[2022-05-30 22:39:13 MetaFG_0] (main.py 265): INFO Train: [2/300][850/1562]	eta 0:03:37 lr 0.000001	time 0.2918 (0.3051)	loss 2.3216 (2.3095)	grad_norm 11.8085 (11.7809)	mem 4879MB
[2022-05-30 22:39:16 MetaFG_0] (main.py 265): INFO Train: [2/300][860/1562]	eta 0:03:34 lr 0.000001	time 0.2923 (0.3051)	loss 2.2976 (2.3095)	grad_norm 8.6114 (11.7693)	mem 4879MB
[2022-05-30 22:39:19 MetaFG_0] (main.py 265): INFO Train: [2/300][870/1562]	eta 0:03:31 lr 0.000001	time 0.2984 (0.3051)	loss 2.2772 (2.3094)	grad_norm 13.6134 (11.7512)	mem 4879MB
[2022-05-30 22:39:22 MetaFG_0] (main.py 265): INFO Train: [2/300][880/1562]	eta 0:03:28 lr 0.000001	time 0.2971 (0.3051)	loss 2.2871 (2.3094)	grad_norm 13.9583 (11.7361)	mem 4879MB
[2022-05-30 22:39:25 MetaFG_0] (main.py 265): INFO Train: [2/300][890/1562]	eta 0:03:25 lr 0.000001	time 0.2939 (0.3051)	loss 2.2997 (2.3094)	grad_norm 12.0146 (11.7272)	mem 4879MB
[2022-05-30 22:39:28 MetaFG_0] (main.py 265): INFO Train: [2/300][900/1562]	eta 0:03:21 lr 0.000001	time 0.2929 (0.3051)	loss 2.3195 (2.3094)	grad_norm 16.2714 (11.7103)	mem 4879MB
[2022-05-30 22:39:31 MetaFG_0] (main.py 265): INFO Train: [2/300][910/1562]	eta 0:03:18 lr 0.000001	time 0.2985 (0.3051)	loss 2.3068 (2.3094)	grad_norm 11.6390 (11.7058)	mem 4879MB
[2022-05-30 22:39:34 MetaFG_0] (main.py 265): INFO Train: [2/300][920/1562]	eta 0:03:15 lr 0.000001	time 0.2985 (0.3050)	loss 2.3125 (2.3093)	grad_norm 6.3629 (11.6771)	mem 4879MB
[2022-05-30 22:39:37 MetaFG_0] (main.py 265): INFO Train: [2/300][930/1562]	eta 0:03:12 lr 0.000001	time 0.2989 (0.3050)	loss 2.3243 (2.3093)	grad_norm 11.8737 (11.6605)	mem 4879MB
[2022-05-30 22:39:40 MetaFG_0] (main.py 265): INFO Train: [2/300][940/1562]	eta 0:03:09 lr 0.000001	time 0.3051 (0.3051)	loss 2.3229 (2.3094)	grad_norm 20.6240 (11.6668)	mem 4879MB
[2022-05-30 22:39:44 MetaFG_0] (main.py 265): INFO Train: [2/300][950/1562]	eta 0:03:06 lr 0.000001	time 0.2936 (0.3051)	loss 2.3067 (2.3094)	grad_norm 10.9368 (11.6438)	mem 4879MB
[2022-05-30 22:39:47 MetaFG_0] (main.py 265): INFO Train: [2/300][960/1562]	eta 0:03:03 lr 0.000001	time 0.2978 (0.3051)	loss 2.3070 (2.3092)	grad_norm 9.9059 (11.6464)	mem 4879MB
[2022-05-30 22:39:50 MetaFG_0] (main.py 265): INFO Train: [2/300][970/1562]	eta 0:03:00 lr 0.000001	time 0.2995 (0.3051)	loss 2.3237 (2.3092)	grad_norm 7.5971 (11.6269)	mem 4879MB
[2022-05-30 22:39:53 MetaFG_0] (main.py 265): INFO Train: [2/300][980/1562]	eta 0:02:57 lr 0.000001	time 0.2997 (0.3051)	loss 2.3001 (2.3091)	grad_norm 14.3583 (11.6186)	mem 4879MB
[2022-05-30 22:39:56 MetaFG_0] (main.py 265): INFO Train: [2/300][990/1562]	eta 0:02:54 lr 0.000001	time 0.2987 (0.3051)	loss 2.3251 (2.3090)	grad_norm 7.7253 (11.5892)	mem 4879MB
[2022-05-30 22:39:59 MetaFG_0] (main.py 265): INFO Train: [2/300][1000/1562]	eta 0:02:51 lr 0.000001	time 0.2923 (0.3051)	loss 2.2817 (2.3090)	grad_norm 9.4709 (11.5686)	mem 4879MB
[2022-05-30 22:40:02 MetaFG_0] (main.py 265): INFO Train: [2/300][1010/1562]	eta 0:02:48 lr 0.000001	time 0.2933 (0.3050)	loss 2.2749 (2.3089)	grad_norm 4.8164 (11.5581)	mem 4879MB
[2022-05-30 22:40:05 MetaFG_0] (main.py 265): INFO Train: [2/300][1020/1562]	eta 0:02:45 lr 0.000001	time 0.2928 (0.3050)	loss 2.3280 (2.3089)	grad_norm 9.4368 (11.5495)	mem 4879MB
[2022-05-30 22:40:08 MetaFG_0] (main.py 265): INFO Train: [2/300][1030/1562]	eta 0:02:42 lr 0.000001	time 0.2934 (0.3050)	loss 2.2883 (2.3088)	grad_norm 7.7749 (11.5517)	mem 4879MB
[2022-05-30 22:40:11 MetaFG_0] (main.py 265): INFO Train: [2/300][1040/1562]	eta 0:02:39 lr 0.000001	time 0.2918 (0.3050)	loss 2.2865 (2.3087)	grad_norm 11.8715 (11.5350)	mem 4879MB
[2022-05-30 22:40:14 MetaFG_0] (main.py 265): INFO Train: [2/300][1050/1562]	eta 0:02:36 lr 0.000001	time 0.2976 (0.3049)	loss 2.3101 (2.3087)	grad_norm 12.2743 (11.5272)	mem 4879MB
[2022-05-30 22:40:17 MetaFG_0] (main.py 265): INFO Train: [2/300][1060/1562]	eta 0:02:33 lr 0.000001	time 0.2973 (0.3049)	loss 2.3133 (2.3086)	grad_norm 6.9290 (11.5199)	mem 4879MB
[2022-05-30 22:40:20 MetaFG_0] (main.py 265): INFO Train: [2/300][1070/1562]	eta 0:02:30 lr 0.000001	time 0.2916 (0.3049)	loss 2.2885 (2.3086)	grad_norm 8.3662 (11.5115)	mem 4879MB
[2022-05-30 22:40:23 MetaFG_0] (main.py 265): INFO Train: [2/300][1080/1562]	eta 0:02:26 lr 0.000001	time 0.2979 (0.3049)	loss 2.3066 (2.3085)	grad_norm 9.6221 (11.4860)	mem 4879MB
[2022-05-30 22:40:26 MetaFG_0] (main.py 265): INFO Train: [2/300][1090/1562]	eta 0:02:23 lr 0.000001	time 0.3019 (0.3049)	loss 2.3057 (2.3084)	grad_norm 11.4933 (11.4697)	mem 4879MB
[2022-05-30 22:40:29 MetaFG_0] (main.py 265): INFO Train: [2/300][1100/1562]	eta 0:02:20 lr 0.000001	time 0.2991 (0.3049)	loss 2.3209 (2.3083)	grad_norm 8.6736 (11.4644)	mem 4879MB
[2022-05-30 22:40:32 MetaFG_0] (main.py 265): INFO Train: [2/300][1110/1562]	eta 0:02:17 lr 0.000001	time 0.2918 (0.3048)	loss 2.3050 (2.3082)	grad_norm 6.2642 (11.4617)	mem 4879MB
[2022-05-30 22:40:35 MetaFG_0] (main.py 265): INFO Train: [2/300][1120/1562]	eta 0:02:14 lr 0.000001	time 0.2917 (0.3048)	loss 2.3026 (2.3081)	grad_norm 6.8173 (11.4409)	mem 4879MB
[2022-05-30 22:40:38 MetaFG_0] (main.py 265): INFO Train: [2/300][1130/1562]	eta 0:02:11 lr 0.000001	time 0.2932 (0.3048)	loss 2.2996 (2.3080)	grad_norm 13.2120 (11.4423)	mem 4879MB
[2022-05-30 22:40:41 MetaFG_0] (main.py 265): INFO Train: [2/300][1140/1562]	eta 0:02:08 lr 0.000001	time 0.2986 (0.3048)	loss 2.3151 (2.3080)	grad_norm 9.7556 (11.4565)	mem 4879MB
[2022-05-30 22:40:44 MetaFG_0] (main.py 265): INFO Train: [2/300][1150/1562]	eta 0:02:05 lr 0.000001	time 0.2989 (0.3048)	loss 2.2945 (2.3079)	grad_norm 10.1370 (11.4376)	mem 4879MB
[2022-05-30 22:40:47 MetaFG_0] (main.py 265): INFO Train: [2/300][1160/1562]	eta 0:02:02 lr 0.000001	time 0.3006 (0.3048)	loss 2.3061 (2.3078)	grad_norm 7.5793 (11.4255)	mem 4879MB
[2022-05-30 22:40:50 MetaFG_0] (main.py 265): INFO Train: [2/300][1170/1562]	eta 0:01:59 lr 0.000001	time 0.2974 (0.3048)	loss 2.3162 (2.3077)	grad_norm 7.9605 (11.4130)	mem 4879MB
[2022-05-30 22:40:53 MetaFG_0] (main.py 265): INFO Train: [2/300][1180/1562]	eta 0:01:56 lr 0.000001	time 0.2957 (0.3048)	loss 2.3027 (2.3077)	grad_norm 8.1828 (11.4036)	mem 4879MB
[2022-05-30 22:40:56 MetaFG_0] (main.py 265): INFO Train: [2/300][1190/1562]	eta 0:01:53 lr 0.000001	time 0.3064 (0.3048)	loss 2.2989 (2.3077)	grad_norm 9.4139 (11.3842)	mem 4879MB
[2022-05-30 22:40:59 MetaFG_0] (main.py 265): INFO Train: [2/300][1200/1562]	eta 0:01:50 lr 0.000001	time 0.2922 (0.3048)	loss 2.3157 (2.3077)	grad_norm 10.4538 (11.3629)	mem 4879MB
[2022-05-30 22:41:03 MetaFG_0] (main.py 265): INFO Train: [2/300][1210/1562]	eta 0:01:47 lr 0.000001	time 0.2918 (0.3048)	loss 2.2900 (2.3076)	grad_norm 9.2781 (11.3642)	mem 4879MB
[2022-05-30 22:41:06 MetaFG_0] (main.py 265): INFO Train: [2/300][1220/1562]	eta 0:01:44 lr 0.000001	time 0.3006 (0.3048)	loss 2.2693 (2.3075)	grad_norm 7.9141 (11.3513)	mem 4879MB
[2022-05-30 22:41:09 MetaFG_0] (main.py 265): INFO Train: [2/300][1230/1562]	eta 0:01:41 lr 0.000001	time 0.2986 (0.3048)	loss 2.3094 (2.3075)	grad_norm 8.9186 (11.3347)	mem 4879MB
[2022-05-30 22:41:12 MetaFG_0] (main.py 265): INFO Train: [2/300][1240/1562]	eta 0:01:38 lr 0.000001	time 0.2987 (0.3048)	loss 2.3077 (2.3074)	grad_norm 4.8634 (11.3187)	mem 4879MB
[2022-05-30 22:41:15 MetaFG_0] (main.py 265): INFO Train: [2/300][1250/1562]	eta 0:01:35 lr 0.000001	time 0.2933 (0.3048)	loss 2.3076 (2.3074)	grad_norm 12.9177 (11.3032)	mem 4879MB
[2022-05-30 22:41:18 MetaFG_0] (main.py 265): INFO Train: [2/300][1260/1562]	eta 0:01:32 lr 0.000001	time 0.2948 (0.3048)	loss 2.2983 (2.3073)	grad_norm 9.2855 (11.3022)	mem 4879MB
[2022-05-30 22:41:21 MetaFG_0] (main.py 265): INFO Train: [2/300][1270/1562]	eta 0:01:28 lr 0.000001	time 0.2988 (0.3048)	loss 2.3062 (2.3073)	grad_norm 8.9601 (11.2929)	mem 4879MB
[2022-05-30 22:41:24 MetaFG_0] (main.py 265): INFO Train: [2/300][1280/1562]	eta 0:01:25 lr 0.000001	time 0.2991 (0.3047)	loss 2.3065 (2.3073)	grad_norm 9.9377 (11.2706)	mem 4879MB
[2022-05-30 22:41:27 MetaFG_0] (main.py 265): INFO Train: [2/300][1290/1562]	eta 0:01:22 lr 0.000001	time 0.2933 (0.3047)	loss 2.2794 (2.3073)	grad_norm 13.9950 (11.2567)	mem 4879MB
[2022-05-30 22:41:30 MetaFG_0] (main.py 265): INFO Train: [2/300][1300/1562]	eta 0:01:19 lr 0.000001	time 0.2938 (0.3047)	loss 2.3049 (2.3073)	grad_norm 7.6609 (11.2455)	mem 4879MB
[2022-05-30 22:41:33 MetaFG_0] (main.py 265): INFO Train: [2/300][1310/1562]	eta 0:01:16 lr 0.000001	time 0.2988 (0.3047)	loss 2.2991 (2.3072)	grad_norm 7.9776 (11.2421)	mem 4879MB
[2022-05-30 22:41:36 MetaFG_0] (main.py 265): INFO Train: [2/300][1320/1562]	eta 0:01:13 lr 0.000001	time 0.2918 (0.3047)	loss 2.2891 (2.3072)	grad_norm 8.1089 (11.2195)	mem 4879MB
[2022-05-30 22:41:39 MetaFG_0] (main.py 265): INFO Train: [2/300][1330/1562]	eta 0:01:10 lr 0.000001	time 0.2992 (0.3047)	loss 2.2997 (2.3072)	grad_norm 11.7436 (11.2199)	mem 4879MB
[2022-05-30 22:41:42 MetaFG_0] (main.py 265): INFO Train: [2/300][1340/1562]	eta 0:01:07 lr 0.000001	time 0.2921 (0.3047)	loss 2.3071 (2.3072)	grad_norm 6.5029 (11.2169)	mem 4879MB
[2022-05-30 22:41:45 MetaFG_0] (main.py 265): INFO Train: [2/300][1350/1562]	eta 0:01:04 lr 0.000001	time 0.2923 (0.3047)	loss 2.3037 (2.3071)	grad_norm 8.7312 (11.2006)	mem 4879MB
[2022-05-30 22:41:48 MetaFG_0] (main.py 265): INFO Train: [2/300][1360/1562]	eta 0:01:01 lr 0.000001	time 0.2924 (0.3047)	loss 2.3206 (2.3071)	grad_norm 8.3943 (11.1801)	mem 4879MB
[2022-05-30 22:41:51 MetaFG_0] (main.py 265): INFO Train: [2/300][1370/1562]	eta 0:00:58 lr 0.000001	time 0.2981 (0.3047)	loss 2.2861 (2.3069)	grad_norm 11.0045 (11.1780)	mem 4879MB
[2022-05-30 22:41:54 MetaFG_0] (main.py 265): INFO Train: [2/300][1380/1562]	eta 0:00:55 lr 0.000001	time 0.2931 (0.3047)	loss 2.2852 (2.3069)	grad_norm 7.9031 (11.1712)	mem 4879MB
[2022-05-30 22:41:57 MetaFG_0] (main.py 265): INFO Train: [2/300][1390/1562]	eta 0:00:52 lr 0.000001	time 0.2998 (0.3047)	loss 2.2814 (2.3068)	grad_norm 9.2029 (11.1614)	mem 4879MB
[2022-05-30 22:42:00 MetaFG_0] (main.py 265): INFO Train: [2/300][1400/1562]	eta 0:00:49 lr 0.000001	time 0.2931 (0.3048)	loss 2.2755 (2.3067)	grad_norm 10.5670 (11.1545)	mem 4879MB
[2022-05-30 22:42:03 MetaFG_0] (main.py 265): INFO Train: [2/300][1410/1562]	eta 0:00:46 lr 0.000001	time 0.2963 (0.3048)	loss 2.3338 (2.3066)	grad_norm 9.9054 (11.1398)	mem 4879MB
[2022-05-30 22:42:06 MetaFG_0] (main.py 265): INFO Train: [2/300][1420/1562]	eta 0:00:43 lr 0.000001	time 0.2928 (0.3048)	loss 2.3138 (2.3066)	grad_norm 9.8110 (11.1291)	mem 4879MB
[2022-05-30 22:42:09 MetaFG_0] (main.py 265): INFO Train: [2/300][1430/1562]	eta 0:00:40 lr 0.000001	time 0.2920 (0.3047)	loss 2.3212 (2.3066)	grad_norm 8.6556 (11.1049)	mem 4879MB
[2022-05-30 22:42:13 MetaFG_0] (main.py 265): INFO Train: [2/300][1440/1562]	eta 0:00:37 lr 0.000001	time 0.2917 (0.3047)	loss 2.2922 (2.3065)	grad_norm 5.4081 (11.0820)	mem 4879MB
[2022-05-30 22:42:16 MetaFG_0] (main.py 265): INFO Train: [2/300][1450/1562]	eta 0:00:34 lr 0.000001	time 0.2919 (0.3047)	loss 2.2717 (2.3065)	grad_norm 9.2226 (11.0729)	mem 4879MB
[2022-05-30 22:42:19 MetaFG_0] (main.py 265): INFO Train: [2/300][1460/1562]	eta 0:00:31 lr 0.000001	time 0.2933 (0.3047)	loss 2.2848 (2.3064)	grad_norm 8.7156 (11.0600)	mem 4879MB
[2022-05-30 22:42:22 MetaFG_0] (main.py 265): INFO Train: [2/300][1470/1562]	eta 0:00:28 lr 0.000001	time 0.2984 (0.3047)	loss 2.3017 (2.3064)	grad_norm 7.8393 (11.0541)	mem 4879MB
[2022-05-30 22:42:25 MetaFG_0] (main.py 265): INFO Train: [2/300][1480/1562]	eta 0:00:24 lr 0.000001	time 0.2978 (0.3047)	loss 2.3082 (2.3064)	grad_norm 8.7823 (11.0359)	mem 4879MB
[2022-05-30 22:42:28 MetaFG_0] (main.py 265): INFO Train: [2/300][1490/1562]	eta 0:00:21 lr 0.000001	time 0.2987 (0.3047)	loss 2.3296 (2.3063)	grad_norm 6.5982 (11.0215)	mem 4879MB
[2022-05-30 22:42:31 MetaFG_0] (main.py 265): INFO Train: [2/300][1500/1562]	eta 0:00:18 lr 0.000001	time 0.2955 (0.3047)	loss 2.3154 (2.3062)	grad_norm 9.0613 (11.0107)	mem 4879MB
[2022-05-30 22:42:34 MetaFG_0] (main.py 265): INFO Train: [2/300][1510/1562]	eta 0:00:15 lr 0.000001	time 0.2988 (0.3047)	loss 2.3252 (2.3062)	grad_norm 7.3692 (10.9938)	mem 4879MB
[2022-05-30 22:42:37 MetaFG_0] (main.py 265): INFO Train: [2/300][1520/1562]	eta 0:00:12 lr 0.000001	time 0.2919 (0.3047)	loss 2.2923 (2.3062)	grad_norm 12.0240 (10.9873)	mem 4879MB
[2022-05-30 22:42:40 MetaFG_0] (main.py 265): INFO Train: [2/300][1530/1562]	eta 0:00:09 lr 0.000001	time 0.2937 (0.3047)	loss 2.3152 (2.3062)	grad_norm 10.1974 (10.9816)	mem 4879MB
[2022-05-30 22:42:43 MetaFG_0] (main.py 265): INFO Train: [2/300][1540/1562]	eta 0:00:06 lr 0.000001	time 0.2920 (0.3047)	loss 2.2980 (2.3061)	grad_norm 7.7559 (10.9668)	mem 4879MB
[2022-05-30 22:42:46 MetaFG_0] (main.py 265): INFO Train: [2/300][1550/1562]	eta 0:00:03 lr 0.000001	time 0.2984 (0.3047)	loss 2.3257 (2.3061)	grad_norm 7.4271 (10.9552)	mem 4879MB
[2022-05-30 22:42:49 MetaFG_0] (main.py 265): INFO Train: [2/300][1560/1562]	eta 0:00:00 lr 0.000001	time 0.2913 (0.3046)	loss 2.2995 (2.3059)	grad_norm 7.2361 (10.9487)	mem 4879MB
[2022-05-30 22:42:49 MetaFG_0] (main.py 272): INFO EPOCH 2 training takes 0:07:55
[2022-05-30 22:42:49 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_2.pth saving......
[2022-05-30 22:42:50 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_2.pth saved !!!
[2022-05-30 22:42:50 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:42:52 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:42:52 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:42:53 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.677 (0.677)	Loss 4.5668 (4.5668)	Acc@1 3.125 (3.125)	Acc@5 12.500 (12.500)	Mem 4879MB
[2022-05-30 22:42:54 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.153)	Loss 4.5635 (4.5363)	Acc@1 3.125 (4.261)	Acc@5 6.250 (11.648)	Mem 4879MB
[2022-05-30 22:42:54 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.125)	Loss 4.5190 (4.5317)	Acc@1 0.000 (4.167)	Acc@5 18.750 (13.095)	Mem 4879MB
[2022-05-30 22:42:55 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.095 (0.115)	Loss 4.5921 (4.5367)	Acc@1 3.125 (3.528)	Acc@5 6.250 (11.895)	Mem 4879MB
[2022-05-30 22:42:56 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.091 (0.109)	Loss 4.5400 (4.5451)	Acc@1 3.125 (2.896)	Acc@5 9.375 (10.747)	Mem 4879MB
[2022-05-30 22:42:57 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.105)	Loss 4.5925 (4.5455)	Acc@1 0.000 (3.002)	Acc@5 0.000 (10.539)	Mem 4879MB
[2022-05-30 22:42:58 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.104)	Loss 4.4847 (4.5407)	Acc@1 3.125 (3.074)	Acc@5 3.125 (10.758)	Mem 4879MB
[2022-05-30 22:42:59 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.102 (0.103)	Loss 4.5174 (4.5396)	Acc@1 0.000 (2.905)	Acc@5 3.125 (10.475)	Mem 4879MB
[2022-05-30 22:43:00 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.101)	Loss 4.5567 (4.5397)	Acc@1 3.125 (2.816)	Acc@5 6.250 (10.455)	Mem 4879MB
[2022-05-30 22:43:01 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.095 (0.100)	Loss 4.4542 (4.5376)	Acc@1 3.125 (2.747)	Acc@5 9.375 (10.508)	Mem 4879MB
[2022-05-30 22:43:02 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.095 (0.100)	Loss 4.5366 (4.5384)	Acc@1 0.000 (2.599)	Acc@5 15.625 (10.520)	Mem 4879MB
[2022-05-30 22:43:03 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.099 (0.099)	Loss 4.5486 (4.5393)	Acc@1 0.000 (2.562)	Acc@5 9.375 (10.501)	Mem 4879MB
[2022-05-30 22:43:04 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.099)	Loss 4.5237 (4.5398)	Acc@1 6.250 (2.608)	Acc@5 15.625 (10.460)	Mem 4879MB
[2022-05-30 22:43:05 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.098)	Loss 4.6024 (4.5409)	Acc@1 0.000 (2.552)	Acc@5 9.375 (10.377)	Mem 4879MB
[2022-05-30 22:43:06 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.098)	Loss 4.5645 (4.5416)	Acc@1 3.125 (2.482)	Acc@5 9.375 (10.262)	Mem 4879MB
[2022-05-30 22:43:07 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 4.6419 (4.5429)	Acc@1 0.000 (2.421)	Acc@5 3.125 (10.141)	Mem 4879MB
[2022-05-30 22:43:07 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.097)	Loss 4.5565 (4.5425)	Acc@1 0.000 (2.465)	Acc@5 6.250 (10.210)	Mem 4879MB
[2022-05-30 22:43:08 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.097)	Loss 4.4965 (4.5417)	Acc@1 3.125 (2.431)	Acc@5 12.500 (10.106)	Mem 4879MB
[2022-05-30 22:43:09 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.100 (0.097)	Loss 4.4989 (4.5425)	Acc@1 0.000 (2.331)	Acc@5 12.500 (9.962)	Mem 4879MB
[2022-05-30 22:43:10 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 4.4592 (4.5438)	Acc@1 3.125 (2.291)	Acc@5 18.750 (9.866)	Mem 4879MB
[2022-05-30 22:43:11 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.088 (0.096)	Loss 4.5236 (4.5451)	Acc@1 3.125 (2.301)	Acc@5 3.125 (9.748)	Mem 4879MB
[2022-05-30 22:43:12 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.096)	Loss 4.5509 (4.5458)	Acc@1 3.125 (2.296)	Acc@5 9.375 (9.730)	Mem 4879MB
[2022-05-30 22:43:13 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.096)	Loss 4.6245 (4.5469)	Acc@1 3.125 (2.262)	Acc@5 3.125 (9.601)	Mem 4879MB
[2022-05-30 22:43:14 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.091 (0.096)	Loss 4.6074 (4.5468)	Acc@1 0.000 (2.246)	Acc@5 3.125 (9.483)	Mem 4879MB
[2022-05-30 22:43:15 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.093 (0.096)	Loss 4.5581 (4.5473)	Acc@1 3.125 (2.191)	Acc@5 12.500 (9.388)	Mem 4879MB
[2022-05-30 22:43:16 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.095 (0.096)	Loss 4.6056 (4.5480)	Acc@1 0.000 (2.154)	Acc@5 3.125 (9.300)	Mem 4879MB
[2022-05-30 22:43:17 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 4.5057 (4.5479)	Acc@1 3.125 (2.131)	Acc@5 12.500 (9.279)	Mem 4879MB
[2022-05-30 22:43:18 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.095)	Loss 4.5457 (4.5485)	Acc@1 0.000 (2.087)	Acc@5 3.125 (9.248)	Mem 4879MB
[2022-05-30 22:43:19 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.095)	Loss 4.5611 (4.5492)	Acc@1 0.000 (2.069)	Acc@5 9.375 (9.253)	Mem 4879MB
[2022-05-30 22:43:20 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.095)	Loss 4.4779 (4.5489)	Acc@1 0.000 (2.051)	Acc@5 12.500 (9.192)	Mem 4879MB
[2022-05-30 22:43:20 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.095)	Loss 4.6561 (4.5490)	Acc@1 0.000 (2.076)	Acc@5 3.125 (9.199)	Mem 4879MB
[2022-05-30 22:43:21 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 4.6434 (4.5493)	Acc@1 0.000 (2.120)	Acc@5 3.125 (9.204)	Mem 4879MB
[2022-05-30 22:43:22 MetaFG_0] (main.py 330): INFO  * Acc@1 2.110 Acc@5 9.210
[2022-05-30 22:43:22 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 2.1%
[2022-05-30 22:43:22 MetaFG_0] (main.py 171): INFO Max accuracy: 2.11%
[2022-05-30 22:43:23 MetaFG_0] (main.py 265): INFO Train: [3/300][0/1562]	eta 0:26:11 lr 0.000001	time 1.0059 (1.0059)	loss 2.3126 (2.3126)	grad_norm 8.0533 (8.0533)	mem 4879MB
[2022-05-30 22:43:26 MetaFG_0] (main.py 265): INFO Train: [3/300][10/1562]	eta 0:09:37 lr 0.000001	time 0.2932 (0.3721)	loss 2.3028 (2.3044)	grad_norm 8.2472 (8.6801)	mem 4879MB
[2022-05-30 22:43:29 MetaFG_0] (main.py 265): INFO Train: [3/300][20/1562]	eta 0:08:43 lr 0.000001	time 0.3012 (0.3397)	loss 2.2913 (2.3005)	grad_norm 5.7758 (8.9834)	mem 4879MB
[2022-05-30 22:43:32 MetaFG_0] (main.py 265): INFO Train: [3/300][30/1562]	eta 0:08:22 lr 0.000001	time 0.2982 (0.3283)	loss 2.2965 (2.2996)	grad_norm 6.7084 (8.9126)	mem 4879MB
[2022-05-30 22:43:35 MetaFG_0] (main.py 265): INFO Train: [3/300][40/1562]	eta 0:08:10 lr 0.000001	time 0.2961 (0.3221)	loss 2.3084 (2.2994)	grad_norm 5.8411 (8.6753)	mem 4879MB
[2022-05-30 22:43:38 MetaFG_0] (main.py 265): INFO Train: [3/300][50/1562]	eta 0:08:01 lr 0.000001	time 0.3039 (0.3187)	loss 2.3165 (2.3000)	grad_norm 9.0361 (8.5917)	mem 4879MB
[2022-05-30 22:43:41 MetaFG_0] (main.py 265): INFO Train: [3/300][60/1562]	eta 0:07:54 lr 0.000001	time 0.2983 (0.3162)	loss 2.3046 (2.2996)	grad_norm 7.8798 (8.6404)	mem 4879MB
[2022-05-30 22:43:44 MetaFG_0] (main.py 265): INFO Train: [3/300][70/1562]	eta 0:07:49 lr 0.000001	time 0.2931 (0.3145)	loss 2.2996 (2.2992)	grad_norm 5.9039 (8.6080)	mem 4879MB
[2022-05-30 22:43:47 MetaFG_0] (main.py 265): INFO Train: [3/300][80/1562]	eta 0:07:44 lr 0.000001	time 0.2933 (0.3132)	loss 2.2995 (2.2990)	grad_norm 9.0932 (8.6628)	mem 4879MB
[2022-05-30 22:43:50 MetaFG_0] (main.py 265): INFO Train: [3/300][90/1562]	eta 0:07:39 lr 0.000001	time 0.2925 (0.3122)	loss 2.3000 (2.2990)	grad_norm 9.7076 (8.6297)	mem 4879MB
[2022-05-30 22:43:53 MetaFG_0] (main.py 265): INFO Train: [3/300][100/1562]	eta 0:07:35 lr 0.000001	time 0.2920 (0.3113)	loss 2.2989 (2.2987)	grad_norm 6.1930 (8.5725)	mem 4879MB
[2022-05-30 22:43:56 MetaFG_0] (main.py 265): INFO Train: [3/300][110/1562]	eta 0:07:30 lr 0.000001	time 0.2997 (0.3105)	loss 2.2960 (2.2989)	grad_norm 10.4263 (8.6250)	mem 4879MB
[2022-05-30 22:43:59 MetaFG_0] (main.py 265): INFO Train: [3/300][120/1562]	eta 0:07:26 lr 0.000001	time 0.2978 (0.3098)	loss 2.2805 (2.2987)	grad_norm 7.5548 (8.5560)	mem 4879MB
[2022-05-30 22:44:02 MetaFG_0] (main.py 265): INFO Train: [3/300][130/1562]	eta 0:07:22 lr 0.000001	time 0.2984 (0.3092)	loss 2.3097 (2.2986)	grad_norm 6.4506 (8.6282)	mem 4879MB
[2022-05-30 22:44:05 MetaFG_0] (main.py 265): INFO Train: [3/300][140/1562]	eta 0:07:19 lr 0.000001	time 0.2917 (0.3088)	loss 2.3171 (2.2989)	grad_norm 7.7566 (8.6451)	mem 4879MB
[2022-05-30 22:44:08 MetaFG_0] (main.py 265): INFO Train: [3/300][150/1562]	eta 0:07:15 lr 0.000001	time 0.2974 (0.3083)	loss 2.3113 (2.2989)	grad_norm 5.1696 (8.6147)	mem 4879MB
[2022-05-30 22:44:11 MetaFG_0] (main.py 265): INFO Train: [3/300][160/1562]	eta 0:07:11 lr 0.000001	time 0.2939 (0.3079)	loss 2.2973 (2.2990)	grad_norm 5.2430 (8.5621)	mem 4879MB
[2022-05-30 22:44:14 MetaFG_0] (main.py 265): INFO Train: [3/300][170/1562]	eta 0:07:08 lr 0.000001	time 0.2930 (0.3078)	loss 2.2843 (2.2988)	grad_norm 8.4712 (8.6051)	mem 4879MB
[2022-05-30 22:44:17 MetaFG_0] (main.py 265): INFO Train: [3/300][180/1562]	eta 0:07:05 lr 0.000001	time 0.2931 (0.3076)	loss 2.3140 (2.2986)	grad_norm 7.2386 (8.6710)	mem 4879MB
[2022-05-30 22:44:20 MetaFG_0] (main.py 265): INFO Train: [3/300][190/1562]	eta 0:07:01 lr 0.000001	time 0.2931 (0.3074)	loss 2.3090 (2.2978)	grad_norm 7.1169 (8.5960)	mem 4879MB
[2022-05-30 22:44:23 MetaFG_0] (main.py 265): INFO Train: [3/300][200/1562]	eta 0:06:59 lr 0.000001	time 0.2945 (0.3077)	loss 2.3043 (2.2978)	grad_norm 9.8371 (8.6404)	mem 4879MB
[2022-05-30 22:44:27 MetaFG_0] (main.py 265): INFO Train: [3/300][210/1562]	eta 0:06:55 lr 0.000001	time 0.3007 (0.3076)	loss 2.3175 (2.2979)	grad_norm 9.0904 (8.6804)	mem 4879MB
[2022-05-30 22:44:30 MetaFG_0] (main.py 265): INFO Train: [3/300][220/1562]	eta 0:06:52 lr 0.000001	time 0.2951 (0.3074)	loss 2.3091 (2.2977)	grad_norm 6.2071 (8.6766)	mem 4879MB
[2022-05-30 22:44:33 MetaFG_0] (main.py 265): INFO Train: [3/300][230/1562]	eta 0:06:49 lr 0.000001	time 0.2934 (0.3073)	loss 2.3183 (2.2975)	grad_norm 9.6103 (8.7004)	mem 4879MB
[2022-05-30 22:44:36 MetaFG_0] (main.py 265): INFO Train: [3/300][240/1562]	eta 0:06:46 lr 0.000001	time 0.2956 (0.3071)	loss 2.2975 (2.2972)	grad_norm 5.8100 (8.6887)	mem 4879MB
[2022-05-30 22:44:39 MetaFG_0] (main.py 265): INFO Train: [3/300][250/1562]	eta 0:06:42 lr 0.000001	time 0.2930 (0.3070)	loss 2.2890 (2.2972)	grad_norm 6.2346 (8.6819)	mem 4879MB
[2022-05-30 22:44:42 MetaFG_0] (main.py 265): INFO Train: [3/300][260/1562]	eta 0:06:39 lr 0.000001	time 0.2925 (0.3069)	loss 2.3038 (2.2971)	grad_norm 7.1225 (8.6537)	mem 4879MB
[2022-05-30 22:44:45 MetaFG_0] (main.py 265): INFO Train: [3/300][270/1562]	eta 0:06:36 lr 0.000001	time 0.2941 (0.3067)	loss 2.3034 (2.2972)	grad_norm 8.4826 (8.6460)	mem 4879MB
[2022-05-30 22:44:48 MetaFG_0] (main.py 265): INFO Train: [3/300][280/1562]	eta 0:06:33 lr 0.000001	time 0.2993 (0.3066)	loss 2.3318 (2.2973)	grad_norm 9.5135 (8.6645)	mem 4879MB
[2022-05-30 22:44:51 MetaFG_0] (main.py 265): INFO Train: [3/300][290/1562]	eta 0:06:29 lr 0.000001	time 0.2922 (0.3066)	loss 2.2984 (2.2972)	grad_norm 4.1970 (8.6027)	mem 4879MB
[2022-05-30 22:44:54 MetaFG_0] (main.py 265): INFO Train: [3/300][300/1562]	eta 0:06:26 lr 0.000001	time 0.2920 (0.3065)	loss 2.3057 (2.2973)	grad_norm 4.6888 (8.5605)	mem 4879MB
[2022-05-30 22:44:57 MetaFG_0] (main.py 265): INFO Train: [3/300][310/1562]	eta 0:06:23 lr 0.000001	time 0.2919 (0.3064)	loss 2.2856 (2.2970)	grad_norm 4.6183 (8.5301)	mem 4879MB
[2022-05-30 22:45:00 MetaFG_0] (main.py 265): INFO Train: [3/300][320/1562]	eta 0:06:20 lr 0.000001	time 0.2920 (0.3064)	loss 2.3037 (2.2971)	grad_norm 12.6100 (8.5222)	mem 4879MB
[2022-05-30 22:45:03 MetaFG_0] (main.py 265): INFO Train: [3/300][330/1562]	eta 0:06:17 lr 0.000001	time 0.2984 (0.3062)	loss 2.3016 (2.2971)	grad_norm 10.9720 (8.5522)	mem 4879MB
[2022-05-30 22:45:06 MetaFG_0] (main.py 265): INFO Train: [3/300][340/1562]	eta 0:06:14 lr 0.000001	time 0.2928 (0.3061)	loss 2.3078 (2.2969)	grad_norm 13.1286 (8.5628)	mem 4879MB
[2022-05-30 22:45:09 MetaFG_0] (main.py 265): INFO Train: [3/300][350/1562]	eta 0:06:10 lr 0.000001	time 0.2923 (0.3060)	loss 2.2991 (2.2966)	grad_norm 4.9552 (8.5834)	mem 4879MB
[2022-05-30 22:45:12 MetaFG_0] (main.py 265): INFO Train: [3/300][360/1562]	eta 0:06:07 lr 0.000001	time 0.2927 (0.3060)	loss 2.2710 (2.2966)	grad_norm 9.5311 (8.5746)	mem 4879MB
[2022-05-30 22:45:15 MetaFG_0] (main.py 265): INFO Train: [3/300][370/1562]	eta 0:06:04 lr 0.000001	time 0.2930 (0.3059)	loss 2.2856 (2.2966)	grad_norm 7.3456 (8.5675)	mem 4879MB
[2022-05-30 22:45:18 MetaFG_0] (main.py 265): INFO Train: [3/300][380/1562]	eta 0:06:01 lr 0.000001	time 0.2978 (0.3059)	loss 2.3082 (2.2966)	grad_norm 5.7190 (8.5993)	mem 4879MB
[2022-05-30 22:45:21 MetaFG_0] (main.py 265): INFO Train: [3/300][390/1562]	eta 0:05:58 lr 0.000001	time 0.2919 (0.3058)	loss 2.3017 (2.2965)	grad_norm 4.7860 (8.5765)	mem 4879MB
[2022-05-30 22:45:24 MetaFG_0] (main.py 265): INFO Train: [3/300][400/1562]	eta 0:05:55 lr 0.000001	time 0.2924 (0.3058)	loss 2.2592 (2.2965)	grad_norm 8.7859 (8.6029)	mem 4879MB
[2022-05-30 22:45:27 MetaFG_0] (main.py 265): INFO Train: [3/300][410/1562]	eta 0:05:52 lr 0.000001	time 0.2978 (0.3057)	loss 2.3057 (2.2965)	grad_norm 6.6360 (8.6089)	mem 4879MB
[2022-05-30 22:45:30 MetaFG_0] (main.py 265): INFO Train: [3/300][420/1562]	eta 0:05:49 lr 0.000001	time 0.2979 (0.3057)	loss 2.3234 (2.2968)	grad_norm 9.6281 (8.6135)	mem 4879MB
[2022-05-30 22:45:33 MetaFG_0] (main.py 265): INFO Train: [3/300][430/1562]	eta 0:05:46 lr 0.000001	time 0.3003 (0.3057)	loss 2.3169 (2.2969)	grad_norm 9.7825 (8.6196)	mem 4879MB
[2022-05-30 22:45:36 MetaFG_0] (main.py 265): INFO Train: [3/300][440/1562]	eta 0:05:42 lr 0.000001	time 0.2922 (0.3057)	loss 2.3020 (2.2969)	grad_norm 7.9504 (8.6268)	mem 4879MB
[2022-05-30 22:45:39 MetaFG_0] (main.py 265): INFO Train: [3/300][450/1562]	eta 0:05:39 lr 0.000001	time 0.2982 (0.3056)	loss 2.2845 (2.2969)	grad_norm 8.7299 (8.6287)	mem 4879MB
[2022-05-30 22:45:43 MetaFG_0] (main.py 265): INFO Train: [3/300][460/1562]	eta 0:05:36 lr 0.000001	time 0.2920 (0.3056)	loss 2.2922 (2.2968)	grad_norm 9.4073 (8.6213)	mem 4879MB
[2022-05-30 22:45:46 MetaFG_0] (main.py 265): INFO Train: [3/300][470/1562]	eta 0:05:33 lr 0.000001	time 0.2994 (0.3055)	loss 2.2950 (2.2967)	grad_norm 5.6825 (8.6295)	mem 4879MB
[2022-05-30 22:45:49 MetaFG_0] (main.py 265): INFO Train: [3/300][480/1562]	eta 0:05:30 lr 0.000001	time 0.2977 (0.3054)	loss 2.2967 (2.2967)	grad_norm 12.3736 (8.6363)	mem 4879MB
[2022-05-30 22:45:52 MetaFG_0] (main.py 265): INFO Train: [3/300][490/1562]	eta 0:05:27 lr 0.000001	time 0.2922 (0.3054)	loss 2.2703 (2.2966)	grad_norm 6.5292 (8.6428)	mem 4879MB
[2022-05-30 22:45:55 MetaFG_0] (main.py 265): INFO Train: [3/300][500/1562]	eta 0:05:24 lr 0.000001	time 0.2979 (0.3054)	loss 2.2832 (2.2965)	grad_norm 7.2504 (8.6455)	mem 4879MB
[2022-05-30 22:45:58 MetaFG_0] (main.py 265): INFO Train: [3/300][510/1562]	eta 0:05:21 lr 0.000001	time 0.2988 (0.3054)	loss 2.2808 (2.2965)	grad_norm 7.4304 (8.6302)	mem 4879MB
[2022-05-30 22:46:01 MetaFG_0] (main.py 265): INFO Train: [3/300][520/1562]	eta 0:05:18 lr 0.000001	time 0.3002 (0.3054)	loss 2.3110 (2.2966)	grad_norm 6.0966 (8.6297)	mem 4879MB
[2022-05-30 22:46:04 MetaFG_0] (main.py 265): INFO Train: [3/300][530/1562]	eta 0:05:15 lr 0.000001	time 0.2919 (0.3053)	loss 2.2964 (2.2966)	grad_norm 6.8364 (8.6296)	mem 4879MB
[2022-05-30 22:46:07 MetaFG_0] (main.py 265): INFO Train: [3/300][540/1562]	eta 0:05:12 lr 0.000001	time 0.2972 (0.3053)	loss 2.2933 (2.2964)	grad_norm 7.3000 (8.6398)	mem 4879MB
[2022-05-30 22:46:10 MetaFG_0] (main.py 265): INFO Train: [3/300][550/1562]	eta 0:05:08 lr 0.000001	time 0.2924 (0.3053)	loss 2.2707 (2.2963)	grad_norm 8.4202 (8.6248)	mem 4879MB
[2022-05-30 22:46:13 MetaFG_0] (main.py 265): INFO Train: [3/300][560/1562]	eta 0:05:05 lr 0.000001	time 0.2920 (0.3053)	loss 2.2761 (2.2962)	grad_norm 7.4115 (8.6108)	mem 4879MB
[2022-05-30 22:46:16 MetaFG_0] (main.py 265): INFO Train: [3/300][570/1562]	eta 0:05:02 lr 0.000001	time 0.2933 (0.3053)	loss 2.2892 (2.2962)	grad_norm 5.5047 (8.6237)	mem 4879MB
[2022-05-30 22:46:19 MetaFG_0] (main.py 265): INFO Train: [3/300][580/1562]	eta 0:04:59 lr 0.000001	time 0.2920 (0.3053)	loss 2.2974 (2.2961)	grad_norm 7.6182 (8.6250)	mem 4879MB
[2022-05-30 22:46:22 MetaFG_0] (main.py 265): INFO Train: [3/300][590/1562]	eta 0:04:56 lr 0.000001	time 0.2935 (0.3052)	loss 2.2904 (2.2960)	grad_norm 7.8284 (8.6157)	mem 4879MB
[2022-05-30 22:46:25 MetaFG_0] (main.py 265): INFO Train: [3/300][600/1562]	eta 0:04:53 lr 0.000001	time 0.2961 (0.3052)	loss 2.3157 (2.2960)	grad_norm 7.6135 (8.6101)	mem 4879MB
[2022-05-30 22:46:28 MetaFG_0] (main.py 265): INFO Train: [3/300][610/1562]	eta 0:04:50 lr 0.000001	time 0.2990 (0.3052)	loss 2.3176 (2.2961)	grad_norm 8.5187 (8.5963)	mem 4879MB
[2022-05-30 22:46:31 MetaFG_0] (main.py 265): INFO Train: [3/300][620/1562]	eta 0:04:47 lr 0.000001	time 0.2952 (0.3052)	loss 2.2999 (2.2960)	grad_norm 6.2129 (8.5759)	mem 4879MB
[2022-05-30 22:46:34 MetaFG_0] (main.py 265): INFO Train: [3/300][630/1562]	eta 0:04:44 lr 0.000001	time 0.3027 (0.3052)	loss 2.2834 (2.2959)	grad_norm 8.5963 (8.5600)	mem 4879MB
[2022-05-30 22:46:37 MetaFG_0] (main.py 265): INFO Train: [3/300][640/1562]	eta 0:04:41 lr 0.000001	time 0.2943 (0.3052)	loss 2.3034 (2.2959)	grad_norm 7.1868 (8.5590)	mem 4879MB
[2022-05-30 22:46:40 MetaFG_0] (main.py 265): INFO Train: [3/300][650/1562]	eta 0:04:38 lr 0.000001	time 0.2921 (0.3051)	loss 2.3122 (2.2959)	grad_norm 6.1939 (8.5600)	mem 4879MB
[2022-05-30 22:46:43 MetaFG_0] (main.py 265): INFO Train: [3/300][660/1562]	eta 0:04:35 lr 0.000001	time 0.2983 (0.3051)	loss 2.2895 (2.2958)	grad_norm 8.1689 (8.5711)	mem 4879MB
[2022-05-30 22:46:46 MetaFG_0] (main.py 265): INFO Train: [3/300][670/1562]	eta 0:04:32 lr 0.000001	time 0.2919 (0.3052)	loss 2.3005 (2.2958)	grad_norm 5.2513 (8.5663)	mem 4879MB
[2022-05-30 22:46:49 MetaFG_0] (main.py 265): INFO Train: [3/300][680/1562]	eta 0:04:29 lr 0.000001	time 0.2920 (0.3052)	loss 2.3029 (2.2958)	grad_norm 10.8733 (8.5597)	mem 4879MB
[2022-05-30 22:46:52 MetaFG_0] (main.py 265): INFO Train: [3/300][690/1562]	eta 0:04:26 lr 0.000001	time 0.2979 (0.3051)	loss 2.3028 (2.2957)	grad_norm 6.3285 (8.5858)	mem 4879MB
[2022-05-30 22:46:56 MetaFG_0] (main.py 265): INFO Train: [3/300][700/1562]	eta 0:04:23 lr 0.000001	time 0.2991 (0.3051)	loss 2.2918 (2.2956)	grad_norm 8.3847 (8.5892)	mem 4879MB
[2022-05-30 22:46:59 MetaFG_0] (main.py 265): INFO Train: [3/300][710/1562]	eta 0:04:19 lr 0.000001	time 0.2928 (0.3051)	loss 2.2847 (2.2956)	grad_norm 5.0280 (8.5893)	mem 4879MB
[2022-05-30 22:47:02 MetaFG_0] (main.py 265): INFO Train: [3/300][720/1562]	eta 0:04:16 lr 0.000001	time 0.2979 (0.3051)	loss 2.2850 (2.2956)	grad_norm 9.7804 (8.5750)	mem 4879MB
[2022-05-30 22:47:05 MetaFG_0] (main.py 265): INFO Train: [3/300][730/1562]	eta 0:04:13 lr 0.000001	time 0.2996 (0.3051)	loss 2.3036 (2.2956)	grad_norm 6.8558 (8.5731)	mem 4879MB
[2022-05-30 22:47:08 MetaFG_0] (main.py 265): INFO Train: [3/300][740/1562]	eta 0:04:10 lr 0.000001	time 0.2993 (0.3051)	loss 2.3031 (2.2956)	grad_norm 6.0581 (8.6008)	mem 4879MB
[2022-05-30 22:47:11 MetaFG_0] (main.py 265): INFO Train: [3/300][750/1562]	eta 0:04:07 lr 0.000001	time 0.3002 (0.3051)	loss 2.2856 (2.2955)	grad_norm 8.9491 (8.5992)	mem 4879MB
[2022-05-30 22:47:14 MetaFG_0] (main.py 265): INFO Train: [3/300][760/1562]	eta 0:04:04 lr 0.000001	time 0.2985 (0.3051)	loss 2.2708 (2.2955)	grad_norm 11.1472 (8.5981)	mem 4879MB
[2022-05-30 22:47:17 MetaFG_0] (main.py 265): INFO Train: [3/300][770/1562]	eta 0:04:01 lr 0.000001	time 0.2931 (0.3050)	loss 2.3174 (2.2955)	grad_norm 10.5175 (8.6019)	mem 4879MB
[2022-05-30 22:47:20 MetaFG_0] (main.py 265): INFO Train: [3/300][780/1562]	eta 0:03:58 lr 0.000001	time 0.2916 (0.3050)	loss 2.2940 (2.2955)	grad_norm 6.0920 (8.6043)	mem 4879MB
[2022-05-30 22:47:23 MetaFG_0] (main.py 265): INFO Train: [3/300][790/1562]	eta 0:03:55 lr 0.000001	time 0.2932 (0.3050)	loss 2.2929 (2.2955)	grad_norm 5.7863 (8.6011)	mem 4879MB
[2022-05-30 22:47:26 MetaFG_0] (main.py 265): INFO Train: [3/300][800/1562]	eta 0:03:52 lr 0.000001	time 0.2994 (0.3050)	loss 2.2959 (2.2955)	grad_norm 9.2553 (8.6140)	mem 4879MB
[2022-05-30 22:47:29 MetaFG_0] (main.py 265): INFO Train: [3/300][810/1562]	eta 0:03:49 lr 0.000001	time 0.2921 (0.3050)	loss 2.2699 (2.2954)	grad_norm 8.8989 (8.6164)	mem 4879MB
[2022-05-30 22:47:32 MetaFG_0] (main.py 265): INFO Train: [3/300][820/1562]	eta 0:03:46 lr 0.000001	time 0.2978 (0.3050)	loss 2.2829 (2.2953)	grad_norm 5.8724 (8.6062)	mem 4879MB
[2022-05-30 22:47:35 MetaFG_0] (main.py 265): INFO Train: [3/300][830/1562]	eta 0:03:43 lr 0.000001	time 0.2920 (0.3049)	loss 2.3107 (2.2953)	grad_norm 8.8210 (8.6115)	mem 4879MB
[2022-05-30 22:47:38 MetaFG_0] (main.py 265): INFO Train: [3/300][840/1562]	eta 0:03:40 lr 0.000001	time 0.2925 (0.3049)	loss 2.2759 (2.2953)	grad_norm 7.6128 (8.6225)	mem 4879MB
[2022-05-30 22:47:41 MetaFG_0] (main.py 265): INFO Train: [3/300][850/1562]	eta 0:03:37 lr 0.000001	time 0.2981 (0.3049)	loss 2.2894 (2.2952)	grad_norm 6.5855 (8.6212)	mem 4879MB
[2022-05-30 22:47:44 MetaFG_0] (main.py 265): INFO Train: [3/300][860/1562]	eta 0:03:34 lr 0.000001	time 0.2939 (0.3049)	loss 2.2844 (2.2952)	grad_norm 5.0865 (8.6277)	mem 4879MB
[2022-05-30 22:47:47 MetaFG_0] (main.py 265): INFO Train: [3/300][870/1562]	eta 0:03:30 lr 0.000001	time 0.2919 (0.3049)	loss 2.2988 (2.2951)	grad_norm 5.8323 (8.6303)	mem 4879MB
[2022-05-30 22:47:50 MetaFG_0] (main.py 265): INFO Train: [3/300][880/1562]	eta 0:03:27 lr 0.000001	time 0.2981 (0.3049)	loss 2.2930 (2.2950)	grad_norm 6.8572 (8.6328)	mem 4879MB
[2022-05-30 22:47:53 MetaFG_0] (main.py 265): INFO Train: [3/300][890/1562]	eta 0:03:24 lr 0.000001	time 0.2926 (0.3049)	loss 2.2782 (2.2949)	grad_norm 5.3534 (8.6313)	mem 4879MB
[2022-05-30 22:47:56 MetaFG_0] (main.py 265): INFO Train: [3/300][900/1562]	eta 0:03:21 lr 0.000001	time 0.3000 (0.3049)	loss 2.2823 (2.2949)	grad_norm 7.2042 (8.6465)	mem 4879MB
[2022-05-30 22:47:59 MetaFG_0] (main.py 265): INFO Train: [3/300][910/1562]	eta 0:03:18 lr 0.000001	time 0.2974 (0.3049)	loss 2.2914 (2.2948)	grad_norm 6.0907 (8.6534)	mem 4879MB
[2022-05-30 22:48:02 MetaFG_0] (main.py 265): INFO Train: [3/300][920/1562]	eta 0:03:15 lr 0.000001	time 0.2919 (0.3048)	loss 2.2888 (2.2948)	grad_norm 7.9410 (8.6651)	mem 4879MB
[2022-05-30 22:48:05 MetaFG_0] (main.py 265): INFO Train: [3/300][930/1562]	eta 0:03:12 lr 0.000001	time 0.2985 (0.3049)	loss 2.2999 (2.2947)	grad_norm 7.4633 (8.6729)	mem 4879MB
[2022-05-30 22:48:08 MetaFG_0] (main.py 265): INFO Train: [3/300][940/1562]	eta 0:03:09 lr 0.000001	time 0.2930 (0.3048)	loss 2.3178 (2.2947)	grad_norm 9.1004 (8.6766)	mem 4879MB
[2022-05-30 22:48:12 MetaFG_0] (main.py 265): INFO Train: [3/300][950/1562]	eta 0:03:06 lr 0.000001	time 0.2990 (0.3048)	loss 2.2703 (2.2946)	grad_norm 6.2570 (8.6730)	mem 4879MB
[2022-05-30 22:48:15 MetaFG_0] (main.py 265): INFO Train: [3/300][960/1562]	eta 0:03:03 lr 0.000001	time 0.2915 (0.3048)	loss 2.2716 (2.2945)	grad_norm 6.9636 (8.6686)	mem 4879MB
[2022-05-30 22:48:18 MetaFG_0] (main.py 265): INFO Train: [3/300][970/1562]	eta 0:03:00 lr 0.000001	time 0.3002 (0.3048)	loss 2.2794 (2.2944)	grad_norm 10.0215 (8.6722)	mem 4879MB
[2022-05-30 22:48:21 MetaFG_0] (main.py 265): INFO Train: [3/300][980/1562]	eta 0:02:57 lr 0.000001	time 0.3003 (0.3048)	loss 2.2946 (2.2944)	grad_norm 4.7260 (8.6589)	mem 4879MB
[2022-05-30 22:48:24 MetaFG_0] (main.py 265): INFO Train: [3/300][990/1562]	eta 0:02:54 lr 0.000001	time 0.2938 (0.3048)	loss 2.3154 (2.2944)	grad_norm 5.4473 (8.6700)	mem 4879MB
[2022-05-30 22:48:27 MetaFG_0] (main.py 265): INFO Train: [3/300][1000/1562]	eta 0:02:51 lr 0.000001	time 0.3013 (0.3048)	loss 2.2870 (2.2943)	grad_norm 5.1205 (8.6575)	mem 4879MB
[2022-05-30 22:48:30 MetaFG_0] (main.py 265): INFO Train: [3/300][1010/1562]	eta 0:02:48 lr 0.000001	time 0.2923 (0.3048)	loss 2.2974 (2.2943)	grad_norm 4.9495 (8.6611)	mem 4879MB
[2022-05-30 22:48:33 MetaFG_0] (main.py 265): INFO Train: [3/300][1020/1562]	eta 0:02:45 lr 0.000001	time 0.2979 (0.3048)	loss 2.2916 (2.2942)	grad_norm 8.0590 (8.6625)	mem 4879MB
[2022-05-30 22:48:36 MetaFG_0] (main.py 265): INFO Train: [3/300][1030/1562]	eta 0:02:42 lr 0.000001	time 0.2922 (0.3048)	loss 2.2965 (2.2941)	grad_norm 7.3141 (8.6659)	mem 4879MB
[2022-05-30 22:48:39 MetaFG_0] (main.py 265): INFO Train: [3/300][1040/1562]	eta 0:02:39 lr 0.000001	time 0.2991 (0.3047)	loss 2.2837 (2.2941)	grad_norm 6.8369 (8.6665)	mem 4879MB
[2022-05-30 22:48:42 MetaFG_0] (main.py 265): INFO Train: [3/300][1050/1562]	eta 0:02:36 lr 0.000001	time 0.2922 (0.3047)	loss 2.2857 (2.2940)	grad_norm 6.9746 (8.6597)	mem 4879MB
[2022-05-30 22:48:45 MetaFG_0] (main.py 265): INFO Train: [3/300][1060/1562]	eta 0:02:32 lr 0.000001	time 0.2916 (0.3047)	loss 2.3034 (2.2939)	grad_norm 6.4308 (8.6552)	mem 4879MB
[2022-05-30 22:48:48 MetaFG_0] (main.py 265): INFO Train: [3/300][1070/1562]	eta 0:02:29 lr 0.000001	time 0.2978 (0.3047)	loss 2.2742 (2.2938)	grad_norm 7.0886 (8.6471)	mem 4879MB
[2022-05-30 22:48:51 MetaFG_0] (main.py 265): INFO Train: [3/300][1080/1562]	eta 0:02:26 lr 0.000001	time 0.2920 (0.3047)	loss 2.3093 (2.2938)	grad_norm 7.1977 (8.6361)	mem 4879MB
[2022-05-30 22:48:54 MetaFG_0] (main.py 265): INFO Train: [3/300][1090/1562]	eta 0:02:23 lr 0.000001	time 0.2921 (0.3047)	loss 2.2686 (2.2938)	grad_norm 5.7035 (8.6320)	mem 4879MB
[2022-05-30 22:48:57 MetaFG_0] (main.py 265): INFO Train: [3/300][1100/1562]	eta 0:02:20 lr 0.000001	time 0.2979 (0.3047)	loss 2.2537 (2.2937)	grad_norm 11.5306 (8.6365)	mem 4879MB
[2022-05-30 22:49:00 MetaFG_0] (main.py 265): INFO Train: [3/300][1110/1562]	eta 0:02:17 lr 0.000001	time 0.2930 (0.3047)	loss 2.2726 (2.2936)	grad_norm 8.4682 (8.6508)	mem 4879MB
[2022-05-30 22:49:03 MetaFG_0] (main.py 265): INFO Train: [3/300][1120/1562]	eta 0:02:14 lr 0.000001	time 0.3087 (0.3048)	loss 2.3319 (2.2936)	grad_norm 6.8963 (8.6458)	mem 4879MB
[2022-05-30 22:49:06 MetaFG_0] (main.py 265): INFO Train: [3/300][1130/1562]	eta 0:02:11 lr 0.000001	time 0.2977 (0.3048)	loss 2.2883 (2.2936)	grad_norm 6.7057 (8.6413)	mem 4879MB
[2022-05-30 22:49:09 MetaFG_0] (main.py 265): INFO Train: [3/300][1140/1562]	eta 0:02:08 lr 0.000001	time 0.2945 (0.3048)	loss 2.2777 (2.2937)	grad_norm 7.7142 (8.6371)	mem 4879MB
[2022-05-30 22:49:12 MetaFG_0] (main.py 265): INFO Train: [3/300][1150/1562]	eta 0:02:05 lr 0.000001	time 0.2944 (0.3048)	loss 2.2998 (2.2936)	grad_norm 15.6061 (8.6496)	mem 4879MB
[2022-05-30 22:49:15 MetaFG_0] (main.py 265): INFO Train: [3/300][1160/1562]	eta 0:02:02 lr 0.000001	time 0.2967 (0.3048)	loss 2.3000 (2.2936)	grad_norm 6.8789 (8.6443)	mem 4879MB
[2022-05-30 22:49:19 MetaFG_0] (main.py 265): INFO Train: [3/300][1170/1562]	eta 0:01:59 lr 0.000001	time 0.3009 (0.3048)	loss 2.3066 (2.2936)	grad_norm 8.7173 (8.6412)	mem 4879MB
[2022-05-30 22:49:22 MetaFG_0] (main.py 265): INFO Train: [3/300][1180/1562]	eta 0:01:56 lr 0.000001	time 0.2924 (0.3047)	loss 2.2839 (2.2935)	grad_norm 7.1821 (8.6398)	mem 4879MB
[2022-05-30 22:49:25 MetaFG_0] (main.py 265): INFO Train: [3/300][1190/1562]	eta 0:01:53 lr 0.000001	time 0.2930 (0.3047)	loss 2.2727 (2.2935)	grad_norm 7.3674 (8.6436)	mem 4879MB
[2022-05-30 22:49:28 MetaFG_0] (main.py 265): INFO Train: [3/300][1200/1562]	eta 0:01:50 lr 0.000001	time 0.2931 (0.3047)	loss 2.2812 (2.2935)	grad_norm 17.2610 (8.6444)	mem 4879MB
[2022-05-30 22:49:31 MetaFG_0] (main.py 265): INFO Train: [3/300][1210/1562]	eta 0:01:47 lr 0.000001	time 0.2981 (0.3047)	loss 2.2450 (2.2933)	grad_norm 9.6261 (8.6543)	mem 4879MB
[2022-05-30 22:49:34 MetaFG_0] (main.py 265): INFO Train: [3/300][1220/1562]	eta 0:01:44 lr 0.000001	time 0.2989 (0.3047)	loss 2.2966 (2.2933)	grad_norm 12.5650 (8.6617)	mem 4879MB
[2022-05-30 22:49:37 MetaFG_0] (main.py 265): INFO Train: [3/300][1230/1562]	eta 0:01:41 lr 0.000001	time 0.2975 (0.3047)	loss 2.2433 (2.2932)	grad_norm 10.3329 (8.6658)	mem 4879MB
[2022-05-30 22:49:40 MetaFG_0] (main.py 265): INFO Train: [3/300][1240/1562]	eta 0:01:38 lr 0.000001	time 0.2942 (0.3047)	loss 2.2403 (2.2931)	grad_norm 9.6556 (8.6623)	mem 4879MB
[2022-05-30 22:49:43 MetaFG_0] (main.py 265): INFO Train: [3/300][1250/1562]	eta 0:01:35 lr 0.000001	time 0.2943 (0.3047)	loss 2.2902 (2.2931)	grad_norm 9.2577 (8.6575)	mem 4879MB
[2022-05-30 22:49:46 MetaFG_0] (main.py 265): INFO Train: [3/300][1260/1562]	eta 0:01:32 lr 0.000001	time 0.2922 (0.3047)	loss 2.2920 (2.2932)	grad_norm 11.4208 (8.6655)	mem 4879MB
[2022-05-30 22:49:49 MetaFG_0] (main.py 265): INFO Train: [3/300][1270/1562]	eta 0:01:28 lr 0.000001	time 0.2972 (0.3047)	loss 2.2931 (2.2931)	grad_norm 7.3069 (8.6648)	mem 4879MB
[2022-05-30 22:49:52 MetaFG_0] (main.py 265): INFO Train: [3/300][1280/1562]	eta 0:01:25 lr 0.000001	time 0.2932 (0.3046)	loss 2.2719 (2.2930)	grad_norm 8.7120 (8.6633)	mem 4879MB
[2022-05-30 22:49:55 MetaFG_0] (main.py 265): INFO Train: [3/300][1290/1562]	eta 0:01:22 lr 0.000001	time 0.2923 (0.3046)	loss 2.3059 (2.2929)	grad_norm 6.9111 (8.6632)	mem 4879MB
[2022-05-30 22:49:58 MetaFG_0] (main.py 265): INFO Train: [3/300][1300/1562]	eta 0:01:19 lr 0.000001	time 0.2978 (0.3046)	loss 2.2899 (2.2929)	grad_norm 8.3254 (8.6567)	mem 4879MB
[2022-05-30 22:50:01 MetaFG_0] (main.py 265): INFO Train: [3/300][1310/1562]	eta 0:01:16 lr 0.000001	time 0.2979 (0.3046)	loss 2.2804 (2.2929)	grad_norm 8.9267 (8.6710)	mem 4879MB
[2022-05-30 22:50:04 MetaFG_0] (main.py 265): INFO Train: [3/300][1320/1562]	eta 0:01:13 lr 0.000001	time 0.3055 (0.3046)	loss 2.2739 (2.2928)	grad_norm 8.3495 (8.6623)	mem 4879MB
[2022-05-30 22:50:07 MetaFG_0] (main.py 265): INFO Train: [3/300][1330/1562]	eta 0:01:10 lr 0.000001	time 0.2941 (0.3046)	loss 2.2471 (2.2928)	grad_norm 7.0227 (8.6645)	mem 4879MB
[2022-05-30 22:50:10 MetaFG_0] (main.py 265): INFO Train: [3/300][1340/1562]	eta 0:01:07 lr 0.000001	time 0.3011 (0.3046)	loss 2.2774 (2.2927)	grad_norm 11.0365 (8.6701)	mem 4879MB
[2022-05-30 22:50:13 MetaFG_0] (main.py 265): INFO Train: [3/300][1350/1562]	eta 0:01:04 lr 0.000001	time 0.2925 (0.3046)	loss 2.2609 (2.2926)	grad_norm 8.0638 (8.6770)	mem 4879MB
[2022-05-30 22:50:16 MetaFG_0] (main.py 265): INFO Train: [3/300][1360/1562]	eta 0:01:01 lr 0.000001	time 0.2929 (0.3046)	loss 2.2806 (2.2926)	grad_norm 9.5507 (8.6726)	mem 4879MB
[2022-05-30 22:50:19 MetaFG_0] (main.py 265): INFO Train: [3/300][1370/1562]	eta 0:00:58 lr 0.000001	time 0.2933 (0.3046)	loss 2.2803 (2.2925)	grad_norm 8.5581 (8.6733)	mem 4879MB
[2022-05-30 22:50:22 MetaFG_0] (main.py 265): INFO Train: [3/300][1380/1562]	eta 0:00:55 lr 0.000001	time 0.2980 (0.3046)	loss 2.3049 (2.2924)	grad_norm 7.6481 (8.6774)	mem 4879MB
[2022-05-30 22:50:25 MetaFG_0] (main.py 265): INFO Train: [3/300][1390/1562]	eta 0:00:52 lr 0.000001	time 0.2933 (0.3046)	loss 2.2918 (2.2923)	grad_norm 5.5216 (8.6750)	mem 4879MB
[2022-05-30 22:50:28 MetaFG_0] (main.py 265): INFO Train: [3/300][1400/1562]	eta 0:00:49 lr 0.000001	time 0.2930 (0.3046)	loss 2.2983 (2.2923)	grad_norm 5.1183 (8.6740)	mem 4879MB
[2022-05-30 22:50:31 MetaFG_0] (main.py 265): INFO Train: [3/300][1410/1562]	eta 0:00:46 lr 0.000001	time 0.2936 (0.3046)	loss 2.2914 (2.2922)	grad_norm 6.8627 (8.6804)	mem 4879MB
[2022-05-30 22:50:34 MetaFG_0] (main.py 265): INFO Train: [3/300][1420/1562]	eta 0:00:43 lr 0.000001	time 0.2925 (0.3046)	loss 2.2441 (2.2921)	grad_norm 11.3522 (8.6933)	mem 4879MB
[2022-05-30 22:50:38 MetaFG_0] (main.py 265): INFO Train: [3/300][1430/1562]	eta 0:00:40 lr 0.000001	time 0.2978 (0.3046)	loss 2.2984 (2.2920)	grad_norm 5.7636 (8.6956)	mem 4879MB
[2022-05-30 22:50:41 MetaFG_0] (main.py 265): INFO Train: [3/300][1440/1562]	eta 0:00:37 lr 0.000001	time 0.2980 (0.3046)	loss 2.2911 (2.2920)	grad_norm 6.3119 (8.6975)	mem 4879MB
[2022-05-30 22:50:44 MetaFG_0] (main.py 265): INFO Train: [3/300][1450/1562]	eta 0:00:34 lr 0.000001	time 0.2914 (0.3046)	loss 2.2745 (2.2919)	grad_norm 8.5255 (8.7053)	mem 4879MB
[2022-05-30 22:50:47 MetaFG_0] (main.py 265): INFO Train: [3/300][1460/1562]	eta 0:00:31 lr 0.000001	time 0.2922 (0.3046)	loss 2.2494 (2.2918)	grad_norm 7.6241 (8.7012)	mem 4879MB
[2022-05-30 22:50:50 MetaFG_0] (main.py 265): INFO Train: [3/300][1470/1562]	eta 0:00:28 lr 0.000001	time 0.2930 (0.3046)	loss 2.2608 (2.2917)	grad_norm 8.9286 (8.7050)	mem 4879MB
[2022-05-30 22:50:53 MetaFG_0] (main.py 265): INFO Train: [3/300][1480/1562]	eta 0:00:24 lr 0.000001	time 0.2922 (0.3045)	loss 2.2708 (2.2917)	grad_norm 8.7342 (8.7109)	mem 4879MB
[2022-05-30 22:50:56 MetaFG_0] (main.py 265): INFO Train: [3/300][1490/1562]	eta 0:00:21 lr 0.000001	time 0.2939 (0.3045)	loss 2.2707 (2.2916)	grad_norm 10.4875 (8.7098)	mem 4879MB
[2022-05-30 22:50:59 MetaFG_0] (main.py 265): INFO Train: [3/300][1500/1562]	eta 0:00:18 lr 0.000001	time 0.2973 (0.3045)	loss 2.2839 (2.2915)	grad_norm 7.6190 (8.7131)	mem 4879MB
[2022-05-30 22:51:02 MetaFG_0] (main.py 265): INFO Train: [3/300][1510/1562]	eta 0:00:15 lr 0.000001	time 0.2998 (0.3045)	loss 2.2912 (2.2915)	grad_norm 7.8306 (8.7182)	mem 4879MB
[2022-05-30 22:51:05 MetaFG_0] (main.py 265): INFO Train: [3/300][1520/1562]	eta 0:00:12 lr 0.000001	time 0.2965 (0.3045)	loss 2.2640 (2.2914)	grad_norm 9.6576 (8.7192)	mem 4879MB
[2022-05-30 22:51:08 MetaFG_0] (main.py 265): INFO Train: [3/300][1530/1562]	eta 0:00:09 lr 0.000001	time 0.2982 (0.3045)	loss 2.2734 (2.2913)	grad_norm 10.2734 (8.7319)	mem 4879MB
[2022-05-30 22:51:11 MetaFG_0] (main.py 265): INFO Train: [3/300][1540/1562]	eta 0:00:06 lr 0.000001	time 0.2975 (0.3045)	loss 2.2741 (2.2912)	grad_norm 9.1468 (8.7355)	mem 4879MB
[2022-05-30 22:51:14 MetaFG_0] (main.py 265): INFO Train: [3/300][1550/1562]	eta 0:00:03 lr 0.000001	time 0.2988 (0.3045)	loss 2.2894 (2.2912)	grad_norm 8.3115 (8.7427)	mem 4879MB
[2022-05-30 22:51:17 MetaFG_0] (main.py 265): INFO Train: [3/300][1560/1562]	eta 0:00:00 lr 0.000001	time 0.2914 (0.3045)	loss 2.3032 (2.2911)	grad_norm 8.5080 (8.7427)	mem 4879MB
[2022-05-30 22:51:17 MetaFG_0] (main.py 272): INFO EPOCH 3 training takes 0:07:55
[2022-05-30 22:51:17 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_3.pth saving......
[2022-05-30 22:51:18 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_3.pth saved !!!
[2022-05-30 22:51:18 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:51:20 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:51:20 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:51:20 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.753 (0.753)	Loss 4.3988 (4.3988)	Acc@1 12.500 (12.500)	Acc@5 21.875 (21.875)	Mem 4879MB
[2022-05-30 22:51:21 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.098 (0.166)	Loss 4.4170 (4.4452)	Acc@1 3.125 (5.966)	Acc@5 12.500 (14.489)	Mem 4879MB
[2022-05-30 22:51:22 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.097 (0.133)	Loss 4.4544 (4.4408)	Acc@1 3.125 (5.506)	Acc@5 21.875 (16.071)	Mem 4879MB
[2022-05-30 22:51:23 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.123)	Loss 4.3617 (4.4446)	Acc@1 9.375 (5.544)	Acc@5 21.875 (16.331)	Mem 4879MB
[2022-05-30 22:51:24 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.116)	Loss 4.4009 (4.4322)	Acc@1 9.375 (5.869)	Acc@5 15.625 (17.226)	Mem 4879MB
[2022-05-30 22:51:25 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.111)	Loss 4.4553 (4.4378)	Acc@1 0.000 (5.515)	Acc@5 12.500 (16.912)	Mem 4879MB
[2022-05-30 22:51:26 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.095 (0.108)	Loss 4.4728 (4.4358)	Acc@1 12.500 (5.482)	Acc@5 18.750 (16.650)	Mem 4879MB
[2022-05-30 22:51:27 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.106)	Loss 4.4679 (4.4345)	Acc@1 6.250 (5.590)	Acc@5 18.750 (16.681)	Mem 4879MB
[2022-05-30 22:51:28 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.098 (0.105)	Loss 4.4840 (4.4341)	Acc@1 6.250 (5.710)	Acc@5 15.625 (16.667)	Mem 4879MB
[2022-05-30 22:51:29 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.104)	Loss 4.4927 (4.4378)	Acc@1 0.000 (5.460)	Acc@5 12.500 (16.415)	Mem 4879MB
[2022-05-30 22:51:30 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.102)	Loss 4.4741 (4.4399)	Acc@1 0.000 (5.507)	Acc@5 6.250 (16.306)	Mem 4879MB
[2022-05-30 22:51:31 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.102)	Loss 4.4263 (4.4382)	Acc@1 9.375 (5.743)	Acc@5 28.125 (16.667)	Mem 4879MB
[2022-05-30 22:51:32 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.101)	Loss 4.4842 (4.4394)	Acc@1 3.125 (5.604)	Acc@5 9.375 (16.632)	Mem 4879MB
[2022-05-30 22:51:33 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.100)	Loss 4.3454 (4.4376)	Acc@1 12.500 (5.677)	Acc@5 21.875 (16.746)	Mem 4879MB
[2022-05-30 22:51:34 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.100)	Loss 4.3334 (4.4369)	Acc@1 6.250 (5.585)	Acc@5 21.875 (16.800)	Mem 4879MB
[2022-05-30 22:51:35 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.099)	Loss 4.4422 (4.4369)	Acc@1 12.500 (5.712)	Acc@5 25.000 (16.991)	Mem 4879MB
[2022-05-30 22:51:36 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.091 (0.099)	Loss 4.4845 (4.4387)	Acc@1 6.250 (5.532)	Acc@5 12.500 (16.828)	Mem 4879MB
[2022-05-30 22:51:36 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.099)	Loss 4.4515 (4.4378)	Acc@1 9.375 (5.556)	Acc@5 15.625 (16.886)	Mem 4879MB
[2022-05-30 22:51:37 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.092 (0.099)	Loss 4.3887 (4.4374)	Acc@1 9.375 (5.490)	Acc@5 21.875 (16.885)	Mem 4879MB
[2022-05-30 22:51:38 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 4.3213 (4.4366)	Acc@1 18.750 (5.448)	Acc@5 25.000 (16.770)	Mem 4879MB
[2022-05-30 22:51:39 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.098)	Loss 4.4115 (4.4365)	Acc@1 6.250 (5.473)	Acc@5 25.000 (16.869)	Mem 4879MB
[2022-05-30 22:51:40 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.098)	Loss 4.4348 (4.4363)	Acc@1 3.125 (5.435)	Acc@5 18.750 (16.810)	Mem 4879MB
[2022-05-30 22:51:41 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.098)	Loss 4.4919 (4.4372)	Acc@1 3.125 (5.430)	Acc@5 12.500 (16.756)	Mem 4879MB
[2022-05-30 22:51:42 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.098)	Loss 4.4139 (4.4384)	Acc@1 0.000 (5.384)	Acc@5 28.125 (16.775)	Mem 4879MB
[2022-05-30 22:51:43 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.093 (0.097)	Loss 4.4019 (4.4385)	Acc@1 6.250 (5.342)	Acc@5 25.000 (16.714)	Mem 4879MB
[2022-05-30 22:51:44 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 4.4183 (4.4383)	Acc@1 6.250 (5.291)	Acc@5 21.875 (16.696)	Mem 4879MB
[2022-05-30 22:51:45 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 4.4137 (4.4373)	Acc@1 6.250 (5.292)	Acc@5 12.500 (16.703)	Mem 4879MB
[2022-05-30 22:51:46 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.097)	Loss 4.4030 (4.4371)	Acc@1 12.500 (5.281)	Acc@5 25.000 (16.905)	Mem 4879MB
[2022-05-30 22:51:47 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 4.4886 (4.4360)	Acc@1 3.125 (5.338)	Acc@5 6.250 (16.915)	Mem 4879MB
[2022-05-30 22:51:48 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.097)	Loss 4.4318 (4.4370)	Acc@1 0.000 (5.273)	Acc@5 12.500 (16.774)	Mem 4879MB
[2022-05-30 22:51:49 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.097)	Loss 4.4668 (4.4366)	Acc@1 3.125 (5.305)	Acc@5 21.875 (16.860)	Mem 4879MB
[2022-05-30 22:51:50 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 4.5214 (4.4359)	Acc@1 3.125 (5.346)	Acc@5 12.500 (16.891)	Mem 4879MB
[2022-05-30 22:51:50 MetaFG_0] (main.py 330): INFO  * Acc@1 5.340 Acc@5 16.930
[2022-05-30 22:51:50 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 5.3%
[2022-05-30 22:51:50 MetaFG_0] (main.py 171): INFO Max accuracy: 5.34%
[2022-05-30 22:51:51 MetaFG_0] (main.py 265): INFO Train: [4/300][0/1562]	eta 0:26:27 lr 0.000001	time 1.0165 (1.0165)	loss 2.2882 (2.2882)	grad_norm 8.8189 (8.8189)	mem 4879MB
[2022-05-30 22:51:54 MetaFG_0] (main.py 265): INFO Train: [4/300][10/1562]	eta 0:09:35 lr 0.000001	time 0.2928 (0.3709)	loss 2.2841 (2.2834)	grad_norm 8.7159 (9.5925)	mem 4879MB
[2022-05-30 22:51:57 MetaFG_0] (main.py 265): INFO Train: [4/300][20/1562]	eta 0:08:42 lr 0.000001	time 0.2983 (0.3387)	loss 2.2858 (2.2846)	grad_norm 9.9903 (9.2190)	mem 4879MB
[2022-05-30 22:52:00 MetaFG_0] (main.py 265): INFO Train: [4/300][30/1562]	eta 0:08:21 lr 0.000001	time 0.2930 (0.3274)	loss 2.2392 (2.2830)	grad_norm 10.8093 (9.3312)	mem 4879MB
[2022-05-30 22:52:03 MetaFG_0] (main.py 265): INFO Train: [4/300][40/1562]	eta 0:08:09 lr 0.000001	time 0.3006 (0.3218)	loss 2.2757 (2.2829)	grad_norm 7.5660 (9.1344)	mem 4879MB
[2022-05-30 22:52:06 MetaFG_0] (main.py 265): INFO Train: [4/300][50/1562]	eta 0:08:01 lr 0.000001	time 0.2949 (0.3183)	loss 2.2824 (2.2830)	grad_norm 8.2242 (9.1501)	mem 4879MB
[2022-05-30 22:52:09 MetaFG_0] (main.py 265): INFO Train: [4/300][60/1562]	eta 0:07:54 lr 0.000001	time 0.2924 (0.3158)	loss 2.3071 (2.2837)	grad_norm 5.2375 (9.0825)	mem 4879MB
[2022-05-30 22:52:12 MetaFG_0] (main.py 265): INFO Train: [4/300][70/1562]	eta 0:07:48 lr 0.000001	time 0.2933 (0.3140)	loss 2.2894 (2.2828)	grad_norm 8.2124 (9.1341)	mem 4879MB
[2022-05-30 22:52:15 MetaFG_0] (main.py 265): INFO Train: [4/300][80/1562]	eta 0:07:43 lr 0.000001	time 0.2918 (0.3126)	loss 2.2796 (2.2825)	grad_norm 6.6775 (9.0910)	mem 4879MB
[2022-05-30 22:52:18 MetaFG_0] (main.py 265): INFO Train: [4/300][90/1562]	eta 0:07:38 lr 0.000001	time 0.2929 (0.3117)	loss 2.2746 (2.2829)	grad_norm 10.0741 (9.1231)	mem 4879MB
[2022-05-30 22:52:21 MetaFG_0] (main.py 265): INFO Train: [4/300][100/1562]	eta 0:07:34 lr 0.000001	time 0.2974 (0.3111)	loss 2.2941 (2.2826)	grad_norm 8.1056 (9.2220)	mem 4879MB
[2022-05-30 22:52:24 MetaFG_0] (main.py 265): INFO Train: [4/300][110/1562]	eta 0:07:31 lr 0.000001	time 0.2933 (0.3106)	loss 2.2647 (2.2818)	grad_norm 6.5405 (9.3696)	mem 4879MB
[2022-05-30 22:52:27 MetaFG_0] (main.py 265): INFO Train: [4/300][120/1562]	eta 0:07:26 lr 0.000001	time 0.2985 (0.3100)	loss 2.3061 (2.2817)	grad_norm 10.7447 (9.5110)	mem 4879MB
[2022-05-30 22:52:30 MetaFG_0] (main.py 265): INFO Train: [4/300][130/1562]	eta 0:07:23 lr 0.000001	time 0.2936 (0.3095)	loss 2.2657 (2.2816)	grad_norm 9.0005 (9.5314)	mem 4879MB
[2022-05-30 22:52:33 MetaFG_0] (main.py 265): INFO Train: [4/300][140/1562]	eta 0:07:19 lr 0.000001	time 0.2988 (0.3092)	loss 2.2572 (2.2811)	grad_norm 7.8467 (9.5888)	mem 4879MB
[2022-05-30 22:52:36 MetaFG_0] (main.py 265): INFO Train: [4/300][150/1562]	eta 0:07:16 lr 0.000001	time 0.2920 (0.3090)	loss 2.3032 (2.2808)	grad_norm 7.3711 (9.5164)	mem 4879MB
[2022-05-30 22:52:39 MetaFG_0] (main.py 265): INFO Train: [4/300][160/1562]	eta 0:07:12 lr 0.000001	time 0.2985 (0.3086)	loss 2.2668 (2.2806)	grad_norm 7.1613 (9.4961)	mem 4879MB
[2022-05-30 22:52:43 MetaFG_0] (main.py 265): INFO Train: [4/300][170/1562]	eta 0:07:09 lr 0.000001	time 0.2939 (0.3083)	loss 2.2986 (2.2808)	grad_norm 9.7227 (9.5253)	mem 4879MB
[2022-05-30 22:52:46 MetaFG_0] (main.py 265): INFO Train: [4/300][180/1562]	eta 0:07:05 lr 0.000001	time 0.2987 (0.3081)	loss 2.2524 (2.2802)	grad_norm 10.0474 (9.5985)	mem 4879MB
[2022-05-30 22:52:49 MetaFG_0] (main.py 265): INFO Train: [4/300][190/1562]	eta 0:07:02 lr 0.000001	time 0.2921 (0.3079)	loss 2.2661 (2.2799)	grad_norm 8.0142 (9.5726)	mem 4879MB
[2022-05-30 22:52:52 MetaFG_0] (main.py 265): INFO Train: [4/300][200/1562]	eta 0:06:59 lr 0.000001	time 0.2979 (0.3077)	loss 2.2890 (2.2796)	grad_norm 10.3863 (9.5837)	mem 4879MB
[2022-05-30 22:52:55 MetaFG_0] (main.py 265): INFO Train: [4/300][210/1562]	eta 0:06:55 lr 0.000001	time 0.2928 (0.3074)	loss 2.3051 (2.2795)	grad_norm 8.7638 (9.5808)	mem 4879MB
[2022-05-30 22:52:58 MetaFG_0] (main.py 265): INFO Train: [4/300][220/1562]	eta 0:06:52 lr 0.000001	time 0.2927 (0.3074)	loss 2.2743 (2.2792)	grad_norm 8.2811 (9.5756)	mem 4879MB
[2022-05-30 22:53:01 MetaFG_0] (main.py 265): INFO Train: [4/300][230/1562]	eta 0:06:49 lr 0.000001	time 0.2995 (0.3072)	loss 2.2688 (2.2785)	grad_norm 7.4083 (9.5872)	mem 4879MB
[2022-05-30 22:53:04 MetaFG_0] (main.py 265): INFO Train: [4/300][240/1562]	eta 0:06:45 lr 0.000001	time 0.2926 (0.3071)	loss 2.2637 (2.2783)	grad_norm 8.6771 (9.5439)	mem 4879MB
[2022-05-30 22:53:07 MetaFG_0] (main.py 265): INFO Train: [4/300][250/1562]	eta 0:06:42 lr 0.000001	time 0.2919 (0.3070)	loss 2.2352 (2.2782)	grad_norm 13.9130 (9.5422)	mem 4879MB
[2022-05-30 22:53:10 MetaFG_0] (main.py 265): INFO Train: [4/300][260/1562]	eta 0:06:39 lr 0.000001	time 0.2939 (0.3069)	loss 2.3065 (2.2783)	grad_norm 6.6375 (9.5466)	mem 4879MB
[2022-05-30 22:53:13 MetaFG_0] (main.py 265): INFO Train: [4/300][270/1562]	eta 0:06:36 lr 0.000001	time 0.2932 (0.3068)	loss 2.2963 (2.2785)	grad_norm 7.2167 (9.5387)	mem 4879MB
[2022-05-30 22:53:16 MetaFG_0] (main.py 265): INFO Train: [4/300][280/1562]	eta 0:06:33 lr 0.000001	time 0.2973 (0.3068)	loss 2.2630 (2.2782)	grad_norm 6.7187 (9.5256)	mem 4879MB
[2022-05-30 22:53:19 MetaFG_0] (main.py 265): INFO Train: [4/300][290/1562]	eta 0:06:30 lr 0.000001	time 0.2996 (0.3068)	loss 2.3089 (2.2784)	grad_norm 9.3020 (9.5306)	mem 4879MB
[2022-05-30 22:53:22 MetaFG_0] (main.py 265): INFO Train: [4/300][300/1562]	eta 0:06:27 lr 0.000001	time 0.2982 (0.3067)	loss 2.3087 (2.2785)	grad_norm 8.7475 (9.5235)	mem 4879MB
[2022-05-30 22:53:25 MetaFG_0] (main.py 265): INFO Train: [4/300][310/1562]	eta 0:06:23 lr 0.000001	time 0.2980 (0.3066)	loss 2.2728 (2.2782)	grad_norm 9.6379 (9.5038)	mem 4879MB
[2022-05-30 22:53:28 MetaFG_0] (main.py 265): INFO Train: [4/300][320/1562]	eta 0:06:20 lr 0.000001	time 0.2932 (0.3065)	loss 2.2839 (2.2779)	grad_norm 5.8237 (9.5483)	mem 4879MB
[2022-05-30 22:53:31 MetaFG_0] (main.py 265): INFO Train: [4/300][330/1562]	eta 0:06:17 lr 0.000001	time 0.2974 (0.3065)	loss 2.2934 (2.2782)	grad_norm 9.3082 (9.5806)	mem 4879MB
[2022-05-30 22:53:34 MetaFG_0] (main.py 265): INFO Train: [4/300][340/1562]	eta 0:06:14 lr 0.000001	time 0.2928 (0.3064)	loss 2.2778 (2.2781)	grad_norm 7.3730 (9.6199)	mem 4879MB
[2022-05-30 22:53:37 MetaFG_0] (main.py 265): INFO Train: [4/300][350/1562]	eta 0:06:11 lr 0.000001	time 0.3175 (0.3065)	loss 2.2843 (2.2778)	grad_norm 8.7137 (9.6399)	mem 4879MB
[2022-05-30 22:53:40 MetaFG_0] (main.py 265): INFO Train: [4/300][360/1562]	eta 0:06:08 lr 0.000001	time 0.2934 (0.3065)	loss 2.2128 (2.2773)	grad_norm 9.7252 (9.6425)	mem 4879MB
[2022-05-30 22:53:43 MetaFG_0] (main.py 265): INFO Train: [4/300][370/1562]	eta 0:06:05 lr 0.000001	time 0.2991 (0.3065)	loss 2.2825 (2.2775)	grad_norm 6.5094 (9.6816)	mem 4879MB
[2022-05-30 22:53:47 MetaFG_0] (main.py 265): INFO Train: [4/300][380/1562]	eta 0:06:02 lr 0.000001	time 0.2972 (0.3064)	loss 2.2553 (2.2771)	grad_norm 10.4168 (9.6798)	mem 4879MB
[2022-05-30 22:53:50 MetaFG_0] (main.py 265): INFO Train: [4/300][390/1562]	eta 0:05:59 lr 0.000001	time 0.2920 (0.3063)	loss 2.2758 (2.2768)	grad_norm 6.5758 (9.6502)	mem 4879MB
[2022-05-30 22:53:53 MetaFG_0] (main.py 265): INFO Train: [4/300][400/1562]	eta 0:05:55 lr 0.000001	time 0.2946 (0.3062)	loss 2.2603 (2.2764)	grad_norm 8.5060 (9.6316)	mem 4879MB
[2022-05-30 22:53:56 MetaFG_0] (main.py 265): INFO Train: [4/300][410/1562]	eta 0:05:52 lr 0.000001	time 0.2919 (0.3062)	loss 2.2672 (2.2765)	grad_norm 11.5602 (9.6458)	mem 4879MB
[2022-05-30 22:53:59 MetaFG_0] (main.py 265): INFO Train: [4/300][420/1562]	eta 0:05:49 lr 0.000001	time 0.2916 (0.3061)	loss 2.2833 (2.2763)	grad_norm 7.3777 (9.6568)	mem 4879MB
[2022-05-30 22:54:02 MetaFG_0] (main.py 265): INFO Train: [4/300][430/1562]	eta 0:05:46 lr 0.000001	time 0.2920 (0.3060)	loss 2.2648 (2.2764)	grad_norm 10.7573 (9.6834)	mem 4879MB
[2022-05-30 22:54:05 MetaFG_0] (main.py 265): INFO Train: [4/300][440/1562]	eta 0:05:43 lr 0.000001	time 0.2918 (0.3059)	loss 2.2362 (2.2762)	grad_norm 9.5572 (9.6769)	mem 4879MB
[2022-05-30 22:54:08 MetaFG_0] (main.py 265): INFO Train: [4/300][450/1562]	eta 0:05:40 lr 0.000001	time 0.2926 (0.3058)	loss 2.2732 (2.2760)	grad_norm 6.5963 (9.6560)	mem 4879MB
[2022-05-30 22:54:11 MetaFG_0] (main.py 265): INFO Train: [4/300][460/1562]	eta 0:05:37 lr 0.000001	time 0.2982 (0.3058)	loss 2.2562 (2.2760)	grad_norm 12.6356 (9.6685)	mem 4879MB
[2022-05-30 22:54:14 MetaFG_0] (main.py 265): INFO Train: [4/300][470/1562]	eta 0:05:33 lr 0.000001	time 0.3004 (0.3058)	loss 2.2890 (2.2761)	grad_norm 7.4072 (9.6687)	mem 4879MB
[2022-05-30 22:54:17 MetaFG_0] (main.py 265): INFO Train: [4/300][480/1562]	eta 0:05:30 lr 0.000001	time 0.2984 (0.3058)	loss 2.2607 (2.2760)	grad_norm 7.2541 (9.6596)	mem 4879MB
[2022-05-30 22:54:20 MetaFG_0] (main.py 265): INFO Train: [4/300][490/1562]	eta 0:05:27 lr 0.000001	time 0.2988 (0.3057)	loss 2.2699 (2.2761)	grad_norm 11.0610 (9.6677)	mem 4879MB
[2022-05-30 22:54:23 MetaFG_0] (main.py 265): INFO Train: [4/300][500/1562]	eta 0:05:24 lr 0.000001	time 0.2930 (0.3057)	loss 2.2452 (2.2757)	grad_norm 9.6688 (9.7251)	mem 4879MB
[2022-05-30 22:54:26 MetaFG_0] (main.py 265): INFO Train: [4/300][510/1562]	eta 0:05:21 lr 0.000001	time 0.2997 (0.3057)	loss 2.2369 (2.2757)	grad_norm 16.1806 (9.7184)	mem 4879MB
[2022-05-30 22:54:29 MetaFG_0] (main.py 265): INFO Train: [4/300][520/1562]	eta 0:05:18 lr 0.000001	time 0.2935 (0.3056)	loss 2.2842 (2.2755)	grad_norm 12.7777 (9.7425)	mem 4879MB
[2022-05-30 22:54:32 MetaFG_0] (main.py 265): INFO Train: [4/300][530/1562]	eta 0:05:15 lr 0.000001	time 0.2920 (0.3056)	loss 2.2826 (2.2755)	grad_norm 8.4979 (9.7525)	mem 4879MB
[2022-05-30 22:54:35 MetaFG_0] (main.py 265): INFO Train: [4/300][540/1562]	eta 0:05:12 lr 0.000001	time 0.2968 (0.3056)	loss 2.2943 (2.2754)	grad_norm 11.2602 (9.7498)	mem 4879MB
[2022-05-30 22:54:38 MetaFG_0] (main.py 265): INFO Train: [4/300][550/1562]	eta 0:05:09 lr 0.000001	time 0.2986 (0.3056)	loss 2.2549 (2.2754)	grad_norm 10.8230 (9.7515)	mem 4879MB
[2022-05-30 22:54:41 MetaFG_0] (main.py 265): INFO Train: [4/300][560/1562]	eta 0:05:06 lr 0.000001	time 0.2920 (0.3055)	loss 2.2776 (2.2754)	grad_norm 6.7687 (9.7534)	mem 4879MB
[2022-05-30 22:54:44 MetaFG_0] (main.py 265): INFO Train: [4/300][570/1562]	eta 0:05:03 lr 0.000001	time 0.2922 (0.3055)	loss 2.2330 (2.2752)	grad_norm 11.6188 (9.7716)	mem 4879MB
[2022-05-30 22:54:47 MetaFG_0] (main.py 265): INFO Train: [4/300][580/1562]	eta 0:04:59 lr 0.000001	time 0.2933 (0.3055)	loss 2.2826 (2.2753)	grad_norm 6.6076 (9.7793)	mem 4879MB
[2022-05-30 22:54:50 MetaFG_0] (main.py 265): INFO Train: [4/300][590/1562]	eta 0:04:56 lr 0.000001	time 0.2924 (0.3054)	loss 2.2606 (2.2750)	grad_norm 9.2121 (9.7725)	mem 4879MB
[2022-05-30 22:54:53 MetaFG_0] (main.py 265): INFO Train: [4/300][600/1562]	eta 0:04:53 lr 0.000001	time 0.2950 (0.3054)	loss 2.2646 (2.2749)	grad_norm 12.2786 (9.7828)	mem 4879MB
[2022-05-30 22:54:56 MetaFG_0] (main.py 265): INFO Train: [4/300][610/1562]	eta 0:04:50 lr 0.000001	time 0.2952 (0.3054)	loss 2.2800 (2.2747)	grad_norm 12.0887 (9.8231)	mem 4879MB
[2022-05-30 22:54:59 MetaFG_0] (main.py 265): INFO Train: [4/300][620/1562]	eta 0:04:47 lr 0.000001	time 0.2917 (0.3053)	loss 2.2558 (2.2742)	grad_norm 6.6498 (9.8031)	mem 4879MB
[2022-05-30 22:55:02 MetaFG_0] (main.py 265): INFO Train: [4/300][630/1562]	eta 0:04:44 lr 0.000001	time 0.2922 (0.3053)	loss 2.2489 (2.2738)	grad_norm 8.8757 (9.8258)	mem 4879MB
[2022-05-30 22:55:05 MetaFG_0] (main.py 265): INFO Train: [4/300][640/1562]	eta 0:04:41 lr 0.000001	time 0.2980 (0.3053)	loss 2.2737 (2.2738)	grad_norm 8.6959 (9.8337)	mem 4879MB
[2022-05-30 22:55:09 MetaFG_0] (main.py 265): INFO Train: [4/300][650/1562]	eta 0:04:38 lr 0.000001	time 0.2932 (0.3053)	loss 2.3028 (2.2738)	grad_norm 7.6382 (9.8347)	mem 4879MB
[2022-05-30 22:55:12 MetaFG_0] (main.py 265): INFO Train: [4/300][660/1562]	eta 0:04:35 lr 0.000001	time 0.2934 (0.3053)	loss 2.2281 (2.2737)	grad_norm 13.2076 (9.8626)	mem 4879MB
[2022-05-30 22:55:15 MetaFG_0] (main.py 265): INFO Train: [4/300][670/1562]	eta 0:04:32 lr 0.000001	time 0.2919 (0.3053)	loss 2.2929 (2.2735)	grad_norm 9.0933 (9.8761)	mem 4879MB
[2022-05-30 22:55:18 MetaFG_0] (main.py 265): INFO Train: [4/300][680/1562]	eta 0:04:29 lr 0.000001	time 0.2925 (0.3053)	loss 2.3018 (2.2733)	grad_norm 9.1598 (9.8859)	mem 4879MB
[2022-05-30 22:55:21 MetaFG_0] (main.py 265): INFO Train: [4/300][690/1562]	eta 0:04:26 lr 0.000001	time 0.2989 (0.3052)	loss 2.2750 (2.2731)	grad_norm 8.4078 (9.9072)	mem 4879MB
[2022-05-30 22:55:24 MetaFG_0] (main.py 265): INFO Train: [4/300][700/1562]	eta 0:04:23 lr 0.000001	time 0.2917 (0.3052)	loss 2.3151 (2.2731)	grad_norm 8.6725 (9.9284)	mem 4879MB
[2022-05-30 22:55:27 MetaFG_0] (main.py 265): INFO Train: [4/300][710/1562]	eta 0:04:19 lr 0.000001	time 0.2921 (0.3052)	loss 2.2851 (2.2726)	grad_norm 10.3981 (9.9330)	mem 4879MB
[2022-05-30 22:55:30 MetaFG_0] (main.py 265): INFO Train: [4/300][720/1562]	eta 0:04:16 lr 0.000001	time 0.3010 (0.3052)	loss 2.2677 (2.2724)	grad_norm 7.3683 (9.9329)	mem 4879MB
[2022-05-30 22:55:33 MetaFG_0] (main.py 265): INFO Train: [4/300][730/1562]	eta 0:04:13 lr 0.000001	time 0.2929 (0.3051)	loss 2.3025 (2.2725)	grad_norm 11.7570 (9.9399)	mem 4879MB
[2022-05-30 22:55:36 MetaFG_0] (main.py 265): INFO Train: [4/300][740/1562]	eta 0:04:10 lr 0.000001	time 0.2937 (0.3051)	loss 2.2422 (2.2720)	grad_norm 9.3221 (9.9455)	mem 4879MB
[2022-05-30 22:55:39 MetaFG_0] (main.py 265): INFO Train: [4/300][750/1562]	eta 0:04:07 lr 0.000001	time 0.2926 (0.3050)	loss 2.1990 (2.2718)	grad_norm 12.6289 (9.9610)	mem 4879MB
[2022-05-30 22:55:42 MetaFG_0] (main.py 265): INFO Train: [4/300][760/1562]	eta 0:04:04 lr 0.000001	time 0.2983 (0.3050)	loss 2.2773 (2.2716)	grad_norm 8.5090 (9.9710)	mem 4879MB
[2022-05-30 22:55:45 MetaFG_0] (main.py 265): INFO Train: [4/300][770/1562]	eta 0:04:01 lr 0.000001	time 0.2938 (0.3050)	loss 2.2367 (2.2716)	grad_norm 12.2632 (9.9891)	mem 4879MB
[2022-05-30 22:55:48 MetaFG_0] (main.py 265): INFO Train: [4/300][780/1562]	eta 0:03:58 lr 0.000001	time 0.2987 (0.3049)	loss 2.2698 (2.2715)	grad_norm 12.4685 (10.0109)	mem 4879MB
[2022-05-30 22:55:51 MetaFG_0] (main.py 265): INFO Train: [4/300][790/1562]	eta 0:03:55 lr 0.000001	time 0.2983 (0.3050)	loss 2.2716 (2.2715)	grad_norm 8.3222 (10.0147)	mem 4879MB
[2022-05-30 22:55:54 MetaFG_0] (main.py 265): INFO Train: [4/300][800/1562]	eta 0:03:52 lr 0.000001	time 0.3035 (0.3050)	loss 2.2537 (2.2712)	grad_norm 9.5662 (10.0093)	mem 4879MB
[2022-05-30 22:55:57 MetaFG_0] (main.py 265): INFO Train: [4/300][810/1562]	eta 0:03:49 lr 0.000001	time 0.2943 (0.3051)	loss 2.3105 (2.2712)	grad_norm 8.0351 (10.0136)	mem 4879MB
[2022-05-30 22:56:00 MetaFG_0] (main.py 265): INFO Train: [4/300][820/1562]	eta 0:03:46 lr 0.000001	time 0.2939 (0.3051)	loss 2.2608 (2.2712)	grad_norm 7.4415 (10.0128)	mem 4879MB
[2022-05-30 22:56:03 MetaFG_0] (main.py 265): INFO Train: [4/300][830/1562]	eta 0:03:43 lr 0.000001	time 0.2983 (0.3050)	loss 2.1925 (2.2708)	grad_norm 11.0492 (10.0736)	mem 4879MB
[2022-05-30 22:56:06 MetaFG_0] (main.py 265): INFO Train: [4/300][840/1562]	eta 0:03:40 lr 0.000001	time 0.2921 (0.3050)	loss 2.2597 (2.2705)	grad_norm 7.3076 (10.0720)	mem 4879MB
[2022-05-30 22:56:09 MetaFG_0] (main.py 265): INFO Train: [4/300][850/1562]	eta 0:03:37 lr 0.000001	time 0.2933 (0.3050)	loss 2.2586 (2.2705)	grad_norm 8.3701 (10.0790)	mem 4879MB
[2022-05-30 22:56:12 MetaFG_0] (main.py 265): INFO Train: [4/300][860/1562]	eta 0:03:34 lr 0.000001	time 0.2936 (0.3050)	loss 2.2359 (2.2703)	grad_norm 10.4145 (10.0910)	mem 4879MB
[2022-05-30 22:56:15 MetaFG_0] (main.py 265): INFO Train: [4/300][870/1562]	eta 0:03:31 lr 0.000001	time 0.2982 (0.3049)	loss 2.2687 (2.2701)	grad_norm 11.1926 (10.1323)	mem 4879MB
[2022-05-30 22:56:18 MetaFG_0] (main.py 265): INFO Train: [4/300][880/1562]	eta 0:03:27 lr 0.000001	time 0.2943 (0.3049)	loss 2.2730 (2.2699)	grad_norm 10.0084 (10.1503)	mem 4879MB
[2022-05-30 22:56:21 MetaFG_0] (main.py 265): INFO Train: [4/300][890/1562]	eta 0:03:24 lr 0.000001	time 0.2917 (0.3049)	loss 2.2702 (2.2698)	grad_norm 9.0802 (10.1551)	mem 4879MB
[2022-05-30 22:56:25 MetaFG_0] (main.py 265): INFO Train: [4/300][900/1562]	eta 0:03:21 lr 0.000001	time 0.2995 (0.3049)	loss 2.2444 (2.2698)	grad_norm 7.2306 (10.1551)	mem 4879MB
[2022-05-30 22:56:28 MetaFG_0] (main.py 265): INFO Train: [4/300][910/1562]	eta 0:03:18 lr 0.000001	time 0.2991 (0.3049)	loss 2.2805 (2.2697)	grad_norm 10.5491 (10.1620)	mem 4879MB
[2022-05-30 22:56:31 MetaFG_0] (main.py 265): INFO Train: [4/300][920/1562]	eta 0:03:15 lr 0.000001	time 0.2942 (0.3049)	loss 2.2687 (2.2697)	grad_norm 8.5268 (10.1544)	mem 4879MB
[2022-05-30 22:56:34 MetaFG_0] (main.py 265): INFO Train: [4/300][930/1562]	eta 0:03:12 lr 0.000001	time 0.3013 (0.3049)	loss 2.2224 (2.2696)	grad_norm 17.9679 (10.1715)	mem 4879MB
[2022-05-30 22:56:37 MetaFG_0] (main.py 265): INFO Train: [4/300][940/1562]	eta 0:03:09 lr 0.000001	time 0.2981 (0.3049)	loss 2.2659 (2.2694)	grad_norm 7.9707 (10.1893)	mem 4879MB
[2022-05-30 22:56:40 MetaFG_0] (main.py 265): INFO Train: [4/300][950/1562]	eta 0:03:06 lr 0.000001	time 0.2922 (0.3049)	loss 2.2416 (2.2691)	grad_norm 10.9761 (10.2046)	mem 4879MB
[2022-05-30 22:56:43 MetaFG_0] (main.py 265): INFO Train: [4/300][960/1562]	eta 0:03:03 lr 0.000001	time 0.2929 (0.3048)	loss 2.2806 (2.2690)	grad_norm 12.1662 (10.2166)	mem 4879MB
[2022-05-30 22:56:46 MetaFG_0] (main.py 265): INFO Train: [4/300][970/1562]	eta 0:03:00 lr 0.000001	time 0.2992 (0.3048)	loss 2.2636 (2.2689)	grad_norm 9.6565 (10.2342)	mem 4879MB
[2022-05-30 22:56:49 MetaFG_0] (main.py 265): INFO Train: [4/300][980/1562]	eta 0:02:57 lr 0.000001	time 0.2988 (0.3048)	loss 2.2203 (2.2687)	grad_norm 9.8258 (10.2432)	mem 4879MB
[2022-05-30 22:56:52 MetaFG_0] (main.py 265): INFO Train: [4/300][990/1562]	eta 0:02:54 lr 0.000001	time 0.2981 (0.3048)	loss 2.2487 (2.2685)	grad_norm 11.9095 (nan)	mem 4879MB
[2022-05-30 22:56:55 MetaFG_0] (main.py 265): INFO Train: [4/300][1000/1562]	eta 0:02:51 lr 0.000001	time 0.2938 (0.3048)	loss 2.2780 (2.2684)	grad_norm 10.3807 (nan)	mem 4879MB
[2022-05-30 22:56:58 MetaFG_0] (main.py 265): INFO Train: [4/300][1010/1562]	eta 0:02:48 lr 0.000001	time 0.2936 (0.3048)	loss 2.2263 (2.2683)	grad_norm 19.5074 (nan)	mem 4879MB
[2022-05-30 22:57:01 MetaFG_0] (main.py 265): INFO Train: [4/300][1020/1562]	eta 0:02:45 lr 0.000001	time 0.2979 (0.3048)	loss 2.2391 (2.2683)	grad_norm 10.3134 (nan)	mem 4879MB
[2022-05-30 22:57:04 MetaFG_0] (main.py 265): INFO Train: [4/300][1030/1562]	eta 0:02:42 lr 0.000001	time 0.2989 (0.3048)	loss 2.2316 (2.2681)	grad_norm 11.3241 (nan)	mem 4879MB
[2022-05-30 22:57:07 MetaFG_0] (main.py 265): INFO Train: [4/300][1040/1562]	eta 0:02:39 lr 0.000001	time 0.2919 (0.3047)	loss 2.2399 (2.2679)	grad_norm 9.9490 (nan)	mem 4879MB
[2022-05-30 22:57:10 MetaFG_0] (main.py 265): INFO Train: [4/300][1050/1562]	eta 0:02:36 lr 0.000001	time 0.2918 (0.3047)	loss 2.2259 (2.2677)	grad_norm 7.4762 (nan)	mem 4879MB
[2022-05-30 22:57:13 MetaFG_0] (main.py 265): INFO Train: [4/300][1060/1562]	eta 0:02:32 lr 0.000001	time 0.2956 (0.3047)	loss 2.2958 (2.2677)	grad_norm 7.9381 (nan)	mem 4879MB
[2022-05-30 22:57:16 MetaFG_0] (main.py 265): INFO Train: [4/300][1070/1562]	eta 0:02:29 lr 0.000001	time 0.2977 (0.3048)	loss 2.2768 (2.2675)	grad_norm 14.4884 (nan)	mem 4879MB
[2022-05-30 22:57:19 MetaFG_0] (main.py 265): INFO Train: [4/300][1080/1562]	eta 0:02:26 lr 0.000001	time 0.2977 (0.3047)	loss 2.2375 (2.2672)	grad_norm 7.4656 (nan)	mem 4879MB
[2022-05-30 22:57:22 MetaFG_0] (main.py 265): INFO Train: [4/300][1090/1562]	eta 0:02:23 lr 0.000001	time 0.2919 (0.3047)	loss 2.2389 (2.2670)	grad_norm 9.8582 (nan)	mem 4879MB
[2022-05-30 22:57:25 MetaFG_0] (main.py 265): INFO Train: [4/300][1100/1562]	eta 0:02:20 lr 0.000001	time 0.3002 (0.3047)	loss 2.2957 (2.2669)	grad_norm 11.8167 (nan)	mem 4879MB
[2022-05-30 22:57:28 MetaFG_0] (main.py 265): INFO Train: [4/300][1110/1562]	eta 0:02:17 lr 0.000001	time 0.2937 (0.3047)	loss 2.2681 (2.2668)	grad_norm 12.3699 (nan)	mem 4879MB
[2022-05-30 22:57:31 MetaFG_0] (main.py 265): INFO Train: [4/300][1120/1562]	eta 0:02:14 lr 0.000001	time 0.2925 (0.3047)	loss 2.2409 (2.2668)	grad_norm 11.6983 (nan)	mem 4879MB
[2022-05-30 22:57:34 MetaFG_0] (main.py 265): INFO Train: [4/300][1130/1562]	eta 0:02:11 lr 0.000001	time 0.2921 (0.3047)	loss 2.2352 (2.2666)	grad_norm 14.5007 (nan)	mem 4879MB
[2022-05-30 22:57:37 MetaFG_0] (main.py 265): INFO Train: [4/300][1140/1562]	eta 0:02:08 lr 0.000001	time 0.2927 (0.3047)	loss 2.2727 (2.2665)	grad_norm 7.0293 (nan)	mem 4879MB
[2022-05-30 22:57:40 MetaFG_0] (main.py 265): INFO Train: [4/300][1150/1562]	eta 0:02:05 lr 0.000001	time 0.2934 (0.3047)	loss 2.2081 (2.2663)	grad_norm 16.4578 (nan)	mem 4879MB
[2022-05-30 22:57:43 MetaFG_0] (main.py 265): INFO Train: [4/300][1160/1562]	eta 0:02:02 lr 0.000001	time 0.2935 (0.3046)	loss 2.2430 (2.2661)	grad_norm 11.9920 (nan)	mem 4879MB
[2022-05-30 22:57:47 MetaFG_0] (main.py 265): INFO Train: [4/300][1170/1562]	eta 0:01:59 lr 0.000001	time 0.2928 (0.3047)	loss 2.2347 (2.2661)	grad_norm 12.4296 (nan)	mem 4879MB
[2022-05-30 22:57:50 MetaFG_0] (main.py 265): INFO Train: [4/300][1180/1562]	eta 0:01:56 lr 0.000001	time 0.2932 (0.3047)	loss 2.2131 (2.2660)	grad_norm 8.3696 (nan)	mem 4879MB
[2022-05-30 22:57:53 MetaFG_0] (main.py 265): INFO Train: [4/300][1190/1562]	eta 0:01:53 lr 0.000001	time 0.2947 (0.3047)	loss 2.2492 (2.2658)	grad_norm 8.9368 (nan)	mem 4879MB
[2022-05-30 22:57:56 MetaFG_0] (main.py 265): INFO Train: [4/300][1200/1562]	eta 0:01:50 lr 0.000001	time 0.2928 (0.3047)	loss 2.2374 (2.2657)	grad_norm 12.4232 (nan)	mem 4879MB
[2022-05-30 22:57:59 MetaFG_0] (main.py 265): INFO Train: [4/300][1210/1562]	eta 0:01:47 lr 0.000001	time 0.2950 (0.3047)	loss 2.2325 (2.2655)	grad_norm 11.6647 (nan)	mem 4879MB
[2022-05-30 22:58:02 MetaFG_0] (main.py 265): INFO Train: [4/300][1220/1562]	eta 0:01:44 lr 0.000001	time 0.2983 (0.3047)	loss 2.3109 (2.2654)	grad_norm 11.8970 (nan)	mem 4879MB
[2022-05-30 22:58:05 MetaFG_0] (main.py 265): INFO Train: [4/300][1230/1562]	eta 0:01:41 lr 0.000002	time 0.2978 (0.3047)	loss 2.2810 (2.2653)	grad_norm 8.0539 (nan)	mem 4879MB
[2022-05-30 22:58:08 MetaFG_0] (main.py 265): INFO Train: [4/300][1240/1562]	eta 0:01:38 lr 0.000002	time 0.2929 (0.3047)	loss 2.2715 (2.2650)	grad_norm 7.8559 (nan)	mem 4879MB
[2022-05-30 22:58:11 MetaFG_0] (main.py 265): INFO Train: [4/300][1250/1562]	eta 0:01:35 lr 0.000002	time 0.2982 (0.3046)	loss 2.2676 (2.2648)	grad_norm 7.7530 (nan)	mem 4879MB
[2022-05-30 22:58:14 MetaFG_0] (main.py 265): INFO Train: [4/300][1260/1562]	eta 0:01:32 lr 0.000002	time 0.2930 (0.3047)	loss 2.2948 (2.2647)	grad_norm 10.2980 (nan)	mem 4879MB
[2022-05-30 22:58:17 MetaFG_0] (main.py 265): INFO Train: [4/300][1270/1562]	eta 0:01:28 lr 0.000002	time 0.2992 (0.3047)	loss 2.2426 (2.2646)	grad_norm 14.9070 (nan)	mem 4879MB
[2022-05-30 22:58:20 MetaFG_0] (main.py 265): INFO Train: [4/300][1280/1562]	eta 0:01:25 lr 0.000002	time 0.2992 (0.3047)	loss 2.2604 (2.2643)	grad_norm 9.8709 (nan)	mem 4879MB
[2022-05-30 22:58:23 MetaFG_0] (main.py 265): INFO Train: [4/300][1290/1562]	eta 0:01:22 lr 0.000002	time 0.2981 (0.3047)	loss 2.1871 (2.2641)	grad_norm 11.7596 (nan)	mem 4879MB
[2022-05-30 22:58:26 MetaFG_0] (main.py 265): INFO Train: [4/300][1300/1562]	eta 0:01:19 lr 0.000002	time 0.2982 (0.3047)	loss 2.1997 (2.2639)	grad_norm 9.9325 (nan)	mem 4879MB
[2022-05-30 22:58:29 MetaFG_0] (main.py 265): INFO Train: [4/300][1310/1562]	eta 0:01:16 lr 0.000002	time 0.2977 (0.3047)	loss 2.2835 (2.2637)	grad_norm 11.9733 (nan)	mem 4879MB
[2022-05-30 22:58:32 MetaFG_0] (main.py 265): INFO Train: [4/300][1320/1562]	eta 0:01:13 lr 0.000002	time 0.2923 (0.3047)	loss 2.2718 (2.2635)	grad_norm 14.0674 (nan)	mem 4879MB
[2022-05-30 22:58:35 MetaFG_0] (main.py 265): INFO Train: [4/300][1330/1562]	eta 0:01:10 lr 0.000002	time 0.2983 (0.3047)	loss 2.2634 (2.2634)	grad_norm 6.2928 (nan)	mem 4879MB
[2022-05-30 22:58:38 MetaFG_0] (main.py 265): INFO Train: [4/300][1340/1562]	eta 0:01:07 lr 0.000002	time 0.2933 (0.3047)	loss 2.1633 (2.2633)	grad_norm 13.0569 (nan)	mem 4879MB
[2022-05-30 22:58:41 MetaFG_0] (main.py 265): INFO Train: [4/300][1350/1562]	eta 0:01:04 lr 0.000002	time 0.2978 (0.3047)	loss 2.2485 (2.2631)	grad_norm 12.5062 (nan)	mem 4879MB
[2022-05-30 22:58:44 MetaFG_0] (main.py 265): INFO Train: [4/300][1360/1562]	eta 0:01:01 lr 0.000002	time 0.2990 (0.3046)	loss 2.2274 (2.2629)	grad_norm 16.3905 (nan)	mem 4879MB
[2022-05-30 22:58:47 MetaFG_0] (main.py 265): INFO Train: [4/300][1370/1562]	eta 0:00:58 lr 0.000002	time 0.2945 (0.3046)	loss 2.2605 (2.2628)	grad_norm 8.7109 (nan)	mem 4879MB
[2022-05-30 22:58:50 MetaFG_0] (main.py 265): INFO Train: [4/300][1380/1562]	eta 0:00:55 lr 0.000002	time 0.2983 (0.3046)	loss 2.2663 (2.2626)	grad_norm 8.6037 (nan)	mem 4879MB
[2022-05-30 22:58:54 MetaFG_0] (main.py 265): INFO Train: [4/300][1390/1562]	eta 0:00:52 lr 0.000002	time 0.2985 (0.3046)	loss 2.2952 (2.2625)	grad_norm 9.5868 (nan)	mem 4879MB
[2022-05-30 22:58:57 MetaFG_0] (main.py 265): INFO Train: [4/300][1400/1562]	eta 0:00:49 lr 0.000002	time 0.2948 (0.3046)	loss 2.2077 (2.2624)	grad_norm 9.4576 (nan)	mem 4879MB
[2022-05-30 22:59:00 MetaFG_0] (main.py 265): INFO Train: [4/300][1410/1562]	eta 0:00:46 lr 0.000002	time 0.2976 (0.3046)	loss 2.2523 (2.2623)	grad_norm 6.1090 (nan)	mem 4879MB
[2022-05-30 22:59:03 MetaFG_0] (main.py 265): INFO Train: [4/300][1420/1562]	eta 0:00:43 lr 0.000002	time 0.3033 (0.3046)	loss 2.2556 (2.2622)	grad_norm 9.4361 (nan)	mem 4879MB
[2022-05-30 22:59:06 MetaFG_0] (main.py 265): INFO Train: [4/300][1430/1562]	eta 0:00:40 lr 0.000002	time 0.2996 (0.3046)	loss 2.2435 (2.2620)	grad_norm 7.1072 (nan)	mem 4879MB
[2022-05-30 22:59:09 MetaFG_0] (main.py 265): INFO Train: [4/300][1440/1562]	eta 0:00:37 lr 0.000002	time 0.2993 (0.3046)	loss 2.1898 (2.2618)	grad_norm 10.9783 (nan)	mem 4879MB
[2022-05-30 22:59:12 MetaFG_0] (main.py 265): INFO Train: [4/300][1450/1562]	eta 0:00:34 lr 0.000002	time 0.2920 (0.3046)	loss 2.2140 (2.2617)	grad_norm 16.4499 (nan)	mem 4879MB
[2022-05-30 22:59:15 MetaFG_0] (main.py 265): INFO Train: [4/300][1460/1562]	eta 0:00:31 lr 0.000002	time 0.2997 (0.3046)	loss 2.2633 (2.2617)	grad_norm 12.8302 (nan)	mem 4879MB
[2022-05-30 22:59:18 MetaFG_0] (main.py 265): INFO Train: [4/300][1470/1562]	eta 0:00:28 lr 0.000002	time 0.2997 (0.3046)	loss 2.2174 (2.2614)	grad_norm 14.3708 (nan)	mem 4879MB
[2022-05-30 22:59:21 MetaFG_0] (main.py 265): INFO Train: [4/300][1480/1562]	eta 0:00:24 lr 0.000002	time 0.2981 (0.3046)	loss 2.2526 (2.2613)	grad_norm 8.9749 (nan)	mem 4879MB
[2022-05-30 22:59:24 MetaFG_0] (main.py 265): INFO Train: [4/300][1490/1562]	eta 0:00:21 lr 0.000002	time 0.2999 (0.3046)	loss 2.2701 (2.2612)	grad_norm 9.3019 (nan)	mem 4879MB
[2022-05-30 22:59:27 MetaFG_0] (main.py 265): INFO Train: [4/300][1500/1562]	eta 0:00:18 lr 0.000002	time 0.2961 (0.3046)	loss 2.2115 (2.2610)	grad_norm 9.5257 (nan)	mem 4879MB
[2022-05-30 22:59:30 MetaFG_0] (main.py 265): INFO Train: [4/300][1510/1562]	eta 0:00:15 lr 0.000002	time 0.3024 (0.3046)	loss 2.1888 (2.2609)	grad_norm 13.3285 (nan)	mem 4879MB
[2022-05-30 22:59:33 MetaFG_0] (main.py 265): INFO Train: [4/300][1520/1562]	eta 0:00:12 lr 0.000002	time 0.2985 (0.3046)	loss 2.2291 (2.2607)	grad_norm 14.3541 (nan)	mem 4879MB
[2022-05-30 22:59:36 MetaFG_0] (main.py 265): INFO Train: [4/300][1530/1562]	eta 0:00:09 lr 0.000002	time 0.2918 (0.3046)	loss 2.2481 (2.2606)	grad_norm 13.3156 (nan)	mem 4879MB
[2022-05-30 22:59:39 MetaFG_0] (main.py 265): INFO Train: [4/300][1540/1562]	eta 0:00:06 lr 0.000002	time 0.2926 (0.3046)	loss 2.2577 (2.2604)	grad_norm 18.4975 (nan)	mem 4879MB
[2022-05-30 22:59:42 MetaFG_0] (main.py 265): INFO Train: [4/300][1550/1562]	eta 0:00:03 lr 0.000002	time 0.2980 (0.3046)	loss 2.1995 (2.2602)	grad_norm 14.6810 (nan)	mem 4879MB
[2022-05-30 22:59:45 MetaFG_0] (main.py 265): INFO Train: [4/300][1560/1562]	eta 0:00:00 lr 0.000002	time 0.2913 (0.3046)	loss 2.1551 (2.2601)	grad_norm 13.4216 (nan)	mem 4879MB
[2022-05-30 22:59:46 MetaFG_0] (main.py 272): INFO EPOCH 4 training takes 0:07:55
[2022-05-30 22:59:46 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_4.pth saving......
[2022-05-30 22:59:47 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_4.pth saved !!!
[2022-05-30 22:59:47 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 22:59:48 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 22:59:48 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 22:59:49 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.561 (0.561)	Loss 4.0833 (4.0833)	Acc@1 9.375 (9.375)	Acc@5 50.000 (50.000)	Mem 4879MB
[2022-05-30 22:59:50 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.143)	Loss 4.2988 (4.1868)	Acc@1 0.000 (9.659)	Acc@5 15.625 (31.818)	Mem 4879MB
[2022-05-30 22:59:51 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.098 (0.118)	Loss 4.2881 (4.1958)	Acc@1 9.375 (10.417)	Acc@5 18.750 (30.804)	Mem 4879MB
[2022-05-30 22:59:52 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.109)	Loss 4.3113 (4.1901)	Acc@1 6.250 (10.282)	Acc@5 28.125 (32.056)	Mem 4879MB
[2022-05-30 22:59:53 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.095 (0.106)	Loss 4.1137 (4.1847)	Acc@1 9.375 (10.595)	Acc@5 31.250 (32.165)	Mem 4879MB
[2022-05-30 22:59:54 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.103)	Loss 4.1135 (4.1816)	Acc@1 12.500 (10.600)	Acc@5 34.375 (32.108)	Mem 4879MB
[2022-05-30 22:59:54 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.102)	Loss 4.2113 (4.1914)	Acc@1 15.625 (9.887)	Acc@5 21.875 (30.635)	Mem 4879MB
[2022-05-30 22:59:55 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.100)	Loss 4.2740 (4.1858)	Acc@1 6.250 (10.475)	Acc@5 25.000 (31.118)	Mem 4879MB
[2022-05-30 22:59:56 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.101 (0.100)	Loss 4.0621 (4.1909)	Acc@1 21.875 (10.378)	Acc@5 46.875 (30.903)	Mem 4879MB
[2022-05-30 22:59:57 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.091 (0.099)	Loss 4.2305 (4.1891)	Acc@1 9.375 (10.405)	Acc@5 34.375 (31.078)	Mem 4879MB
[2022-05-30 22:59:58 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.098)	Loss 4.3498 (4.1921)	Acc@1 3.125 (10.303)	Acc@5 15.625 (30.662)	Mem 4879MB
[2022-05-30 22:59:59 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.093 (0.098)	Loss 4.1304 (4.1946)	Acc@1 21.875 (10.248)	Acc@5 43.750 (30.518)	Mem 4879MB
[2022-05-30 23:00:00 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.097)	Loss 4.2445 (4.1938)	Acc@1 9.375 (10.460)	Acc@5 34.375 (30.914)	Mem 4879MB
[2022-05-30 23:00:01 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.097)	Loss 4.2095 (4.1931)	Acc@1 12.500 (10.425)	Acc@5 28.125 (31.035)	Mem 4879MB
[2022-05-30 23:00:02 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.097)	Loss 4.2742 (4.1946)	Acc@1 3.125 (10.306)	Acc@5 28.125 (30.807)	Mem 4879MB
[2022-05-30 23:00:03 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.097)	Loss 4.0393 (4.1928)	Acc@1 18.750 (10.410)	Acc@5 40.625 (30.919)	Mem 4879MB
[2022-05-30 23:00:04 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.097)	Loss 4.2107 (4.1923)	Acc@1 9.375 (10.540)	Acc@5 25.000 (30.881)	Mem 4879MB
[2022-05-30 23:00:05 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.097)	Loss 4.1772 (4.1932)	Acc@1 9.375 (10.453)	Acc@5 25.000 (30.702)	Mem 4879MB
[2022-05-30 23:00:06 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.096)	Loss 4.2201 (4.1933)	Acc@1 9.375 (10.532)	Acc@5 28.125 (30.663)	Mem 4879MB
[2022-05-30 23:00:07 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.095 (0.096)	Loss 4.1824 (4.1920)	Acc@1 6.250 (10.618)	Acc@5 34.375 (30.939)	Mem 4879MB
[2022-05-30 23:00:08 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.095 (0.096)	Loss 4.2021 (4.1929)	Acc@1 6.250 (10.619)	Acc@5 21.875 (30.970)	Mem 4879MB
[2022-05-30 23:00:09 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.096)	Loss 4.0267 (4.1926)	Acc@1 15.625 (10.619)	Acc@5 40.625 (30.909)	Mem 4879MB
[2022-05-30 23:00:09 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.096)	Loss 4.3022 (4.1920)	Acc@1 9.375 (10.732)	Acc@5 18.750 (30.967)	Mem 4879MB
[2022-05-30 23:00:10 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.095 (0.096)	Loss 4.2393 (4.1933)	Acc@1 15.625 (10.823)	Acc@5 28.125 (30.925)	Mem 4879MB
[2022-05-30 23:00:11 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 4.1481 (4.1921)	Acc@1 12.500 (10.905)	Acc@5 31.250 (31.017)	Mem 4879MB
[2022-05-30 23:00:12 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 4.0804 (4.1907)	Acc@1 15.625 (10.944)	Acc@5 37.500 (31.051)	Mem 4879MB
[2022-05-30 23:00:13 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.095)	Loss 4.1450 (4.1882)	Acc@1 9.375 (11.063)	Acc@5 37.500 (31.250)	Mem 4879MB
[2022-05-30 23:00:14 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.093 (0.095)	Loss 4.1216 (4.1886)	Acc@1 15.625 (11.024)	Acc@5 37.500 (31.285)	Mem 4879MB
[2022-05-30 23:00:15 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.101 (0.095)	Loss 4.2184 (4.1879)	Acc@1 6.250 (11.088)	Acc@5 31.250 (31.339)	Mem 4879MB
[2022-05-30 23:00:16 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.095)	Loss 4.1449 (4.1880)	Acc@1 12.500 (11.061)	Acc@5 34.375 (31.218)	Mem 4879MB
[2022-05-30 23:00:17 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.093 (0.095)	Loss 4.2365 (4.1874)	Acc@1 6.250 (11.067)	Acc@5 25.000 (31.333)	Mem 4879MB
[2022-05-30 23:00:18 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 4.2986 (4.1865)	Acc@1 3.125 (11.133)	Acc@5 25.000 (31.411)	Mem 4879MB
[2022-05-30 23:00:18 MetaFG_0] (main.py 330): INFO  * Acc@1 11.160 Acc@5 31.460
[2022-05-30 23:00:18 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 11.2%
[2022-05-30 23:00:18 MetaFG_0] (main.py 171): INFO Max accuracy: 11.16%
[2022-05-30 23:00:19 MetaFG_0] (main.py 265): INFO Train: [5/300][0/1562]	eta 0:26:41 lr 0.000002	time 1.0251 (1.0251)	loss 2.2161 (2.2161)	grad_norm 9.2086 (9.2086)	mem 4879MB
[2022-05-30 23:00:22 MetaFG_0] (main.py 265): INFO Train: [5/300][10/1562]	eta 0:09:42 lr 0.000002	time 0.2920 (0.3752)	loss 2.2565 (2.2346)	grad_norm 12.4159 (12.5950)	mem 4879MB
[2022-05-30 23:00:25 MetaFG_0] (main.py 265): INFO Train: [5/300][20/1562]	eta 0:08:44 lr 0.000002	time 0.2938 (0.3404)	loss 2.2132 (2.2334)	grad_norm 14.8341 (12.7995)	mem 4879MB
[2022-05-30 23:00:28 MetaFG_0] (main.py 265): INFO Train: [5/300][30/1562]	eta 0:08:27 lr 0.000002	time 0.2920 (0.3315)	loss 2.2794 (2.2344)	grad_norm 14.9079 (12.8766)	mem 4879MB
[2022-05-30 23:00:31 MetaFG_0] (main.py 265): INFO Train: [5/300][40/1562]	eta 0:08:14 lr 0.000002	time 0.2962 (0.3252)	loss 2.2326 (2.2384)	grad_norm 16.4752 (12.8265)	mem 4879MB
[2022-05-30 23:00:34 MetaFG_0] (main.py 265): INFO Train: [5/300][50/1562]	eta 0:08:04 lr 0.000002	time 0.2975 (0.3205)	loss 2.2064 (2.2351)	grad_norm 10.8556 (12.8495)	mem 4879MB
[2022-05-30 23:00:37 MetaFG_0] (main.py 265): INFO Train: [5/300][60/1562]	eta 0:07:57 lr 0.000002	time 0.2935 (0.3178)	loss 2.2310 (2.2333)	grad_norm 12.3990 (12.7484)	mem 4879MB
[2022-05-30 23:00:40 MetaFG_0] (main.py 265): INFO Train: [5/300][70/1562]	eta 0:07:51 lr 0.000002	time 0.2993 (0.3157)	loss 2.1987 (2.2341)	grad_norm 12.3305 (12.7311)	mem 4879MB
[2022-05-30 23:00:43 MetaFG_0] (main.py 265): INFO Train: [5/300][80/1562]	eta 0:07:45 lr 0.000002	time 0.2943 (0.3142)	loss 2.2348 (2.2321)	grad_norm 9.3437 (12.4348)	mem 4879MB
[2022-05-30 23:00:47 MetaFG_0] (main.py 265): INFO Train: [5/300][90/1562]	eta 0:07:40 lr 0.000002	time 0.2973 (0.3132)	loss 2.2434 (2.2326)	grad_norm 9.8782 (12.4486)	mem 4879MB
[2022-05-30 23:00:50 MetaFG_0] (main.py 265): INFO Train: [5/300][100/1562]	eta 0:07:36 lr 0.000002	time 0.2918 (0.3121)	loss 2.2037 (2.2330)	grad_norm 11.9222 (12.3177)	mem 4879MB
[2022-05-30 23:00:53 MetaFG_0] (main.py 265): INFO Train: [5/300][110/1562]	eta 0:07:32 lr 0.000002	time 0.2974 (0.3116)	loss 2.2231 (2.2318)	grad_norm 12.6482 (12.2467)	mem 4879MB
[2022-05-30 23:00:56 MetaFG_0] (main.py 265): INFO Train: [5/300][120/1562]	eta 0:07:28 lr 0.000002	time 0.2955 (0.3109)	loss 2.2218 (2.2311)	grad_norm 11.0093 (12.3183)	mem 4879MB
[2022-05-30 23:00:59 MetaFG_0] (main.py 265): INFO Train: [5/300][130/1562]	eta 0:07:24 lr 0.000002	time 0.2920 (0.3102)	loss 2.2845 (2.2322)	grad_norm 13.6305 (12.3534)	mem 4879MB
[2022-05-30 23:01:02 MetaFG_0] (main.py 265): INFO Train: [5/300][140/1562]	eta 0:07:20 lr 0.000002	time 0.2921 (0.3096)	loss 2.2305 (2.2320)	grad_norm 12.2146 (12.4988)	mem 4879MB
[2022-05-30 23:01:05 MetaFG_0] (main.py 265): INFO Train: [5/300][150/1562]	eta 0:07:16 lr 0.000002	time 0.2932 (0.3091)	loss 2.2914 (2.2324)	grad_norm 11.5070 (12.5650)	mem 4879MB
[2022-05-30 23:01:08 MetaFG_0] (main.py 265): INFO Train: [5/300][160/1562]	eta 0:07:12 lr 0.000002	time 0.3021 (0.3088)	loss 2.1743 (2.2313)	grad_norm 13.2656 (12.6243)	mem 4879MB
[2022-05-30 23:01:11 MetaFG_0] (main.py 265): INFO Train: [5/300][170/1562]	eta 0:07:09 lr 0.000002	time 0.2977 (0.3086)	loss 2.2547 (2.2312)	grad_norm 9.4504 (12.6260)	mem 4879MB
[2022-05-30 23:01:14 MetaFG_0] (main.py 265): INFO Train: [5/300][180/1562]	eta 0:07:06 lr 0.000002	time 0.2918 (0.3083)	loss 2.1781 (2.2319)	grad_norm 13.7233 (12.6116)	mem 4879MB
[2022-05-30 23:01:17 MetaFG_0] (main.py 265): INFO Train: [5/300][190/1562]	eta 0:07:02 lr 0.000002	time 0.2935 (0.3080)	loss 2.2363 (2.2312)	grad_norm 17.3730 (12.6689)	mem 4879MB
[2022-05-30 23:01:20 MetaFG_0] (main.py 265): INFO Train: [5/300][200/1562]	eta 0:06:59 lr 0.000002	time 0.2974 (0.3078)	loss 2.2419 (2.2304)	grad_norm 13.4519 (12.6740)	mem 4879MB
[2022-05-30 23:01:23 MetaFG_0] (main.py 265): INFO Train: [5/300][210/1562]	eta 0:06:56 lr 0.000002	time 0.2978 (0.3077)	loss 2.2074 (2.2299)	grad_norm 24.1076 (12.7073)	mem 4879MB
[2022-05-30 23:01:26 MetaFG_0] (main.py 265): INFO Train: [5/300][220/1562]	eta 0:06:52 lr 0.000002	time 0.2919 (0.3076)	loss 2.2070 (2.2290)	grad_norm 11.3123 (12.7081)	mem 4879MB
[2022-05-30 23:01:29 MetaFG_0] (main.py 265): INFO Train: [5/300][230/1562]	eta 0:06:49 lr 0.000002	time 0.2938 (0.3074)	loss 2.2485 (2.2293)	grad_norm 9.1257 (12.7276)	mem 4879MB
[2022-05-30 23:01:32 MetaFG_0] (main.py 265): INFO Train: [5/300][240/1562]	eta 0:06:46 lr 0.000002	time 0.2977 (0.3073)	loss 2.2043 (2.2288)	grad_norm 13.4411 (12.7122)	mem 4879MB
[2022-05-30 23:01:35 MetaFG_0] (main.py 265): INFO Train: [5/300][250/1562]	eta 0:06:42 lr 0.000002	time 0.2914 (0.3071)	loss 2.1812 (2.2283)	grad_norm 8.7113 (12.7887)	mem 4879MB
[2022-05-30 23:01:38 MetaFG_0] (main.py 265): INFO Train: [5/300][260/1562]	eta 0:06:39 lr 0.000002	time 0.2918 (0.3069)	loss 2.2363 (2.2285)	grad_norm 13.2346 (12.7623)	mem 4879MB
[2022-05-30 23:01:41 MetaFG_0] (main.py 265): INFO Train: [5/300][270/1562]	eta 0:06:36 lr 0.000002	time 0.2918 (0.3068)	loss 2.2213 (2.2286)	grad_norm 14.8186 (12.7912)	mem 4879MB
[2022-05-30 23:01:44 MetaFG_0] (main.py 265): INFO Train: [5/300][280/1562]	eta 0:06:33 lr 0.000002	time 0.2980 (0.3066)	loss 2.2420 (2.2290)	grad_norm 12.5144 (12.8160)	mem 4879MB
[2022-05-30 23:01:47 MetaFG_0] (main.py 265): INFO Train: [5/300][290/1562]	eta 0:06:29 lr 0.000002	time 0.2992 (0.3065)	loss 2.2031 (2.2293)	grad_norm 10.5222 (12.8001)	mem 4879MB
[2022-05-30 23:01:50 MetaFG_0] (main.py 265): INFO Train: [5/300][300/1562]	eta 0:06:26 lr 0.000002	time 0.2925 (0.3064)	loss 2.2182 (2.2290)	grad_norm 16.0514 (12.8435)	mem 4879MB
[2022-05-30 23:01:53 MetaFG_0] (main.py 265): INFO Train: [5/300][310/1562]	eta 0:06:23 lr 0.000002	time 0.2975 (0.3063)	loss 2.2290 (2.2290)	grad_norm 10.6742 (12.8251)	mem 4879MB
[2022-05-30 23:01:56 MetaFG_0] (main.py 265): INFO Train: [5/300][320/1562]	eta 0:06:20 lr 0.000002	time 0.2975 (0.3062)	loss 2.2037 (2.2288)	grad_norm 12.2583 (12.8099)	mem 4879MB
[2022-05-30 23:01:59 MetaFG_0] (main.py 265): INFO Train: [5/300][330/1562]	eta 0:06:17 lr 0.000002	time 0.2931 (0.3061)	loss 2.1862 (2.2287)	grad_norm 11.5101 (12.8014)	mem 4879MB
[2022-05-30 23:02:02 MetaFG_0] (main.py 265): INFO Train: [5/300][340/1562]	eta 0:06:14 lr 0.000002	time 0.2960 (0.3061)	loss 2.2946 (2.2289)	grad_norm 13.1408 (12.8191)	mem 4879MB
[2022-05-30 23:02:05 MetaFG_0] (main.py 265): INFO Train: [5/300][350/1562]	eta 0:06:10 lr 0.000002	time 0.2985 (0.3061)	loss 2.2548 (2.2287)	grad_norm 9.8141 (12.8398)	mem 4879MB
[2022-05-30 23:02:09 MetaFG_0] (main.py 265): INFO Train: [5/300][360/1562]	eta 0:06:07 lr 0.000002	time 0.2928 (0.3061)	loss 2.2793 (2.2286)	grad_norm 10.4354 (12.8522)	mem 4879MB
[2022-05-30 23:02:12 MetaFG_0] (main.py 265): INFO Train: [5/300][370/1562]	eta 0:06:04 lr 0.000002	time 0.2937 (0.3061)	loss 2.2213 (2.2285)	grad_norm 10.4789 (12.8430)	mem 4879MB
[2022-05-30 23:02:15 MetaFG_0] (main.py 265): INFO Train: [5/300][380/1562]	eta 0:06:01 lr 0.000002	time 0.2955 (0.3060)	loss 2.2670 (2.2288)	grad_norm 13.9528 (12.8476)	mem 4879MB
[2022-05-30 23:02:18 MetaFG_0] (main.py 265): INFO Train: [5/300][390/1562]	eta 0:05:58 lr 0.000002	time 0.2980 (0.3059)	loss 2.2664 (2.2286)	grad_norm 10.7830 (12.8734)	mem 4879MB
[2022-05-30 23:02:21 MetaFG_0] (main.py 265): INFO Train: [5/300][400/1562]	eta 0:05:55 lr 0.000002	time 0.2930 (0.3058)	loss 2.1801 (2.2282)	grad_norm 12.9476 (12.9004)	mem 4879MB
[2022-05-30 23:02:24 MetaFG_0] (main.py 265): INFO Train: [5/300][410/1562]	eta 0:05:52 lr 0.000002	time 0.3040 (0.3058)	loss 2.2407 (2.2280)	grad_norm 12.7586 (12.8836)	mem 4879MB
[2022-05-30 23:02:27 MetaFG_0] (main.py 265): INFO Train: [5/300][420/1562]	eta 0:05:49 lr 0.000002	time 0.2964 (0.3057)	loss 2.1940 (2.2279)	grad_norm 11.9641 (12.9063)	mem 4879MB
[2022-05-30 23:02:30 MetaFG_0] (main.py 265): INFO Train: [5/300][430/1562]	eta 0:05:45 lr 0.000002	time 0.2935 (0.3056)	loss 2.2973 (2.2276)	grad_norm 14.7372 (12.9063)	mem 4879MB
[2022-05-30 23:02:33 MetaFG_0] (main.py 265): INFO Train: [5/300][440/1562]	eta 0:05:42 lr 0.000002	time 0.3002 (0.3056)	loss 2.2268 (2.2273)	grad_norm 8.1411 (12.9025)	mem 4879MB
[2022-05-30 23:02:36 MetaFG_0] (main.py 265): INFO Train: [5/300][450/1562]	eta 0:05:39 lr 0.000002	time 0.2927 (0.3055)	loss 2.1796 (2.2271)	grad_norm 10.4921 (12.8788)	mem 4879MB
[2022-05-30 23:02:39 MetaFG_0] (main.py 265): INFO Train: [5/300][460/1562]	eta 0:05:36 lr 0.000002	time 0.2983 (0.3054)	loss 2.2242 (2.2273)	grad_norm 9.6443 (12.8625)	mem 4879MB
[2022-05-30 23:02:42 MetaFG_0] (main.py 265): INFO Train: [5/300][470/1562]	eta 0:05:33 lr 0.000002	time 0.3320 (0.3056)	loss 2.1937 (2.2271)	grad_norm 11.8178 (12.8495)	mem 4879MB
[2022-05-30 23:02:45 MetaFG_0] (main.py 265): INFO Train: [5/300][480/1562]	eta 0:05:30 lr 0.000002	time 0.2919 (0.3055)	loss 2.2696 (2.2272)	grad_norm 9.7838 (12.8638)	mem 4879MB
[2022-05-30 23:02:48 MetaFG_0] (main.py 265): INFO Train: [5/300][490/1562]	eta 0:05:27 lr 0.000002	time 0.3041 (0.3055)	loss 2.2025 (2.2268)	grad_norm 17.9588 (12.8763)	mem 4879MB
[2022-05-30 23:02:51 MetaFG_0] (main.py 265): INFO Train: [5/300][500/1562]	eta 0:05:24 lr 0.000002	time 0.2930 (0.3054)	loss 2.2637 (2.2267)	grad_norm 9.8886 (12.8425)	mem 4879MB
[2022-05-30 23:02:54 MetaFG_0] (main.py 265): INFO Train: [5/300][510/1562]	eta 0:05:21 lr 0.000002	time 0.2929 (0.3053)	loss 2.2078 (2.2263)	grad_norm 12.6102 (12.8601)	mem 4879MB
[2022-05-30 23:02:57 MetaFG_0] (main.py 265): INFO Train: [5/300][520/1562]	eta 0:05:18 lr 0.000002	time 0.2936 (0.3053)	loss 2.1831 (2.2262)	grad_norm 11.8839 (12.8721)	mem 4879MB
[2022-05-30 23:03:00 MetaFG_0] (main.py 265): INFO Train: [5/300][530/1562]	eta 0:05:15 lr 0.000002	time 0.2929 (0.3053)	loss 2.1806 (2.2261)	grad_norm 14.2076 (12.8931)	mem 4879MB
[2022-05-30 23:03:03 MetaFG_0] (main.py 265): INFO Train: [5/300][540/1562]	eta 0:05:11 lr 0.000002	time 0.2920 (0.3052)	loss 2.2208 (2.2257)	grad_norm 14.0575 (12.8988)	mem 4879MB
[2022-05-30 23:03:06 MetaFG_0] (main.py 265): INFO Train: [5/300][550/1562]	eta 0:05:08 lr 0.000002	time 0.2979 (0.3052)	loss 2.2191 (2.2255)	grad_norm 10.2170 (12.9006)	mem 4879MB
[2022-05-30 23:03:09 MetaFG_0] (main.py 265): INFO Train: [5/300][560/1562]	eta 0:05:05 lr 0.000002	time 0.2929 (0.3052)	loss 2.2240 (2.2254)	grad_norm 12.7184 (12.9312)	mem 4879MB
[2022-05-30 23:03:12 MetaFG_0] (main.py 265): INFO Train: [5/300][570/1562]	eta 0:05:02 lr 0.000002	time 0.2923 (0.3052)	loss 2.2519 (2.2254)	grad_norm 12.6254 (12.9253)	mem 4879MB
[2022-05-30 23:03:15 MetaFG_0] (main.py 265): INFO Train: [5/300][580/1562]	eta 0:04:59 lr 0.000002	time 0.2917 (0.3052)	loss 2.2228 (2.2256)	grad_norm 10.1987 (12.9126)	mem 4879MB
[2022-05-30 23:03:18 MetaFG_0] (main.py 265): INFO Train: [5/300][590/1562]	eta 0:04:56 lr 0.000002	time 0.2931 (0.3051)	loss 2.1693 (2.2252)	grad_norm 16.2158 (12.9231)	mem 4879MB
[2022-05-30 23:03:21 MetaFG_0] (main.py 265): INFO Train: [5/300][600/1562]	eta 0:04:53 lr 0.000002	time 0.2981 (0.3051)	loss 2.2157 (2.2251)	grad_norm 18.2140 (12.9210)	mem 4879MB
[2022-05-30 23:03:24 MetaFG_0] (main.py 265): INFO Train: [5/300][610/1562]	eta 0:04:50 lr 0.000002	time 0.2939 (0.3050)	loss 2.2548 (2.2247)	grad_norm 15.1703 (12.9417)	mem 4879MB
[2022-05-30 23:03:27 MetaFG_0] (main.py 265): INFO Train: [5/300][620/1562]	eta 0:04:47 lr 0.000002	time 0.2949 (0.3050)	loss 2.1918 (2.2246)	grad_norm 14.4801 (12.9351)	mem 4879MB
[2022-05-30 23:03:30 MetaFG_0] (main.py 265): INFO Train: [5/300][630/1562]	eta 0:04:44 lr 0.000002	time 0.2929 (0.3050)	loss 2.1964 (2.2245)	grad_norm 9.5356 (12.9380)	mem 4879MB
[2022-05-30 23:03:34 MetaFG_0] (main.py 265): INFO Train: [5/300][640/1562]	eta 0:04:41 lr 0.000002	time 0.2920 (0.3050)	loss 2.1989 (2.2244)	grad_norm 13.4577 (12.9409)	mem 4879MB
[2022-05-30 23:03:37 MetaFG_0] (main.py 265): INFO Train: [5/300][650/1562]	eta 0:04:38 lr 0.000002	time 0.2919 (0.3049)	loss 2.2360 (2.2242)	grad_norm 10.6845 (12.9401)	mem 4879MB
[2022-05-30 23:03:40 MetaFG_0] (main.py 265): INFO Train: [5/300][660/1562]	eta 0:04:35 lr 0.000002	time 0.2932 (0.3049)	loss 2.1826 (2.2240)	grad_norm 9.9822 (12.9354)	mem 4879MB
[2022-05-30 23:03:43 MetaFG_0] (main.py 265): INFO Train: [5/300][670/1562]	eta 0:04:31 lr 0.000002	time 0.2915 (0.3049)	loss 2.1968 (2.2241)	grad_norm 8.8639 (12.9487)	mem 4879MB
[2022-05-30 23:03:46 MetaFG_0] (main.py 265): INFO Train: [5/300][680/1562]	eta 0:04:28 lr 0.000002	time 0.2930 (0.3049)	loss 2.1604 (2.2236)	grad_norm 19.1662 (12.9723)	mem 4879MB
[2022-05-30 23:03:49 MetaFG_0] (main.py 265): INFO Train: [5/300][690/1562]	eta 0:04:25 lr 0.000002	time 0.2988 (0.3049)	loss 2.1973 (2.2237)	grad_norm 7.7221 (13.0016)	mem 4879MB
[2022-05-30 23:03:52 MetaFG_0] (main.py 265): INFO Train: [5/300][700/1562]	eta 0:04:22 lr 0.000002	time 0.2918 (0.3049)	loss 2.1992 (2.2237)	grad_norm 8.7922 (12.9969)	mem 4879MB
[2022-05-30 23:03:55 MetaFG_0] (main.py 265): INFO Train: [5/300][710/1562]	eta 0:04:19 lr 0.000002	time 0.2994 (0.3048)	loss 2.1846 (2.2234)	grad_norm 6.7082 (13.0087)	mem 4879MB
[2022-05-30 23:03:58 MetaFG_0] (main.py 265): INFO Train: [5/300][720/1562]	eta 0:04:16 lr 0.000002	time 0.3003 (0.3048)	loss 2.2471 (2.2238)	grad_norm 14.2234 (13.0006)	mem 4879MB
[2022-05-30 23:04:01 MetaFG_0] (main.py 265): INFO Train: [5/300][730/1562]	eta 0:04:13 lr 0.000002	time 0.2983 (0.3048)	loss 2.2457 (2.2238)	grad_norm 10.4624 (13.0021)	mem 4879MB
[2022-05-30 23:04:04 MetaFG_0] (main.py 265): INFO Train: [5/300][740/1562]	eta 0:04:10 lr 0.000002	time 0.2926 (0.3048)	loss 2.2166 (2.2235)	grad_norm 18.6265 (13.0161)	mem 4879MB
[2022-05-30 23:04:07 MetaFG_0] (main.py 265): INFO Train: [5/300][750/1562]	eta 0:04:07 lr 0.000002	time 0.2974 (0.3048)	loss 2.2556 (2.2234)	grad_norm 14.5618 (13.0181)	mem 4879MB
[2022-05-30 23:04:10 MetaFG_0] (main.py 265): INFO Train: [5/300][760/1562]	eta 0:04:04 lr 0.000002	time 0.2918 (0.3048)	loss 2.2560 (2.2233)	grad_norm 10.9554 (13.0137)	mem 4879MB
[2022-05-30 23:04:13 MetaFG_0] (main.py 265): INFO Train: [5/300][770/1562]	eta 0:04:01 lr 0.000002	time 0.2927 (0.3048)	loss 2.1981 (2.2231)	grad_norm 10.6242 (13.0075)	mem 4879MB
[2022-05-30 23:04:16 MetaFG_0] (main.py 265): INFO Train: [5/300][780/1562]	eta 0:03:58 lr 0.000002	time 0.2942 (0.3047)	loss 2.2547 (2.2229)	grad_norm 12.9934 (13.0032)	mem 4879MB
[2022-05-30 23:04:19 MetaFG_0] (main.py 265): INFO Train: [5/300][790/1562]	eta 0:03:55 lr 0.000002	time 0.3019 (0.3047)	loss 2.2177 (2.2230)	grad_norm 14.8545 (13.0023)	mem 4879MB
[2022-05-30 23:04:22 MetaFG_0] (main.py 265): INFO Train: [5/300][800/1562]	eta 0:03:52 lr 0.000002	time 0.2938 (0.3047)	loss 2.2452 (2.2230)	grad_norm 10.3441 (13.0035)	mem 4879MB
[2022-05-30 23:04:25 MetaFG_0] (main.py 265): INFO Train: [5/300][810/1562]	eta 0:03:49 lr 0.000002	time 0.2976 (0.3047)	loss 2.2771 (2.2231)	grad_norm 15.2996 (13.0088)	mem 4879MB
[2022-05-30 23:04:28 MetaFG_0] (main.py 265): INFO Train: [5/300][820/1562]	eta 0:03:46 lr 0.000002	time 0.2986 (0.3047)	loss 2.2038 (2.2229)	grad_norm 13.3398 (13.0235)	mem 4879MB
[2022-05-30 23:04:31 MetaFG_0] (main.py 265): INFO Train: [5/300][830/1562]	eta 0:03:43 lr 0.000002	time 0.2939 (0.3047)	loss 2.2409 (2.2227)	grad_norm 8.6226 (13.0106)	mem 4879MB
[2022-05-30 23:04:34 MetaFG_0] (main.py 265): INFO Train: [5/300][840/1562]	eta 0:03:39 lr 0.000002	time 0.2945 (0.3047)	loss 2.2382 (2.2225)	grad_norm 11.0759 (13.0169)	mem 4879MB
[2022-05-30 23:04:37 MetaFG_0] (main.py 265): INFO Train: [5/300][850/1562]	eta 0:03:36 lr 0.000002	time 0.2973 (0.3046)	loss 2.2155 (2.2222)	grad_norm 8.3474 (13.0244)	mem 4879MB
[2022-05-30 23:04:40 MetaFG_0] (main.py 265): INFO Train: [5/300][860/1562]	eta 0:03:33 lr 0.000002	time 0.2995 (0.3046)	loss 2.1716 (2.2219)	grad_norm 12.7207 (13.0206)	mem 4879MB
[2022-05-30 23:04:43 MetaFG_0] (main.py 265): INFO Train: [5/300][870/1562]	eta 0:03:30 lr 0.000002	time 0.3021 (0.3046)	loss 2.2329 (2.2216)	grad_norm 16.8011 (13.0102)	mem 4879MB
[2022-05-30 23:04:46 MetaFG_0] (main.py 265): INFO Train: [5/300][880/1562]	eta 0:03:27 lr 0.000002	time 0.2931 (0.3046)	loss 2.1190 (2.2213)	grad_norm 12.9771 (13.0083)	mem 4879MB
[2022-05-30 23:04:49 MetaFG_0] (main.py 265): INFO Train: [5/300][890/1562]	eta 0:03:24 lr 0.000002	time 0.3017 (0.3046)	loss 2.2357 (2.2212)	grad_norm 11.4597 (13.0131)	mem 4879MB
[2022-05-30 23:04:53 MetaFG_0] (main.py 265): INFO Train: [5/300][900/1562]	eta 0:03:21 lr 0.000002	time 0.2932 (0.3046)	loss 2.1822 (2.2210)	grad_norm 17.3509 (13.0282)	mem 4879MB
[2022-05-30 23:04:56 MetaFG_0] (main.py 265): INFO Train: [5/300][910/1562]	eta 0:03:18 lr 0.000002	time 0.2985 (0.3046)	loss 2.2062 (2.2209)	grad_norm 19.6386 (13.0430)	mem 4879MB
[2022-05-30 23:04:59 MetaFG_0] (main.py 265): INFO Train: [5/300][920/1562]	eta 0:03:15 lr 0.000002	time 0.2987 (0.3047)	loss 2.2292 (2.2208)	grad_norm 12.6757 (13.0501)	mem 4879MB
[2022-05-30 23:05:02 MetaFG_0] (main.py 265): INFO Train: [5/300][930/1562]	eta 0:03:12 lr 0.000002	time 0.2920 (0.3047)	loss 2.1516 (2.2208)	grad_norm 11.6627 (13.0401)	mem 4879MB
[2022-05-30 23:05:05 MetaFG_0] (main.py 265): INFO Train: [5/300][940/1562]	eta 0:03:09 lr 0.000002	time 0.3003 (0.3047)	loss 2.1859 (2.2207)	grad_norm 12.3865 (13.0440)	mem 4879MB
[2022-05-30 23:05:08 MetaFG_0] (main.py 265): INFO Train: [5/300][950/1562]	eta 0:03:06 lr 0.000002	time 0.2932 (0.3047)	loss 2.1774 (2.2208)	grad_norm 9.3884 (13.0373)	mem 4879MB
[2022-05-30 23:05:11 MetaFG_0] (main.py 265): INFO Train: [5/300][960/1562]	eta 0:03:03 lr 0.000002	time 0.2927 (0.3047)	loss 2.1438 (2.2204)	grad_norm 18.4713 (13.0595)	mem 4879MB
[2022-05-30 23:05:14 MetaFG_0] (main.py 265): INFO Train: [5/300][970/1562]	eta 0:03:00 lr 0.000002	time 0.2932 (0.3047)	loss 2.2429 (2.2203)	grad_norm 10.3522 (13.0586)	mem 4879MB
[2022-05-30 23:05:17 MetaFG_0] (main.py 265): INFO Train: [5/300][980/1562]	eta 0:02:57 lr 0.000002	time 0.3044 (0.3047)	loss 2.2265 (2.2203)	grad_norm 11.4852 (13.0692)	mem 4879MB
[2022-05-30 23:05:20 MetaFG_0] (main.py 265): INFO Train: [5/300][990/1562]	eta 0:02:54 lr 0.000002	time 0.2919 (0.3047)	loss 2.1740 (2.2200)	grad_norm 15.7335 (13.0764)	mem 4879MB
[2022-05-30 23:05:23 MetaFG_0] (main.py 265): INFO Train: [5/300][1000/1562]	eta 0:02:51 lr 0.000002	time 0.2944 (0.3047)	loss 2.2257 (2.2197)	grad_norm 16.8253 (13.0684)	mem 4879MB
[2022-05-30 23:05:26 MetaFG_0] (main.py 265): INFO Train: [5/300][1010/1562]	eta 0:02:48 lr 0.000002	time 0.2935 (0.3047)	loss 2.1679 (2.2196)	grad_norm 11.7411 (13.0634)	mem 4879MB
[2022-05-30 23:05:29 MetaFG_0] (main.py 265): INFO Train: [5/300][1020/1562]	eta 0:02:45 lr 0.000002	time 0.2936 (0.3047)	loss 2.2001 (2.2194)	grad_norm 15.4055 (13.0803)	mem 4879MB
[2022-05-30 23:05:32 MetaFG_0] (main.py 265): INFO Train: [5/300][1030/1562]	eta 0:02:42 lr 0.000002	time 0.2924 (0.3046)	loss 2.1876 (2.2193)	grad_norm 15.0912 (13.0962)	mem 4879MB
[2022-05-30 23:05:35 MetaFG_0] (main.py 265): INFO Train: [5/300][1040/1562]	eta 0:02:39 lr 0.000002	time 0.2986 (0.3046)	loss 2.2776 (2.2190)	grad_norm 11.3174 (13.1114)	mem 4879MB
[2022-05-30 23:05:38 MetaFG_0] (main.py 265): INFO Train: [5/300][1050/1562]	eta 0:02:35 lr 0.000002	time 0.2920 (0.3046)	loss 2.1965 (2.2192)	grad_norm 10.6459 (13.1089)	mem 4879MB
[2022-05-30 23:05:41 MetaFG_0] (main.py 265): INFO Train: [5/300][1060/1562]	eta 0:02:32 lr 0.000002	time 0.2916 (0.3046)	loss 2.2257 (2.2190)	grad_norm 11.9211 (13.1083)	mem 4879MB
[2022-05-30 23:05:44 MetaFG_0] (main.py 265): INFO Train: [5/300][1070/1562]	eta 0:02:29 lr 0.000002	time 0.2974 (0.3046)	loss 2.1484 (2.2188)	grad_norm 18.7526 (13.1205)	mem 4879MB
[2022-05-30 23:05:47 MetaFG_0] (main.py 265): INFO Train: [5/300][1080/1562]	eta 0:02:26 lr 0.000002	time 0.2918 (0.3046)	loss 2.1644 (2.2186)	grad_norm 15.5937 (13.1353)	mem 4879MB
[2022-05-30 23:05:50 MetaFG_0] (main.py 265): INFO Train: [5/300][1090/1562]	eta 0:02:23 lr 0.000002	time 0.2923 (0.3046)	loss 2.1893 (2.2184)	grad_norm 8.1698 (13.1357)	mem 4879MB
[2022-05-30 23:05:53 MetaFG_0] (main.py 265): INFO Train: [5/300][1100/1562]	eta 0:02:20 lr 0.000002	time 0.2927 (0.3046)	loss 2.2289 (2.2181)	grad_norm 10.5390 (13.1325)	mem 4879MB
[2022-05-30 23:05:56 MetaFG_0] (main.py 265): INFO Train: [5/300][1110/1562]	eta 0:02:17 lr 0.000002	time 0.2978 (0.3046)	loss 2.2097 (2.2178)	grad_norm 15.4042 (13.1465)	mem 4879MB
[2022-05-30 23:05:59 MetaFG_0] (main.py 265): INFO Train: [5/300][1120/1562]	eta 0:02:14 lr 0.000002	time 0.2945 (0.3046)	loss 2.2652 (2.2177)	grad_norm 11.0016 (13.1399)	mem 4879MB
[2022-05-30 23:06:03 MetaFG_0] (main.py 265): INFO Train: [5/300][1130/1562]	eta 0:02:11 lr 0.000002	time 0.2921 (0.3046)	loss 2.2098 (2.2176)	grad_norm 9.9913 (13.1308)	mem 4879MB
[2022-05-30 23:06:06 MetaFG_0] (main.py 265): INFO Train: [5/300][1140/1562]	eta 0:02:08 lr 0.000002	time 0.2985 (0.3046)	loss 2.1851 (2.2172)	grad_norm 17.7335 (13.1506)	mem 4879MB
[2022-05-30 23:06:09 MetaFG_0] (main.py 265): INFO Train: [5/300][1150/1562]	eta 0:02:05 lr 0.000002	time 0.2915 (0.3046)	loss 2.2110 (2.2171)	grad_norm 15.5360 (13.1553)	mem 4879MB
[2022-05-30 23:06:12 MetaFG_0] (main.py 265): INFO Train: [5/300][1160/1562]	eta 0:02:02 lr 0.000002	time 0.2913 (0.3046)	loss 2.2302 (2.2172)	grad_norm 10.5093 (13.1590)	mem 4879MB
[2022-05-30 23:06:15 MetaFG_0] (main.py 265): INFO Train: [5/300][1170/1562]	eta 0:01:59 lr 0.000002	time 0.2986 (0.3045)	loss 2.2867 (2.2167)	grad_norm 13.4279 (13.1578)	mem 4879MB
[2022-05-30 23:06:18 MetaFG_0] (main.py 265): INFO Train: [5/300][1180/1562]	eta 0:01:56 lr 0.000002	time 0.2922 (0.3045)	loss 2.2620 (2.2167)	grad_norm 12.1206 (13.1837)	mem 4879MB
[2022-05-30 23:06:21 MetaFG_0] (main.py 265): INFO Train: [5/300][1190/1562]	eta 0:01:53 lr 0.000002	time 0.2928 (0.3045)	loss 2.1587 (2.2163)	grad_norm 16.0030 (13.2020)	mem 4879MB
[2022-05-30 23:06:24 MetaFG_0] (main.py 265): INFO Train: [5/300][1200/1562]	eta 0:01:50 lr 0.000002	time 0.2990 (0.3045)	loss 2.2398 (2.2160)	grad_norm 12.6031 (13.2146)	mem 4879MB
[2022-05-30 23:06:27 MetaFG_0] (main.py 265): INFO Train: [5/300][1210/1562]	eta 0:01:47 lr 0.000002	time 0.2943 (0.3045)	loss 2.2028 (2.2159)	grad_norm 15.5397 (13.2176)	mem 4879MB
[2022-05-30 23:06:30 MetaFG_0] (main.py 265): INFO Train: [5/300][1220/1562]	eta 0:01:44 lr 0.000002	time 0.2992 (0.3045)	loss 2.2276 (2.2157)	grad_norm 13.4458 (13.2394)	mem 4879MB
[2022-05-30 23:06:33 MetaFG_0] (main.py 265): INFO Train: [5/300][1230/1562]	eta 0:01:41 lr 0.000002	time 0.2920 (0.3045)	loss 2.2316 (2.2154)	grad_norm 13.9623 (13.2381)	mem 4879MB
[2022-05-30 23:06:36 MetaFG_0] (main.py 265): INFO Train: [5/300][1240/1562]	eta 0:01:38 lr 0.000002	time 0.2921 (0.3045)	loss 2.0599 (2.2148)	grad_norm 17.1724 (13.2636)	mem 4879MB
[2022-05-30 23:06:39 MetaFG_0] (main.py 265): INFO Train: [5/300][1250/1562]	eta 0:01:34 lr 0.000002	time 0.2974 (0.3045)	loss 2.2245 (2.2147)	grad_norm 12.9025 (13.2721)	mem 4879MB
[2022-05-30 23:06:42 MetaFG_0] (main.py 265): INFO Train: [5/300][1260/1562]	eta 0:01:31 lr 0.000002	time 0.2917 (0.3045)	loss 2.0932 (2.2144)	grad_norm 15.2778 (13.2762)	mem 4879MB
[2022-05-30 23:06:45 MetaFG_0] (main.py 265): INFO Train: [5/300][1270/1562]	eta 0:01:28 lr 0.000002	time 0.2977 (0.3044)	loss 2.2114 (2.2142)	grad_norm 13.1074 (13.2806)	mem 4879MB
[2022-05-30 23:06:48 MetaFG_0] (main.py 265): INFO Train: [5/300][1280/1562]	eta 0:01:25 lr 0.000002	time 0.2975 (0.3044)	loss 2.2120 (2.2140)	grad_norm 11.4691 (13.2944)	mem 4879MB
[2022-05-30 23:06:51 MetaFG_0] (main.py 265): INFO Train: [5/300][1290/1562]	eta 0:01:22 lr 0.000002	time 0.2979 (0.3044)	loss 2.1711 (2.2136)	grad_norm 11.3914 (13.2979)	mem 4879MB
[2022-05-30 23:06:54 MetaFG_0] (main.py 265): INFO Train: [5/300][1300/1562]	eta 0:01:19 lr 0.000002	time 0.2914 (0.3044)	loss 2.2668 (2.2137)	grad_norm 14.2473 (13.2983)	mem 4879MB
[2022-05-30 23:06:57 MetaFG_0] (main.py 265): INFO Train: [5/300][1310/1562]	eta 0:01:16 lr 0.000002	time 0.2982 (0.3044)	loss 2.1331 (2.2136)	grad_norm 11.7722 (13.3106)	mem 4879MB
[2022-05-30 23:07:00 MetaFG_0] (main.py 265): INFO Train: [5/300][1320/1562]	eta 0:01:13 lr 0.000002	time 0.2931 (0.3044)	loss 2.1293 (2.2133)	grad_norm 15.7775 (13.3296)	mem 4879MB
[2022-05-30 23:07:03 MetaFG_0] (main.py 265): INFO Train: [5/300][1330/1562]	eta 0:01:10 lr 0.000002	time 0.2978 (0.3044)	loss 2.1469 (2.2131)	grad_norm 13.0302 (13.3454)	mem 4879MB
[2022-05-30 23:07:06 MetaFG_0] (main.py 265): INFO Train: [5/300][1340/1562]	eta 0:01:07 lr 0.000002	time 0.2918 (0.3044)	loss 2.1565 (2.2128)	grad_norm 14.6134 (13.3620)	mem 4879MB
[2022-05-30 23:07:09 MetaFG_0] (main.py 265): INFO Train: [5/300][1350/1562]	eta 0:01:04 lr 0.000002	time 0.3172 (0.3044)	loss 2.1854 (2.2125)	grad_norm 13.4049 (13.3649)	mem 4879MB
[2022-05-30 23:07:12 MetaFG_0] (main.py 265): INFO Train: [5/300][1360/1562]	eta 0:01:01 lr 0.000002	time 0.2976 (0.3044)	loss 2.2275 (2.2125)	grad_norm 8.0697 (13.3865)	mem 4879MB
[2022-05-30 23:07:15 MetaFG_0] (main.py 265): INFO Train: [5/300][1370/1562]	eta 0:00:58 lr 0.000002	time 0.2956 (0.3044)	loss 2.0949 (2.2121)	grad_norm 15.2653 (13.4009)	mem 4879MB
[2022-05-30 23:07:18 MetaFG_0] (main.py 265): INFO Train: [5/300][1380/1562]	eta 0:00:55 lr 0.000002	time 0.2983 (0.3044)	loss 2.0458 (2.2118)	grad_norm 15.7028 (13.4132)	mem 4879MB
[2022-05-30 23:07:21 MetaFG_0] (main.py 265): INFO Train: [5/300][1390/1562]	eta 0:00:52 lr 0.000002	time 0.2934 (0.3044)	loss 2.1009 (2.2116)	grad_norm 10.9091 (13.4217)	mem 4879MB
[2022-05-30 23:07:25 MetaFG_0] (main.py 265): INFO Train: [5/300][1400/1562]	eta 0:00:49 lr 0.000002	time 0.3102 (0.3044)	loss 2.2186 (2.2114)	grad_norm 10.3713 (13.4182)	mem 4879MB
[2022-05-30 23:07:28 MetaFG_0] (main.py 265): INFO Train: [5/300][1410/1562]	eta 0:00:46 lr 0.000002	time 0.2929 (0.3044)	loss 2.1802 (2.2112)	grad_norm 14.1350 (13.4225)	mem 4879MB
[2022-05-30 23:07:31 MetaFG_0] (main.py 265): INFO Train: [5/300][1420/1562]	eta 0:00:43 lr 0.000002	time 0.2924 (0.3044)	loss 2.2117 (2.2109)	grad_norm 7.0145 (13.4286)	mem 4879MB
[2022-05-30 23:07:34 MetaFG_0] (main.py 265): INFO Train: [5/300][1430/1562]	eta 0:00:40 lr 0.000002	time 0.2943 (0.3044)	loss 2.2398 (2.2108)	grad_norm 14.1947 (13.4339)	mem 4879MB
[2022-05-30 23:07:37 MetaFG_0] (main.py 265): INFO Train: [5/300][1440/1562]	eta 0:00:37 lr 0.000002	time 0.2931 (0.3043)	loss 2.2198 (2.2105)	grad_norm 12.9435 (13.4455)	mem 4879MB
[2022-05-30 23:07:40 MetaFG_0] (main.py 265): INFO Train: [5/300][1450/1562]	eta 0:00:34 lr 0.000002	time 0.2934 (0.3043)	loss 2.2634 (2.2103)	grad_norm 9.8420 (13.4410)	mem 4879MB
[2022-05-30 23:07:43 MetaFG_0] (main.py 265): INFO Train: [5/300][1460/1562]	eta 0:00:31 lr 0.000002	time 0.2925 (0.3043)	loss 2.0907 (2.2101)	grad_norm 20.3216 (13.4495)	mem 4879MB
[2022-05-30 23:07:46 MetaFG_0] (main.py 265): INFO Train: [5/300][1470/1562]	eta 0:00:27 lr 0.000002	time 0.2927 (0.3043)	loss 2.2147 (2.2100)	grad_norm 10.4764 (13.4443)	mem 4879MB
[2022-05-30 23:07:49 MetaFG_0] (main.py 265): INFO Train: [5/300][1480/1562]	eta 0:00:24 lr 0.000002	time 0.2932 (0.3043)	loss 2.0842 (2.2098)	grad_norm 16.5681 (13.4491)	mem 4879MB
[2022-05-30 23:07:52 MetaFG_0] (main.py 265): INFO Train: [5/300][1490/1562]	eta 0:00:21 lr 0.000002	time 0.2930 (0.3043)	loss 2.2145 (2.2095)	grad_norm 8.7916 (13.4498)	mem 4879MB
[2022-05-30 23:07:55 MetaFG_0] (main.py 265): INFO Train: [5/300][1500/1562]	eta 0:00:18 lr 0.000002	time 0.2925 (0.3043)	loss 2.2436 (2.2094)	grad_norm 9.6964 (13.4664)	mem 4879MB
[2022-05-30 23:07:58 MetaFG_0] (main.py 265): INFO Train: [5/300][1510/1562]	eta 0:00:15 lr 0.000002	time 0.2995 (0.3043)	loss 2.1746 (2.2091)	grad_norm 12.7195 (13.4706)	mem 4879MB
[2022-05-30 23:08:01 MetaFG_0] (main.py 265): INFO Train: [5/300][1520/1562]	eta 0:00:12 lr 0.000002	time 0.2947 (0.3043)	loss 2.1421 (2.2087)	grad_norm 14.7026 (13.4959)	mem 4879MB
[2022-05-30 23:08:04 MetaFG_0] (main.py 265): INFO Train: [5/300][1530/1562]	eta 0:00:09 lr 0.000002	time 0.2973 (0.3043)	loss 2.1843 (2.2085)	grad_norm 13.3536 (13.4981)	mem 4879MB
[2022-05-30 23:08:07 MetaFG_0] (main.py 265): INFO Train: [5/300][1540/1562]	eta 0:00:06 lr 0.000002	time 0.2938 (0.3043)	loss 2.2173 (2.2082)	grad_norm 16.4523 (13.5069)	mem 4879MB
[2022-05-30 23:08:10 MetaFG_0] (main.py 265): INFO Train: [5/300][1550/1562]	eta 0:00:03 lr 0.000002	time 0.2941 (0.3043)	loss 2.1571 (2.2080)	grad_norm 12.3713 (13.5142)	mem 4879MB
[2022-05-30 23:08:13 MetaFG_0] (main.py 265): INFO Train: [5/300][1560/1562]	eta 0:00:00 lr 0.000002	time 0.2925 (0.3043)	loss 2.2515 (2.2079)	grad_norm 16.6170 (13.5155)	mem 4879MB
[2022-05-30 23:08:13 MetaFG_0] (main.py 272): INFO EPOCH 5 training takes 0:07:55
[2022-05-30 23:08:13 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_5.pth saving......
[2022-05-30 23:08:14 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_5.pth saved !!!
[2022-05-30 23:08:14 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:08:16 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:08:16 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:08:17 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.758 (0.758)	Loss 4.0484 (4.0484)	Acc@1 15.625 (15.625)	Acc@5 37.500 (37.500)	Mem 4879MB
[2022-05-30 23:08:18 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.157)	Loss 3.9847 (3.8992)	Acc@1 9.375 (19.318)	Acc@5 43.750 (51.705)	Mem 4879MB
[2022-05-30 23:08:18 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.126)	Loss 3.8541 (3.9054)	Acc@1 12.500 (19.196)	Acc@5 56.250 (50.298)	Mem 4879MB
[2022-05-30 23:08:19 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.116)	Loss 3.9591 (3.9042)	Acc@1 15.625 (19.052)	Acc@5 56.250 (50.101)	Mem 4879MB
[2022-05-30 23:08:20 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.111)	Loss 4.1865 (3.9181)	Acc@1 3.125 (18.598)	Acc@5 25.000 (48.704)	Mem 4879MB
[2022-05-30 23:08:21 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.092 (0.107)	Loss 3.8884 (3.9100)	Acc@1 18.750 (19.118)	Acc@5 50.000 (49.020)	Mem 4879MB
[2022-05-30 23:08:22 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.093 (0.105)	Loss 3.7806 (3.9081)	Acc@1 21.875 (18.750)	Acc@5 56.250 (49.488)	Mem 4879MB
[2022-05-30 23:08:23 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.104)	Loss 3.8400 (3.9059)	Acc@1 18.750 (18.926)	Acc@5 40.625 (49.648)	Mem 4879MB
[2022-05-30 23:08:24 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.103)	Loss 3.7726 (3.9052)	Acc@1 31.250 (18.904)	Acc@5 53.125 (49.614)	Mem 4879MB
[2022-05-30 23:08:25 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.090 (0.101)	Loss 3.9362 (3.9055)	Acc@1 15.625 (18.784)	Acc@5 50.000 (49.416)	Mem 4879MB
[2022-05-30 23:08:26 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.101)	Loss 3.8548 (3.9061)	Acc@1 12.500 (18.874)	Acc@5 53.125 (49.103)	Mem 4879MB
[2022-05-30 23:08:27 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.093 (0.100)	Loss 4.0024 (3.9085)	Acc@1 28.125 (19.060)	Acc@5 37.500 (48.902)	Mem 4879MB
[2022-05-30 23:08:28 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.089 (0.099)	Loss 3.9957 (3.9097)	Acc@1 18.750 (19.215)	Acc@5 34.375 (48.657)	Mem 4879MB
[2022-05-30 23:08:29 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 3.8530 (3.9053)	Acc@1 25.000 (19.323)	Acc@5 53.125 (48.855)	Mem 4879MB
[2022-05-30 23:08:30 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 3.9941 (3.9040)	Acc@1 21.875 (19.592)	Acc@5 40.625 (48.892)	Mem 4879MB
[2022-05-30 23:08:31 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 3.8989 (3.9017)	Acc@1 15.625 (19.723)	Acc@5 56.250 (48.945)	Mem 4879MB
[2022-05-30 23:08:32 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 3.8202 (3.8981)	Acc@1 15.625 (19.876)	Acc@5 59.375 (49.224)	Mem 4879MB
[2022-05-30 23:08:33 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.104 (0.098)	Loss 3.7452 (3.8977)	Acc@1 25.000 (19.755)	Acc@5 65.625 (49.452)	Mem 4879MB
[2022-05-30 23:08:33 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 3.9912 (3.9007)	Acc@1 15.625 (19.700)	Acc@5 28.125 (48.947)	Mem 4879MB
[2022-05-30 23:08:34 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.093 (0.098)	Loss 3.8912 (3.9002)	Acc@1 21.875 (19.650)	Acc@5 53.125 (49.002)	Mem 4879MB
[2022-05-30 23:08:35 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.089 (0.097)	Loss 3.9092 (3.8996)	Acc@1 25.000 (19.714)	Acc@5 53.125 (48.927)	Mem 4879MB
[2022-05-30 23:08:36 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 3.9263 (3.9015)	Acc@1 18.750 (19.742)	Acc@5 56.250 (48.741)	Mem 4879MB
[2022-05-30 23:08:37 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 3.7159 (3.8991)	Acc@1 28.125 (19.825)	Acc@5 62.500 (48.855)	Mem 4879MB
[2022-05-30 23:08:38 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.097)	Loss 3.9260 (3.8986)	Acc@1 21.875 (19.981)	Acc@5 56.250 (49.012)	Mem 4879MB
[2022-05-30 23:08:39 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.109 (0.097)	Loss 3.9714 (3.8992)	Acc@1 15.625 (19.930)	Acc@5 50.000 (48.989)	Mem 4879MB
[2022-05-30 23:08:40 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.097)	Loss 3.8503 (3.8976)	Acc@1 25.000 (20.020)	Acc@5 56.250 (49.066)	Mem 4879MB
[2022-05-30 23:08:41 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.096)	Loss 3.9441 (3.8998)	Acc@1 21.875 (19.911)	Acc@5 43.750 (48.910)	Mem 4879MB
[2022-05-30 23:08:42 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 4.0073 (3.8986)	Acc@1 6.250 (19.995)	Acc@5 37.500 (48.951)	Mem 4879MB
[2022-05-30 23:08:43 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.093 (0.096)	Loss 3.9947 (3.8995)	Acc@1 9.375 (19.973)	Acc@5 40.625 (48.843)	Mem 4879MB
[2022-05-30 23:08:44 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.100 (0.096)	Loss 3.8991 (3.8997)	Acc@1 21.875 (19.931)	Acc@5 50.000 (48.787)	Mem 4879MB
[2022-05-30 23:08:45 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 4.0652 (3.8996)	Acc@1 12.500 (19.944)	Acc@5 37.500 (48.879)	Mem 4879MB
[2022-05-30 23:08:46 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 3.8453 (3.8999)	Acc@1 25.000 (19.865)	Acc@5 59.375 (48.895)	Mem 4879MB
[2022-05-30 23:08:46 MetaFG_0] (main.py 330): INFO  * Acc@1 19.870 Acc@5 48.890
[2022-05-30 23:08:46 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 19.9%
[2022-05-30 23:08:46 MetaFG_0] (main.py 171): INFO Max accuracy: 19.87%
[2022-05-30 23:08:47 MetaFG_0] (main.py 265): INFO Train: [6/300][0/1562]	eta 0:28:13 lr 0.000002	time 1.0841 (1.0841)	loss 2.1733 (2.1733)	grad_norm 11.4236 (11.4236)	mem 4879MB
[2022-05-30 23:08:50 MetaFG_0] (main.py 265): INFO Train: [6/300][10/1562]	eta 0:09:39 lr 0.000002	time 0.2935 (0.3731)	loss 2.2540 (2.1837)	grad_norm 19.6890 (13.9268)	mem 4879MB
[2022-05-30 23:08:53 MetaFG_0] (main.py 265): INFO Train: [6/300][20/1562]	eta 0:08:44 lr 0.000002	time 0.2988 (0.3404)	loss 2.1713 (2.1792)	grad_norm 10.1155 (13.2325)	mem 4879MB
[2022-05-30 23:08:56 MetaFG_0] (main.py 265): INFO Train: [6/300][30/1562]	eta 0:08:22 lr 0.000002	time 0.2940 (0.3283)	loss 2.1521 (2.1761)	grad_norm 13.6032 (13.7935)	mem 4879MB
[2022-05-30 23:08:59 MetaFG_0] (main.py 265): INFO Train: [6/300][40/1562]	eta 0:08:10 lr 0.000002	time 0.2988 (0.3222)	loss 2.1466 (2.1768)	grad_norm 15.7887 (13.7983)	mem 4879MB
[2022-05-30 23:09:02 MetaFG_0] (main.py 265): INFO Train: [6/300][50/1562]	eta 0:08:01 lr 0.000002	time 0.2922 (0.3188)	loss 2.2270 (2.1817)	grad_norm 11.6934 (13.6995)	mem 4879MB
[2022-05-30 23:09:05 MetaFG_0] (main.py 265): INFO Train: [6/300][60/1562]	eta 0:07:55 lr 0.000002	time 0.2981 (0.3163)	loss 2.1350 (2.1786)	grad_norm 15.2544 (13.7681)	mem 4879MB
[2022-05-30 23:09:08 MetaFG_0] (main.py 265): INFO Train: [6/300][70/1562]	eta 0:07:49 lr 0.000002	time 0.2980 (0.3145)	loss 2.2188 (2.1771)	grad_norm 8.3742 (13.7820)	mem 4879MB
[2022-05-30 23:09:11 MetaFG_0] (main.py 265): INFO Train: [6/300][80/1562]	eta 0:07:43 lr 0.000002	time 0.2937 (0.3130)	loss 2.1543 (2.1750)	grad_norm 19.2246 (13.8943)	mem 4879MB
[2022-05-30 23:09:14 MetaFG_0] (main.py 265): INFO Train: [6/300][90/1562]	eta 0:07:39 lr 0.000002	time 0.2931 (0.3121)	loss 2.1420 (2.1739)	grad_norm 10.4696 (14.0518)	mem 4879MB
[2022-05-30 23:09:17 MetaFG_0] (main.py 265): INFO Train: [6/300][100/1562]	eta 0:07:34 lr 0.000002	time 0.2922 (0.3110)	loss 2.1163 (2.1737)	grad_norm 10.9936 (14.0225)	mem 4879MB
[2022-05-30 23:09:20 MetaFG_0] (main.py 265): INFO Train: [6/300][110/1562]	eta 0:07:30 lr 0.000002	time 0.2939 (0.3106)	loss 2.1674 (2.1755)	grad_norm 17.9128 (14.1271)	mem 4879MB
[2022-05-30 23:09:23 MetaFG_0] (main.py 265): INFO Train: [6/300][120/1562]	eta 0:07:27 lr 0.000002	time 0.3005 (0.3100)	loss 2.1858 (2.1761)	grad_norm 19.8474 (14.1872)	mem 4879MB
[2022-05-30 23:09:27 MetaFG_0] (main.py 265): INFO Train: [6/300][130/1562]	eta 0:07:24 lr 0.000002	time 0.2920 (0.3104)	loss 2.1004 (2.1749)	grad_norm 18.3854 (14.2365)	mem 4879MB
[2022-05-30 23:09:30 MetaFG_0] (main.py 265): INFO Train: [6/300][140/1562]	eta 0:07:20 lr 0.000002	time 0.2994 (0.3100)	loss 2.1212 (2.1736)	grad_norm 24.3531 (14.5309)	mem 4879MB
[2022-05-30 23:09:33 MetaFG_0] (main.py 265): INFO Train: [6/300][150/1562]	eta 0:07:17 lr 0.000002	time 0.2930 (0.3096)	loss 2.1225 (2.1731)	grad_norm 10.8054 (14.5685)	mem 4879MB
[2022-05-30 23:09:36 MetaFG_0] (main.py 265): INFO Train: [6/300][160/1562]	eta 0:07:13 lr 0.000002	time 0.2923 (0.3092)	loss 2.1775 (2.1728)	grad_norm 17.8141 (14.5611)	mem 4879MB
[2022-05-30 23:09:39 MetaFG_0] (main.py 265): INFO Train: [6/300][170/1562]	eta 0:07:09 lr 0.000002	time 0.2925 (0.3089)	loss 2.1812 (2.1728)	grad_norm 10.2483 (14.6267)	mem 4879MB
[2022-05-30 23:09:42 MetaFG_0] (main.py 265): INFO Train: [6/300][180/1562]	eta 0:07:06 lr 0.000002	time 0.2918 (0.3085)	loss 2.1683 (2.1719)	grad_norm 14.5326 (14.7208)	mem 4879MB
[2022-05-30 23:09:45 MetaFG_0] (main.py 265): INFO Train: [6/300][190/1562]	eta 0:07:02 lr 0.000002	time 0.2986 (0.3083)	loss 2.1859 (2.1720)	grad_norm 15.3404 (14.6682)	mem 4879MB
[2022-05-30 23:09:48 MetaFG_0] (main.py 265): INFO Train: [6/300][200/1562]	eta 0:06:59 lr 0.000002	time 0.2976 (0.3081)	loss 2.1760 (2.1721)	grad_norm 24.6344 (14.7192)	mem 4879MB
[2022-05-30 23:09:51 MetaFG_0] (main.py 265): INFO Train: [6/300][210/1562]	eta 0:06:56 lr 0.000002	time 0.2990 (0.3079)	loss 2.1323 (2.1715)	grad_norm 11.2825 (14.6454)	mem 4879MB
[2022-05-30 23:09:54 MetaFG_0] (main.py 265): INFO Train: [6/300][220/1562]	eta 0:06:52 lr 0.000002	time 0.2931 (0.3077)	loss 2.1835 (2.1708)	grad_norm 15.6204 (14.7044)	mem 4879MB
[2022-05-30 23:09:57 MetaFG_0] (main.py 265): INFO Train: [6/300][230/1562]	eta 0:06:49 lr 0.000002	time 0.2926 (0.3075)	loss 2.1212 (2.1696)	grad_norm 13.1112 (14.7672)	mem 4879MB
[2022-05-30 23:10:00 MetaFG_0] (main.py 265): INFO Train: [6/300][240/1562]	eta 0:06:46 lr 0.000002	time 0.2976 (0.3074)	loss 2.1520 (2.1710)	grad_norm 10.7622 (14.7431)	mem 4879MB
[2022-05-30 23:10:03 MetaFG_0] (main.py 265): INFO Train: [6/300][250/1562]	eta 0:06:43 lr 0.000002	time 0.2977 (0.3072)	loss 2.1671 (2.1702)	grad_norm 13.6363 (14.7667)	mem 4879MB
[2022-05-30 23:10:06 MetaFG_0] (main.py 265): INFO Train: [6/300][260/1562]	eta 0:06:39 lr 0.000002	time 0.2945 (0.3070)	loss 2.1308 (2.1705)	grad_norm 18.4982 (14.8434)	mem 4879MB
[2022-05-30 23:10:09 MetaFG_0] (main.py 265): INFO Train: [6/300][270/1562]	eta 0:06:36 lr 0.000002	time 0.3026 (0.3070)	loss 2.2389 (2.1693)	grad_norm 11.4496 (14.9478)	mem 4879MB
[2022-05-30 23:10:12 MetaFG_0] (main.py 265): INFO Train: [6/300][280/1562]	eta 0:06:33 lr 0.000002	time 0.2981 (0.3069)	loss 2.1575 (2.1692)	grad_norm 21.9595 (14.9660)	mem 4879MB
[2022-05-30 23:10:15 MetaFG_0] (main.py 265): INFO Train: [6/300][290/1562]	eta 0:06:30 lr 0.000002	time 0.2922 (0.3067)	loss 2.1678 (2.1693)	grad_norm 14.3774 (14.9921)	mem 4879MB
[2022-05-30 23:10:18 MetaFG_0] (main.py 265): INFO Train: [6/300][300/1562]	eta 0:06:26 lr 0.000002	time 0.2922 (0.3066)	loss 2.0880 (2.1694)	grad_norm 14.8448 (14.9906)	mem 4879MB
[2022-05-30 23:10:21 MetaFG_0] (main.py 265): INFO Train: [6/300][310/1562]	eta 0:06:23 lr 0.000002	time 0.2997 (0.3065)	loss 2.1775 (2.1699)	grad_norm 15.1902 (14.9597)	mem 4879MB
[2022-05-30 23:10:24 MetaFG_0] (main.py 265): INFO Train: [6/300][320/1562]	eta 0:06:20 lr 0.000002	time 0.2981 (0.3065)	loss 2.2806 (2.1699)	grad_norm 18.3171 (14.9749)	mem 4879MB
[2022-05-30 23:10:27 MetaFG_0] (main.py 265): INFO Train: [6/300][330/1562]	eta 0:06:17 lr 0.000002	time 0.2998 (0.3064)	loss 2.2010 (2.1700)	grad_norm 14.2147 (14.9527)	mem 4879MB
[2022-05-30 23:10:30 MetaFG_0] (main.py 265): INFO Train: [6/300][340/1562]	eta 0:06:14 lr 0.000002	time 0.2948 (0.3063)	loss 2.1957 (2.1703)	grad_norm 8.7280 (14.9038)	mem 4879MB
[2022-05-30 23:10:33 MetaFG_0] (main.py 265): INFO Train: [6/300][350/1562]	eta 0:06:11 lr 0.000002	time 0.2913 (0.3062)	loss 2.2410 (2.1699)	grad_norm 9.0627 (14.9590)	mem 4879MB
[2022-05-30 23:10:36 MetaFG_0] (main.py 265): INFO Train: [6/300][360/1562]	eta 0:06:08 lr 0.000002	time 0.2977 (0.3062)	loss 2.1583 (2.1696)	grad_norm 11.7603 (14.9182)	mem 4879MB
[2022-05-30 23:10:39 MetaFG_0] (main.py 265): INFO Train: [6/300][370/1562]	eta 0:06:04 lr 0.000002	time 0.2920 (0.3061)	loss 2.1863 (2.1695)	grad_norm 13.5623 (14.9210)	mem 4879MB
[2022-05-30 23:10:42 MetaFG_0] (main.py 265): INFO Train: [6/300][380/1562]	eta 0:06:01 lr 0.000002	time 0.2941 (0.3060)	loss 2.1175 (2.1695)	grad_norm 12.8458 (14.8822)	mem 4879MB
[2022-05-30 23:10:45 MetaFG_0] (main.py 265): INFO Train: [6/300][390/1562]	eta 0:05:58 lr 0.000002	time 0.2929 (0.3060)	loss 2.1362 (2.1692)	grad_norm 10.9426 (14.8673)	mem 4879MB
[2022-05-30 23:10:49 MetaFG_0] (main.py 265): INFO Train: [6/300][400/1562]	eta 0:05:55 lr 0.000002	time 0.2932 (0.3059)	loss 2.2146 (2.1689)	grad_norm 12.8816 (14.8600)	mem 4879MB
[2022-05-30 23:10:52 MetaFG_0] (main.py 265): INFO Train: [6/300][410/1562]	eta 0:05:52 lr 0.000002	time 0.2926 (0.3058)	loss 2.1182 (2.1677)	grad_norm 11.9437 (14.8236)	mem 4879MB
[2022-05-30 23:10:55 MetaFG_0] (main.py 265): INFO Train: [6/300][420/1562]	eta 0:05:49 lr 0.000002	time 0.2919 (0.3058)	loss 2.1670 (2.1676)	grad_norm 18.0634 (14.8223)	mem 4879MB
[2022-05-30 23:10:58 MetaFG_0] (main.py 265): INFO Train: [6/300][430/1562]	eta 0:05:46 lr 0.000002	time 0.2931 (0.3057)	loss 2.1575 (2.1675)	grad_norm 17.3348 (14.8234)	mem 4879MB
[2022-05-30 23:11:01 MetaFG_0] (main.py 265): INFO Train: [6/300][440/1562]	eta 0:05:42 lr 0.000002	time 0.2981 (0.3057)	loss 2.0312 (2.1667)	grad_norm 12.8485 (14.7998)	mem 4879MB
[2022-05-30 23:11:04 MetaFG_0] (main.py 265): INFO Train: [6/300][450/1562]	eta 0:05:39 lr 0.000002	time 0.2993 (0.3056)	loss 2.0874 (2.1669)	grad_norm 13.8713 (14.7962)	mem 4879MB
[2022-05-30 23:11:07 MetaFG_0] (main.py 265): INFO Train: [6/300][460/1562]	eta 0:05:36 lr 0.000002	time 0.2931 (0.3056)	loss 2.1208 (2.1667)	grad_norm 12.8638 (14.8001)	mem 4879MB
[2022-05-30 23:11:10 MetaFG_0] (main.py 265): INFO Train: [6/300][470/1562]	eta 0:05:33 lr 0.000002	time 0.2930 (0.3056)	loss 2.1411 (2.1664)	grad_norm 15.9792 (14.7429)	mem 4879MB
[2022-05-30 23:11:13 MetaFG_0] (main.py 265): INFO Train: [6/300][480/1562]	eta 0:05:30 lr 0.000002	time 0.2917 (0.3056)	loss 2.1108 (2.1660)	grad_norm 15.5482 (14.7264)	mem 4879MB
[2022-05-30 23:11:16 MetaFG_0] (main.py 265): INFO Train: [6/300][490/1562]	eta 0:05:27 lr 0.000002	time 0.2914 (0.3055)	loss 2.2247 (2.1659)	grad_norm 12.3171 (14.7042)	mem 4879MB
[2022-05-30 23:11:19 MetaFG_0] (main.py 265): INFO Train: [6/300][500/1562]	eta 0:05:24 lr 0.000002	time 0.2920 (0.3055)	loss 2.0261 (2.1655)	grad_norm 13.9087 (14.7674)	mem 4879MB
[2022-05-30 23:11:22 MetaFG_0] (main.py 265): INFO Train: [6/300][510/1562]	eta 0:05:21 lr 0.000002	time 0.2931 (0.3054)	loss 2.2232 (2.1659)	grad_norm 11.7644 (14.7589)	mem 4879MB
[2022-05-30 23:11:25 MetaFG_0] (main.py 265): INFO Train: [6/300][520/1562]	eta 0:05:18 lr 0.000002	time 0.2996 (0.3054)	loss 2.1199 (2.1657)	grad_norm 17.1976 (14.7402)	mem 4879MB
[2022-05-30 23:11:28 MetaFG_0] (main.py 265): INFO Train: [6/300][530/1562]	eta 0:05:15 lr 0.000002	time 0.2985 (0.3054)	loss 2.1065 (2.1646)	grad_norm 14.4943 (14.7503)	mem 4879MB
[2022-05-30 23:11:31 MetaFG_0] (main.py 265): INFO Train: [6/300][540/1562]	eta 0:05:12 lr 0.000002	time 0.2974 (0.3054)	loss 2.1912 (2.1647)	grad_norm 18.0474 (14.7567)	mem 4879MB
[2022-05-30 23:11:34 MetaFG_0] (main.py 265): INFO Train: [6/300][550/1562]	eta 0:05:09 lr 0.000002	time 0.2978 (0.3054)	loss 2.2460 (2.1646)	grad_norm 14.8113 (14.7883)	mem 4879MB
[2022-05-30 23:11:37 MetaFG_0] (main.py 265): INFO Train: [6/300][560/1562]	eta 0:05:05 lr 0.000002	time 0.2919 (0.3053)	loss 2.1901 (2.1648)	grad_norm 10.0010 (14.7729)	mem 4879MB
[2022-05-30 23:11:40 MetaFG_0] (main.py 265): INFO Train: [6/300][570/1562]	eta 0:05:03 lr 0.000002	time 0.3059 (0.3055)	loss 2.1883 (2.1646)	grad_norm 12.3440 (14.7953)	mem 4879MB
[2022-05-30 23:11:43 MetaFG_0] (main.py 265): INFO Train: [6/300][580/1562]	eta 0:04:59 lr 0.000002	time 0.2913 (0.3055)	loss 2.1748 (2.1645)	grad_norm 15.0587 (14.7906)	mem 4879MB
[2022-05-30 23:11:46 MetaFG_0] (main.py 265): INFO Train: [6/300][590/1562]	eta 0:04:56 lr 0.000002	time 0.2976 (0.3054)	loss 2.1232 (2.1641)	grad_norm 10.4506 (14.8208)	mem 4879MB
[2022-05-30 23:11:49 MetaFG_0] (main.py 265): INFO Train: [6/300][600/1562]	eta 0:04:53 lr 0.000002	time 0.2987 (0.3054)	loss 2.0659 (2.1640)	grad_norm 14.7639 (14.8176)	mem 4879MB
[2022-05-30 23:11:52 MetaFG_0] (main.py 265): INFO Train: [6/300][610/1562]	eta 0:04:50 lr 0.000002	time 0.2933 (0.3054)	loss 2.1723 (2.1644)	grad_norm 16.7372 (14.8398)	mem 4879MB
[2022-05-30 23:11:55 MetaFG_0] (main.py 265): INFO Train: [6/300][620/1562]	eta 0:04:47 lr 0.000002	time 0.2915 (0.3054)	loss 2.0762 (2.1639)	grad_norm 19.2228 (14.8536)	mem 4879MB
[2022-05-30 23:11:59 MetaFG_0] (main.py 265): INFO Train: [6/300][630/1562]	eta 0:04:44 lr 0.000002	time 0.2919 (0.3054)	loss 2.1268 (2.1638)	grad_norm 25.6457 (14.8619)	mem 4879MB
[2022-05-30 23:12:02 MetaFG_0] (main.py 265): INFO Train: [6/300][640/1562]	eta 0:04:41 lr 0.000002	time 0.2929 (0.3053)	loss 2.1350 (2.1638)	grad_norm 13.7523 (14.8612)	mem 4879MB
[2022-05-30 23:12:05 MetaFG_0] (main.py 265): INFO Train: [6/300][650/1562]	eta 0:04:38 lr 0.000002	time 0.2928 (0.3053)	loss 2.1586 (2.1638)	grad_norm 16.7585 (14.8546)	mem 4879MB
[2022-05-30 23:12:08 MetaFG_0] (main.py 265): INFO Train: [6/300][660/1562]	eta 0:04:35 lr 0.000002	time 0.2918 (0.3053)	loss 2.1931 (2.1639)	grad_norm 16.8483 (14.8808)	mem 4879MB
[2022-05-30 23:12:11 MetaFG_0] (main.py 265): INFO Train: [6/300][670/1562]	eta 0:04:32 lr 0.000002	time 0.3039 (0.3053)	loss 2.0555 (2.1639)	grad_norm 14.4251 (14.8858)	mem 4879MB
[2022-05-30 23:12:14 MetaFG_0] (main.py 265): INFO Train: [6/300][680/1562]	eta 0:04:29 lr 0.000002	time 0.2933 (0.3053)	loss 2.1824 (2.1635)	grad_norm 12.0297 (14.8762)	mem 4879MB
[2022-05-30 23:12:17 MetaFG_0] (main.py 265): INFO Train: [6/300][690/1562]	eta 0:04:26 lr 0.000002	time 0.2989 (0.3052)	loss 2.1416 (2.1637)	grad_norm 9.9700 (14.8663)	mem 4879MB
[2022-05-30 23:12:20 MetaFG_0] (main.py 265): INFO Train: [6/300][700/1562]	eta 0:04:23 lr 0.000002	time 0.2922 (0.3052)	loss 2.1600 (2.1637)	grad_norm 11.3418 (14.8462)	mem 4879MB
[2022-05-30 23:12:23 MetaFG_0] (main.py 265): INFO Train: [6/300][710/1562]	eta 0:04:20 lr 0.000002	time 0.2980 (0.3052)	loss 2.1971 (2.1636)	grad_norm 16.3169 (14.8661)	mem 4879MB
[2022-05-30 23:12:26 MetaFG_0] (main.py 265): INFO Train: [6/300][720/1562]	eta 0:04:16 lr 0.000002	time 0.2979 (0.3052)	loss 2.1658 (2.1630)	grad_norm 7.9907 (14.8377)	mem 4879MB
[2022-05-30 23:12:29 MetaFG_0] (main.py 265): INFO Train: [6/300][730/1562]	eta 0:04:13 lr 0.000002	time 0.2937 (0.3052)	loss 2.1754 (2.1628)	grad_norm 15.2129 (14.8773)	mem 4879MB
[2022-05-30 23:12:32 MetaFG_0] (main.py 265): INFO Train: [6/300][740/1562]	eta 0:04:10 lr 0.000002	time 0.2926 (0.3052)	loss 2.0992 (2.1627)	grad_norm 14.1299 (14.8708)	mem 4879MB
[2022-05-30 23:12:35 MetaFG_0] (main.py 265): INFO Train: [6/300][750/1562]	eta 0:04:07 lr 0.000002	time 0.2982 (0.3051)	loss 2.1317 (2.1625)	grad_norm 11.8629 (14.8598)	mem 4879MB
[2022-05-30 23:12:38 MetaFG_0] (main.py 265): INFO Train: [6/300][760/1562]	eta 0:04:04 lr 0.000002	time 0.2926 (0.3051)	loss 2.2009 (2.1621)	grad_norm 13.6474 (14.8442)	mem 4879MB
[2022-05-30 23:12:41 MetaFG_0] (main.py 265): INFO Train: [6/300][770/1562]	eta 0:04:01 lr 0.000002	time 0.2978 (0.3051)	loss 2.0589 (2.1620)	grad_norm 18.2464 (14.8627)	mem 4879MB
[2022-05-30 23:12:44 MetaFG_0] (main.py 265): INFO Train: [6/300][780/1562]	eta 0:03:58 lr 0.000002	time 0.2972 (0.3051)	loss 2.1933 (2.1617)	grad_norm 12.4232 (14.8678)	mem 4879MB
[2022-05-30 23:12:47 MetaFG_0] (main.py 265): INFO Train: [6/300][790/1562]	eta 0:03:55 lr 0.000002	time 0.2918 (0.3050)	loss 2.0470 (2.1610)	grad_norm 15.4095 (14.8814)	mem 4879MB
[2022-05-30 23:12:50 MetaFG_0] (main.py 265): INFO Train: [6/300][800/1562]	eta 0:03:52 lr 0.000002	time 0.2999 (0.3050)	loss 2.1151 (2.1608)	grad_norm 17.3822 (14.8772)	mem 4879MB
[2022-05-30 23:12:53 MetaFG_0] (main.py 265): INFO Train: [6/300][810/1562]	eta 0:03:49 lr 0.000002	time 0.3003 (0.3050)	loss 2.0211 (2.1607)	grad_norm 23.7668 (inf)	mem 4879MB
[2022-05-30 23:12:56 MetaFG_0] (main.py 265): INFO Train: [6/300][820/1562]	eta 0:03:46 lr 0.000002	time 0.3035 (0.3050)	loss 2.1758 (2.1606)	grad_norm 12.9033 (inf)	mem 4879MB
[2022-05-30 23:12:59 MetaFG_0] (main.py 265): INFO Train: [6/300][830/1562]	eta 0:03:43 lr 0.000002	time 0.2931 (0.3050)	loss 2.1933 (2.1607)	grad_norm 14.8879 (inf)	mem 4879MB
[2022-05-30 23:13:02 MetaFG_0] (main.py 265): INFO Train: [6/300][840/1562]	eta 0:03:40 lr 0.000002	time 0.2916 (0.3049)	loss 2.1634 (2.1604)	grad_norm 15.0954 (inf)	mem 4879MB
[2022-05-30 23:13:05 MetaFG_0] (main.py 265): INFO Train: [6/300][850/1562]	eta 0:03:37 lr 0.000002	time 0.2918 (0.3049)	loss 2.1674 (2.1600)	grad_norm 19.8491 (inf)	mem 4879MB
[2022-05-30 23:13:08 MetaFG_0] (main.py 265): INFO Train: [6/300][860/1562]	eta 0:03:34 lr 0.000002	time 0.2939 (0.3049)	loss 2.1408 (2.1598)	grad_norm 15.3471 (inf)	mem 4879MB
[2022-05-30 23:13:11 MetaFG_0] (main.py 265): INFO Train: [6/300][870/1562]	eta 0:03:30 lr 0.000002	time 0.2979 (0.3049)	loss 2.2151 (2.1595)	grad_norm 14.1042 (inf)	mem 4879MB
[2022-05-30 23:13:14 MetaFG_0] (main.py 265): INFO Train: [6/300][880/1562]	eta 0:03:27 lr 0.000002	time 0.2935 (0.3049)	loss 2.1633 (2.1598)	grad_norm 23.8503 (inf)	mem 4879MB
[2022-05-30 23:13:17 MetaFG_0] (main.py 265): INFO Train: [6/300][890/1562]	eta 0:03:24 lr 0.000002	time 0.2977 (0.3049)	loss 2.1707 (2.1597)	grad_norm 17.6705 (inf)	mem 4879MB
[2022-05-30 23:13:20 MetaFG_0] (main.py 265): INFO Train: [6/300][900/1562]	eta 0:03:21 lr 0.000002	time 0.2976 (0.3048)	loss 1.9813 (2.1595)	grad_norm 18.0592 (inf)	mem 4879MB
[2022-05-30 23:13:24 MetaFG_0] (main.py 265): INFO Train: [6/300][910/1562]	eta 0:03:18 lr 0.000002	time 0.2984 (0.3048)	loss 2.0794 (2.1592)	grad_norm 13.0101 (inf)	mem 4879MB
[2022-05-30 23:13:27 MetaFG_0] (main.py 265): INFO Train: [6/300][920/1562]	eta 0:03:15 lr 0.000002	time 0.2917 (0.3048)	loss 2.1685 (2.1591)	grad_norm 14.6151 (inf)	mem 4879MB
[2022-05-30 23:13:30 MetaFG_0] (main.py 265): INFO Train: [6/300][930/1562]	eta 0:03:12 lr 0.000002	time 0.2926 (0.3048)	loss 2.1350 (2.1592)	grad_norm 11.7666 (inf)	mem 4879MB
[2022-05-30 23:13:33 MetaFG_0] (main.py 265): INFO Train: [6/300][940/1562]	eta 0:03:09 lr 0.000002	time 0.2946 (0.3048)	loss 2.0852 (2.1590)	grad_norm 13.3120 (inf)	mem 4879MB
[2022-05-30 23:13:36 MetaFG_0] (main.py 265): INFO Train: [6/300][950/1562]	eta 0:03:06 lr 0.000002	time 0.2991 (0.3048)	loss 2.0901 (2.1586)	grad_norm 15.1124 (inf)	mem 4879MB
[2022-05-30 23:13:39 MetaFG_0] (main.py 265): INFO Train: [6/300][960/1562]	eta 0:03:03 lr 0.000002	time 0.2974 (0.3048)	loss 2.1593 (2.1584)	grad_norm 13.4307 (inf)	mem 4879MB
[2022-05-30 23:13:42 MetaFG_0] (main.py 265): INFO Train: [6/300][970/1562]	eta 0:03:00 lr 0.000002	time 0.2921 (0.3048)	loss 2.1012 (2.1580)	grad_norm 16.8267 (inf)	mem 4879MB
[2022-05-30 23:13:45 MetaFG_0] (main.py 265): INFO Train: [6/300][980/1562]	eta 0:02:57 lr 0.000002	time 0.2919 (0.3048)	loss 2.1869 (2.1578)	grad_norm 16.4184 (inf)	mem 4879MB
[2022-05-30 23:13:48 MetaFG_0] (main.py 265): INFO Train: [6/300][990/1562]	eta 0:02:54 lr 0.000002	time 0.2950 (0.3048)	loss 2.0436 (2.1577)	grad_norm 13.3062 (inf)	mem 4879MB
[2022-05-30 23:13:51 MetaFG_0] (main.py 265): INFO Train: [6/300][1000/1562]	eta 0:02:51 lr 0.000002	time 0.2996 (0.3048)	loss 2.1367 (2.1575)	grad_norm 13.2753 (inf)	mem 4879MB
[2022-05-30 23:13:54 MetaFG_0] (main.py 265): INFO Train: [6/300][1010/1562]	eta 0:02:48 lr 0.000002	time 0.2918 (0.3048)	loss 2.2487 (2.1573)	grad_norm 11.4658 (inf)	mem 4879MB
[2022-05-30 23:13:57 MetaFG_0] (main.py 265): INFO Train: [6/300][1020/1562]	eta 0:02:45 lr 0.000002	time 0.2918 (0.3048)	loss 2.0568 (2.1570)	grad_norm 21.1202 (inf)	mem 4879MB
[2022-05-30 23:14:00 MetaFG_0] (main.py 265): INFO Train: [6/300][1030/1562]	eta 0:02:42 lr 0.000002	time 0.2929 (0.3048)	loss 2.0159 (2.1568)	grad_norm 15.3083 (inf)	mem 4879MB
[2022-05-30 23:14:03 MetaFG_0] (main.py 265): INFO Train: [6/300][1040/1562]	eta 0:02:39 lr 0.000002	time 0.3007 (0.3048)	loss 2.0713 (2.1567)	grad_norm 12.2842 (inf)	mem 4879MB
[2022-05-30 23:14:06 MetaFG_0] (main.py 265): INFO Train: [6/300][1050/1562]	eta 0:02:36 lr 0.000002	time 0.2918 (0.3048)	loss 2.0176 (2.1564)	grad_norm 18.0501 (inf)	mem 4879MB
[2022-05-30 23:14:09 MetaFG_0] (main.py 265): INFO Train: [6/300][1060/1562]	eta 0:02:32 lr 0.000002	time 0.2991 (0.3048)	loss 2.1793 (2.1559)	grad_norm 12.5159 (inf)	mem 4879MB
[2022-05-30 23:14:12 MetaFG_0] (main.py 265): INFO Train: [6/300][1070/1562]	eta 0:02:29 lr 0.000002	time 0.2934 (0.3048)	loss 2.1628 (2.1557)	grad_norm 10.5387 (inf)	mem 4879MB
[2022-05-30 23:14:15 MetaFG_0] (main.py 265): INFO Train: [6/300][1080/1562]	eta 0:02:26 lr 0.000002	time 0.2914 (0.3048)	loss 2.1260 (2.1554)	grad_norm 15.0914 (inf)	mem 4879MB
[2022-05-30 23:14:18 MetaFG_0] (main.py 265): INFO Train: [6/300][1090/1562]	eta 0:02:23 lr 0.000002	time 0.2988 (0.3047)	loss 2.2249 (2.1552)	grad_norm 13.5443 (inf)	mem 4879MB
[2022-05-30 23:14:21 MetaFG_0] (main.py 265): INFO Train: [6/300][1100/1562]	eta 0:02:20 lr 0.000002	time 0.2983 (0.3047)	loss 2.1340 (2.1553)	grad_norm 12.7963 (inf)	mem 4879MB
[2022-05-30 23:14:24 MetaFG_0] (main.py 265): INFO Train: [6/300][1110/1562]	eta 0:02:17 lr 0.000002	time 0.2923 (0.3047)	loss 2.1400 (2.1552)	grad_norm 11.3351 (inf)	mem 4879MB
[2022-05-30 23:14:27 MetaFG_0] (main.py 265): INFO Train: [6/300][1120/1562]	eta 0:02:14 lr 0.000002	time 0.2934 (0.3047)	loss 2.2041 (2.1550)	grad_norm 12.1820 (inf)	mem 4879MB
[2022-05-30 23:14:30 MetaFG_0] (main.py 265): INFO Train: [6/300][1130/1562]	eta 0:02:11 lr 0.000002	time 0.2938 (0.3047)	loss 2.1373 (2.1547)	grad_norm 10.2267 (inf)	mem 4879MB
[2022-05-30 23:14:34 MetaFG_0] (main.py 265): INFO Train: [6/300][1140/1562]	eta 0:02:08 lr 0.000002	time 0.2919 (0.3047)	loss 2.1561 (2.1549)	grad_norm 23.4107 (inf)	mem 4879MB
[2022-05-30 23:14:37 MetaFG_0] (main.py 265): INFO Train: [6/300][1150/1562]	eta 0:02:05 lr 0.000002	time 0.2982 (0.3047)	loss 2.0937 (2.1545)	grad_norm 15.3989 (inf)	mem 4879MB
[2022-05-30 23:14:40 MetaFG_0] (main.py 265): INFO Train: [6/300][1160/1562]	eta 0:02:02 lr 0.000002	time 0.2983 (0.3047)	loss 2.1019 (2.1543)	grad_norm 17.0778 (inf)	mem 4879MB
[2022-05-30 23:14:43 MetaFG_0] (main.py 265): INFO Train: [6/300][1170/1562]	eta 0:01:59 lr 0.000002	time 0.2977 (0.3047)	loss 2.2234 (2.1539)	grad_norm 15.8531 (inf)	mem 4879MB
[2022-05-30 23:14:46 MetaFG_0] (main.py 265): INFO Train: [6/300][1180/1562]	eta 0:01:56 lr 0.000002	time 0.2918 (0.3047)	loss 2.1861 (2.1537)	grad_norm 11.9205 (inf)	mem 4879MB
[2022-05-30 23:14:49 MetaFG_0] (main.py 265): INFO Train: [6/300][1190/1562]	eta 0:01:53 lr 0.000002	time 0.2983 (0.3047)	loss 2.0306 (2.1529)	grad_norm 20.9436 (inf)	mem 4879MB
[2022-05-30 23:14:52 MetaFG_0] (main.py 265): INFO Train: [6/300][1200/1562]	eta 0:01:50 lr 0.000002	time 0.2946 (0.3047)	loss 2.0738 (2.1525)	grad_norm 15.0149 (inf)	mem 4879MB
[2022-05-30 23:14:55 MetaFG_0] (main.py 265): INFO Train: [6/300][1210/1562]	eta 0:01:47 lr 0.000002	time 0.2985 (0.3046)	loss 2.1474 (2.1524)	grad_norm 17.7387 (inf)	mem 4879MB
[2022-05-30 23:14:58 MetaFG_0] (main.py 265): INFO Train: [6/300][1220/1562]	eta 0:01:44 lr 0.000002	time 0.2919 (0.3047)	loss 2.1480 (2.1519)	grad_norm 9.3246 (inf)	mem 4879MB
[2022-05-30 23:15:01 MetaFG_0] (main.py 265): INFO Train: [6/300][1230/1562]	eta 0:01:41 lr 0.000002	time 0.3009 (0.3047)	loss 2.0436 (2.1514)	grad_norm 11.4234 (inf)	mem 4879MB
[2022-05-30 23:15:04 MetaFG_0] (main.py 265): INFO Train: [6/300][1240/1562]	eta 0:01:38 lr 0.000002	time 0.2921 (0.3046)	loss 2.1478 (2.1511)	grad_norm 13.8550 (inf)	mem 4879MB
[2022-05-30 23:15:07 MetaFG_0] (main.py 265): INFO Train: [6/300][1250/1562]	eta 0:01:35 lr 0.000002	time 0.2920 (0.3046)	loss 1.9901 (2.1508)	grad_norm 12.5722 (inf)	mem 4879MB
[2022-05-30 23:15:10 MetaFG_0] (main.py 265): INFO Train: [6/300][1260/1562]	eta 0:01:31 lr 0.000002	time 0.2934 (0.3046)	loss 2.2017 (2.1509)	grad_norm 14.1896 (inf)	mem 4879MB
[2022-05-30 23:15:13 MetaFG_0] (main.py 265): INFO Train: [6/300][1270/1562]	eta 0:01:28 lr 0.000002	time 0.2964 (0.3046)	loss 2.0956 (2.1506)	grad_norm 15.1729 (inf)	mem 4879MB
[2022-05-30 23:15:16 MetaFG_0] (main.py 265): INFO Train: [6/300][1280/1562]	eta 0:01:25 lr 0.000002	time 0.2980 (0.3046)	loss 2.1275 (2.1502)	grad_norm 19.0417 (inf)	mem 4879MB
[2022-05-30 23:15:19 MetaFG_0] (main.py 265): INFO Train: [6/300][1290/1562]	eta 0:01:22 lr 0.000002	time 0.2915 (0.3046)	loss 2.0312 (2.1500)	grad_norm 15.6861 (inf)	mem 4879MB
[2022-05-30 23:15:22 MetaFG_0] (main.py 265): INFO Train: [6/300][1300/1562]	eta 0:01:19 lr 0.000002	time 0.2978 (0.3046)	loss 2.1517 (2.1500)	grad_norm 17.5136 (inf)	mem 4879MB
[2022-05-30 23:15:25 MetaFG_0] (main.py 265): INFO Train: [6/300][1310/1562]	eta 0:01:16 lr 0.000002	time 0.2924 (0.3046)	loss 2.0488 (2.1498)	grad_norm 15.2120 (inf)	mem 4879MB
[2022-05-30 23:15:28 MetaFG_0] (main.py 265): INFO Train: [6/300][1320/1562]	eta 0:01:13 lr 0.000002	time 0.2919 (0.3046)	loss 2.1776 (2.1497)	grad_norm 16.2806 (inf)	mem 4879MB
[2022-05-30 23:15:31 MetaFG_0] (main.py 265): INFO Train: [6/300][1330/1562]	eta 0:01:10 lr 0.000002	time 0.2975 (0.3046)	loss 2.1727 (2.1496)	grad_norm 12.2447 (inf)	mem 4879MB
[2022-05-30 23:15:34 MetaFG_0] (main.py 265): INFO Train: [6/300][1340/1562]	eta 0:01:07 lr 0.000002	time 0.2915 (0.3046)	loss 2.0988 (2.1495)	grad_norm 18.0657 (inf)	mem 4879MB
[2022-05-30 23:15:37 MetaFG_0] (main.py 265): INFO Train: [6/300][1350/1562]	eta 0:01:04 lr 0.000002	time 0.2916 (0.3046)	loss 2.1705 (2.1493)	grad_norm 11.4808 (inf)	mem 4879MB
[2022-05-30 23:15:40 MetaFG_0] (main.py 265): INFO Train: [6/300][1360/1562]	eta 0:01:01 lr 0.000002	time 0.2921 (0.3045)	loss 2.1563 (2.1491)	grad_norm 15.0604 (inf)	mem 4879MB
[2022-05-30 23:15:43 MetaFG_0] (main.py 265): INFO Train: [6/300][1370/1562]	eta 0:00:58 lr 0.000002	time 0.2972 (0.3045)	loss 2.0435 (2.1488)	grad_norm 22.3954 (inf)	mem 4879MB
[2022-05-30 23:15:46 MetaFG_0] (main.py 265): INFO Train: [6/300][1380/1562]	eta 0:00:55 lr 0.000002	time 0.2918 (0.3045)	loss 2.1145 (2.1487)	grad_norm 12.7409 (inf)	mem 4879MB
[2022-05-30 23:15:49 MetaFG_0] (main.py 265): INFO Train: [6/300][1390/1562]	eta 0:00:52 lr 0.000002	time 0.2980 (0.3045)	loss 2.1371 (2.1484)	grad_norm 16.7325 (inf)	mem 4879MB
[2022-05-30 23:15:53 MetaFG_0] (main.py 265): INFO Train: [6/300][1400/1562]	eta 0:00:49 lr 0.000002	time 0.3009 (0.3045)	loss 2.1308 (2.1480)	grad_norm 13.7033 (inf)	mem 4879MB
[2022-05-30 23:15:56 MetaFG_0] (main.py 265): INFO Train: [6/300][1410/1562]	eta 0:00:46 lr 0.000002	time 0.2930 (0.3045)	loss 2.1403 (2.1479)	grad_norm 10.9314 (inf)	mem 4879MB
[2022-05-30 23:15:59 MetaFG_0] (main.py 265): INFO Train: [6/300][1420/1562]	eta 0:00:43 lr 0.000002	time 0.2927 (0.3045)	loss 2.1818 (2.1478)	grad_norm 8.5727 (inf)	mem 4879MB
[2022-05-30 23:16:02 MetaFG_0] (main.py 265): INFO Train: [6/300][1430/1562]	eta 0:00:40 lr 0.000002	time 0.2988 (0.3045)	loss 2.1459 (2.1474)	grad_norm 13.0215 (inf)	mem 4879MB
[2022-05-30 23:16:05 MetaFG_0] (main.py 265): INFO Train: [6/300][1440/1562]	eta 0:00:37 lr 0.000002	time 0.2917 (0.3045)	loss 1.9899 (2.1471)	grad_norm 30.3229 (inf)	mem 4879MB
[2022-05-30 23:16:08 MetaFG_0] (main.py 265): INFO Train: [6/300][1450/1562]	eta 0:00:34 lr 0.000002	time 0.2985 (0.3045)	loss 2.1593 (2.1468)	grad_norm 15.3121 (inf)	mem 4879MB
[2022-05-30 23:16:11 MetaFG_0] (main.py 265): INFO Train: [6/300][1460/1562]	eta 0:00:31 lr 0.000002	time 0.2930 (0.3045)	loss 2.1056 (2.1464)	grad_norm 18.2521 (inf)	mem 4879MB
[2022-05-30 23:16:14 MetaFG_0] (main.py 265): INFO Train: [6/300][1470/1562]	eta 0:00:28 lr 0.000002	time 0.3072 (0.3045)	loss 2.0998 (2.1461)	grad_norm 11.4282 (inf)	mem 4879MB
[2022-05-30 23:16:17 MetaFG_0] (main.py 265): INFO Train: [6/300][1480/1562]	eta 0:00:24 lr 0.000002	time 0.2945 (0.3046)	loss 2.1185 (2.1461)	grad_norm 9.2366 (inf)	mem 4879MB
[2022-05-30 23:16:20 MetaFG_0] (main.py 265): INFO Train: [6/300][1490/1562]	eta 0:00:21 lr 0.000002	time 0.2927 (0.3046)	loss 2.0210 (2.1460)	grad_norm 10.4294 (inf)	mem 4879MB
[2022-05-30 23:16:23 MetaFG_0] (main.py 265): INFO Train: [6/300][1500/1562]	eta 0:00:18 lr 0.000002	time 0.2934 (0.3046)	loss 2.1299 (2.1459)	grad_norm 10.7450 (inf)	mem 4879MB
[2022-05-30 23:16:26 MetaFG_0] (main.py 265): INFO Train: [6/300][1510/1562]	eta 0:00:15 lr 0.000002	time 0.2984 (0.3046)	loss 2.0507 (2.1457)	grad_norm 14.4473 (inf)	mem 4879MB
[2022-05-30 23:16:29 MetaFG_0] (main.py 265): INFO Train: [6/300][1520/1562]	eta 0:00:12 lr 0.000002	time 0.2978 (0.3046)	loss 2.2113 (2.1455)	grad_norm 17.7277 (inf)	mem 4879MB
[2022-05-30 23:16:32 MetaFG_0] (main.py 265): INFO Train: [6/300][1530/1562]	eta 0:00:09 lr 0.000002	time 0.2925 (0.3046)	loss 2.1153 (2.1449)	grad_norm 12.8389 (inf)	mem 4879MB
[2022-05-30 23:16:35 MetaFG_0] (main.py 265): INFO Train: [6/300][1540/1562]	eta 0:00:06 lr 0.000002	time 0.2917 (0.3046)	loss 2.0721 (2.1446)	grad_norm 23.5996 (inf)	mem 4879MB
[2022-05-30 23:16:38 MetaFG_0] (main.py 265): INFO Train: [6/300][1550/1562]	eta 0:00:03 lr 0.000002	time 0.2933 (0.3046)	loss 2.0651 (2.1444)	grad_norm 19.7920 (inf)	mem 4879MB
[2022-05-30 23:16:41 MetaFG_0] (main.py 265): INFO Train: [6/300][1560/1562]	eta 0:00:00 lr 0.000002	time 0.2910 (0.3046)	loss 2.1547 (2.1441)	grad_norm 17.3003 (inf)	mem 4879MB
[2022-05-30 23:16:42 MetaFG_0] (main.py 272): INFO EPOCH 6 training takes 0:07:55
[2022-05-30 23:16:42 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_6.pth saving......
[2022-05-30 23:16:43 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_6.pth saved !!!
[2022-05-30 23:16:43 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:16:44 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:16:44 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:16:45 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.649 (0.649)	Loss 3.5589 (3.5589)	Acc@1 40.625 (40.625)	Acc@5 53.125 (53.125)	Mem 4879MB
[2022-05-30 23:16:46 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.099 (0.159)	Loss 3.5861 (3.5398)	Acc@1 25.000 (31.250)	Acc@5 53.125 (60.511)	Mem 4879MB
[2022-05-30 23:16:47 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.127)	Loss 3.6827 (3.5451)	Acc@1 25.000 (32.292)	Acc@5 59.375 (61.458)	Mem 4879MB
[2022-05-30 23:16:48 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.100 (0.116)	Loss 3.5592 (3.5656)	Acc@1 37.500 (29.738)	Acc@5 56.250 (62.298)	Mem 4879MB
[2022-05-30 23:16:49 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.111)	Loss 3.7667 (3.5781)	Acc@1 21.875 (29.345)	Acc@5 59.375 (62.043)	Mem 4879MB
[2022-05-30 23:16:50 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.108)	Loss 3.5987 (3.5707)	Acc@1 40.625 (29.228)	Acc@5 62.500 (62.439)	Mem 4879MB
[2022-05-30 23:16:51 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.106)	Loss 3.5423 (3.5705)	Acc@1 31.250 (29.867)	Acc@5 68.750 (62.756)	Mem 4879MB
[2022-05-30 23:16:52 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.090 (0.104)	Loss 3.5222 (3.5660)	Acc@1 31.250 (29.665)	Acc@5 65.625 (62.984)	Mem 4879MB
[2022-05-30 23:16:52 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.103)	Loss 3.5567 (3.5678)	Acc@1 28.125 (29.475)	Acc@5 56.250 (63.310)	Mem 4879MB
[2022-05-30 23:16:53 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.102 (0.102)	Loss 3.3572 (3.5702)	Acc@1 50.000 (29.396)	Acc@5 71.875 (63.221)	Mem 4879MB
[2022-05-30 23:16:54 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.101)	Loss 3.6126 (3.5746)	Acc@1 21.875 (28.960)	Acc@5 65.625 (63.181)	Mem 4879MB
[2022-05-30 23:16:55 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.111 (0.101)	Loss 3.5115 (3.5723)	Acc@1 28.125 (29.167)	Acc@5 75.000 (63.514)	Mem 4879MB
[2022-05-30 23:16:56 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.100)	Loss 3.5815 (3.5744)	Acc@1 37.500 (29.132)	Acc@5 75.000 (63.275)	Mem 4879MB
[2022-05-30 23:16:57 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 3.6442 (3.5775)	Acc@1 25.000 (29.079)	Acc@5 53.125 (63.120)	Mem 4879MB
[2022-05-30 23:16:58 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 3.6180 (3.5796)	Acc@1 28.125 (29.145)	Acc@5 53.125 (62.832)	Mem 4879MB
[2022-05-30 23:16:59 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.095 (0.099)	Loss 3.5833 (3.5846)	Acc@1 21.875 (28.746)	Acc@5 59.375 (62.376)	Mem 4879MB
[2022-05-30 23:17:00 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 3.5218 (3.5821)	Acc@1 34.375 (28.746)	Acc@5 68.750 (62.364)	Mem 4879MB
[2022-05-30 23:17:01 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 3.6062 (3.5856)	Acc@1 31.250 (28.509)	Acc@5 62.500 (62.354)	Mem 4879MB
[2022-05-30 23:17:02 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.101 (0.098)	Loss 3.6739 (3.5873)	Acc@1 28.125 (28.453)	Acc@5 56.250 (62.310)	Mem 4879MB
[2022-05-30 23:17:03 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 3.7034 (3.5886)	Acc@1 25.000 (28.321)	Acc@5 46.875 (62.026)	Mem 4879MB
[2022-05-30 23:17:04 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.095 (0.097)	Loss 3.6455 (3.5896)	Acc@1 31.250 (28.234)	Acc@5 50.000 (61.925)	Mem 4879MB
[2022-05-30 23:17:05 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 3.5558 (3.5889)	Acc@1 37.500 (28.406)	Acc@5 62.500 (61.922)	Mem 4879MB
[2022-05-30 23:17:06 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 3.4731 (3.5884)	Acc@1 28.125 (28.436)	Acc@5 65.625 (61.934)	Mem 4879MB
[2022-05-30 23:17:07 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.097)	Loss 3.4587 (3.5891)	Acc@1 34.375 (28.409)	Acc@5 78.125 (61.918)	Mem 4879MB
[2022-05-30 23:17:07 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.095 (0.097)	Loss 3.5762 (3.5887)	Acc@1 34.375 (28.449)	Acc@5 59.375 (61.955)	Mem 4879MB
[2022-05-30 23:17:08 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.097)	Loss 3.6516 (3.5913)	Acc@1 34.375 (28.424)	Acc@5 56.250 (61.728)	Mem 4879MB
[2022-05-30 23:17:09 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 3.3595 (3.5907)	Acc@1 50.000 (28.532)	Acc@5 81.250 (61.830)	Mem 4879MB
[2022-05-30 23:17:10 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.093 (0.096)	Loss 3.5642 (3.5884)	Acc@1 25.000 (28.667)	Acc@5 65.625 (61.923)	Mem 4879MB
[2022-05-30 23:17:11 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.095 (0.096)	Loss 3.6040 (3.5882)	Acc@1 28.125 (28.581)	Acc@5 59.375 (61.955)	Mem 4879MB
[2022-05-30 23:17:12 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 3.6255 (3.5876)	Acc@1 31.250 (28.555)	Acc@5 68.750 (62.017)	Mem 4879MB
[2022-05-30 23:17:13 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.096)	Loss 3.5742 (3.5876)	Acc@1 31.250 (28.592)	Acc@5 65.625 (62.157)	Mem 4879MB
[2022-05-30 23:17:14 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 3.5639 (3.5872)	Acc@1 31.250 (28.738)	Acc@5 59.375 (62.199)	Mem 4879MB
[2022-05-30 23:17:14 MetaFG_0] (main.py 330): INFO  * Acc@1 28.770 Acc@5 62.200
[2022-05-30 23:17:14 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 28.8%
[2022-05-30 23:17:14 MetaFG_0] (main.py 171): INFO Max accuracy: 28.77%
[2022-05-30 23:17:15 MetaFG_0] (main.py 265): INFO Train: [7/300][0/1562]	eta 0:24:50 lr 0.000002	time 0.9544 (0.9544)	loss 2.0521 (2.0521)	grad_norm 12.7153 (12.7153)	mem 4879MB
[2022-05-30 23:17:18 MetaFG_0] (main.py 265): INFO Train: [7/300][10/1562]	eta 0:09:33 lr 0.000002	time 0.2958 (0.3697)	loss 2.1983 (2.1353)	grad_norm 12.4347 (15.5547)	mem 4879MB
[2022-05-30 23:17:21 MetaFG_0] (main.py 265): INFO Train: [7/300][20/1562]	eta 0:08:40 lr 0.000002	time 0.2924 (0.3374)	loss 2.1550 (2.1294)	grad_norm 16.2966 (16.2393)	mem 4879MB
[2022-05-30 23:17:24 MetaFG_0] (main.py 265): INFO Train: [7/300][30/1562]	eta 0:08:20 lr 0.000002	time 0.2975 (0.3264)	loss 2.1445 (2.1236)	grad_norm 8.4022 (16.1993)	mem 4879MB
[2022-05-30 23:17:27 MetaFG_0] (main.py 265): INFO Train: [7/300][40/1562]	eta 0:08:08 lr 0.000002	time 0.2922 (0.3212)	loss 2.1666 (2.1205)	grad_norm 7.6804 (16.2248)	mem 4879MB
[2022-05-30 23:17:30 MetaFG_0] (main.py 265): INFO Train: [7/300][50/1562]	eta 0:08:00 lr 0.000002	time 0.2971 (0.3178)	loss 1.9671 (2.1154)	grad_norm 16.7607 (16.0858)	mem 4879MB
[2022-05-30 23:17:34 MetaFG_0] (main.py 265): INFO Train: [7/300][60/1562]	eta 0:07:53 lr 0.000002	time 0.2924 (0.3152)	loss 2.1648 (2.1110)	grad_norm 11.8198 (15.9964)	mem 4879MB
[2022-05-30 23:17:37 MetaFG_0] (main.py 265): INFO Train: [7/300][70/1562]	eta 0:07:48 lr 0.000002	time 0.2985 (0.3137)	loss 2.1306 (2.1132)	grad_norm 21.5106 (15.8975)	mem 4879MB
[2022-05-30 23:17:40 MetaFG_0] (main.py 265): INFO Train: [7/300][80/1562]	eta 0:07:43 lr 0.000002	time 0.3008 (0.3126)	loss 2.1833 (2.1142)	grad_norm 16.9804 (15.8491)	mem 4879MB
[2022-05-30 23:17:43 MetaFG_0] (main.py 265): INFO Train: [7/300][90/1562]	eta 0:07:38 lr 0.000002	time 0.2978 (0.3116)	loss 2.1621 (2.1159)	grad_norm 19.2161 (15.8888)	mem 4879MB
[2022-05-30 23:17:46 MetaFG_0] (main.py 265): INFO Train: [7/300][100/1562]	eta 0:07:34 lr 0.000002	time 0.2946 (0.3107)	loss 2.0076 (2.1132)	grad_norm 13.7219 (15.9360)	mem 4879MB
[2022-05-30 23:17:49 MetaFG_0] (main.py 265): INFO Train: [7/300][110/1562]	eta 0:07:30 lr 0.000002	time 0.2922 (0.3101)	loss 2.1162 (2.1144)	grad_norm 17.9246 (15.8810)	mem 4879MB
[2022-05-30 23:17:52 MetaFG_0] (main.py 265): INFO Train: [7/300][120/1562]	eta 0:07:26 lr 0.000002	time 0.2917 (0.3094)	loss 2.1332 (2.1100)	grad_norm 10.5016 (15.8538)	mem 4879MB
[2022-05-30 23:17:55 MetaFG_0] (main.py 265): INFO Train: [7/300][130/1562]	eta 0:07:22 lr 0.000002	time 0.2922 (0.3088)	loss 2.1437 (2.1123)	grad_norm 15.5740 (15.8844)	mem 4879MB
[2022-05-30 23:17:58 MetaFG_0] (main.py 265): INFO Train: [7/300][140/1562]	eta 0:07:18 lr 0.000002	time 0.3017 (0.3083)	loss 2.1394 (2.1122)	grad_norm 20.8995 (15.9241)	mem 4879MB
[2022-05-30 23:18:01 MetaFG_0] (main.py 265): INFO Train: [7/300][150/1562]	eta 0:07:15 lr 0.000002	time 0.3039 (0.3081)	loss 2.0647 (2.1104)	grad_norm 12.0446 (15.7748)	mem 4879MB
[2022-05-30 23:18:04 MetaFG_0] (main.py 265): INFO Train: [7/300][160/1562]	eta 0:07:11 lr 0.000002	time 0.3002 (0.3078)	loss 2.1425 (2.1110)	grad_norm 11.1978 (15.6118)	mem 4879MB
[2022-05-30 23:18:07 MetaFG_0] (main.py 265): INFO Train: [7/300][170/1562]	eta 0:07:08 lr 0.000002	time 0.3002 (0.3076)	loss 2.0811 (2.1141)	grad_norm 12.3040 (15.6516)	mem 4879MB
[2022-05-30 23:18:10 MetaFG_0] (main.py 265): INFO Train: [7/300][180/1562]	eta 0:07:04 lr 0.000002	time 0.2919 (0.3074)	loss 2.0524 (2.1103)	grad_norm 27.8678 (15.7081)	mem 4879MB
[2022-05-30 23:18:13 MetaFG_0] (main.py 265): INFO Train: [7/300][190/1562]	eta 0:07:01 lr 0.000002	time 0.2999 (0.3072)	loss 2.0553 (2.1086)	grad_norm 11.4267 (15.6261)	mem 4879MB
[2022-05-30 23:18:16 MetaFG_0] (main.py 265): INFO Train: [7/300][200/1562]	eta 0:06:58 lr 0.000002	time 0.2974 (0.3071)	loss 2.0362 (2.1099)	grad_norm 18.6369 (15.6610)	mem 4879MB
[2022-05-30 23:18:19 MetaFG_0] (main.py 265): INFO Train: [7/300][210/1562]	eta 0:06:55 lr 0.000002	time 0.3030 (0.3070)	loss 2.1539 (2.1093)	grad_norm 13.0885 (15.5603)	mem 4879MB
[2022-05-30 23:18:22 MetaFG_0] (main.py 265): INFO Train: [7/300][220/1562]	eta 0:06:51 lr 0.000002	time 0.2972 (0.3069)	loss 2.0723 (2.1097)	grad_norm 11.5932 (15.5168)	mem 4879MB
[2022-05-30 23:18:25 MetaFG_0] (main.py 265): INFO Train: [7/300][230/1562]	eta 0:06:48 lr 0.000002	time 0.2988 (0.3067)	loss 2.0371 (2.1085)	grad_norm 20.2866 (15.4740)	mem 4879MB
[2022-05-30 23:18:28 MetaFG_0] (main.py 265): INFO Train: [7/300][240/1562]	eta 0:06:45 lr 0.000002	time 0.2975 (0.3067)	loss 1.9801 (2.1070)	grad_norm 14.9621 (15.5638)	mem 4879MB
[2022-05-30 23:18:31 MetaFG_0] (main.py 265): INFO Train: [7/300][250/1562]	eta 0:06:42 lr 0.000002	time 0.3157 (0.3069)	loss 2.1427 (2.1079)	grad_norm 18.2015 (15.5791)	mem 4879MB
[2022-05-30 23:18:34 MetaFG_0] (main.py 265): INFO Train: [7/300][260/1562]	eta 0:06:39 lr 0.000002	time 0.2931 (0.3067)	loss 2.0105 (2.1071)	grad_norm 16.0766 (15.5853)	mem 4879MB
[2022-05-30 23:18:37 MetaFG_0] (main.py 265): INFO Train: [7/300][270/1562]	eta 0:06:36 lr 0.000002	time 0.2983 (0.3066)	loss 2.1546 (2.1076)	grad_norm 14.5092 (15.5091)	mem 4879MB
[2022-05-30 23:18:40 MetaFG_0] (main.py 265): INFO Train: [7/300][280/1562]	eta 0:06:32 lr 0.000002	time 0.2917 (0.3065)	loss 2.0894 (2.1062)	grad_norm 10.0981 (15.6244)	mem 4879MB
[2022-05-30 23:18:43 MetaFG_0] (main.py 265): INFO Train: [7/300][290/1562]	eta 0:06:29 lr 0.000002	time 0.3018 (0.3063)	loss 1.8741 (2.1045)	grad_norm 13.8071 (15.6982)	mem 4879MB
[2022-05-30 23:18:46 MetaFG_0] (main.py 265): INFO Train: [7/300][300/1562]	eta 0:06:26 lr 0.000002	time 0.2924 (0.3062)	loss 2.1034 (2.1048)	grad_norm 12.8665 (15.6643)	mem 4879MB
[2022-05-30 23:18:49 MetaFG_0] (main.py 265): INFO Train: [7/300][310/1562]	eta 0:06:23 lr 0.000002	time 0.2919 (0.3060)	loss 2.0257 (2.1042)	grad_norm 14.7836 (15.7145)	mem 4879MB
[2022-05-30 23:18:52 MetaFG_0] (main.py 265): INFO Train: [7/300][320/1562]	eta 0:06:19 lr 0.000002	time 0.2939 (0.3059)	loss 2.1148 (2.1034)	grad_norm 11.7431 (15.7720)	mem 4879MB
[2022-05-30 23:18:56 MetaFG_0] (main.py 265): INFO Train: [7/300][330/1562]	eta 0:06:16 lr 0.000002	time 0.2939 (0.3058)	loss 2.1050 (2.1025)	grad_norm 14.7887 (15.7504)	mem 4879MB
[2022-05-30 23:18:59 MetaFG_0] (main.py 265): INFO Train: [7/300][340/1562]	eta 0:06:13 lr 0.000002	time 0.2929 (0.3058)	loss 2.0526 (2.1020)	grad_norm 11.1909 (15.7185)	mem 4879MB
[2022-05-30 23:19:02 MetaFG_0] (main.py 265): INFO Train: [7/300][350/1562]	eta 0:06:10 lr 0.000002	time 0.2925 (0.3057)	loss 2.1463 (2.1010)	grad_norm 17.4942 (15.7308)	mem 4879MB
[2022-05-30 23:19:05 MetaFG_0] (main.py 265): INFO Train: [7/300][360/1562]	eta 0:06:07 lr 0.000002	time 0.2979 (0.3056)	loss 2.1044 (2.1010)	grad_norm 13.3899 (15.8240)	mem 4879MB
[2022-05-30 23:19:08 MetaFG_0] (main.py 265): INFO Train: [7/300][370/1562]	eta 0:06:04 lr 0.000002	time 0.2928 (0.3055)	loss 1.9932 (2.1005)	grad_norm 14.9084 (15.7875)	mem 4879MB
[2022-05-30 23:19:11 MetaFG_0] (main.py 265): INFO Train: [7/300][380/1562]	eta 0:06:01 lr 0.000002	time 0.2931 (0.3055)	loss 2.0942 (2.1009)	grad_norm 12.8085 (15.7865)	mem 4879MB
[2022-05-30 23:19:14 MetaFG_0] (main.py 265): INFO Train: [7/300][390/1562]	eta 0:05:58 lr 0.000002	time 0.2935 (0.3055)	loss 2.0986 (2.1010)	grad_norm 12.4919 (15.8239)	mem 4879MB
[2022-05-30 23:19:17 MetaFG_0] (main.py 265): INFO Train: [7/300][400/1562]	eta 0:05:54 lr 0.000002	time 0.2936 (0.3055)	loss 2.0980 (2.1004)	grad_norm 13.3768 (15.7541)	mem 4879MB
[2022-05-30 23:19:20 MetaFG_0] (main.py 265): INFO Train: [7/300][410/1562]	eta 0:05:51 lr 0.000002	time 0.3010 (0.3055)	loss 1.9747 (2.1006)	grad_norm 14.1112 (15.7374)	mem 4879MB
[2022-05-30 23:19:23 MetaFG_0] (main.py 265): INFO Train: [7/300][420/1562]	eta 0:05:48 lr 0.000002	time 0.2919 (0.3054)	loss 2.1107 (2.1003)	grad_norm 14.9408 (15.7701)	mem 4879MB
[2022-05-30 23:19:26 MetaFG_0] (main.py 265): INFO Train: [7/300][430/1562]	eta 0:05:45 lr 0.000002	time 0.3003 (0.3054)	loss 2.2064 (2.1004)	grad_norm 9.2420 (15.7565)	mem 4879MB
[2022-05-30 23:19:29 MetaFG_0] (main.py 265): INFO Train: [7/300][440/1562]	eta 0:05:42 lr 0.000002	time 0.2936 (0.3053)	loss 2.0897 (2.1002)	grad_norm 12.0252 (15.7620)	mem 4879MB
[2022-05-30 23:19:32 MetaFG_0] (main.py 265): INFO Train: [7/300][450/1562]	eta 0:05:39 lr 0.000002	time 0.3037 (0.3053)	loss 2.1864 (2.1002)	grad_norm 19.5849 (15.7705)	mem 4879MB
[2022-05-30 23:19:35 MetaFG_0] (main.py 265): INFO Train: [7/300][460/1562]	eta 0:05:36 lr 0.000002	time 0.2988 (0.3053)	loss 2.0304 (2.0990)	grad_norm 13.0533 (15.7848)	mem 4879MB
[2022-05-30 23:19:38 MetaFG_0] (main.py 265): INFO Train: [7/300][470/1562]	eta 0:05:33 lr 0.000002	time 0.2980 (0.3052)	loss 2.1268 (2.0986)	grad_norm 8.0203 (15.7960)	mem 4879MB
[2022-05-30 23:19:41 MetaFG_0] (main.py 265): INFO Train: [7/300][480/1562]	eta 0:05:30 lr 0.000002	time 0.2921 (0.3052)	loss 2.1069 (2.0990)	grad_norm 13.2762 (15.7969)	mem 4879MB
[2022-05-30 23:19:44 MetaFG_0] (main.py 265): INFO Train: [7/300][490/1562]	eta 0:05:27 lr 0.000002	time 0.2950 (0.3052)	loss 2.0847 (2.0991)	grad_norm 11.9895 (15.7869)	mem 4879MB
[2022-05-30 23:19:47 MetaFG_0] (main.py 265): INFO Train: [7/300][500/1562]	eta 0:05:24 lr 0.000002	time 0.2919 (0.3051)	loss 2.1581 (2.0990)	grad_norm 13.0748 (15.7665)	mem 4879MB
[2022-05-30 23:19:50 MetaFG_0] (main.py 265): INFO Train: [7/300][510/1562]	eta 0:05:20 lr 0.000002	time 0.2931 (0.3051)	loss 2.0446 (2.0983)	grad_norm 22.2515 (15.8230)	mem 4879MB
[2022-05-30 23:19:53 MetaFG_0] (main.py 265): INFO Train: [7/300][520/1562]	eta 0:05:17 lr 0.000002	time 0.2917 (0.3051)	loss 2.1826 (2.0986)	grad_norm 14.3617 (15.8525)	mem 4879MB
[2022-05-30 23:19:56 MetaFG_0] (main.py 265): INFO Train: [7/300][530/1562]	eta 0:05:14 lr 0.000002	time 0.2973 (0.3051)	loss 2.0855 (2.0987)	grad_norm 15.5131 (15.8451)	mem 4879MB
[2022-05-30 23:19:59 MetaFG_0] (main.py 265): INFO Train: [7/300][540/1562]	eta 0:05:11 lr 0.000002	time 0.2988 (0.3051)	loss 2.0523 (2.0980)	grad_norm 17.1056 (15.8828)	mem 4879MB
[2022-05-30 23:20:02 MetaFG_0] (main.py 265): INFO Train: [7/300][550/1562]	eta 0:05:08 lr 0.000002	time 0.2981 (0.3051)	loss 2.1784 (2.0981)	grad_norm 11.1634 (15.8620)	mem 4879MB
[2022-05-30 23:20:05 MetaFG_0] (main.py 265): INFO Train: [7/300][560/1562]	eta 0:05:05 lr 0.000002	time 0.3006 (0.3050)	loss 2.1238 (2.0989)	grad_norm 17.3589 (15.8480)	mem 4879MB
[2022-05-30 23:20:08 MetaFG_0] (main.py 265): INFO Train: [7/300][570/1562]	eta 0:05:02 lr 0.000002	time 0.2979 (0.3050)	loss 2.1636 (2.0994)	grad_norm 17.9076 (15.8137)	mem 4879MB
[2022-05-30 23:20:11 MetaFG_0] (main.py 265): INFO Train: [7/300][580/1562]	eta 0:04:59 lr 0.000002	time 0.2924 (0.3050)	loss 2.0118 (2.0991)	grad_norm 17.2610 (15.8205)	mem 4879MB
[2022-05-30 23:20:15 MetaFG_0] (main.py 265): INFO Train: [7/300][590/1562]	eta 0:04:56 lr 0.000002	time 0.2927 (0.3050)	loss 2.0916 (2.0994)	grad_norm 15.2816 (15.8183)	mem 4879MB
[2022-05-30 23:20:18 MetaFG_0] (main.py 265): INFO Train: [7/300][600/1562]	eta 0:04:53 lr 0.000002	time 0.2988 (0.3050)	loss 2.1942 (2.0989)	grad_norm 10.0423 (15.7896)	mem 4879MB
[2022-05-30 23:20:21 MetaFG_0] (main.py 265): INFO Train: [7/300][610/1562]	eta 0:04:50 lr 0.000002	time 0.2990 (0.3050)	loss 2.1600 (2.0986)	grad_norm 14.7910 (15.7802)	mem 4879MB
[2022-05-30 23:20:24 MetaFG_0] (main.py 265): INFO Train: [7/300][620/1562]	eta 0:04:47 lr 0.000002	time 0.2931 (0.3050)	loss 2.0622 (2.0992)	grad_norm 17.5904 (15.7729)	mem 4879MB
[2022-05-30 23:20:27 MetaFG_0] (main.py 265): INFO Train: [7/300][630/1562]	eta 0:04:44 lr 0.000002	time 0.2934 (0.3049)	loss 2.2123 (2.0984)	grad_norm 11.6235 (15.7651)	mem 4879MB
[2022-05-30 23:20:30 MetaFG_0] (main.py 265): INFO Train: [7/300][640/1562]	eta 0:04:41 lr 0.000002	time 0.2956 (0.3049)	loss 2.0386 (2.0980)	grad_norm 16.8350 (15.7846)	mem 4879MB
[2022-05-30 23:20:33 MetaFG_0] (main.py 265): INFO Train: [7/300][650/1562]	eta 0:04:38 lr 0.000002	time 0.2932 (0.3049)	loss 2.0917 (2.0973)	grad_norm 10.8840 (15.8016)	mem 4879MB
[2022-05-30 23:20:36 MetaFG_0] (main.py 265): INFO Train: [7/300][660/1562]	eta 0:04:34 lr 0.000002	time 0.2916 (0.3048)	loss 1.9666 (2.0969)	grad_norm 20.8827 (15.8118)	mem 4879MB
[2022-05-30 23:20:39 MetaFG_0] (main.py 265): INFO Train: [7/300][670/1562]	eta 0:04:31 lr 0.000002	time 0.2982 (0.3048)	loss 2.0886 (2.0968)	grad_norm 11.8542 (15.8003)	mem 4879MB
[2022-05-30 23:20:42 MetaFG_0] (main.py 265): INFO Train: [7/300][680/1562]	eta 0:04:28 lr 0.000002	time 0.2917 (0.3049)	loss 2.0836 (2.0968)	grad_norm 17.8561 (15.8287)	mem 4879MB
[2022-05-30 23:20:45 MetaFG_0] (main.py 265): INFO Train: [7/300][690/1562]	eta 0:04:25 lr 0.000002	time 0.2988 (0.3049)	loss 2.1315 (2.0964)	grad_norm 14.2122 (15.8498)	mem 4879MB
[2022-05-30 23:20:48 MetaFG_0] (main.py 265): INFO Train: [7/300][700/1562]	eta 0:04:22 lr 0.000002	time 0.2933 (0.3049)	loss 2.0826 (2.0958)	grad_norm 17.7945 (15.8817)	mem 4879MB
[2022-05-30 23:20:51 MetaFG_0] (main.py 265): INFO Train: [7/300][710/1562]	eta 0:04:19 lr 0.000002	time 0.2928 (0.3049)	loss 1.9540 (2.0956)	grad_norm 15.8839 (15.8838)	mem 4879MB
[2022-05-30 23:20:54 MetaFG_0] (main.py 265): INFO Train: [7/300][720/1562]	eta 0:04:16 lr 0.000002	time 0.2995 (0.3048)	loss 2.1556 (2.0955)	grad_norm 10.4201 (15.8694)	mem 4879MB
[2022-05-30 23:20:57 MetaFG_0] (main.py 265): INFO Train: [7/300][730/1562]	eta 0:04:13 lr 0.000002	time 0.3010 (0.3048)	loss 2.1720 (2.0957)	grad_norm 16.9508 (15.8692)	mem 4879MB
[2022-05-30 23:21:00 MetaFG_0] (main.py 265): INFO Train: [7/300][740/1562]	eta 0:04:10 lr 0.000002	time 0.3001 (0.3048)	loss 2.0595 (2.0951)	grad_norm 10.1413 (15.8962)	mem 4879MB
[2022-05-30 23:21:03 MetaFG_0] (main.py 265): INFO Train: [7/300][750/1562]	eta 0:04:07 lr 0.000002	time 0.2931 (0.3048)	loss 2.0766 (2.0947)	grad_norm 22.4042 (15.9222)	mem 4879MB
[2022-05-30 23:21:06 MetaFG_0] (main.py 265): INFO Train: [7/300][760/1562]	eta 0:04:04 lr 0.000002	time 0.2920 (0.3048)	loss 2.0787 (2.0947)	grad_norm 21.8278 (15.9289)	mem 4879MB
[2022-05-30 23:21:09 MetaFG_0] (main.py 265): INFO Train: [7/300][770/1562]	eta 0:04:01 lr 0.000002	time 0.2996 (0.3047)	loss 2.0195 (2.0945)	grad_norm 12.4599 (15.9267)	mem 4879MB
[2022-05-30 23:21:12 MetaFG_0] (main.py 265): INFO Train: [7/300][780/1562]	eta 0:03:58 lr 0.000002	time 0.2934 (0.3048)	loss 2.0353 (2.0946)	grad_norm 11.6571 (15.9191)	mem 4879MB
[2022-05-30 23:21:15 MetaFG_0] (main.py 265): INFO Train: [7/300][790/1562]	eta 0:03:55 lr 0.000002	time 0.2979 (0.3048)	loss 1.9955 (2.0937)	grad_norm 14.0306 (15.9150)	mem 4879MB
[2022-05-30 23:21:18 MetaFG_0] (main.py 265): INFO Train: [7/300][800/1562]	eta 0:03:52 lr 0.000002	time 0.2977 (0.3048)	loss 2.1032 (2.0932)	grad_norm 14.1392 (15.9096)	mem 4879MB
[2022-05-30 23:21:21 MetaFG_0] (main.py 265): INFO Train: [7/300][810/1562]	eta 0:03:49 lr 0.000002	time 0.2924 (0.3047)	loss 2.1516 (2.0931)	grad_norm 15.8251 (15.9239)	mem 4879MB
[2022-05-30 23:21:24 MetaFG_0] (main.py 265): INFO Train: [7/300][820/1562]	eta 0:03:46 lr 0.000002	time 0.2932 (0.3047)	loss 1.9904 (2.0927)	grad_norm 23.0226 (15.9178)	mem 4879MB
[2022-05-30 23:21:27 MetaFG_0] (main.py 265): INFO Train: [7/300][830/1562]	eta 0:03:43 lr 0.000002	time 0.2988 (0.3047)	loss 2.0901 (2.0926)	grad_norm 10.5445 (15.9283)	mem 4879MB
[2022-05-30 23:21:31 MetaFG_0] (main.py 265): INFO Train: [7/300][840/1562]	eta 0:03:39 lr 0.000002	time 0.2930 (0.3047)	loss 2.2327 (2.0924)	grad_norm 16.5731 (15.9484)	mem 4879MB
[2022-05-30 23:21:34 MetaFG_0] (main.py 265): INFO Train: [7/300][850/1562]	eta 0:03:36 lr 0.000002	time 0.2973 (0.3047)	loss 2.1809 (2.0925)	grad_norm 11.8632 (15.9150)	mem 4879MB
[2022-05-30 23:21:37 MetaFG_0] (main.py 265): INFO Train: [7/300][860/1562]	eta 0:03:33 lr 0.000002	time 0.2941 (0.3047)	loss 2.0496 (2.0919)	grad_norm 16.3323 (15.9245)	mem 4879MB
[2022-05-30 23:21:40 MetaFG_0] (main.py 265): INFO Train: [7/300][870/1562]	eta 0:03:30 lr 0.000002	time 0.2920 (0.3047)	loss 2.1575 (2.0918)	grad_norm 22.1716 (15.9420)	mem 4879MB
[2022-05-30 23:21:43 MetaFG_0] (main.py 265): INFO Train: [7/300][880/1562]	eta 0:03:27 lr 0.000002	time 0.2983 (0.3047)	loss 2.1355 (2.0917)	grad_norm 17.0955 (15.9469)	mem 4879MB
[2022-05-30 23:21:46 MetaFG_0] (main.py 265): INFO Train: [7/300][890/1562]	eta 0:03:24 lr 0.000002	time 0.2973 (0.3047)	loss 1.9322 (2.0915)	grad_norm 14.6798 (15.9273)	mem 4879MB
[2022-05-30 23:21:49 MetaFG_0] (main.py 265): INFO Train: [7/300][900/1562]	eta 0:03:21 lr 0.000002	time 0.2923 (0.3047)	loss 2.0851 (2.0910)	grad_norm 20.1784 (15.9293)	mem 4879MB
[2022-05-30 23:21:52 MetaFG_0] (main.py 265): INFO Train: [7/300][910/1562]	eta 0:03:18 lr 0.000002	time 0.2920 (0.3046)	loss 2.0574 (2.0906)	grad_norm 11.7953 (15.9014)	mem 4879MB
[2022-05-30 23:21:55 MetaFG_0] (main.py 265): INFO Train: [7/300][920/1562]	eta 0:03:15 lr 0.000002	time 0.2920 (0.3046)	loss 2.0577 (2.0903)	grad_norm 14.4958 (15.8899)	mem 4879MB
[2022-05-30 23:21:58 MetaFG_0] (main.py 265): INFO Train: [7/300][930/1562]	eta 0:03:12 lr 0.000002	time 0.2921 (0.3046)	loss 2.0038 (2.0897)	grad_norm 16.3507 (15.9040)	mem 4879MB
[2022-05-30 23:22:01 MetaFG_0] (main.py 265): INFO Train: [7/300][940/1562]	eta 0:03:09 lr 0.000002	time 0.2933 (0.3046)	loss 2.0797 (2.0894)	grad_norm 9.8406 (15.9052)	mem 4879MB
[2022-05-30 23:22:04 MetaFG_0] (main.py 265): INFO Train: [7/300][950/1562]	eta 0:03:06 lr 0.000002	time 0.2972 (0.3046)	loss 1.9481 (2.0889)	grad_norm 18.4182 (15.9052)	mem 4879MB
[2022-05-30 23:22:07 MetaFG_0] (main.py 265): INFO Train: [7/300][960/1562]	eta 0:03:03 lr 0.000002	time 0.2939 (0.3046)	loss 2.1750 (2.0885)	grad_norm 10.7478 (15.8982)	mem 4879MB
[2022-05-30 23:22:10 MetaFG_0] (main.py 265): INFO Train: [7/300][970/1562]	eta 0:03:00 lr 0.000002	time 0.2973 (0.3046)	loss 2.0259 (2.0886)	grad_norm 19.9774 (15.8902)	mem 4879MB
[2022-05-30 23:22:13 MetaFG_0] (main.py 265): INFO Train: [7/300][980/1562]	eta 0:02:57 lr 0.000002	time 0.2927 (0.3045)	loss 2.0059 (2.0881)	grad_norm 18.6785 (15.9051)	mem 4879MB
[2022-05-30 23:22:16 MetaFG_0] (main.py 265): INFO Train: [7/300][990/1562]	eta 0:02:54 lr 0.000002	time 0.2921 (0.3045)	loss 1.9752 (2.0880)	grad_norm 16.5420 (15.9514)	mem 4879MB
[2022-05-30 23:22:19 MetaFG_0] (main.py 265): INFO Train: [7/300][1000/1562]	eta 0:02:51 lr 0.000002	time 0.2970 (0.3045)	loss 2.0177 (2.0878)	grad_norm 14.8575 (15.9470)	mem 4879MB
[2022-05-30 23:22:22 MetaFG_0] (main.py 265): INFO Train: [7/300][1010/1562]	eta 0:02:48 lr 0.000002	time 0.2922 (0.3045)	loss 1.8951 (2.0873)	grad_norm 23.0852 (15.9494)	mem 4879MB
[2022-05-30 23:22:25 MetaFG_0] (main.py 265): INFO Train: [7/300][1020/1562]	eta 0:02:45 lr 0.000002	time 0.2936 (0.3045)	loss 2.2023 (2.0873)	grad_norm 7.9962 (15.9493)	mem 4879MB
[2022-05-30 23:22:28 MetaFG_0] (main.py 265): INFO Train: [7/300][1030/1562]	eta 0:02:41 lr 0.000002	time 0.2919 (0.3045)	loss 2.2265 (2.0872)	grad_norm 15.7493 (15.9658)	mem 4879MB
[2022-05-30 23:22:31 MetaFG_0] (main.py 265): INFO Train: [7/300][1040/1562]	eta 0:02:38 lr 0.000002	time 0.2972 (0.3045)	loss 2.1423 (2.0874)	grad_norm 9.6050 (15.9570)	mem 4879MB
[2022-05-30 23:22:34 MetaFG_0] (main.py 265): INFO Train: [7/300][1050/1562]	eta 0:02:35 lr 0.000002	time 0.2920 (0.3045)	loss 1.9706 (2.0870)	grad_norm 10.6035 (15.9514)	mem 4879MB
[2022-05-30 23:22:37 MetaFG_0] (main.py 265): INFO Train: [7/300][1060/1562]	eta 0:02:32 lr 0.000002	time 0.2994 (0.3045)	loss 2.1217 (2.0871)	grad_norm 13.7564 (15.9633)	mem 4879MB
[2022-05-30 23:22:40 MetaFG_0] (main.py 265): INFO Train: [7/300][1070/1562]	eta 0:02:29 lr 0.000002	time 0.2936 (0.3045)	loss 2.1733 (2.0872)	grad_norm 19.7899 (15.9681)	mem 4879MB
[2022-05-30 23:22:43 MetaFG_0] (main.py 265): INFO Train: [7/300][1080/1562]	eta 0:02:26 lr 0.000002	time 0.2937 (0.3044)	loss 2.0817 (2.0871)	grad_norm 12.2319 (15.9703)	mem 4879MB
[2022-05-30 23:22:46 MetaFG_0] (main.py 265): INFO Train: [7/300][1090/1562]	eta 0:02:23 lr 0.000002	time 0.2932 (0.3044)	loss 2.0798 (2.0870)	grad_norm 18.3391 (15.9651)	mem 4879MB
[2022-05-30 23:22:49 MetaFG_0] (main.py 265): INFO Train: [7/300][1100/1562]	eta 0:02:20 lr 0.000002	time 0.2993 (0.3044)	loss 1.9461 (2.0865)	grad_norm 14.4825 (15.9661)	mem 4879MB
[2022-05-30 23:22:52 MetaFG_0] (main.py 265): INFO Train: [7/300][1110/1562]	eta 0:02:17 lr 0.000002	time 0.2922 (0.3044)	loss 1.9225 (2.0864)	grad_norm 20.2125 (15.9613)	mem 4879MB
[2022-05-30 23:22:56 MetaFG_0] (main.py 265): INFO Train: [7/300][1120/1562]	eta 0:02:14 lr 0.000002	time 0.3110 (0.3044)	loss 2.0710 (2.0863)	grad_norm 19.9753 (15.9635)	mem 4879MB
[2022-05-30 23:22:59 MetaFG_0] (main.py 265): INFO Train: [7/300][1130/1562]	eta 0:02:11 lr 0.000002	time 0.2916 (0.3045)	loss 1.9217 (2.0861)	grad_norm 21.3165 (15.9838)	mem 4879MB
[2022-05-30 23:23:02 MetaFG_0] (main.py 265): INFO Train: [7/300][1140/1562]	eta 0:02:08 lr 0.000002	time 0.2949 (0.3045)	loss 2.1593 (2.0863)	grad_norm 12.7685 (15.9987)	mem 4879MB
[2022-05-30 23:23:05 MetaFG_0] (main.py 265): INFO Train: [7/300][1150/1562]	eta 0:02:05 lr 0.000002	time 0.2914 (0.3044)	loss 2.0788 (2.0860)	grad_norm 11.5894 (16.0041)	mem 4879MB
[2022-05-30 23:23:08 MetaFG_0] (main.py 265): INFO Train: [7/300][1160/1562]	eta 0:02:02 lr 0.000002	time 0.2966 (0.3044)	loss 1.9182 (2.0855)	grad_norm 14.9734 (16.0246)	mem 4879MB
[2022-05-30 23:23:11 MetaFG_0] (main.py 265): INFO Train: [7/300][1170/1562]	eta 0:01:59 lr 0.000002	time 0.2931 (0.3044)	loss 2.0230 (2.0848)	grad_norm 13.8164 (16.0262)	mem 4879MB
[2022-05-30 23:23:14 MetaFG_0] (main.py 265): INFO Train: [7/300][1180/1562]	eta 0:01:56 lr 0.000002	time 0.2917 (0.3044)	loss 2.1379 (2.0847)	grad_norm 20.6233 (16.0414)	mem 4879MB
[2022-05-30 23:23:17 MetaFG_0] (main.py 265): INFO Train: [7/300][1190/1562]	eta 0:01:53 lr 0.000002	time 0.2999 (0.3044)	loss 2.1057 (2.0844)	grad_norm 9.8897 (16.0438)	mem 4879MB
[2022-05-30 23:23:20 MetaFG_0] (main.py 265): INFO Train: [7/300][1200/1562]	eta 0:01:50 lr 0.000002	time 0.2978 (0.3044)	loss 2.1705 (2.0841)	grad_norm 11.2052 (16.0499)	mem 4879MB
[2022-05-30 23:23:23 MetaFG_0] (main.py 265): INFO Train: [7/300][1210/1562]	eta 0:01:47 lr 0.000002	time 0.2919 (0.3044)	loss 2.0033 (2.0836)	grad_norm 11.1587 (16.0476)	mem 4879MB
[2022-05-30 23:23:26 MetaFG_0] (main.py 265): INFO Train: [7/300][1220/1562]	eta 0:01:44 lr 0.000002	time 0.2931 (0.3044)	loss 2.1370 (2.0836)	grad_norm 13.4682 (16.0786)	mem 4879MB
[2022-05-30 23:23:29 MetaFG_0] (main.py 265): INFO Train: [7/300][1230/1562]	eta 0:01:41 lr 0.000002	time 0.2981 (0.3044)	loss 2.0679 (2.0834)	grad_norm 11.1995 (16.0657)	mem 4879MB
[2022-05-30 23:23:32 MetaFG_0] (main.py 265): INFO Train: [7/300][1240/1562]	eta 0:01:38 lr 0.000002	time 0.2970 (0.3044)	loss 1.9801 (2.0830)	grad_norm 16.9078 (16.0653)	mem 4879MB
[2022-05-30 23:23:35 MetaFG_0] (main.py 265): INFO Train: [7/300][1250/1562]	eta 0:01:34 lr 0.000002	time 0.2931 (0.3044)	loss 1.8399 (2.0823)	grad_norm 11.4216 (16.0653)	mem 4879MB
[2022-05-30 23:23:38 MetaFG_0] (main.py 265): INFO Train: [7/300][1260/1562]	eta 0:01:31 lr 0.000002	time 0.2994 (0.3044)	loss 2.1925 (2.0818)	grad_norm 10.8947 (16.0881)	mem 4879MB
[2022-05-30 23:23:41 MetaFG_0] (main.py 265): INFO Train: [7/300][1270/1562]	eta 0:01:28 lr 0.000002	time 0.2981 (0.3044)	loss 1.9394 (2.0814)	grad_norm 16.2334 (16.0973)	mem 4879MB
[2022-05-30 23:23:44 MetaFG_0] (main.py 265): INFO Train: [7/300][1280/1562]	eta 0:01:25 lr 0.000002	time 0.2916 (0.3044)	loss 2.1187 (2.0812)	grad_norm 12.3536 (16.0980)	mem 4879MB
[2022-05-30 23:23:47 MetaFG_0] (main.py 265): INFO Train: [7/300][1290/1562]	eta 0:01:22 lr 0.000002	time 0.3009 (0.3044)	loss 1.9645 (2.0809)	grad_norm 20.6713 (nan)	mem 4879MB
[2022-05-30 23:23:50 MetaFG_0] (main.py 265): INFO Train: [7/300][1300/1562]	eta 0:01:19 lr 0.000002	time 0.2919 (0.3044)	loss 2.0787 (2.0809)	grad_norm 17.5322 (nan)	mem 4879MB
[2022-05-30 23:23:53 MetaFG_0] (main.py 265): INFO Train: [7/300][1310/1562]	eta 0:01:16 lr 0.000002	time 0.2920 (0.3044)	loss 2.0033 (2.0806)	grad_norm 14.4546 (nan)	mem 4879MB
[2022-05-30 23:23:56 MetaFG_0] (main.py 265): INFO Train: [7/300][1320/1562]	eta 0:01:13 lr 0.000002	time 0.2990 (0.3044)	loss 1.9301 (2.0803)	grad_norm 16.6166 (nan)	mem 4879MB
[2022-05-30 23:23:59 MetaFG_0] (main.py 265): INFO Train: [7/300][1330/1562]	eta 0:01:10 lr 0.000002	time 0.2921 (0.3044)	loss 2.1936 (2.0804)	grad_norm 13.2965 (nan)	mem 4879MB
[2022-05-30 23:24:02 MetaFG_0] (main.py 265): INFO Train: [7/300][1340/1562]	eta 0:01:07 lr 0.000002	time 0.2972 (0.3044)	loss 1.9562 (2.0801)	grad_norm 13.5044 (nan)	mem 4879MB
[2022-05-30 23:24:05 MetaFG_0] (main.py 265): INFO Train: [7/300][1350/1562]	eta 0:01:04 lr 0.000002	time 0.2979 (0.3043)	loss 2.0149 (2.0798)	grad_norm 21.1965 (nan)	mem 4879MB
[2022-05-30 23:24:08 MetaFG_0] (main.py 265): INFO Train: [7/300][1360/1562]	eta 0:01:01 lr 0.000002	time 0.2914 (0.3043)	loss 1.9422 (2.0797)	grad_norm 22.4071 (nan)	mem 4879MB
[2022-05-30 23:24:12 MetaFG_0] (main.py 265): INFO Train: [7/300][1370/1562]	eta 0:00:58 lr 0.000002	time 0.2974 (0.3043)	loss 1.9574 (2.0796)	grad_norm 12.4789 (nan)	mem 4879MB
[2022-05-30 23:24:15 MetaFG_0] (main.py 265): INFO Train: [7/300][1380/1562]	eta 0:00:55 lr 0.000002	time 0.2936 (0.3043)	loss 2.0099 (2.0793)	grad_norm 24.0457 (nan)	mem 4879MB
[2022-05-30 23:24:18 MetaFG_0] (main.py 265): INFO Train: [7/300][1390/1562]	eta 0:00:52 lr 0.000002	time 0.2932 (0.3043)	loss 2.1504 (2.0794)	grad_norm 13.2507 (nan)	mem 4879MB
[2022-05-30 23:24:21 MetaFG_0] (main.py 265): INFO Train: [7/300][1400/1562]	eta 0:00:49 lr 0.000002	time 0.2915 (0.3043)	loss 2.0729 (2.0788)	grad_norm 15.1358 (nan)	mem 4879MB
[2022-05-30 23:24:24 MetaFG_0] (main.py 265): INFO Train: [7/300][1410/1562]	eta 0:00:46 lr 0.000002	time 0.2979 (0.3043)	loss 2.0231 (2.0790)	grad_norm 25.0896 (nan)	mem 4879MB
[2022-05-30 23:24:27 MetaFG_0] (main.py 265): INFO Train: [7/300][1420/1562]	eta 0:00:43 lr 0.000002	time 0.2926 (0.3043)	loss 2.1585 (2.0785)	grad_norm 9.2493 (nan)	mem 4879MB
[2022-05-30 23:24:30 MetaFG_0] (main.py 265): INFO Train: [7/300][1430/1562]	eta 0:00:40 lr 0.000002	time 0.2984 (0.3043)	loss 1.9714 (2.0782)	grad_norm 13.1690 (nan)	mem 4879MB
[2022-05-30 23:24:33 MetaFG_0] (main.py 265): INFO Train: [7/300][1440/1562]	eta 0:00:37 lr 0.000002	time 0.2920 (0.3043)	loss 1.9795 (2.0777)	grad_norm 13.3065 (nan)	mem 4879MB
[2022-05-30 23:24:36 MetaFG_0] (main.py 265): INFO Train: [7/300][1450/1562]	eta 0:00:34 lr 0.000002	time 0.2991 (0.3043)	loss 1.9572 (2.0776)	grad_norm 19.4686 (nan)	mem 4879MB
[2022-05-30 23:24:39 MetaFG_0] (main.py 265): INFO Train: [7/300][1460/1562]	eta 0:00:31 lr 0.000002	time 0.2953 (0.3043)	loss 2.0830 (2.0774)	grad_norm 13.6694 (nan)	mem 4879MB
[2022-05-30 23:24:42 MetaFG_0] (main.py 265): INFO Train: [7/300][1470/1562]	eta 0:00:27 lr 0.000002	time 0.2918 (0.3043)	loss 2.0978 (2.0771)	grad_norm 16.0250 (nan)	mem 4879MB
[2022-05-30 23:24:45 MetaFG_0] (main.py 265): INFO Train: [7/300][1480/1562]	eta 0:00:24 lr 0.000002	time 0.2935 (0.3043)	loss 1.8392 (2.0770)	grad_norm 15.5433 (nan)	mem 4879MB
[2022-05-30 23:24:48 MetaFG_0] (main.py 265): INFO Train: [7/300][1490/1562]	eta 0:00:21 lr 0.000002	time 0.2915 (0.3043)	loss 2.0256 (2.0767)	grad_norm 30.9920 (nan)	mem 4879MB
[2022-05-30 23:24:51 MetaFG_0] (main.py 265): INFO Train: [7/300][1500/1562]	eta 0:00:18 lr 0.000002	time 0.2997 (0.3043)	loss 1.9700 (2.0761)	grad_norm 20.1974 (nan)	mem 4879MB
[2022-05-30 23:24:54 MetaFG_0] (main.py 265): INFO Train: [7/300][1510/1562]	eta 0:00:15 lr 0.000002	time 0.2988 (0.3043)	loss 2.0447 (2.0759)	grad_norm 12.5457 (nan)	mem 4879MB
[2022-05-30 23:24:57 MetaFG_0] (main.py 265): INFO Train: [7/300][1520/1562]	eta 0:00:12 lr 0.000002	time 0.2935 (0.3043)	loss 1.9569 (2.0757)	grad_norm 22.4304 (nan)	mem 4879MB
[2022-05-30 23:25:00 MetaFG_0] (main.py 265): INFO Train: [7/300][1530/1562]	eta 0:00:09 lr 0.000002	time 0.2928 (0.3043)	loss 2.0985 (2.0758)	grad_norm 16.4447 (nan)	mem 4879MB
[2022-05-30 23:25:03 MetaFG_0] (main.py 265): INFO Train: [7/300][1540/1562]	eta 0:00:06 lr 0.000002	time 0.2940 (0.3043)	loss 1.9645 (2.0756)	grad_norm 32.6489 (nan)	mem 4879MB
[2022-05-30 23:25:06 MetaFG_0] (main.py 265): INFO Train: [7/300][1550/1562]	eta 0:00:03 lr 0.000003	time 0.2984 (0.3043)	loss 1.9438 (2.0754)	grad_norm 18.8278 (nan)	mem 4879MB
[2022-05-30 23:25:09 MetaFG_0] (main.py 265): INFO Train: [7/300][1560/1562]	eta 0:00:00 lr 0.000003	time 0.2913 (0.3043)	loss 2.0618 (2.0753)	grad_norm 16.4439 (nan)	mem 4879MB
[2022-05-30 23:25:10 MetaFG_0] (main.py 272): INFO EPOCH 7 training takes 0:07:55
[2022-05-30 23:25:10 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_7.pth saving......
[2022-05-30 23:25:10 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_7.pth saved !!!
[2022-05-30 23:25:10 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:25:12 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:25:12 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:25:13 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.636 (0.636)	Loss 3.3849 (3.3849)	Acc@1 28.125 (28.125)	Acc@5 71.875 (71.875)	Mem 4879MB
[2022-05-30 23:25:14 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.087 (0.148)	Loss 3.4223 (3.3136)	Acc@1 21.875 (32.102)	Acc@5 78.125 (71.591)	Mem 4879MB
[2022-05-30 23:25:14 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.122)	Loss 3.1929 (3.2605)	Acc@1 31.250 (35.565)	Acc@5 65.625 (71.726)	Mem 4879MB
[2022-05-30 23:25:15 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.093 (0.113)	Loss 3.2509 (3.2526)	Acc@1 28.125 (35.383)	Acc@5 78.125 (72.681)	Mem 4879MB
[2022-05-30 23:25:16 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.109)	Loss 3.2967 (3.2529)	Acc@1 37.500 (36.509)	Acc@5 71.875 (72.485)	Mem 4879MB
[2022-05-30 23:25:17 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.114 (0.106)	Loss 3.2639 (3.2551)	Acc@1 40.625 (36.091)	Acc@5 68.750 (72.304)	Mem 4879MB
[2022-05-30 23:25:18 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.102 (0.104)	Loss 3.2882 (3.2491)	Acc@1 28.125 (36.168)	Acc@5 59.375 (71.875)	Mem 4879MB
[2022-05-30 23:25:19 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.104 (0.103)	Loss 3.3201 (3.2625)	Acc@1 18.750 (35.739)	Acc@5 65.625 (71.303)	Mem 4879MB
[2022-05-30 23:25:20 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 3.3327 (3.2644)	Acc@1 34.375 (35.957)	Acc@5 71.875 (71.335)	Mem 4879MB
[2022-05-30 23:25:21 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 2.9656 (3.2716)	Acc@1 53.125 (35.714)	Acc@5 90.625 (70.913)	Mem 4879MB
[2022-05-30 23:25:22 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 3.1400 (3.2741)	Acc@1 50.000 (36.015)	Acc@5 75.000 (70.823)	Mem 4879MB
[2022-05-30 23:25:23 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.090 (0.100)	Loss 3.1981 (3.2713)	Acc@1 31.250 (36.515)	Acc@5 71.875 (70.890)	Mem 4879MB
[2022-05-30 23:25:24 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 3.2099 (3.2701)	Acc@1 37.500 (36.131)	Acc@5 71.875 (71.074)	Mem 4879MB
[2022-05-30 23:25:25 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.105 (0.099)	Loss 3.1764 (3.2628)	Acc@1 34.375 (36.450)	Acc@5 71.875 (71.398)	Mem 4879MB
[2022-05-30 23:25:26 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 3.3103 (3.2567)	Acc@1 40.625 (36.613)	Acc@5 71.875 (71.698)	Mem 4879MB
[2022-05-30 23:25:27 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.093 (0.098)	Loss 3.3928 (3.2575)	Acc@1 34.375 (36.486)	Acc@5 62.500 (71.606)	Mem 4879MB
[2022-05-30 23:25:28 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.089 (0.098)	Loss 3.4176 (3.2627)	Acc@1 31.250 (36.297)	Acc@5 75.000 (71.467)	Mem 4879MB
[2022-05-30 23:25:29 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 3.4467 (3.2648)	Acc@1 37.500 (36.166)	Acc@5 65.625 (71.345)	Mem 4879MB
[2022-05-30 23:25:30 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.100 (0.097)	Loss 3.0680 (3.2678)	Acc@1 40.625 (35.981)	Acc@5 84.375 (71.236)	Mem 4879MB
[2022-05-30 23:25:30 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.097)	Loss 2.8988 (3.2631)	Acc@1 56.250 (36.158)	Acc@5 93.750 (71.466)	Mem 4879MB
[2022-05-30 23:25:31 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.092 (0.097)	Loss 3.3479 (3.2655)	Acc@1 31.250 (36.101)	Acc@5 68.750 (71.346)	Mem 4879MB
[2022-05-30 23:25:32 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.089 (0.097)	Loss 3.2303 (3.2673)	Acc@1 34.375 (36.137)	Acc@5 68.750 (71.194)	Mem 4879MB
[2022-05-30 23:25:33 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.096)	Loss 3.4141 (3.2654)	Acc@1 34.375 (36.256)	Acc@5 53.125 (71.239)	Mem 4879MB
[2022-05-30 23:25:34 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.096)	Loss 3.2550 (3.2644)	Acc@1 34.375 (36.147)	Acc@5 78.125 (71.226)	Mem 4879MB
[2022-05-30 23:25:35 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.096)	Loss 3.1428 (3.2662)	Acc@1 50.000 (36.061)	Acc@5 78.125 (71.201)	Mem 4879MB
[2022-05-30 23:25:36 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.096)	Loss 3.4195 (3.2673)	Acc@1 28.125 (35.981)	Acc@5 50.000 (70.991)	Mem 4879MB
[2022-05-30 23:25:37 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 3.4517 (3.2669)	Acc@1 28.125 (36.171)	Acc@5 59.375 (70.953)	Mem 4879MB
[2022-05-30 23:25:38 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.091 (0.096)	Loss 3.2326 (3.2653)	Acc@1 37.500 (36.197)	Acc@5 84.375 (71.102)	Mem 4879MB
[2022-05-30 23:25:39 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 3.5197 (3.2642)	Acc@1 28.125 (36.277)	Acc@5 56.250 (71.208)	Mem 4879MB
[2022-05-30 23:25:40 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.095 (0.096)	Loss 3.2515 (3.2645)	Acc@1 37.500 (36.201)	Acc@5 78.125 (71.263)	Mem 4879MB
[2022-05-30 23:25:41 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.096)	Loss 3.2789 (3.2635)	Acc@1 43.750 (36.327)	Acc@5 81.250 (71.377)	Mem 4879MB
[2022-05-30 23:25:42 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 3.3072 (3.2612)	Acc@1 31.250 (36.515)	Acc@5 71.875 (71.463)	Mem 4879MB
[2022-05-30 23:25:42 MetaFG_0] (main.py 330): INFO  * Acc@1 36.490 Acc@5 71.410
[2022-05-30 23:25:42 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 36.5%
[2022-05-30 23:25:42 MetaFG_0] (main.py 171): INFO Max accuracy: 36.49%
[2022-05-30 23:25:43 MetaFG_0] (main.py 265): INFO Train: [8/300][0/1562]	eta 0:27:46 lr 0.000003	time 1.0672 (1.0672)	loss 2.1705 (2.1705)	grad_norm 18.1316 (18.1316)	mem 4879MB
[2022-05-30 23:25:46 MetaFG_0] (main.py 265): INFO Train: [8/300][10/1562]	eta 0:09:41 lr 0.000003	time 0.2974 (0.3748)	loss 2.0919 (2.0437)	grad_norm 16.5368 (16.0847)	mem 4879MB
[2022-05-30 23:25:49 MetaFG_0] (main.py 265): INFO Train: [8/300][20/1562]	eta 0:08:44 lr 0.000003	time 0.2928 (0.3402)	loss 2.0438 (2.0501)	grad_norm 15.6555 (15.6481)	mem 4879MB
[2022-05-30 23:25:52 MetaFG_0] (main.py 265): INFO Train: [8/300][30/1562]	eta 0:08:22 lr 0.000003	time 0.2920 (0.3281)	loss 2.0958 (2.0654)	grad_norm 14.7966 (15.3790)	mem 4879MB
[2022-05-30 23:25:55 MetaFG_0] (main.py 265): INFO Train: [8/300][40/1562]	eta 0:08:10 lr 0.000003	time 0.2946 (0.3224)	loss 1.9085 (2.0520)	grad_norm 14.9828 (15.6179)	mem 4879MB
[2022-05-30 23:25:58 MetaFG_0] (main.py 265): INFO Train: [8/300][50/1562]	eta 0:08:01 lr 0.000003	time 0.2977 (0.3187)	loss 2.1481 (2.0559)	grad_norm 10.0697 (15.5288)	mem 4879MB
[2022-05-30 23:26:01 MetaFG_0] (main.py 265): INFO Train: [8/300][60/1562]	eta 0:07:54 lr 0.000003	time 0.2987 (0.3161)	loss 1.8869 (2.0419)	grad_norm 30.4226 (16.0084)	mem 4879MB
[2022-05-30 23:26:04 MetaFG_0] (main.py 265): INFO Train: [8/300][70/1562]	eta 0:07:48 lr 0.000003	time 0.2921 (0.3143)	loss 1.9942 (2.0410)	grad_norm 19.6021 (15.9599)	mem 4879MB
[2022-05-30 23:26:07 MetaFG_0] (main.py 265): INFO Train: [8/300][80/1562]	eta 0:07:44 lr 0.000003	time 0.2972 (0.3134)	loss 2.0359 (2.0430)	grad_norm 19.4968 (16.1166)	mem 4879MB
[2022-05-30 23:26:10 MetaFG_0] (main.py 265): INFO Train: [8/300][90/1562]	eta 0:07:39 lr 0.000003	time 0.2919 (0.3124)	loss 2.0600 (2.0441)	grad_norm 16.1339 (15.9581)	mem 4879MB
[2022-05-30 23:26:13 MetaFG_0] (main.py 265): INFO Train: [8/300][100/1562]	eta 0:07:35 lr 0.000003	time 0.3003 (0.3117)	loss 2.0317 (2.0435)	grad_norm 11.7869 (16.1847)	mem 4879MB
[2022-05-30 23:26:16 MetaFG_0] (main.py 265): INFO Train: [8/300][110/1562]	eta 0:07:31 lr 0.000003	time 0.2984 (0.3111)	loss 2.0795 (2.0458)	grad_norm 10.0141 (16.2860)	mem 4879MB
[2022-05-30 23:26:19 MetaFG_0] (main.py 265): INFO Train: [8/300][120/1562]	eta 0:07:27 lr 0.000003	time 0.2934 (0.3106)	loss 2.0790 (2.0455)	grad_norm 9.6461 (16.2801)	mem 4879MB
[2022-05-30 23:26:22 MetaFG_0] (main.py 265): INFO Train: [8/300][130/1562]	eta 0:07:24 lr 0.000003	time 0.2955 (0.3101)	loss 1.9554 (2.0445)	grad_norm 16.2030 (16.3039)	mem 4879MB
[2022-05-30 23:26:26 MetaFG_0] (main.py 265): INFO Train: [8/300][140/1562]	eta 0:07:20 lr 0.000003	time 0.2965 (0.3096)	loss 1.8791 (2.0438)	grad_norm 15.9077 (16.2106)	mem 4879MB
[2022-05-30 23:26:29 MetaFG_0] (main.py 265): INFO Train: [8/300][150/1562]	eta 0:07:16 lr 0.000003	time 0.2983 (0.3091)	loss 1.7735 (2.0415)	grad_norm 15.2372 (16.3365)	mem 4879MB
[2022-05-30 23:26:32 MetaFG_0] (main.py 265): INFO Train: [8/300][160/1562]	eta 0:07:12 lr 0.000003	time 0.2930 (0.3087)	loss 1.8795 (2.0419)	grad_norm 25.0358 (16.3919)	mem 4879MB
[2022-05-30 23:26:35 MetaFG_0] (main.py 265): INFO Train: [8/300][170/1562]	eta 0:07:09 lr 0.000003	time 0.2917 (0.3084)	loss 2.0652 (2.0412)	grad_norm 20.3896 (16.4360)	mem 4879MB
[2022-05-30 23:26:38 MetaFG_0] (main.py 265): INFO Train: [8/300][180/1562]	eta 0:07:05 lr 0.000003	time 0.2931 (0.3080)	loss 2.0318 (2.0434)	grad_norm 21.0048 (16.3855)	mem 4879MB
[2022-05-30 23:26:41 MetaFG_0] (main.py 265): INFO Train: [8/300][190/1562]	eta 0:07:02 lr 0.000003	time 0.2992 (0.3080)	loss 2.1031 (2.0434)	grad_norm 16.2637 (16.3372)	mem 4879MB
[2022-05-30 23:26:44 MetaFG_0] (main.py 265): INFO Train: [8/300][200/1562]	eta 0:06:59 lr 0.000003	time 0.2920 (0.3078)	loss 1.8956 (2.0433)	grad_norm 17.9339 (16.2790)	mem 4879MB
[2022-05-30 23:26:47 MetaFG_0] (main.py 265): INFO Train: [8/300][210/1562]	eta 0:06:56 lr 0.000003	time 0.2929 (0.3077)	loss 1.9400 (2.0427)	grad_norm 20.1293 (16.2346)	mem 4879MB
[2022-05-30 23:26:50 MetaFG_0] (main.py 265): INFO Train: [8/300][220/1562]	eta 0:06:52 lr 0.000003	time 0.2930 (0.3076)	loss 2.1326 (2.0427)	grad_norm 18.7722 (16.2231)	mem 4879MB
[2022-05-30 23:26:53 MetaFG_0] (main.py 265): INFO Train: [8/300][230/1562]	eta 0:06:49 lr 0.000003	time 0.2975 (0.3074)	loss 2.0836 (2.0429)	grad_norm 19.3954 (16.1430)	mem 4879MB
[2022-05-30 23:26:56 MetaFG_0] (main.py 265): INFO Train: [8/300][240/1562]	eta 0:06:46 lr 0.000003	time 0.2997 (0.3072)	loss 1.9266 (2.0419)	grad_norm 16.3328 (16.2085)	mem 4879MB
[2022-05-30 23:26:59 MetaFG_0] (main.py 265): INFO Train: [8/300][250/1562]	eta 0:06:42 lr 0.000003	time 0.2917 (0.3071)	loss 2.0438 (2.0397)	grad_norm 13.9067 (16.2108)	mem 4879MB
[2022-05-30 23:27:02 MetaFG_0] (main.py 265): INFO Train: [8/300][260/1562]	eta 0:06:39 lr 0.000003	time 0.2974 (0.3070)	loss 1.9811 (2.0403)	grad_norm 14.5525 (16.2085)	mem 4879MB
[2022-05-30 23:27:05 MetaFG_0] (main.py 265): INFO Train: [8/300][270/1562]	eta 0:06:36 lr 0.000003	time 0.2976 (0.3069)	loss 2.1741 (2.0405)	grad_norm 19.5567 (16.2458)	mem 4879MB
[2022-05-30 23:27:08 MetaFG_0] (main.py 265): INFO Train: [8/300][280/1562]	eta 0:06:33 lr 0.000003	time 0.2934 (0.3067)	loss 2.1148 (2.0400)	grad_norm 17.1395 (16.2888)	mem 4879MB
[2022-05-30 23:27:11 MetaFG_0] (main.py 265): INFO Train: [8/300][290/1562]	eta 0:06:29 lr 0.000003	time 0.2981 (0.3066)	loss 1.9380 (2.0379)	grad_norm 23.7735 (16.2595)	mem 4879MB
[2022-05-30 23:27:14 MetaFG_0] (main.py 265): INFO Train: [8/300][300/1562]	eta 0:06:26 lr 0.000003	time 0.2926 (0.3064)	loss 1.8809 (2.0378)	grad_norm 22.8997 (16.4218)	mem 4879MB
[2022-05-30 23:27:17 MetaFG_0] (main.py 265): INFO Train: [8/300][310/1562]	eta 0:06:23 lr 0.000003	time 0.2979 (0.3064)	loss 2.0774 (2.0371)	grad_norm 14.4325 (16.3567)	mem 4879MB
[2022-05-30 23:27:20 MetaFG_0] (main.py 265): INFO Train: [8/300][320/1562]	eta 0:06:20 lr 0.000003	time 0.2976 (0.3063)	loss 2.1420 (2.0388)	grad_norm 13.3812 (16.3297)	mem 4879MB
[2022-05-30 23:27:23 MetaFG_0] (main.py 265): INFO Train: [8/300][330/1562]	eta 0:06:17 lr 0.000003	time 0.2935 (0.3063)	loss 2.1798 (2.0398)	grad_norm 22.1606 (16.4067)	mem 4879MB
[2022-05-30 23:27:26 MetaFG_0] (main.py 265): INFO Train: [8/300][340/1562]	eta 0:06:14 lr 0.000003	time 0.2981 (0.3064)	loss 1.9042 (2.0402)	grad_norm 13.0680 (16.4506)	mem 4879MB
[2022-05-30 23:27:29 MetaFG_0] (main.py 265): INFO Train: [8/300][350/1562]	eta 0:06:11 lr 0.000003	time 0.2992 (0.3063)	loss 2.0869 (2.0402)	grad_norm 21.0787 (16.4954)	mem 4879MB
[2022-05-30 23:27:32 MetaFG_0] (main.py 265): INFO Train: [8/300][360/1562]	eta 0:06:08 lr 0.000003	time 0.2924 (0.3063)	loss 2.0820 (2.0391)	grad_norm 35.0996 (16.5273)	mem 4879MB
[2022-05-30 23:27:35 MetaFG_0] (main.py 265): INFO Train: [8/300][370/1562]	eta 0:06:04 lr 0.000003	time 0.2935 (0.3062)	loss 2.1886 (2.0388)	grad_norm 11.5683 (16.5351)	mem 4879MB
[2022-05-30 23:27:38 MetaFG_0] (main.py 265): INFO Train: [8/300][380/1562]	eta 0:06:01 lr 0.000003	time 0.2979 (0.3061)	loss 2.0500 (2.0380)	grad_norm 17.4615 (16.5773)	mem 4879MB
[2022-05-30 23:27:42 MetaFG_0] (main.py 265): INFO Train: [8/300][390/1562]	eta 0:05:58 lr 0.000003	time 0.2993 (0.3061)	loss 2.0761 (2.0373)	grad_norm 19.3086 (16.5776)	mem 4879MB
[2022-05-30 23:27:45 MetaFG_0] (main.py 265): INFO Train: [8/300][400/1562]	eta 0:05:55 lr 0.000003	time 0.2950 (0.3061)	loss 2.1082 (2.0371)	grad_norm 12.2309 (16.5296)	mem 4879MB
[2022-05-30 23:27:48 MetaFG_0] (main.py 265): INFO Train: [8/300][410/1562]	eta 0:05:52 lr 0.000003	time 0.2926 (0.3060)	loss 2.0570 (2.0379)	grad_norm 19.8154 (16.5558)	mem 4879MB
[2022-05-30 23:27:51 MetaFG_0] (main.py 265): INFO Train: [8/300][420/1562]	eta 0:05:49 lr 0.000003	time 0.2929 (0.3060)	loss 1.9746 (2.0377)	grad_norm 14.7906 (16.5796)	mem 4879MB
[2022-05-30 23:27:54 MetaFG_0] (main.py 265): INFO Train: [8/300][430/1562]	eta 0:05:46 lr 0.000003	time 0.2979 (0.3060)	loss 2.0264 (2.0383)	grad_norm 11.7570 (16.6268)	mem 4879MB
[2022-05-30 23:27:57 MetaFG_0] (main.py 265): INFO Train: [8/300][440/1562]	eta 0:05:43 lr 0.000003	time 0.3004 (0.3061)	loss 2.1214 (2.0371)	grad_norm 15.0622 (16.6284)	mem 4879MB
[2022-05-30 23:28:00 MetaFG_0] (main.py 265): INFO Train: [8/300][450/1562]	eta 0:05:40 lr 0.000003	time 0.2988 (0.3060)	loss 2.0646 (2.0363)	grad_norm 11.9887 (16.6314)	mem 4879MB
[2022-05-30 23:28:03 MetaFG_0] (main.py 265): INFO Train: [8/300][460/1562]	eta 0:05:37 lr 0.000003	time 0.2932 (0.3060)	loss 1.9765 (2.0360)	grad_norm 16.1429 (16.6425)	mem 4879MB
[2022-05-30 23:28:06 MetaFG_0] (main.py 265): INFO Train: [8/300][470/1562]	eta 0:05:34 lr 0.000003	time 0.2922 (0.3059)	loss 2.0644 (2.0361)	grad_norm 17.6103 (16.6424)	mem 4879MB
[2022-05-30 23:28:09 MetaFG_0] (main.py 265): INFO Train: [8/300][480/1562]	eta 0:05:30 lr 0.000003	time 0.3006 (0.3058)	loss 2.0310 (2.0353)	grad_norm 11.3382 (16.6414)	mem 4879MB
[2022-05-30 23:28:12 MetaFG_0] (main.py 265): INFO Train: [8/300][490/1562]	eta 0:05:27 lr 0.000003	time 0.2976 (0.3058)	loss 2.0473 (2.0348)	grad_norm 17.1253 (16.6162)	mem 4879MB
[2022-05-30 23:28:15 MetaFG_0] (main.py 265): INFO Train: [8/300][500/1562]	eta 0:05:24 lr 0.000003	time 0.2936 (0.3057)	loss 1.7927 (2.0329)	grad_norm 13.7915 (16.6508)	mem 4879MB
[2022-05-30 23:28:18 MetaFG_0] (main.py 265): INFO Train: [8/300][510/1562]	eta 0:05:21 lr 0.000003	time 0.2981 (0.3057)	loss 1.9489 (2.0325)	grad_norm 14.3640 (16.6452)	mem 4879MB
[2022-05-30 23:28:21 MetaFG_0] (main.py 265): INFO Train: [8/300][520/1562]	eta 0:05:18 lr 0.000003	time 0.2933 (0.3057)	loss 2.0062 (2.0326)	grad_norm 14.0776 (16.6164)	mem 4879MB
[2022-05-30 23:28:24 MetaFG_0] (main.py 265): INFO Train: [8/300][530/1562]	eta 0:05:15 lr 0.000003	time 0.2973 (0.3056)	loss 2.0780 (2.0331)	grad_norm 15.1657 (16.5973)	mem 4879MB
[2022-05-30 23:28:27 MetaFG_0] (main.py 265): INFO Train: [8/300][540/1562]	eta 0:05:12 lr 0.000003	time 0.2922 (0.3056)	loss 1.8793 (2.0321)	grad_norm 12.8053 (16.6091)	mem 4879MB
[2022-05-30 23:28:30 MetaFG_0] (main.py 265): INFO Train: [8/300][550/1562]	eta 0:05:09 lr 0.000003	time 0.2939 (0.3055)	loss 2.0734 (2.0325)	grad_norm 21.3662 (16.6049)	mem 4879MB
[2022-05-30 23:28:33 MetaFG_0] (main.py 265): INFO Train: [8/300][560/1562]	eta 0:05:06 lr 0.000003	time 0.2918 (0.3055)	loss 2.0629 (2.0329)	grad_norm 11.9038 (16.5974)	mem 4879MB
[2022-05-30 23:28:36 MetaFG_0] (main.py 265): INFO Train: [8/300][570/1562]	eta 0:05:02 lr 0.000003	time 0.2924 (0.3054)	loss 2.0748 (2.0320)	grad_norm 15.7150 (16.6688)	mem 4879MB
[2022-05-30 23:28:39 MetaFG_0] (main.py 265): INFO Train: [8/300][580/1562]	eta 0:04:59 lr 0.000003	time 0.2936 (0.3054)	loss 2.0298 (2.0316)	grad_norm 12.1938 (16.6793)	mem 4879MB
[2022-05-30 23:28:42 MetaFG_0] (main.py 265): INFO Train: [8/300][590/1562]	eta 0:04:56 lr 0.000003	time 0.2922 (0.3054)	loss 2.1183 (2.0311)	grad_norm 14.2249 (16.6815)	mem 4879MB
[2022-05-30 23:28:45 MetaFG_0] (main.py 265): INFO Train: [8/300][600/1562]	eta 0:04:53 lr 0.000003	time 0.2930 (0.3053)	loss 2.0186 (2.0313)	grad_norm 26.4792 (16.6895)	mem 4879MB
[2022-05-30 23:28:48 MetaFG_0] (main.py 265): INFO Train: [8/300][610/1562]	eta 0:04:50 lr 0.000003	time 0.2981 (0.3053)	loss 1.9784 (2.0319)	grad_norm 13.9447 (16.6339)	mem 4879MB
[2022-05-30 23:28:51 MetaFG_0] (main.py 265): INFO Train: [8/300][620/1562]	eta 0:04:47 lr 0.000003	time 0.3000 (0.3053)	loss 1.7401 (2.0303)	grad_norm 17.0186 (16.6761)	mem 4879MB
[2022-05-30 23:28:54 MetaFG_0] (main.py 265): INFO Train: [8/300][630/1562]	eta 0:04:44 lr 0.000003	time 0.2918 (0.3053)	loss 1.9602 (2.0304)	grad_norm 11.2495 (16.6361)	mem 4879MB
[2022-05-30 23:28:58 MetaFG_0] (main.py 265): INFO Train: [8/300][640/1562]	eta 0:04:41 lr 0.000003	time 0.2936 (0.3053)	loss 2.0878 (2.0302)	grad_norm 14.3668 (16.6327)	mem 4879MB
[2022-05-30 23:29:01 MetaFG_0] (main.py 265): INFO Train: [8/300][650/1562]	eta 0:04:38 lr 0.000003	time 0.2929 (0.3053)	loss 2.1085 (2.0302)	grad_norm 9.8850 (16.6247)	mem 4879MB
[2022-05-30 23:29:04 MetaFG_0] (main.py 265): INFO Train: [8/300][660/1562]	eta 0:04:35 lr 0.000003	time 0.3009 (0.3053)	loss 1.9415 (2.0307)	grad_norm 19.4203 (16.6242)	mem 4879MB
[2022-05-30 23:29:07 MetaFG_0] (main.py 265): INFO Train: [8/300][670/1562]	eta 0:04:32 lr 0.000003	time 0.2986 (0.3053)	loss 1.9962 (2.0296)	grad_norm 19.9025 (16.6615)	mem 4879MB
[2022-05-30 23:29:10 MetaFG_0] (main.py 265): INFO Train: [8/300][680/1562]	eta 0:04:29 lr 0.000003	time 0.2990 (0.3053)	loss 1.9996 (2.0292)	grad_norm 28.0833 (16.6806)	mem 4879MB
[2022-05-30 23:29:13 MetaFG_0] (main.py 265): INFO Train: [8/300][690/1562]	eta 0:04:26 lr 0.000003	time 0.2922 (0.3053)	loss 2.0138 (2.0297)	grad_norm 12.6110 (16.6958)	mem 4879MB
[2022-05-30 23:29:16 MetaFG_0] (main.py 265): INFO Train: [8/300][700/1562]	eta 0:04:23 lr 0.000003	time 0.2945 (0.3052)	loss 2.0825 (2.0286)	grad_norm 13.0397 (16.6825)	mem 4879MB
[2022-05-30 23:29:19 MetaFG_0] (main.py 265): INFO Train: [8/300][710/1562]	eta 0:04:20 lr 0.000003	time 0.2918 (0.3052)	loss 1.9867 (2.0279)	grad_norm 10.3045 (16.6664)	mem 4879MB
[2022-05-30 23:29:22 MetaFG_0] (main.py 265): INFO Train: [8/300][720/1562]	eta 0:04:17 lr 0.000003	time 0.2916 (0.3052)	loss 2.1783 (2.0284)	grad_norm 18.9273 (16.6598)	mem 4879MB
[2022-05-30 23:29:25 MetaFG_0] (main.py 265): INFO Train: [8/300][730/1562]	eta 0:04:13 lr 0.000003	time 0.2991 (0.3053)	loss 2.0794 (2.0280)	grad_norm 17.5138 (16.6719)	mem 4879MB
[2022-05-30 23:29:28 MetaFG_0] (main.py 265): INFO Train: [8/300][740/1562]	eta 0:04:10 lr 0.000003	time 0.2920 (0.3053)	loss 1.9099 (2.0278)	grad_norm 19.2231 (16.6566)	mem 4879MB
[2022-05-30 23:29:31 MetaFG_0] (main.py 265): INFO Train: [8/300][750/1562]	eta 0:04:07 lr 0.000003	time 0.2974 (0.3053)	loss 1.8817 (2.0269)	grad_norm 22.4719 (16.6615)	mem 4879MB
[2022-05-30 23:29:34 MetaFG_0] (main.py 265): INFO Train: [8/300][760/1562]	eta 0:04:04 lr 0.000003	time 0.2918 (0.3053)	loss 1.9509 (2.0266)	grad_norm 15.2099 (16.6652)	mem 4879MB
[2022-05-30 23:29:37 MetaFG_0] (main.py 265): INFO Train: [8/300][770/1562]	eta 0:04:01 lr 0.000003	time 0.2990 (0.3053)	loss 1.9953 (2.0264)	grad_norm 12.9371 (16.6556)	mem 4879MB
[2022-05-30 23:29:40 MetaFG_0] (main.py 265): INFO Train: [8/300][780/1562]	eta 0:03:58 lr 0.000003	time 0.2982 (0.3054)	loss 2.0378 (2.0257)	grad_norm 11.3081 (16.6713)	mem 4879MB
[2022-05-30 23:29:43 MetaFG_0] (main.py 265): INFO Train: [8/300][790/1562]	eta 0:03:55 lr 0.000003	time 0.3014 (0.3054)	loss 2.1167 (2.0252)	grad_norm 14.0265 (16.6862)	mem 4879MB
[2022-05-30 23:29:46 MetaFG_0] (main.py 265): INFO Train: [8/300][800/1562]	eta 0:03:52 lr 0.000003	time 0.2940 (0.3054)	loss 1.9464 (2.0249)	grad_norm 16.3977 (16.6881)	mem 4879MB
[2022-05-30 23:29:49 MetaFG_0] (main.py 265): INFO Train: [8/300][810/1562]	eta 0:03:49 lr 0.000003	time 0.2925 (0.3054)	loss 2.0936 (2.0241)	grad_norm 14.2835 (16.7110)	mem 4879MB
[2022-05-30 23:29:53 MetaFG_0] (main.py 265): INFO Train: [8/300][820/1562]	eta 0:03:46 lr 0.000003	time 0.2924 (0.3053)	loss 1.9525 (2.0234)	grad_norm 13.6804 (16.7244)	mem 4879MB
[2022-05-30 23:29:56 MetaFG_0] (main.py 265): INFO Train: [8/300][830/1562]	eta 0:03:43 lr 0.000003	time 0.2920 (0.3053)	loss 1.8928 (2.0231)	grad_norm 10.7542 (16.7033)	mem 4879MB
[2022-05-30 23:29:59 MetaFG_0] (main.py 265): INFO Train: [8/300][840/1562]	eta 0:03:40 lr 0.000003	time 0.3033 (0.3053)	loss 1.7526 (2.0222)	grad_norm 17.3521 (16.6864)	mem 4879MB
[2022-05-30 23:30:02 MetaFG_0] (main.py 265): INFO Train: [8/300][850/1562]	eta 0:03:37 lr 0.000003	time 0.2933 (0.3052)	loss 1.9897 (2.0223)	grad_norm 10.8705 (16.6622)	mem 4879MB
[2022-05-30 23:30:05 MetaFG_0] (main.py 265): INFO Train: [8/300][860/1562]	eta 0:03:34 lr 0.000003	time 0.2919 (0.3052)	loss 1.9382 (2.0222)	grad_norm 14.0701 (16.6527)	mem 4879MB
[2022-05-30 23:30:08 MetaFG_0] (main.py 265): INFO Train: [8/300][870/1562]	eta 0:03:31 lr 0.000003	time 0.2928 (0.3052)	loss 2.1174 (2.0222)	grad_norm 10.7787 (16.6417)	mem 4879MB
[2022-05-30 23:30:11 MetaFG_0] (main.py 265): INFO Train: [8/300][880/1562]	eta 0:03:28 lr 0.000003	time 0.2930 (0.3052)	loss 2.0120 (2.0225)	grad_norm 13.8080 (16.6399)	mem 4879MB
[2022-05-30 23:30:14 MetaFG_0] (main.py 265): INFO Train: [8/300][890/1562]	eta 0:03:25 lr 0.000003	time 0.2932 (0.3051)	loss 2.0718 (2.0226)	grad_norm 18.2806 (16.6416)	mem 4879MB
[2022-05-30 23:30:17 MetaFG_0] (main.py 265): INFO Train: [8/300][900/1562]	eta 0:03:22 lr 0.000003	time 0.2990 (0.3051)	loss 2.0945 (2.0222)	grad_norm 14.7933 (16.6369)	mem 4879MB
[2022-05-30 23:30:20 MetaFG_0] (main.py 265): INFO Train: [8/300][910/1562]	eta 0:03:18 lr 0.000003	time 0.2950 (0.3051)	loss 1.8782 (2.0205)	grad_norm 15.0404 (16.6651)	mem 4879MB
[2022-05-30 23:30:23 MetaFG_0] (main.py 265): INFO Train: [8/300][920/1562]	eta 0:03:15 lr 0.000003	time 0.2988 (0.3051)	loss 2.1448 (2.0206)	grad_norm 13.7652 (16.6581)	mem 4879MB
[2022-05-30 23:30:26 MetaFG_0] (main.py 265): INFO Train: [8/300][930/1562]	eta 0:03:12 lr 0.000003	time 0.2932 (0.3051)	loss 2.0671 (2.0202)	grad_norm 17.6841 (16.6654)	mem 4879MB
[2022-05-30 23:30:29 MetaFG_0] (main.py 265): INFO Train: [8/300][940/1562]	eta 0:03:09 lr 0.000003	time 0.3007 (0.3051)	loss 2.1497 (2.0208)	grad_norm 13.4048 (16.6675)	mem 4879MB
[2022-05-30 23:30:32 MetaFG_0] (main.py 265): INFO Train: [8/300][950/1562]	eta 0:03:06 lr 0.000003	time 0.2984 (0.3051)	loss 2.0577 (2.0210)	grad_norm 10.2056 (16.6728)	mem 4879MB
[2022-05-30 23:30:35 MetaFG_0] (main.py 265): INFO Train: [8/300][960/1562]	eta 0:03:03 lr 0.000003	time 0.3057 (0.3051)	loss 1.9676 (2.0209)	grad_norm 12.7352 (16.6666)	mem 4879MB
[2022-05-30 23:30:38 MetaFG_0] (main.py 265): INFO Train: [8/300][970/1562]	eta 0:03:00 lr 0.000003	time 0.2980 (0.3051)	loss 2.1067 (2.0212)	grad_norm 14.0235 (16.6739)	mem 4879MB
[2022-05-30 23:30:41 MetaFG_0] (main.py 265): INFO Train: [8/300][980/1562]	eta 0:02:57 lr 0.000003	time 0.2992 (0.3051)	loss 1.9799 (2.0209)	grad_norm 14.5091 (16.6838)	mem 4879MB
[2022-05-30 23:30:44 MetaFG_0] (main.py 265): INFO Train: [8/300][990/1562]	eta 0:02:54 lr 0.000003	time 0.2928 (0.3051)	loss 1.8580 (2.0204)	grad_norm 40.8690 (16.7375)	mem 4879MB
[2022-05-30 23:30:47 MetaFG_0] (main.py 265): INFO Train: [8/300][1000/1562]	eta 0:02:51 lr 0.000003	time 0.2992 (0.3051)	loss 1.8640 (2.0202)	grad_norm 19.3770 (16.7381)	mem 4879MB
[2022-05-30 23:30:50 MetaFG_0] (main.py 265): INFO Train: [8/300][1010/1562]	eta 0:02:48 lr 0.000003	time 0.2920 (0.3051)	loss 2.0463 (2.0199)	grad_norm 11.6705 (16.7235)	mem 4879MB
[2022-05-30 23:30:53 MetaFG_0] (main.py 265): INFO Train: [8/300][1020/1562]	eta 0:02:45 lr 0.000003	time 0.2931 (0.3050)	loss 2.0919 (2.0196)	grad_norm 16.1710 (16.7259)	mem 4879MB
[2022-05-30 23:30:56 MetaFG_0] (main.py 265): INFO Train: [8/300][1030/1562]	eta 0:02:42 lr 0.000003	time 0.2998 (0.3050)	loss 2.1116 (2.0185)	grad_norm 15.3362 (16.7259)	mem 4879MB
[2022-05-30 23:30:59 MetaFG_0] (main.py 265): INFO Train: [8/300][1040/1562]	eta 0:02:39 lr 0.000003	time 0.2932 (0.3050)	loss 2.0321 (2.0186)	grad_norm 11.9829 (16.7135)	mem 4879MB
[2022-05-30 23:31:02 MetaFG_0] (main.py 265): INFO Train: [8/300][1050/1562]	eta 0:02:36 lr 0.000003	time 0.2981 (0.3050)	loss 2.0204 (2.0184)	grad_norm 18.0596 (16.7025)	mem 4879MB
[2022-05-30 23:31:05 MetaFG_0] (main.py 265): INFO Train: [8/300][1060/1562]	eta 0:02:33 lr 0.000003	time 0.2990 (0.3050)	loss 1.8777 (2.0184)	grad_norm 14.3576 (16.6930)	mem 4879MB
[2022-05-30 23:31:08 MetaFG_0] (main.py 265): INFO Train: [8/300][1070/1562]	eta 0:02:30 lr 0.000003	time 0.2933 (0.3050)	loss 1.9419 (2.0178)	grad_norm 17.5093 (16.7021)	mem 4879MB
[2022-05-30 23:31:12 MetaFG_0] (main.py 265): INFO Train: [8/300][1080/1562]	eta 0:02:26 lr 0.000003	time 0.2935 (0.3050)	loss 2.0420 (2.0178)	grad_norm 7.8156 (16.7113)	mem 4879MB
[2022-05-30 23:31:15 MetaFG_0] (main.py 265): INFO Train: [8/300][1090/1562]	eta 0:02:23 lr 0.000003	time 0.2930 (0.3049)	loss 2.0505 (2.0174)	grad_norm 12.5322 (16.7462)	mem 4879MB
[2022-05-30 23:31:18 MetaFG_0] (main.py 265): INFO Train: [8/300][1100/1562]	eta 0:02:20 lr 0.000003	time 0.2992 (0.3049)	loss 2.0379 (2.0169)	grad_norm 16.0447 (16.7406)	mem 4879MB
[2022-05-30 23:31:21 MetaFG_0] (main.py 265): INFO Train: [8/300][1110/1562]	eta 0:02:17 lr 0.000003	time 0.2923 (0.3049)	loss 1.8840 (2.0166)	grad_norm 15.7638 (16.7268)	mem 4879MB
[2022-05-30 23:31:24 MetaFG_0] (main.py 265): INFO Train: [8/300][1120/1562]	eta 0:02:14 lr 0.000003	time 0.2918 (0.3049)	loss 1.9485 (2.0166)	grad_norm 19.6385 (16.7198)	mem 4879MB
[2022-05-30 23:31:27 MetaFG_0] (main.py 265): INFO Train: [8/300][1130/1562]	eta 0:02:11 lr 0.000003	time 0.2937 (0.3049)	loss 2.0592 (2.0166)	grad_norm 11.7865 (16.7110)	mem 4879MB
[2022-05-30 23:31:30 MetaFG_0] (main.py 265): INFO Train: [8/300][1140/1562]	eta 0:02:08 lr 0.000003	time 0.2983 (0.3049)	loss 2.0823 (2.0159)	grad_norm 13.2451 (16.7021)	mem 4879MB
[2022-05-30 23:31:33 MetaFG_0] (main.py 265): INFO Train: [8/300][1150/1562]	eta 0:02:05 lr 0.000003	time 0.2923 (0.3049)	loss 2.1220 (2.0153)	grad_norm 17.5253 (16.7246)	mem 4879MB
[2022-05-30 23:31:36 MetaFG_0] (main.py 265): INFO Train: [8/300][1160/1562]	eta 0:02:02 lr 0.000003	time 0.2917 (0.3049)	loss 1.9944 (2.0147)	grad_norm 13.1032 (16.7258)	mem 4879MB
[2022-05-30 23:31:39 MetaFG_0] (main.py 265): INFO Train: [8/300][1170/1562]	eta 0:01:59 lr 0.000003	time 0.2915 (0.3048)	loss 2.0899 (2.0147)	grad_norm 12.3606 (16.7169)	mem 4879MB
[2022-05-30 23:31:42 MetaFG_0] (main.py 265): INFO Train: [8/300][1180/1562]	eta 0:01:56 lr 0.000003	time 0.2986 (0.3048)	loss 1.8546 (2.0147)	grad_norm 16.8319 (16.7014)	mem 4879MB
[2022-05-30 23:31:45 MetaFG_0] (main.py 265): INFO Train: [8/300][1190/1562]	eta 0:01:53 lr 0.000003	time 0.2929 (0.3048)	loss 1.8540 (2.0149)	grad_norm 15.2975 (16.6835)	mem 4879MB
[2022-05-30 23:31:48 MetaFG_0] (main.py 265): INFO Train: [8/300][1200/1562]	eta 0:01:50 lr 0.000003	time 0.2937 (0.3048)	loss 1.9992 (2.0145)	grad_norm 16.8155 (16.6893)	mem 4879MB
[2022-05-30 23:31:51 MetaFG_0] (main.py 265): INFO Train: [8/300][1210/1562]	eta 0:01:47 lr 0.000003	time 0.2915 (0.3048)	loss 1.8655 (2.0140)	grad_norm 17.1763 (16.6958)	mem 4879MB
[2022-05-30 23:31:54 MetaFG_0] (main.py 265): INFO Train: [8/300][1220/1562]	eta 0:01:44 lr 0.000003	time 0.2915 (0.3048)	loss 1.9951 (2.0141)	grad_norm 11.3396 (16.6911)	mem 4879MB
[2022-05-30 23:31:57 MetaFG_0] (main.py 265): INFO Train: [8/300][1230/1562]	eta 0:01:41 lr 0.000003	time 0.2937 (0.3048)	loss 1.9839 (2.0140)	grad_norm 10.3708 (16.7074)	mem 4879MB
[2022-05-30 23:32:00 MetaFG_0] (main.py 265): INFO Train: [8/300][1240/1562]	eta 0:01:38 lr 0.000003	time 0.2988 (0.3048)	loss 2.1289 (2.0135)	grad_norm 31.3011 (16.7222)	mem 4879MB
[2022-05-30 23:32:03 MetaFG_0] (main.py 265): INFO Train: [8/300][1250/1562]	eta 0:01:35 lr 0.000003	time 0.2929 (0.3048)	loss 1.9975 (2.0137)	grad_norm 12.4757 (16.7261)	mem 4879MB
[2022-05-30 23:32:06 MetaFG_0] (main.py 265): INFO Train: [8/300][1260/1562]	eta 0:01:32 lr 0.000003	time 0.2917 (0.3048)	loss 1.8187 (2.0135)	grad_norm 18.1653 (16.7200)	mem 4879MB
[2022-05-30 23:32:09 MetaFG_0] (main.py 265): INFO Train: [8/300][1270/1562]	eta 0:01:28 lr 0.000003	time 0.2932 (0.3048)	loss 1.7901 (2.0131)	grad_norm 21.8981 (16.7076)	mem 4879MB
[2022-05-30 23:32:12 MetaFG_0] (main.py 265): INFO Train: [8/300][1280/1562]	eta 0:01:25 lr 0.000003	time 0.3014 (0.3048)	loss 2.0957 (2.0132)	grad_norm 13.0938 (16.7155)	mem 4879MB
[2022-05-30 23:32:15 MetaFG_0] (main.py 265): INFO Train: [8/300][1290/1562]	eta 0:01:22 lr 0.000003	time 0.2924 (0.3048)	loss 1.7244 (2.0128)	grad_norm 20.2106 (16.7284)	mem 4879MB
[2022-05-30 23:32:18 MetaFG_0] (main.py 265): INFO Train: [8/300][1300/1562]	eta 0:01:19 lr 0.000003	time 0.2926 (0.3048)	loss 2.0697 (2.0129)	grad_norm 14.3604 (16.7237)	mem 4879MB
[2022-05-30 23:32:21 MetaFG_0] (main.py 265): INFO Train: [8/300][1310/1562]	eta 0:01:16 lr 0.000003	time 0.2934 (0.3048)	loss 2.0146 (2.0127)	grad_norm 11.0181 (16.7053)	mem 4879MB
[2022-05-30 23:32:24 MetaFG_0] (main.py 265): INFO Train: [8/300][1320/1562]	eta 0:01:13 lr 0.000003	time 0.2988 (0.3048)	loss 1.8939 (2.0119)	grad_norm 16.5016 (16.7340)	mem 4879MB
[2022-05-30 23:32:27 MetaFG_0] (main.py 265): INFO Train: [8/300][1330/1562]	eta 0:01:10 lr 0.000003	time 0.2934 (0.3047)	loss 1.9919 (2.0117)	grad_norm 12.1397 (16.7280)	mem 4879MB
[2022-05-30 23:32:30 MetaFG_0] (main.py 265): INFO Train: [8/300][1340/1562]	eta 0:01:07 lr 0.000003	time 0.2920 (0.3047)	loss 2.0348 (2.0113)	grad_norm 11.1674 (16.7162)	mem 4879MB
[2022-05-30 23:32:34 MetaFG_0] (main.py 265): INFO Train: [8/300][1350/1562]	eta 0:01:04 lr 0.000003	time 0.2936 (0.3047)	loss 1.9768 (2.0112)	grad_norm 10.2662 (16.7248)	mem 4879MB
[2022-05-30 23:32:37 MetaFG_0] (main.py 265): INFO Train: [8/300][1360/1562]	eta 0:01:01 lr 0.000003	time 0.3046 (0.3047)	loss 2.0892 (2.0115)	grad_norm 24.1538 (16.7406)	mem 4879MB
[2022-05-30 23:32:40 MetaFG_0] (main.py 265): INFO Train: [8/300][1370/1562]	eta 0:00:58 lr 0.000003	time 0.2929 (0.3047)	loss 2.1105 (2.0112)	grad_norm 15.2753 (16.7477)	mem 4879MB
[2022-05-30 23:32:43 MetaFG_0] (main.py 265): INFO Train: [8/300][1380/1562]	eta 0:00:55 lr 0.000003	time 0.2920 (0.3047)	loss 1.8787 (2.0110)	grad_norm 15.3419 (16.7430)	mem 4879MB
[2022-05-30 23:32:46 MetaFG_0] (main.py 265): INFO Train: [8/300][1390/1562]	eta 0:00:52 lr 0.000003	time 0.2921 (0.3046)	loss 1.7317 (2.0103)	grad_norm 15.5639 (16.7418)	mem 4879MB
[2022-05-30 23:32:49 MetaFG_0] (main.py 265): INFO Train: [8/300][1400/1562]	eta 0:00:49 lr 0.000003	time 0.2920 (0.3046)	loss 2.0457 (2.0103)	grad_norm 12.1601 (16.7363)	mem 4879MB
[2022-05-30 23:32:52 MetaFG_0] (main.py 265): INFO Train: [8/300][1410/1562]	eta 0:00:46 lr 0.000003	time 0.2989 (0.3046)	loss 2.1265 (2.0100)	grad_norm 12.9663 (16.7354)	mem 4879MB
[2022-05-30 23:32:55 MetaFG_0] (main.py 265): INFO Train: [8/300][1420/1562]	eta 0:00:43 lr 0.000003	time 0.2934 (0.3046)	loss 1.9723 (2.0096)	grad_norm 10.6464 (16.7179)	mem 4879MB
[2022-05-30 23:32:58 MetaFG_0] (main.py 265): INFO Train: [8/300][1430/1562]	eta 0:00:40 lr 0.000003	time 0.3018 (0.3046)	loss 1.9081 (2.0092)	grad_norm 9.7298 (16.7220)	mem 4879MB
[2022-05-30 23:33:01 MetaFG_0] (main.py 265): INFO Train: [8/300][1440/1562]	eta 0:00:37 lr 0.000003	time 0.2980 (0.3046)	loss 2.0989 (2.0088)	grad_norm 8.6474 (16.7174)	mem 4879MB
[2022-05-30 23:33:04 MetaFG_0] (main.py 265): INFO Train: [8/300][1450/1562]	eta 0:00:34 lr 0.000003	time 0.2992 (0.3046)	loss 1.9395 (2.0087)	grad_norm 13.5907 (16.7088)	mem 4879MB
[2022-05-30 23:33:07 MetaFG_0] (main.py 265): INFO Train: [8/300][1460/1562]	eta 0:00:31 lr 0.000003	time 0.3019 (0.3046)	loss 1.8321 (2.0080)	grad_norm 12.5939 (16.6945)	mem 4879MB
[2022-05-30 23:33:10 MetaFG_0] (main.py 265): INFO Train: [8/300][1470/1562]	eta 0:00:28 lr 0.000003	time 0.2929 (0.3046)	loss 2.0289 (2.0078)	grad_norm 10.5112 (16.7066)	mem 4879MB
[2022-05-30 23:33:13 MetaFG_0] (main.py 265): INFO Train: [8/300][1480/1562]	eta 0:00:24 lr 0.000003	time 0.2941 (0.3046)	loss 1.8243 (2.0074)	grad_norm 17.1763 (16.7061)	mem 4879MB
[2022-05-30 23:33:16 MetaFG_0] (main.py 265): INFO Train: [8/300][1490/1562]	eta 0:00:21 lr 0.000003	time 0.2935 (0.3046)	loss 2.0625 (2.0076)	grad_norm 10.7138 (16.7024)	mem 4879MB
[2022-05-30 23:33:19 MetaFG_0] (main.py 265): INFO Train: [8/300][1500/1562]	eta 0:00:18 lr 0.000003	time 0.2922 (0.3046)	loss 1.8761 (2.0076)	grad_norm 11.8368 (16.7003)	mem 4879MB
[2022-05-30 23:33:22 MetaFG_0] (main.py 265): INFO Train: [8/300][1510/1562]	eta 0:00:15 lr 0.000003	time 0.2926 (0.3046)	loss 2.0340 (2.0076)	grad_norm 17.8139 (16.6930)	mem 4879MB
[2022-05-30 23:33:25 MetaFG_0] (main.py 265): INFO Train: [8/300][1520/1562]	eta 0:00:12 lr 0.000003	time 0.2946 (0.3046)	loss 2.0073 (2.0074)	grad_norm 13.2290 (16.6984)	mem 4879MB
[2022-05-30 23:33:28 MetaFG_0] (main.py 265): INFO Train: [8/300][1530/1562]	eta 0:00:09 lr 0.000003	time 0.2915 (0.3046)	loss 2.0576 (2.0075)	grad_norm 25.4476 (16.7085)	mem 4879MB
[2022-05-30 23:33:31 MetaFG_0] (main.py 265): INFO Train: [8/300][1540/1562]	eta 0:00:06 lr 0.000003	time 0.2999 (0.3045)	loss 1.9891 (2.0072)	grad_norm 9.1439 (16.7171)	mem 4879MB
[2022-05-30 23:33:34 MetaFG_0] (main.py 265): INFO Train: [8/300][1550/1562]	eta 0:00:03 lr 0.000003	time 0.2916 (0.3045)	loss 2.0850 (2.0069)	grad_norm 13.6488 (16.7272)	mem 4879MB
[2022-05-30 23:33:37 MetaFG_0] (main.py 265): INFO Train: [8/300][1560/1562]	eta 0:00:00 lr 0.000003	time 0.2920 (0.3045)	loss 1.9089 (2.0064)	grad_norm 18.7192 (16.7404)	mem 4879MB
[2022-05-30 23:33:38 MetaFG_0] (main.py 272): INFO EPOCH 8 training takes 0:07:55
[2022-05-30 23:33:38 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_8.pth saving......
[2022-05-30 23:33:38 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_8.pth saved !!!
[2022-05-30 23:33:38 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:33:40 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:33:40 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:33:41 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.621 (0.621)	Loss 2.8530 (2.8530)	Acc@1 43.750 (43.750)	Acc@5 78.125 (78.125)	Mem 4879MB
[2022-05-30 23:33:42 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.087 (0.143)	Loss 3.1192 (2.9917)	Acc@1 40.625 (42.330)	Acc@5 75.000 (74.432)	Mem 4879MB
[2022-05-30 23:33:43 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.121)	Loss 3.1991 (3.0166)	Acc@1 34.375 (39.881)	Acc@5 71.875 (76.042)	Mem 4879MB
[2022-05-30 23:33:44 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.095 (0.112)	Loss 2.9589 (2.9876)	Acc@1 50.000 (42.238)	Acc@5 78.125 (76.512)	Mem 4879MB
[2022-05-30 23:33:45 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.108)	Loss 2.7164 (2.9526)	Acc@1 59.375 (43.674)	Acc@5 87.500 (78.125)	Mem 4879MB
[2022-05-30 23:33:45 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.105)	Loss 2.8685 (2.9373)	Acc@1 46.875 (44.608)	Acc@5 78.125 (78.799)	Mem 4879MB
[2022-05-30 23:33:46 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.103)	Loss 2.6843 (2.9400)	Acc@1 50.000 (44.416)	Acc@5 87.500 (78.586)	Mem 4879MB
[2022-05-30 23:33:47 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.101)	Loss 2.8746 (2.9319)	Acc@1 50.000 (44.410)	Acc@5 75.000 (78.565)	Mem 4879MB
[2022-05-30 23:33:48 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.100)	Loss 3.0990 (2.9404)	Acc@1 43.750 (44.213)	Acc@5 62.500 (78.125)	Mem 4879MB
[2022-05-30 23:33:49 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.099)	Loss 2.9131 (2.9407)	Acc@1 50.000 (44.299)	Acc@5 84.375 (78.091)	Mem 4879MB
[2022-05-30 23:33:50 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.098)	Loss 2.9745 (2.9443)	Acc@1 40.625 (44.090)	Acc@5 71.875 (77.877)	Mem 4879MB
[2022-05-30 23:33:51 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.095 (0.098)	Loss 2.4286 (2.9401)	Acc@1 56.250 (44.200)	Acc@5 84.375 (77.928)	Mem 4879MB
[2022-05-30 23:33:52 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.098)	Loss 3.0894 (2.9446)	Acc@1 34.375 (43.569)	Acc@5 71.875 (77.531)	Mem 4879MB
[2022-05-30 23:33:53 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.097)	Loss 3.0172 (2.9490)	Acc@1 40.625 (43.488)	Acc@5 68.750 (77.576)	Mem 4879MB
[2022-05-30 23:33:54 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.097)	Loss 2.8887 (2.9484)	Acc@1 40.625 (43.329)	Acc@5 87.500 (77.549)	Mem 4879MB
[2022-05-30 23:33:55 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.090 (0.097)	Loss 2.8768 (2.9428)	Acc@1 53.125 (43.647)	Acc@5 75.000 (77.773)	Mem 4879MB
[2022-05-30 23:33:56 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.096)	Loss 3.0418 (2.9367)	Acc@1 59.375 (43.847)	Acc@5 71.875 (78.106)	Mem 4879MB
[2022-05-30 23:33:57 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.096)	Loss 2.8861 (2.9344)	Acc@1 46.875 (43.713)	Acc@5 71.875 (78.344)	Mem 4879MB
[2022-05-30 23:33:57 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.092 (0.096)	Loss 3.0890 (2.9381)	Acc@1 40.625 (43.629)	Acc@5 75.000 (78.194)	Mem 4879MB
[2022-05-30 23:33:58 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.093 (0.096)	Loss 2.9230 (2.9406)	Acc@1 56.250 (43.406)	Acc@5 78.125 (78.190)	Mem 4879MB
[2022-05-30 23:33:59 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.091 (0.096)	Loss 2.7375 (2.9394)	Acc@1 65.625 (43.486)	Acc@5 81.250 (78.203)	Mem 4879MB
[2022-05-30 23:34:00 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.096)	Loss 2.9030 (2.9398)	Acc@1 40.625 (43.350)	Acc@5 81.250 (78.243)	Mem 4879MB
[2022-05-30 23:34:01 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.096)	Loss 3.1111 (2.9403)	Acc@1 46.875 (43.396)	Acc@5 68.750 (78.238)	Mem 4879MB
[2022-05-30 23:34:02 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.095 (0.096)	Loss 2.9216 (2.9432)	Acc@1 46.875 (43.222)	Acc@5 75.000 (78.139)	Mem 4879MB
[2022-05-30 23:34:03 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.100 (0.096)	Loss 2.9324 (2.9432)	Acc@1 43.750 (43.115)	Acc@5 84.375 (78.190)	Mem 4879MB
[2022-05-30 23:34:04 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.095 (0.096)	Loss 3.0547 (2.9460)	Acc@1 37.500 (43.078)	Acc@5 71.875 (78.100)	Mem 4879MB
[2022-05-30 23:34:05 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.090 (0.096)	Loss 3.1107 (2.9474)	Acc@1 21.875 (42.912)	Acc@5 78.125 (78.005)	Mem 4879MB
[2022-05-30 23:34:06 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.100 (0.095)	Loss 3.0403 (2.9447)	Acc@1 34.375 (42.908)	Acc@5 81.250 (77.987)	Mem 4879MB
[2022-05-30 23:34:07 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.095)	Loss 3.0478 (2.9443)	Acc@1 50.000 (43.049)	Acc@5 78.125 (77.925)	Mem 4879MB
[2022-05-30 23:34:08 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.092 (0.095)	Loss 2.7395 (2.9455)	Acc@1 50.000 (42.998)	Acc@5 81.250 (77.899)	Mem 4879MB
[2022-05-30 23:34:09 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.095)	Loss 3.0142 (2.9449)	Acc@1 40.625 (43.002)	Acc@5 81.250 (77.917)	Mem 4879MB
[2022-05-30 23:34:10 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.095)	Loss 2.7190 (2.9431)	Acc@1 50.000 (43.087)	Acc@5 90.625 (78.085)	Mem 4879MB
[2022-05-30 23:34:10 MetaFG_0] (main.py 330): INFO  * Acc@1 43.100 Acc@5 78.070
[2022-05-30 23:34:10 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 43.1%
[2022-05-30 23:34:10 MetaFG_0] (main.py 171): INFO Max accuracy: 43.10%
[2022-05-30 23:34:11 MetaFG_0] (main.py 265): INFO Train: [9/300][0/1562]	eta 0:29:11 lr 0.000003	time 1.1211 (1.1211)	loss 1.8798 (1.8798)	grad_norm 11.5421 (11.5421)	mem 4879MB
[2022-05-30 23:34:14 MetaFG_0] (main.py 265): INFO Train: [9/300][10/1562]	eta 0:10:28 lr 0.000003	time 0.2945 (0.4050)	loss 2.0497 (2.0275)	grad_norm 11.0537 (14.8644)	mem 4879MB
[2022-05-30 23:34:17 MetaFG_0] (main.py 265): INFO Train: [9/300][20/1562]	eta 0:09:09 lr 0.000003	time 0.2921 (0.3562)	loss 1.9219 (2.0063)	grad_norm 17.6364 (15.0802)	mem 4879MB
[2022-05-30 23:34:20 MetaFG_0] (main.py 265): INFO Train: [9/300][30/1562]	eta 0:08:40 lr 0.000003	time 0.2937 (0.3394)	loss 2.0165 (2.0085)	grad_norm 27.5720 (15.8083)	mem 4879MB
[2022-05-30 23:34:24 MetaFG_0] (main.py 265): INFO Train: [9/300][40/1562]	eta 0:08:23 lr 0.000003	time 0.2927 (0.3307)	loss 2.0253 (2.0052)	grad_norm 14.8661 (16.2855)	mem 4879MB
[2022-05-30 23:34:27 MetaFG_0] (main.py 265): INFO Train: [9/300][50/1562]	eta 0:08:11 lr 0.000003	time 0.2921 (0.3253)	loss 1.7920 (1.9917)	grad_norm 16.6578 (16.8556)	mem 4879MB
[2022-05-30 23:34:30 MetaFG_0] (main.py 265): INFO Train: [9/300][60/1562]	eta 0:08:03 lr 0.000003	time 0.2992 (0.3220)	loss 2.0478 (1.9814)	grad_norm 24.5033 (16.9696)	mem 4879MB
[2022-05-30 23:34:33 MetaFG_0] (main.py 265): INFO Train: [9/300][70/1562]	eta 0:07:56 lr 0.000003	time 0.2915 (0.3192)	loss 2.0539 (1.9814)	grad_norm 15.9452 (16.9013)	mem 4879MB
[2022-05-30 23:34:36 MetaFG_0] (main.py 265): INFO Train: [9/300][80/1562]	eta 0:07:50 lr 0.000003	time 0.2917 (0.3173)	loss 2.0677 (1.9798)	grad_norm 15.0033 (16.7342)	mem 4879MB
[2022-05-30 23:34:39 MetaFG_0] (main.py 265): INFO Train: [9/300][90/1562]	eta 0:07:44 lr 0.000003	time 0.2930 (0.3157)	loss 2.0644 (1.9867)	grad_norm 23.4870 (16.6420)	mem 4879MB
[2022-05-30 23:34:42 MetaFG_0] (main.py 265): INFO Train: [9/300][100/1562]	eta 0:07:39 lr 0.000003	time 0.2977 (0.3144)	loss 1.9945 (1.9846)	grad_norm 14.9851 (16.6269)	mem 4879MB
[2022-05-30 23:34:45 MetaFG_0] (main.py 265): INFO Train: [9/300][110/1562]	eta 0:07:35 lr 0.000003	time 0.2942 (0.3134)	loss 1.7943 (1.9843)	grad_norm 16.1547 (16.5651)	mem 4879MB
[2022-05-30 23:34:48 MetaFG_0] (main.py 265): INFO Train: [9/300][120/1562]	eta 0:07:30 lr 0.000003	time 0.2936 (0.3127)	loss 2.0159 (1.9807)	grad_norm 13.5201 (16.6028)	mem 4879MB
[2022-05-30 23:34:51 MetaFG_0] (main.py 265): INFO Train: [9/300][130/1562]	eta 0:07:26 lr 0.000003	time 0.2998 (0.3121)	loss 1.8736 (1.9783)	grad_norm 16.1415 (16.6324)	mem 4879MB
[2022-05-30 23:34:54 MetaFG_0] (main.py 265): INFO Train: [9/300][140/1562]	eta 0:07:23 lr 0.000003	time 0.2990 (0.3116)	loss 2.1279 (1.9824)	grad_norm 16.1761 (16.8381)	mem 4879MB
[2022-05-30 23:34:57 MetaFG_0] (main.py 265): INFO Train: [9/300][150/1562]	eta 0:07:19 lr 0.000003	time 0.2964 (0.3110)	loss 1.9522 (1.9789)	grad_norm 16.1314 (16.9072)	mem 4879MB
[2022-05-30 23:35:00 MetaFG_0] (main.py 265): INFO Train: [9/300][160/1562]	eta 0:07:15 lr 0.000003	time 0.2928 (0.3105)	loss 2.1062 (1.9755)	grad_norm 11.3283 (16.9465)	mem 4879MB
[2022-05-30 23:35:03 MetaFG_0] (main.py 265): INFO Train: [9/300][170/1562]	eta 0:07:11 lr 0.000003	time 0.2948 (0.3100)	loss 1.9406 (1.9711)	grad_norm 14.0466 (16.9748)	mem 4879MB
[2022-05-30 23:35:06 MetaFG_0] (main.py 265): INFO Train: [9/300][180/1562]	eta 0:07:08 lr 0.000003	time 0.2934 (0.3098)	loss 2.0006 (1.9722)	grad_norm 14.3644 (17.0572)	mem 4879MB
[2022-05-30 23:35:09 MetaFG_0] (main.py 265): INFO Train: [9/300][190/1562]	eta 0:07:04 lr 0.000003	time 0.2917 (0.3095)	loss 2.0119 (1.9716)	grad_norm 17.0179 (17.0812)	mem 4879MB
[2022-05-30 23:35:12 MetaFG_0] (main.py 265): INFO Train: [9/300][200/1562]	eta 0:07:01 lr 0.000003	time 0.2935 (0.3092)	loss 1.9859 (1.9703)	grad_norm 11.5719 (17.0320)	mem 4879MB
[2022-05-30 23:35:15 MetaFG_0] (main.py 265): INFO Train: [9/300][210/1562]	eta 0:06:57 lr 0.000003	time 0.2988 (0.3089)	loss 1.8424 (1.9690)	grad_norm 16.1460 (17.0092)	mem 4879MB
[2022-05-30 23:35:18 MetaFG_0] (main.py 265): INFO Train: [9/300][220/1562]	eta 0:06:54 lr 0.000003	time 0.2984 (0.3088)	loss 1.9641 (1.9703)	grad_norm 9.7058 (16.9972)	mem 4879MB
[2022-05-30 23:35:21 MetaFG_0] (main.py 265): INFO Train: [9/300][230/1562]	eta 0:06:50 lr 0.000003	time 0.2980 (0.3085)	loss 1.8207 (1.9686)	grad_norm 13.6804 (16.9710)	mem 4879MB
[2022-05-30 23:35:24 MetaFG_0] (main.py 265): INFO Train: [9/300][240/1562]	eta 0:06:47 lr 0.000003	time 0.2916 (0.3083)	loss 1.8356 (1.9689)	grad_norm 11.9200 (16.9198)	mem 4879MB
[2022-05-30 23:35:27 MetaFG_0] (main.py 265): INFO Train: [9/300][250/1562]	eta 0:06:44 lr 0.000003	time 0.2916 (0.3081)	loss 1.9286 (1.9669)	grad_norm 13.7186 (16.8966)	mem 4879MB
[2022-05-30 23:35:30 MetaFG_0] (main.py 265): INFO Train: [9/300][260/1562]	eta 0:06:40 lr 0.000003	time 0.2916 (0.3079)	loss 2.0378 (1.9665)	grad_norm 12.1370 (16.8134)	mem 4879MB
[2022-05-30 23:35:33 MetaFG_0] (main.py 265): INFO Train: [9/300][270/1562]	eta 0:06:37 lr 0.000003	time 0.2931 (0.3077)	loss 1.8849 (1.9663)	grad_norm 13.7223 (16.7687)	mem 4879MB
[2022-05-30 23:35:36 MetaFG_0] (main.py 265): INFO Train: [9/300][280/1562]	eta 0:06:34 lr 0.000003	time 0.2916 (0.3074)	loss 1.8515 (1.9651)	grad_norm 13.0941 (16.7706)	mem 4879MB
[2022-05-30 23:35:39 MetaFG_0] (main.py 265): INFO Train: [9/300][290/1562]	eta 0:06:30 lr 0.000003	time 0.2977 (0.3073)	loss 2.1573 (1.9650)	grad_norm 14.8570 (inf)	mem 4879MB
[2022-05-30 23:35:42 MetaFG_0] (main.py 265): INFO Train: [9/300][300/1562]	eta 0:06:27 lr 0.000003	time 0.2985 (0.3073)	loss 2.1027 (1.9648)	grad_norm 13.2810 (inf)	mem 4879MB
[2022-05-30 23:35:45 MetaFG_0] (main.py 265): INFO Train: [9/300][310/1562]	eta 0:06:24 lr 0.000003	time 0.2937 (0.3071)	loss 2.0183 (1.9655)	grad_norm 11.0998 (inf)	mem 4879MB
[2022-05-30 23:35:49 MetaFG_0] (main.py 265): INFO Train: [9/300][320/1562]	eta 0:06:21 lr 0.000003	time 0.3007 (0.3070)	loss 2.0697 (1.9665)	grad_norm 14.8327 (inf)	mem 4879MB
[2022-05-30 23:35:52 MetaFG_0] (main.py 265): INFO Train: [9/300][330/1562]	eta 0:06:18 lr 0.000003	time 0.2940 (0.3069)	loss 2.1075 (1.9663)	grad_norm 17.2591 (inf)	mem 4879MB
[2022-05-30 23:35:55 MetaFG_0] (main.py 265): INFO Train: [9/300][340/1562]	eta 0:06:15 lr 0.000003	time 0.2933 (0.3069)	loss 2.0726 (1.9676)	grad_norm 17.6088 (inf)	mem 4879MB
[2022-05-30 23:35:58 MetaFG_0] (main.py 265): INFO Train: [9/300][350/1562]	eta 0:06:11 lr 0.000003	time 0.2979 (0.3067)	loss 2.0150 (1.9679)	grad_norm 12.8661 (inf)	mem 4879MB
[2022-05-30 23:36:01 MetaFG_0] (main.py 265): INFO Train: [9/300][360/1562]	eta 0:06:08 lr 0.000003	time 0.2919 (0.3067)	loss 2.1327 (1.9673)	grad_norm 17.6927 (inf)	mem 4879MB
[2022-05-30 23:36:04 MetaFG_0] (main.py 265): INFO Train: [9/300][370/1562]	eta 0:06:05 lr 0.000003	time 0.2981 (0.3066)	loss 2.0067 (1.9683)	grad_norm 12.8257 (inf)	mem 4879MB
[2022-05-30 23:36:07 MetaFG_0] (main.py 265): INFO Train: [9/300][380/1562]	eta 0:06:02 lr 0.000003	time 0.2922 (0.3065)	loss 2.0371 (1.9694)	grad_norm 10.9066 (inf)	mem 4879MB
[2022-05-30 23:36:10 MetaFG_0] (main.py 265): INFO Train: [9/300][390/1562]	eta 0:05:59 lr 0.000003	time 0.2933 (0.3065)	loss 2.0622 (1.9687)	grad_norm 11.6979 (inf)	mem 4879MB
[2022-05-30 23:36:13 MetaFG_0] (main.py 265): INFO Train: [9/300][400/1562]	eta 0:05:56 lr 0.000003	time 0.2978 (0.3064)	loss 2.0646 (1.9698)	grad_norm 13.8853 (inf)	mem 4879MB
[2022-05-30 23:36:16 MetaFG_0] (main.py 265): INFO Train: [9/300][410/1562]	eta 0:05:52 lr 0.000003	time 0.2991 (0.3064)	loss 1.9257 (1.9706)	grad_norm 10.8208 (inf)	mem 4879MB
[2022-05-30 23:36:19 MetaFG_0] (main.py 265): INFO Train: [9/300][420/1562]	eta 0:05:49 lr 0.000003	time 0.3038 (0.3064)	loss 1.9853 (1.9710)	grad_norm 21.5943 (inf)	mem 4879MB
[2022-05-30 23:36:22 MetaFG_0] (main.py 265): INFO Train: [9/300][430/1562]	eta 0:05:46 lr 0.000003	time 0.3005 (0.3063)	loss 2.1246 (1.9716)	grad_norm 21.0333 (inf)	mem 4879MB
[2022-05-30 23:36:25 MetaFG_0] (main.py 265): INFO Train: [9/300][440/1562]	eta 0:05:43 lr 0.000003	time 0.2993 (0.3062)	loss 2.0080 (1.9716)	grad_norm 16.7561 (inf)	mem 4879MB
[2022-05-30 23:36:28 MetaFG_0] (main.py 265): INFO Train: [9/300][450/1562]	eta 0:05:40 lr 0.000003	time 0.2920 (0.3064)	loss 1.8947 (1.9710)	grad_norm 12.2530 (inf)	mem 4879MB
[2022-05-30 23:36:31 MetaFG_0] (main.py 265): INFO Train: [9/300][460/1562]	eta 0:05:37 lr 0.000003	time 0.2924 (0.3063)	loss 2.0114 (1.9720)	grad_norm 22.5996 (inf)	mem 4879MB
[2022-05-30 23:36:34 MetaFG_0] (main.py 265): INFO Train: [9/300][470/1562]	eta 0:05:34 lr 0.000003	time 0.2928 (0.3062)	loss 1.6932 (1.9716)	grad_norm 23.8281 (inf)	mem 4879MB
[2022-05-30 23:36:37 MetaFG_0] (main.py 265): INFO Train: [9/300][480/1562]	eta 0:05:31 lr 0.000003	time 0.2923 (0.3062)	loss 2.0765 (1.9721)	grad_norm 14.9622 (inf)	mem 4879MB
[2022-05-30 23:36:40 MetaFG_0] (main.py 265): INFO Train: [9/300][490/1562]	eta 0:05:28 lr 0.000003	time 0.2928 (0.3061)	loss 1.9466 (1.9708)	grad_norm 14.7558 (inf)	mem 4879MB
[2022-05-30 23:36:43 MetaFG_0] (main.py 265): INFO Train: [9/300][500/1562]	eta 0:05:25 lr 0.000003	time 0.2934 (0.3061)	loss 1.8028 (1.9715)	grad_norm 27.4065 (inf)	mem 4879MB
[2022-05-30 23:36:46 MetaFG_0] (main.py 265): INFO Train: [9/300][510/1562]	eta 0:05:21 lr 0.000003	time 0.2919 (0.3060)	loss 1.7975 (1.9705)	grad_norm 25.4199 (inf)	mem 4879MB
[2022-05-30 23:36:49 MetaFG_0] (main.py 265): INFO Train: [9/300][520/1562]	eta 0:05:18 lr 0.000003	time 0.2931 (0.3060)	loss 1.9932 (1.9705)	grad_norm 13.3458 (inf)	mem 4879MB
[2022-05-30 23:36:52 MetaFG_0] (main.py 265): INFO Train: [9/300][530/1562]	eta 0:05:15 lr 0.000003	time 0.2930 (0.3059)	loss 2.1109 (1.9702)	grad_norm 15.4367 (inf)	mem 4879MB
[2022-05-30 23:36:55 MetaFG_0] (main.py 265): INFO Train: [9/300][540/1562]	eta 0:05:12 lr 0.000003	time 0.2942 (0.3059)	loss 2.1034 (1.9699)	grad_norm 8.9541 (inf)	mem 4879MB
[2022-05-30 23:36:58 MetaFG_0] (main.py 265): INFO Train: [9/300][550/1562]	eta 0:05:09 lr 0.000003	time 0.2924 (0.3058)	loss 2.0154 (1.9703)	grad_norm 9.7723 (inf)	mem 4879MB
[2022-05-30 23:37:02 MetaFG_0] (main.py 265): INFO Train: [9/300][560/1562]	eta 0:05:06 lr 0.000003	time 0.2937 (0.3058)	loss 1.9915 (1.9708)	grad_norm 12.9821 (inf)	mem 4879MB
[2022-05-30 23:37:05 MetaFG_0] (main.py 265): INFO Train: [9/300][570/1562]	eta 0:05:03 lr 0.000003	time 0.2975 (0.3058)	loss 2.0514 (1.9705)	grad_norm 15.1287 (inf)	mem 4879MB
[2022-05-30 23:37:08 MetaFG_0] (main.py 265): INFO Train: [9/300][580/1562]	eta 0:05:00 lr 0.000003	time 0.2940 (0.3057)	loss 1.8495 (1.9690)	grad_norm 19.0017 (inf)	mem 4879MB
[2022-05-30 23:37:11 MetaFG_0] (main.py 265): INFO Train: [9/300][590/1562]	eta 0:04:57 lr 0.000003	time 0.2930 (0.3057)	loss 2.0221 (1.9680)	grad_norm 17.4355 (inf)	mem 4879MB
[2022-05-30 23:37:14 MetaFG_0] (main.py 265): INFO Train: [9/300][600/1562]	eta 0:04:54 lr 0.000003	time 0.2922 (0.3056)	loss 2.0847 (1.9687)	grad_norm 22.0523 (inf)	mem 4879MB
[2022-05-30 23:37:17 MetaFG_0] (main.py 265): INFO Train: [9/300][610/1562]	eta 0:04:50 lr 0.000003	time 0.2924 (0.3056)	loss 2.0122 (1.9680)	grad_norm 18.4450 (inf)	mem 4879MB
[2022-05-30 23:37:20 MetaFG_0] (main.py 265): INFO Train: [9/300][620/1562]	eta 0:04:47 lr 0.000003	time 0.2918 (0.3055)	loss 1.9266 (1.9675)	grad_norm 11.7893 (inf)	mem 4879MB
[2022-05-30 23:37:23 MetaFG_0] (main.py 265): INFO Train: [9/300][630/1562]	eta 0:04:44 lr 0.000003	time 0.2985 (0.3055)	loss 2.1044 (1.9671)	grad_norm 24.2311 (inf)	mem 4879MB
[2022-05-30 23:37:26 MetaFG_0] (main.py 265): INFO Train: [9/300][640/1562]	eta 0:04:41 lr 0.000003	time 0.2969 (0.3054)	loss 2.0078 (1.9653)	grad_norm 15.4739 (inf)	mem 4879MB
[2022-05-30 23:37:29 MetaFG_0] (main.py 265): INFO Train: [9/300][650/1562]	eta 0:04:38 lr 0.000003	time 0.2988 (0.3054)	loss 2.0954 (1.9655)	grad_norm 14.2481 (inf)	mem 4879MB
[2022-05-30 23:37:32 MetaFG_0] (main.py 265): INFO Train: [9/300][660/1562]	eta 0:04:35 lr 0.000003	time 0.2989 (0.3054)	loss 1.7785 (1.9645)	grad_norm 25.4355 (inf)	mem 4879MB
[2022-05-30 23:37:35 MetaFG_0] (main.py 265): INFO Train: [9/300][670/1562]	eta 0:04:32 lr 0.000003	time 0.2940 (0.3053)	loss 1.9814 (1.9632)	grad_norm 10.9907 (inf)	mem 4879MB
[2022-05-30 23:37:38 MetaFG_0] (main.py 265): INFO Train: [9/300][680/1562]	eta 0:04:29 lr 0.000003	time 0.2946 (0.3053)	loss 2.1012 (1.9634)	grad_norm 12.0104 (inf)	mem 4879MB
[2022-05-30 23:37:41 MetaFG_0] (main.py 265): INFO Train: [9/300][690/1562]	eta 0:04:26 lr 0.000003	time 0.2976 (0.3053)	loss 2.0565 (1.9633)	grad_norm 17.2843 (inf)	mem 4879MB
[2022-05-30 23:37:44 MetaFG_0] (main.py 265): INFO Train: [9/300][700/1562]	eta 0:04:23 lr 0.000003	time 0.2927 (0.3053)	loss 2.0057 (1.9633)	grad_norm 11.2168 (inf)	mem 4879MB
[2022-05-30 23:37:47 MetaFG_0] (main.py 265): INFO Train: [9/300][710/1562]	eta 0:04:20 lr 0.000003	time 0.2930 (0.3053)	loss 1.9801 (1.9625)	grad_norm 19.4395 (inf)	mem 4879MB
[2022-05-30 23:37:50 MetaFG_0] (main.py 265): INFO Train: [9/300][720/1562]	eta 0:04:17 lr 0.000003	time 0.2918 (0.3053)	loss 1.9251 (1.9613)	grad_norm 17.9756 (inf)	mem 4879MB
[2022-05-30 23:37:53 MetaFG_0] (main.py 265): INFO Train: [9/300][730/1562]	eta 0:04:13 lr 0.000003	time 0.2988 (0.3052)	loss 1.9666 (1.9609)	grad_norm 12.6211 (inf)	mem 4879MB
[2022-05-30 23:37:56 MetaFG_0] (main.py 265): INFO Train: [9/300][740/1562]	eta 0:04:10 lr 0.000003	time 0.2920 (0.3052)	loss 2.0270 (1.9614)	grad_norm 22.6459 (inf)	mem 4879MB
[2022-05-30 23:37:59 MetaFG_0] (main.py 265): INFO Train: [9/300][750/1562]	eta 0:04:07 lr 0.000003	time 0.2945 (0.3052)	loss 2.0295 (1.9609)	grad_norm 25.9346 (inf)	mem 4879MB
[2022-05-30 23:38:02 MetaFG_0] (main.py 265): INFO Train: [9/300][760/1562]	eta 0:04:04 lr 0.000003	time 0.3031 (0.3052)	loss 1.9106 (1.9601)	grad_norm 28.7224 (inf)	mem 4879MB
[2022-05-30 23:38:05 MetaFG_0] (main.py 265): INFO Train: [9/300][770/1562]	eta 0:04:01 lr 0.000003	time 0.2975 (0.3051)	loss 1.9298 (1.9602)	grad_norm 14.5131 (inf)	mem 4879MB
[2022-05-30 23:38:08 MetaFG_0] (main.py 265): INFO Train: [9/300][780/1562]	eta 0:03:58 lr 0.000003	time 0.2975 (0.3051)	loss 1.9661 (1.9603)	grad_norm 25.0499 (inf)	mem 4879MB
[2022-05-30 23:38:11 MetaFG_0] (main.py 265): INFO Train: [9/300][790/1562]	eta 0:03:55 lr 0.000003	time 0.2920 (0.3051)	loss 2.0174 (1.9603)	grad_norm 17.6261 (inf)	mem 4879MB
[2022-05-30 23:38:14 MetaFG_0] (main.py 265): INFO Train: [9/300][800/1562]	eta 0:03:52 lr 0.000003	time 0.2921 (0.3050)	loss 2.0848 (1.9598)	grad_norm 17.6318 (inf)	mem 4879MB
[2022-05-30 23:38:17 MetaFG_0] (main.py 265): INFO Train: [9/300][810/1562]	eta 0:03:49 lr 0.000003	time 0.2924 (0.3050)	loss 2.0661 (1.9600)	grad_norm 14.4837 (inf)	mem 4879MB
[2022-05-30 23:38:20 MetaFG_0] (main.py 265): INFO Train: [9/300][820/1562]	eta 0:03:46 lr 0.000003	time 0.2958 (0.3050)	loss 2.0226 (1.9603)	grad_norm 13.2544 (inf)	mem 4879MB
[2022-05-30 23:38:23 MetaFG_0] (main.py 265): INFO Train: [9/300][830/1562]	eta 0:03:43 lr 0.000003	time 0.2937 (0.3050)	loss 2.1343 (1.9602)	grad_norm 15.6122 (inf)	mem 4879MB
[2022-05-30 23:38:26 MetaFG_0] (main.py 265): INFO Train: [9/300][840/1562]	eta 0:03:40 lr 0.000003	time 0.2920 (0.3050)	loss 2.0433 (1.9597)	grad_norm 10.2731 (inf)	mem 4879MB
[2022-05-30 23:38:29 MetaFG_0] (main.py 265): INFO Train: [9/300][850/1562]	eta 0:03:37 lr 0.000003	time 0.2920 (0.3049)	loss 1.9761 (1.9599)	grad_norm 19.4038 (inf)	mem 4879MB
[2022-05-30 23:38:33 MetaFG_0] (main.py 265): INFO Train: [9/300][860/1562]	eta 0:03:34 lr 0.000003	time 0.2931 (0.3050)	loss 2.0348 (1.9595)	grad_norm 12.8243 (inf)	mem 4879MB
[2022-05-30 23:38:36 MetaFG_0] (main.py 265): INFO Train: [9/300][870/1562]	eta 0:03:31 lr 0.000003	time 0.2990 (0.3050)	loss 1.7587 (1.9594)	grad_norm 16.7587 (inf)	mem 4879MB
[2022-05-30 23:38:39 MetaFG_0] (main.py 265): INFO Train: [9/300][880/1562]	eta 0:03:28 lr 0.000003	time 0.3052 (0.3051)	loss 1.9093 (1.9590)	grad_norm 22.4218 (inf)	mem 4879MB
[2022-05-30 23:38:42 MetaFG_0] (main.py 265): INFO Train: [9/300][890/1562]	eta 0:03:24 lr 0.000003	time 0.2920 (0.3051)	loss 1.8628 (1.9588)	grad_norm 14.1889 (inf)	mem 4879MB
[2022-05-30 23:38:45 MetaFG_0] (main.py 265): INFO Train: [9/300][900/1562]	eta 0:03:21 lr 0.000003	time 0.2928 (0.3050)	loss 2.0507 (1.9589)	grad_norm 18.0978 (inf)	mem 4879MB
[2022-05-30 23:38:48 MetaFG_0] (main.py 265): INFO Train: [9/300][910/1562]	eta 0:03:18 lr 0.000003	time 0.2923 (0.3050)	loss 1.9331 (1.9585)	grad_norm 17.0275 (inf)	mem 4879MB
[2022-05-30 23:38:51 MetaFG_0] (main.py 265): INFO Train: [9/300][920/1562]	eta 0:03:15 lr 0.000003	time 0.2927 (0.3050)	loss 2.1043 (1.9583)	grad_norm 10.2498 (inf)	mem 4879MB
[2022-05-30 23:38:54 MetaFG_0] (main.py 265): INFO Train: [9/300][930/1562]	eta 0:03:12 lr 0.000003	time 0.2914 (0.3050)	loss 2.0166 (1.9583)	grad_norm 10.5641 (inf)	mem 4879MB
[2022-05-30 23:38:57 MetaFG_0] (main.py 265): INFO Train: [9/300][940/1562]	eta 0:03:09 lr 0.000003	time 0.2980 (0.3050)	loss 1.9406 (1.9582)	grad_norm 18.5778 (inf)	mem 4879MB
[2022-05-30 23:39:00 MetaFG_0] (main.py 265): INFO Train: [9/300][950/1562]	eta 0:03:06 lr 0.000003	time 0.2943 (0.3050)	loss 2.0082 (1.9583)	grad_norm 16.5339 (inf)	mem 4879MB
[2022-05-30 23:39:03 MetaFG_0] (main.py 265): INFO Train: [9/300][960/1562]	eta 0:03:03 lr 0.000003	time 0.2948 (0.3050)	loss 1.8496 (1.9576)	grad_norm 40.5739 (inf)	mem 4879MB
[2022-05-30 23:39:06 MetaFG_0] (main.py 265): INFO Train: [9/300][970/1562]	eta 0:03:00 lr 0.000003	time 0.2930 (0.3050)	loss 1.6926 (1.9575)	grad_norm 21.0097 (inf)	mem 4879MB
[2022-05-30 23:39:09 MetaFG_0] (main.py 265): INFO Train: [9/300][980/1562]	eta 0:02:57 lr 0.000003	time 0.2988 (0.3050)	loss 1.8524 (1.9576)	grad_norm 18.1842 (inf)	mem 4879MB
[2022-05-30 23:39:12 MetaFG_0] (main.py 265): INFO Train: [9/300][990/1562]	eta 0:02:54 lr 0.000003	time 0.2980 (0.3050)	loss 1.8872 (1.9573)	grad_norm 12.5322 (inf)	mem 4879MB
[2022-05-30 23:39:15 MetaFG_0] (main.py 265): INFO Train: [9/300][1000/1562]	eta 0:02:51 lr 0.000003	time 0.2948 (0.3049)	loss 2.0897 (1.9567)	grad_norm 9.3642 (inf)	mem 4879MB
[2022-05-30 23:39:18 MetaFG_0] (main.py 265): INFO Train: [9/300][1010/1562]	eta 0:02:48 lr 0.000003	time 0.2978 (0.3049)	loss 1.8928 (1.9561)	grad_norm 15.7051 (inf)	mem 4879MB
[2022-05-30 23:39:21 MetaFG_0] (main.py 265): INFO Train: [9/300][1020/1562]	eta 0:02:45 lr 0.000003	time 0.2927 (0.3049)	loss 2.0756 (1.9560)	grad_norm 10.7243 (inf)	mem 4879MB
[2022-05-30 23:39:24 MetaFG_0] (main.py 265): INFO Train: [9/300][1030/1562]	eta 0:02:42 lr 0.000003	time 0.2951 (0.3049)	loss 1.9512 (1.9556)	grad_norm 12.7739 (inf)	mem 4879MB
[2022-05-30 23:39:27 MetaFG_0] (main.py 265): INFO Train: [9/300][1040/1562]	eta 0:02:39 lr 0.000003	time 0.2921 (0.3049)	loss 1.9424 (1.9555)	grad_norm 10.7864 (inf)	mem 4879MB
[2022-05-30 23:39:30 MetaFG_0] (main.py 265): INFO Train: [9/300][1050/1562]	eta 0:02:36 lr 0.000003	time 0.2928 (0.3049)	loss 2.0193 (1.9553)	grad_norm 16.8470 (inf)	mem 4879MB
[2022-05-30 23:39:33 MetaFG_0] (main.py 265): INFO Train: [9/300][1060/1562]	eta 0:02:33 lr 0.000003	time 0.2999 (0.3049)	loss 1.6131 (1.9552)	grad_norm 15.8697 (inf)	mem 4879MB
[2022-05-30 23:39:36 MetaFG_0] (main.py 265): INFO Train: [9/300][1070/1562]	eta 0:02:30 lr 0.000003	time 0.3042 (0.3049)	loss 1.7951 (1.9550)	grad_norm 31.8869 (inf)	mem 4879MB
[2022-05-30 23:39:40 MetaFG_0] (main.py 265): INFO Train: [9/300][1080/1562]	eta 0:02:26 lr 0.000003	time 0.2930 (0.3049)	loss 2.0240 (1.9547)	grad_norm 20.8884 (inf)	mem 4879MB
[2022-05-30 23:39:43 MetaFG_0] (main.py 265): INFO Train: [9/300][1090/1562]	eta 0:02:23 lr 0.000003	time 0.2939 (0.3049)	loss 1.8894 (1.9548)	grad_norm 15.3788 (inf)	mem 4879MB
[2022-05-30 23:39:46 MetaFG_0] (main.py 265): INFO Train: [9/300][1100/1562]	eta 0:02:20 lr 0.000003	time 0.2923 (0.3049)	loss 1.8315 (1.9551)	grad_norm 17.9855 (inf)	mem 4879MB
[2022-05-30 23:39:49 MetaFG_0] (main.py 265): INFO Train: [9/300][1110/1562]	eta 0:02:17 lr 0.000003	time 0.2929 (0.3049)	loss 1.9023 (1.9548)	grad_norm 15.1758 (inf)	mem 4879MB
[2022-05-30 23:39:52 MetaFG_0] (main.py 265): INFO Train: [9/300][1120/1562]	eta 0:02:14 lr 0.000003	time 0.2939 (0.3048)	loss 2.1410 (1.9546)	grad_norm 9.4124 (inf)	mem 4879MB
[2022-05-30 23:39:55 MetaFG_0] (main.py 265): INFO Train: [9/300][1130/1562]	eta 0:02:11 lr 0.000003	time 0.2984 (0.3048)	loss 2.0699 (1.9546)	grad_norm 20.3273 (inf)	mem 4879MB
[2022-05-30 23:39:58 MetaFG_0] (main.py 265): INFO Train: [9/300][1140/1562]	eta 0:02:08 lr 0.000003	time 0.2987 (0.3048)	loss 2.0370 (1.9543)	grad_norm 14.0571 (inf)	mem 4879MB
[2022-05-30 23:40:01 MetaFG_0] (main.py 265): INFO Train: [9/300][1150/1562]	eta 0:02:05 lr 0.000003	time 0.2930 (0.3048)	loss 1.7523 (1.9538)	grad_norm 19.8387 (inf)	mem 4879MB
[2022-05-30 23:40:04 MetaFG_0] (main.py 265): INFO Train: [9/300][1160/1562]	eta 0:02:02 lr 0.000003	time 0.2944 (0.3048)	loss 1.8499 (1.9533)	grad_norm 22.7765 (inf)	mem 4879MB
[2022-05-30 23:40:07 MetaFG_0] (main.py 265): INFO Train: [9/300][1170/1562]	eta 0:01:59 lr 0.000003	time 0.2940 (0.3048)	loss 1.8984 (1.9529)	grad_norm 21.2494 (inf)	mem 4879MB
[2022-05-30 23:40:10 MetaFG_0] (main.py 265): INFO Train: [9/300][1180/1562]	eta 0:01:56 lr 0.000003	time 0.2935 (0.3048)	loss 1.8410 (1.9525)	grad_norm 15.3484 (inf)	mem 4879MB
[2022-05-30 23:40:13 MetaFG_0] (main.py 265): INFO Train: [9/300][1190/1562]	eta 0:01:53 lr 0.000003	time 0.2994 (0.3048)	loss 1.9606 (1.9529)	grad_norm 10.8154 (inf)	mem 4879MB
[2022-05-30 23:40:16 MetaFG_0] (main.py 265): INFO Train: [9/300][1200/1562]	eta 0:01:50 lr 0.000003	time 0.3040 (0.3048)	loss 1.7999 (1.9527)	grad_norm 25.3575 (inf)	mem 4879MB
[2022-05-30 23:40:19 MetaFG_0] (main.py 265): INFO Train: [9/300][1210/1562]	eta 0:01:47 lr 0.000003	time 0.2922 (0.3048)	loss 1.9699 (1.9527)	grad_norm 11.6418 (inf)	mem 4879MB
[2022-05-30 23:40:22 MetaFG_0] (main.py 265): INFO Train: [9/300][1220/1562]	eta 0:01:44 lr 0.000003	time 0.2982 (0.3048)	loss 1.8729 (1.9531)	grad_norm 16.8208 (inf)	mem 4879MB
[2022-05-30 23:40:25 MetaFG_0] (main.py 265): INFO Train: [9/300][1230/1562]	eta 0:01:41 lr 0.000003	time 0.2926 (0.3048)	loss 2.0334 (1.9531)	grad_norm 10.5740 (inf)	mem 4879MB
[2022-05-30 23:40:28 MetaFG_0] (main.py 265): INFO Train: [9/300][1240/1562]	eta 0:01:38 lr 0.000003	time 0.2971 (0.3048)	loss 1.7354 (1.9532)	grad_norm 21.7052 (inf)	mem 4879MB
[2022-05-30 23:40:31 MetaFG_0] (main.py 265): INFO Train: [9/300][1250/1562]	eta 0:01:35 lr 0.000003	time 0.2976 (0.3047)	loss 1.7575 (1.9528)	grad_norm 20.1878 (inf)	mem 4879MB
[2022-05-30 23:40:34 MetaFG_0] (main.py 265): INFO Train: [9/300][1260/1562]	eta 0:01:32 lr 0.000003	time 0.2934 (0.3047)	loss 1.7514 (1.9529)	grad_norm 16.9813 (inf)	mem 4879MB
[2022-05-30 23:40:37 MetaFG_0] (main.py 265): INFO Train: [9/300][1270/1562]	eta 0:01:28 lr 0.000003	time 0.2983 (0.3047)	loss 2.0143 (1.9526)	grad_norm 18.2132 (inf)	mem 4879MB
[2022-05-30 23:40:40 MetaFG_0] (main.py 265): INFO Train: [9/300][1280/1562]	eta 0:01:25 lr 0.000003	time 0.3002 (0.3047)	loss 1.9606 (1.9523)	grad_norm 22.6990 (inf)	mem 4879MB
[2022-05-30 23:40:43 MetaFG_0] (main.py 265): INFO Train: [9/300][1290/1562]	eta 0:01:22 lr 0.000003	time 0.2915 (0.3047)	loss 1.6580 (1.9516)	grad_norm 13.9570 (inf)	mem 4879MB
[2022-05-30 23:40:46 MetaFG_0] (main.py 265): INFO Train: [9/300][1300/1562]	eta 0:01:19 lr 0.000003	time 0.2926 (0.3047)	loss 1.9536 (1.9521)	grad_norm 17.5904 (inf)	mem 4879MB
[2022-05-30 23:40:49 MetaFG_0] (main.py 265): INFO Train: [9/300][1310/1562]	eta 0:01:16 lr 0.000003	time 0.2920 (0.3047)	loss 1.9434 (1.9518)	grad_norm 13.9461 (inf)	mem 4879MB
[2022-05-30 23:40:52 MetaFG_0] (main.py 265): INFO Train: [9/300][1320/1562]	eta 0:01:13 lr 0.000003	time 0.2995 (0.3047)	loss 1.6676 (1.9519)	grad_norm 10.7755 (inf)	mem 4879MB
[2022-05-30 23:40:56 MetaFG_0] (main.py 265): INFO Train: [9/300][1330/1562]	eta 0:01:10 lr 0.000003	time 0.2927 (0.3048)	loss 1.8376 (1.9518)	grad_norm 21.2391 (inf)	mem 4879MB
[2022-05-30 23:40:59 MetaFG_0] (main.py 265): INFO Train: [9/300][1340/1562]	eta 0:01:07 lr 0.000003	time 0.2942 (0.3048)	loss 2.0414 (1.9512)	grad_norm 24.4847 (inf)	mem 4879MB
[2022-05-30 23:41:02 MetaFG_0] (main.py 265): INFO Train: [9/300][1350/1562]	eta 0:01:04 lr 0.000003	time 0.2934 (0.3048)	loss 1.9856 (1.9510)	grad_norm 12.2845 (inf)	mem 4879MB
[2022-05-30 23:41:05 MetaFG_0] (main.py 265): INFO Train: [9/300][1360/1562]	eta 0:01:01 lr 0.000003	time 0.2938 (0.3048)	loss 2.0988 (1.9510)	grad_norm 12.6614 (inf)	mem 4879MB
[2022-05-30 23:41:08 MetaFG_0] (main.py 265): INFO Train: [9/300][1370/1562]	eta 0:00:58 lr 0.000003	time 0.2934 (0.3047)	loss 1.9915 (1.9507)	grad_norm 19.5928 (inf)	mem 4879MB
[2022-05-30 23:41:11 MetaFG_0] (main.py 265): INFO Train: [9/300][1380/1562]	eta 0:00:55 lr 0.000003	time 0.2985 (0.3048)	loss 1.8192 (1.9503)	grad_norm 13.7029 (inf)	mem 4879MB
[2022-05-30 23:41:14 MetaFG_0] (main.py 265): INFO Train: [9/300][1390/1562]	eta 0:00:52 lr 0.000003	time 0.2918 (0.3048)	loss 2.0086 (1.9504)	grad_norm 12.2134 (inf)	mem 4879MB
[2022-05-30 23:41:17 MetaFG_0] (main.py 265): INFO Train: [9/300][1400/1562]	eta 0:00:49 lr 0.000003	time 0.3010 (0.3047)	loss 1.6321 (1.9500)	grad_norm 15.2567 (inf)	mem 4879MB
[2022-05-30 23:41:20 MetaFG_0] (main.py 265): INFO Train: [9/300][1410/1562]	eta 0:00:46 lr 0.000003	time 0.2973 (0.3048)	loss 1.8059 (1.9497)	grad_norm 16.5727 (inf)	mem 4879MB
[2022-05-30 23:41:23 MetaFG_0] (main.py 265): INFO Train: [9/300][1420/1562]	eta 0:00:43 lr 0.000003	time 0.2935 (0.3048)	loss 1.7493 (1.9496)	grad_norm 21.9874 (inf)	mem 4879MB
[2022-05-30 23:41:26 MetaFG_0] (main.py 265): INFO Train: [9/300][1430/1562]	eta 0:00:40 lr 0.000003	time 0.2934 (0.3047)	loss 2.0402 (1.9497)	grad_norm 12.1606 (inf)	mem 4879MB
[2022-05-30 23:41:29 MetaFG_0] (main.py 265): INFO Train: [9/300][1440/1562]	eta 0:00:37 lr 0.000003	time 0.3007 (0.3047)	loss 1.9737 (1.9499)	grad_norm 10.2064 (inf)	mem 4879MB
[2022-05-30 23:41:32 MetaFG_0] (main.py 265): INFO Train: [9/300][1450/1562]	eta 0:00:34 lr 0.000003	time 0.2920 (0.3047)	loss 1.7836 (1.9495)	grad_norm 19.5101 (inf)	mem 4879MB
[2022-05-30 23:41:35 MetaFG_0] (main.py 265): INFO Train: [9/300][1460/1562]	eta 0:00:31 lr 0.000003	time 0.2921 (0.3047)	loss 2.0307 (1.9496)	grad_norm 19.2020 (inf)	mem 4879MB
[2022-05-30 23:41:38 MetaFG_0] (main.py 265): INFO Train: [9/300][1470/1562]	eta 0:00:28 lr 0.000003	time 0.2916 (0.3047)	loss 1.7191 (1.9494)	grad_norm 11.6770 (inf)	mem 4879MB
[2022-05-30 23:41:41 MetaFG_0] (main.py 265): INFO Train: [9/300][1480/1562]	eta 0:00:24 lr 0.000003	time 0.2989 (0.3047)	loss 1.7635 (1.9491)	grad_norm 14.1713 (inf)	mem 4879MB
[2022-05-30 23:41:44 MetaFG_0] (main.py 265): INFO Train: [9/300][1490/1562]	eta 0:00:21 lr 0.000003	time 0.2932 (0.3047)	loss 1.9061 (1.9490)	grad_norm 27.7934 (inf)	mem 4879MB
[2022-05-30 23:41:47 MetaFG_0] (main.py 265): INFO Train: [9/300][1500/1562]	eta 0:00:18 lr 0.000003	time 0.2914 (0.3047)	loss 1.7345 (1.9485)	grad_norm 25.2829 (inf)	mem 4879MB
[2022-05-30 23:41:50 MetaFG_0] (main.py 265): INFO Train: [9/300][1510/1562]	eta 0:00:15 lr 0.000003	time 0.2997 (0.3047)	loss 1.9606 (1.9486)	grad_norm 17.5082 (inf)	mem 4879MB
[2022-05-30 23:41:53 MetaFG_0] (main.py 265): INFO Train: [9/300][1520/1562]	eta 0:00:12 lr 0.000003	time 0.2914 (0.3047)	loss 2.0523 (1.9486)	grad_norm 14.9856 (inf)	mem 4879MB
[2022-05-30 23:41:56 MetaFG_0] (main.py 265): INFO Train: [9/300][1530/1562]	eta 0:00:09 lr 0.000003	time 0.2951 (0.3047)	loss 1.8003 (1.9481)	grad_norm 14.8167 (inf)	mem 4879MB
[2022-05-30 23:41:59 MetaFG_0] (main.py 265): INFO Train: [9/300][1540/1562]	eta 0:00:06 lr 0.000003	time 0.2932 (0.3047)	loss 1.8628 (1.9475)	grad_norm 13.1791 (inf)	mem 4879MB
[2022-05-30 23:42:02 MetaFG_0] (main.py 265): INFO Train: [9/300][1550/1562]	eta 0:00:03 lr 0.000003	time 0.2926 (0.3046)	loss 1.8810 (1.9475)	grad_norm 10.3651 (inf)	mem 4879MB
[2022-05-30 23:42:05 MetaFG_0] (main.py 265): INFO Train: [9/300][1560/1562]	eta 0:00:00 lr 0.000003	time 0.2928 (0.3046)	loss 1.9317 (1.9475)	grad_norm 19.0210 (inf)	mem 4879MB
[2022-05-30 23:42:06 MetaFG_0] (main.py 272): INFO EPOCH 9 training takes 0:07:55
[2022-05-30 23:42:06 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_9.pth saving......
[2022-05-30 23:42:07 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_9.pth saved !!!
[2022-05-30 23:42:07 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:42:08 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:42:08 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:42:09 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.673 (0.673)	Loss 2.5433 (2.5433)	Acc@1 56.250 (56.250)	Acc@5 84.375 (84.375)	Mem 4879MB
[2022-05-30 23:42:10 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.087 (0.151)	Loss 2.6103 (2.5549)	Acc@1 56.250 (52.841)	Acc@5 78.125 (84.375)	Mem 4879MB
[2022-05-30 23:42:11 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.124)	Loss 2.3965 (2.5822)	Acc@1 59.375 (51.339)	Acc@5 90.625 (84.970)	Mem 4879MB
[2022-05-30 23:42:12 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.118 (0.116)	Loss 2.6152 (2.6112)	Acc@1 50.000 (50.101)	Acc@5 78.125 (83.972)	Mem 4879MB
[2022-05-30 23:42:13 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.110)	Loss 2.7889 (2.6350)	Acc@1 40.625 (49.238)	Acc@5 75.000 (83.689)	Mem 4879MB
[2022-05-30 23:42:14 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.107)	Loss 2.5688 (2.6259)	Acc@1 59.375 (49.387)	Acc@5 90.625 (83.762)	Mem 4879MB
[2022-05-30 23:42:15 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 2.6294 (2.6385)	Acc@1 56.250 (49.129)	Acc@5 87.500 (84.324)	Mem 4879MB
[2022-05-30 23:42:16 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.092 (0.103)	Loss 2.9960 (2.6567)	Acc@1 28.125 (48.680)	Acc@5 78.125 (83.451)	Mem 4879MB
[2022-05-30 23:42:17 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 2.6276 (2.6628)	Acc@1 53.125 (48.380)	Acc@5 87.500 (83.410)	Mem 4879MB
[2022-05-30 23:42:18 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 2.9888 (2.6671)	Acc@1 53.125 (48.111)	Acc@5 71.875 (83.139)	Mem 4879MB
[2022-05-30 23:42:19 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.101)	Loss 2.6179 (2.6693)	Acc@1 43.750 (47.587)	Acc@5 87.500 (82.921)	Mem 4879MB
[2022-05-30 23:42:20 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.091 (0.100)	Loss 2.5530 (2.6733)	Acc@1 40.625 (47.410)	Acc@5 93.750 (82.461)	Mem 4879MB
[2022-05-30 23:42:20 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.099)	Loss 2.7700 (2.6762)	Acc@1 40.625 (47.262)	Acc@5 81.250 (82.180)	Mem 4879MB
[2022-05-30 23:42:21 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 2.5850 (2.6707)	Acc@1 50.000 (47.615)	Acc@5 75.000 (82.061)	Mem 4879MB
[2022-05-30 23:42:22 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.097 (0.099)	Loss 2.6073 (2.6651)	Acc@1 53.125 (48.050)	Acc@5 78.125 (82.181)	Mem 4879MB
[2022-05-30 23:42:23 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 2.8282 (2.6674)	Acc@1 37.500 (47.661)	Acc@5 71.875 (82.057)	Mem 4879MB
[2022-05-30 23:42:24 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 2.6684 (2.6603)	Acc@1 40.625 (47.981)	Acc@5 84.375 (82.356)	Mem 4879MB
[2022-05-30 23:42:25 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 2.5918 (2.6626)	Acc@1 56.250 (47.971)	Acc@5 81.250 (82.273)	Mem 4879MB
[2022-05-30 23:42:26 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 2.6290 (2.6592)	Acc@1 53.125 (48.170)	Acc@5 81.250 (82.338)	Mem 4879MB
[2022-05-30 23:42:27 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.098)	Loss 2.5021 (2.6598)	Acc@1 53.125 (47.906)	Acc@5 84.375 (82.346)	Mem 4879MB
[2022-05-30 23:42:28 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 2.7309 (2.6618)	Acc@1 46.875 (47.683)	Acc@5 84.375 (82.307)	Mem 4879MB
[2022-05-30 23:42:29 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 2.1240 (2.6606)	Acc@1 56.250 (47.823)	Acc@5 93.750 (82.390)	Mem 4879MB
[2022-05-30 23:42:30 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 2.8112 (2.6597)	Acc@1 43.750 (47.879)	Acc@5 75.000 (82.339)	Mem 4879MB
[2022-05-30 23:42:31 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 2.6278 (2.6590)	Acc@1 46.875 (47.944)	Acc@5 71.875 (82.359)	Mem 4879MB
[2022-05-30 23:42:32 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.093 (0.097)	Loss 2.6519 (2.6589)	Acc@1 46.875 (48.068)	Acc@5 90.625 (82.300)	Mem 4879MB
[2022-05-30 23:42:33 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.097)	Loss 2.7878 (2.6586)	Acc@1 43.750 (48.108)	Acc@5 81.250 (82.371)	Mem 4879MB
[2022-05-30 23:42:34 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.097 (0.097)	Loss 2.7087 (2.6592)	Acc@1 37.500 (48.048)	Acc@5 71.875 (82.399)	Mem 4879MB
[2022-05-30 23:42:35 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 2.7999 (2.6567)	Acc@1 43.750 (47.959)	Acc@5 65.625 (82.415)	Mem 4879MB
[2022-05-30 23:42:36 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.095 (0.097)	Loss 2.6494 (2.6566)	Acc@1 53.125 (48.009)	Acc@5 75.000 (82.384)	Mem 4879MB
[2022-05-30 23:42:36 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 2.5921 (2.6554)	Acc@1 50.000 (47.992)	Acc@5 81.250 (82.463)	Mem 4879MB
[2022-05-30 23:42:37 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.098 (0.096)	Loss 2.7581 (2.6572)	Acc@1 46.875 (47.944)	Acc@5 75.000 (82.350)	Mem 4879MB
[2022-05-30 23:42:38 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 2.8149 (2.6582)	Acc@1 43.750 (47.960)	Acc@5 84.375 (82.295)	Mem 4879MB
[2022-05-30 23:42:39 MetaFG_0] (main.py 330): INFO  * Acc@1 47.980 Acc@5 82.300
[2022-05-30 23:42:39 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 48.0%
[2022-05-30 23:42:39 MetaFG_0] (main.py 171): INFO Max accuracy: 47.98%
[2022-05-30 23:42:40 MetaFG_0] (main.py 265): INFO Train: [10/300][0/1562]	eta 0:27:56 lr 0.000003	time 1.0733 (1.0733)	loss 2.0263 (2.0263)	grad_norm 23.4795 (23.4795)	mem 4879MB
[2022-05-30 23:42:43 MetaFG_0] (main.py 265): INFO Train: [10/300][10/1562]	eta 0:09:46 lr 0.000003	time 0.2935 (0.3778)	loss 1.7379 (1.9325)	grad_norm 18.8702 (20.4933)	mem 4879MB
[2022-05-30 23:42:46 MetaFG_0] (main.py 265): INFO Train: [10/300][20/1562]	eta 0:08:48 lr 0.000003	time 0.2930 (0.3425)	loss 2.0826 (1.9287)	grad_norm 12.9257 (19.0966)	mem 4879MB
[2022-05-30 23:42:49 MetaFG_0] (main.py 265): INFO Train: [10/300][30/1562]	eta 0:08:25 lr 0.000003	time 0.2983 (0.3297)	loss 1.8247 (1.9082)	grad_norm 14.7875 (19.2045)	mem 4879MB
[2022-05-30 23:42:52 MetaFG_0] (main.py 265): INFO Train: [10/300][40/1562]	eta 0:08:11 lr 0.000003	time 0.2940 (0.3231)	loss 2.0296 (1.9176)	grad_norm 12.5239 (18.7108)	mem 4879MB
[2022-05-30 23:42:55 MetaFG_0] (main.py 265): INFO Train: [10/300][50/1562]	eta 0:08:02 lr 0.000003	time 0.2932 (0.3192)	loss 2.0114 (1.9209)	grad_norm 22.6135 (18.5271)	mem 4879MB
[2022-05-30 23:42:58 MetaFG_0] (main.py 265): INFO Train: [10/300][60/1562]	eta 0:07:55 lr 0.000003	time 0.2951 (0.3168)	loss 1.6953 (1.9199)	grad_norm 13.1282 (18.1866)	mem 4879MB
[2022-05-30 23:43:01 MetaFG_0] (main.py 265): INFO Train: [10/300][70/1562]	eta 0:07:49 lr 0.000003	time 0.2978 (0.3147)	loss 1.8388 (1.9139)	grad_norm 18.4112 (18.5803)	mem 4879MB
[2022-05-30 23:43:04 MetaFG_0] (main.py 265): INFO Train: [10/300][80/1562]	eta 0:07:45 lr 0.000003	time 0.2998 (0.3141)	loss 2.0375 (1.9125)	grad_norm 14.9832 (18.4448)	mem 4879MB
[2022-05-30 23:43:07 MetaFG_0] (main.py 265): INFO Train: [10/300][90/1562]	eta 0:07:40 lr 0.000003	time 0.2932 (0.3129)	loss 1.9660 (1.9164)	grad_norm 13.0844 (18.4178)	mem 4879MB
[2022-05-30 23:43:10 MetaFG_0] (main.py 265): INFO Train: [10/300][100/1562]	eta 0:07:36 lr 0.000003	time 0.3185 (0.3124)	loss 1.8375 (1.9198)	grad_norm 13.5192 (18.4172)	mem 4879MB
[2022-05-30 23:43:13 MetaFG_0] (main.py 265): INFO Train: [10/300][110/1562]	eta 0:07:33 lr 0.000003	time 0.2923 (0.3122)	loss 1.7833 (1.9168)	grad_norm 24.1978 (18.6844)	mem 4879MB
[2022-05-30 23:43:16 MetaFG_0] (main.py 265): INFO Train: [10/300][120/1562]	eta 0:07:29 lr 0.000003	time 0.2922 (0.3114)	loss 1.6767 (1.9140)	grad_norm 14.2741 (18.7020)	mem 4879MB
[2022-05-30 23:43:19 MetaFG_0] (main.py 265): INFO Train: [10/300][130/1562]	eta 0:07:25 lr 0.000003	time 0.2924 (0.3109)	loss 1.8913 (1.9158)	grad_norm 11.3808 (18.4513)	mem 4879MB
[2022-05-30 23:43:22 MetaFG_0] (main.py 265): INFO Train: [10/300][140/1562]	eta 0:07:21 lr 0.000003	time 0.2921 (0.3105)	loss 1.8781 (1.9150)	grad_norm 18.4755 (18.3857)	mem 4879MB
[2022-05-30 23:43:25 MetaFG_0] (main.py 265): INFO Train: [10/300][150/1562]	eta 0:07:18 lr 0.000003	time 0.2988 (0.3103)	loss 2.0021 (1.9150)	grad_norm 9.8933 (18.2389)	mem 4879MB
[2022-05-30 23:43:28 MetaFG_0] (main.py 265): INFO Train: [10/300][160/1562]	eta 0:07:14 lr 0.000003	time 0.2919 (0.3099)	loss 1.9377 (1.9109)	grad_norm 20.9704 (18.1642)	mem 4879MB
[2022-05-30 23:43:32 MetaFG_0] (main.py 265): INFO Train: [10/300][170/1562]	eta 0:07:10 lr 0.000003	time 0.2925 (0.3094)	loss 1.6685 (1.9070)	grad_norm 17.3849 (18.1449)	mem 4879MB
[2022-05-30 23:43:35 MetaFG_0] (main.py 265): INFO Train: [10/300][180/1562]	eta 0:07:07 lr 0.000003	time 0.2939 (0.3092)	loss 1.9315 (1.9080)	grad_norm 10.7276 (17.9883)	mem 4879MB
[2022-05-30 23:43:38 MetaFG_0] (main.py 265): INFO Train: [10/300][190/1562]	eta 0:07:04 lr 0.000003	time 0.2919 (0.3091)	loss 1.7500 (1.9060)	grad_norm 25.5233 (17.9479)	mem 4879MB
[2022-05-30 23:43:41 MetaFG_0] (main.py 265): INFO Train: [10/300][200/1562]	eta 0:07:00 lr 0.000003	time 0.2924 (0.3088)	loss 1.7448 (1.9089)	grad_norm 12.3490 (17.9272)	mem 4879MB
[2022-05-30 23:43:44 MetaFG_0] (main.py 265): INFO Train: [10/300][210/1562]	eta 0:06:57 lr 0.000003	time 0.2979 (0.3085)	loss 1.8176 (1.9095)	grad_norm 14.9949 (17.8652)	mem 4879MB
[2022-05-30 23:43:47 MetaFG_0] (main.py 265): INFO Train: [10/300][220/1562]	eta 0:06:53 lr 0.000003	time 0.2932 (0.3082)	loss 1.9211 (1.9110)	grad_norm 11.7904 (17.7953)	mem 4879MB
[2022-05-30 23:43:50 MetaFG_0] (main.py 265): INFO Train: [10/300][230/1562]	eta 0:06:50 lr 0.000003	time 0.2923 (0.3080)	loss 1.7362 (1.9103)	grad_norm 18.9127 (17.8379)	mem 4879MB
[2022-05-30 23:43:53 MetaFG_0] (main.py 265): INFO Train: [10/300][240/1562]	eta 0:06:46 lr 0.000003	time 0.2925 (0.3078)	loss 1.8378 (1.9126)	grad_norm 18.7842 (17.7860)	mem 4879MB
[2022-05-30 23:43:56 MetaFG_0] (main.py 265): INFO Train: [10/300][250/1562]	eta 0:06:43 lr 0.000003	time 0.2931 (0.3076)	loss 2.0465 (1.9142)	grad_norm 14.5091 (17.7515)	mem 4879MB
[2022-05-30 23:43:59 MetaFG_0] (main.py 265): INFO Train: [10/300][260/1562]	eta 0:06:40 lr 0.000003	time 0.2920 (0.3074)	loss 2.0104 (1.9151)	grad_norm 22.2642 (17.7265)	mem 4879MB
[2022-05-30 23:44:02 MetaFG_0] (main.py 265): INFO Train: [10/300][270/1562]	eta 0:06:36 lr 0.000003	time 0.2931 (0.3072)	loss 2.0936 (1.9145)	grad_norm 14.6819 (17.7969)	mem 4879MB
[2022-05-30 23:44:05 MetaFG_0] (main.py 265): INFO Train: [10/300][280/1562]	eta 0:06:33 lr 0.000003	time 0.2937 (0.3070)	loss 1.7548 (1.9147)	grad_norm 18.3370 (17.7660)	mem 4879MB
[2022-05-30 23:44:08 MetaFG_0] (main.py 265): INFO Train: [10/300][290/1562]	eta 0:06:30 lr 0.000003	time 0.2926 (0.3069)	loss 1.8082 (1.9137)	grad_norm 18.9423 (17.8356)	mem 4879MB
[2022-05-30 23:44:11 MetaFG_0] (main.py 265): INFO Train: [10/300][300/1562]	eta 0:06:27 lr 0.000003	time 0.2918 (0.3068)	loss 1.6888 (1.9108)	grad_norm 18.6083 (17.9162)	mem 4879MB
[2022-05-30 23:44:14 MetaFG_0] (main.py 265): INFO Train: [10/300][310/1562]	eta 0:06:24 lr 0.000003	time 0.2925 (0.3067)	loss 1.7461 (1.9078)	grad_norm 14.8076 (17.9578)	mem 4879MB
[2022-05-30 23:44:17 MetaFG_0] (main.py 265): INFO Train: [10/300][320/1562]	eta 0:06:20 lr 0.000003	time 0.2924 (0.3066)	loss 2.1760 (1.9080)	grad_norm 16.3456 (18.0156)	mem 4879MB
[2022-05-30 23:44:20 MetaFG_0] (main.py 265): INFO Train: [10/300][330/1562]	eta 0:06:17 lr 0.000003	time 0.3043 (0.3066)	loss 1.7174 (1.9057)	grad_norm 24.1633 (18.0403)	mem 4879MB
[2022-05-30 23:44:23 MetaFG_0] (main.py 265): INFO Train: [10/300][340/1562]	eta 0:06:14 lr 0.000003	time 0.2988 (0.3066)	loss 1.7322 (1.9054)	grad_norm 19.4595 (18.0095)	mem 4879MB
[2022-05-30 23:44:26 MetaFG_0] (main.py 265): INFO Train: [10/300][350/1562]	eta 0:06:11 lr 0.000003	time 0.2986 (0.3065)	loss 1.9530 (1.9054)	grad_norm 14.1184 (18.0053)	mem 4879MB
[2022-05-30 23:44:29 MetaFG_0] (main.py 265): INFO Train: [10/300][360/1562]	eta 0:06:08 lr 0.000003	time 0.2989 (0.3064)	loss 1.9583 (1.9052)	grad_norm 19.3484 (17.9960)	mem 4879MB
[2022-05-30 23:44:32 MetaFG_0] (main.py 265): INFO Train: [10/300][370/1562]	eta 0:06:05 lr 0.000003	time 0.2995 (0.3064)	loss 1.7725 (1.9037)	grad_norm 18.2794 (18.0223)	mem 4879MB
[2022-05-30 23:44:35 MetaFG_0] (main.py 265): INFO Train: [10/300][380/1562]	eta 0:06:02 lr 0.000003	time 0.2925 (0.3063)	loss 1.9777 (1.9040)	grad_norm 13.6426 (18.0488)	mem 4879MB
[2022-05-30 23:44:38 MetaFG_0] (main.py 265): INFO Train: [10/300][390/1562]	eta 0:05:58 lr 0.000003	time 0.2944 (0.3063)	loss 1.7230 (1.9031)	grad_norm 21.1400 (18.0305)	mem 4879MB
[2022-05-30 23:44:41 MetaFG_0] (main.py 265): INFO Train: [10/300][400/1562]	eta 0:05:55 lr 0.000003	time 0.2961 (0.3062)	loss 2.0005 (1.9034)	grad_norm 24.5002 (18.0646)	mem 4879MB
[2022-05-30 23:44:44 MetaFG_0] (main.py 265): INFO Train: [10/300][410/1562]	eta 0:05:52 lr 0.000003	time 0.2974 (0.3062)	loss 1.9206 (1.9032)	grad_norm 21.5582 (18.0498)	mem 4879MB
[2022-05-30 23:44:47 MetaFG_0] (main.py 265): INFO Train: [10/300][420/1562]	eta 0:05:49 lr 0.000003	time 0.2951 (0.3061)	loss 1.6074 (1.9030)	grad_norm 23.7968 (18.0482)	mem 4879MB
[2022-05-30 23:44:51 MetaFG_0] (main.py 265): INFO Train: [10/300][430/1562]	eta 0:05:46 lr 0.000003	time 0.2939 (0.3061)	loss 2.0679 (1.9025)	grad_norm 19.6179 (18.0516)	mem 4879MB
[2022-05-30 23:44:54 MetaFG_0] (main.py 265): INFO Train: [10/300][440/1562]	eta 0:05:43 lr 0.000003	time 0.2920 (0.3060)	loss 2.0109 (1.9017)	grad_norm 10.3085 (18.0993)	mem 4879MB
[2022-05-30 23:44:57 MetaFG_0] (main.py 265): INFO Train: [10/300][450/1562]	eta 0:05:40 lr 0.000003	time 0.2924 (0.3059)	loss 1.9877 (1.9011)	grad_norm 11.6253 (18.1578)	mem 4879MB
[2022-05-30 23:45:00 MetaFG_0] (main.py 265): INFO Train: [10/300][460/1562]	eta 0:05:37 lr 0.000003	time 0.2999 (0.3058)	loss 1.9844 (1.9019)	grad_norm 13.3565 (18.1351)	mem 4879MB
[2022-05-30 23:45:03 MetaFG_0] (main.py 265): INFO Train: [10/300][470/1562]	eta 0:05:33 lr 0.000003	time 0.2981 (0.3058)	loss 2.0415 (1.9014)	grad_norm 17.9383 (18.1785)	mem 4879MB
[2022-05-30 23:45:06 MetaFG_0] (main.py 265): INFO Train: [10/300][480/1562]	eta 0:05:30 lr 0.000003	time 0.2941 (0.3058)	loss 1.7130 (1.9003)	grad_norm 12.9647 (18.2154)	mem 4879MB
[2022-05-30 23:45:09 MetaFG_0] (main.py 265): INFO Train: [10/300][490/1562]	eta 0:05:27 lr 0.000003	time 0.2939 (0.3057)	loss 2.0565 (1.9008)	grad_norm 15.6641 (18.1354)	mem 4879MB
[2022-05-30 23:45:12 MetaFG_0] (main.py 265): INFO Train: [10/300][500/1562]	eta 0:05:24 lr 0.000003	time 0.2930 (0.3057)	loss 1.8644 (1.9015)	grad_norm 16.9344 (18.1305)	mem 4879MB
[2022-05-30 23:45:15 MetaFG_0] (main.py 265): INFO Train: [10/300][510/1562]	eta 0:05:21 lr 0.000003	time 0.2924 (0.3057)	loss 1.8722 (1.9017)	grad_norm 14.8313 (18.1396)	mem 4879MB
[2022-05-30 23:45:18 MetaFG_0] (main.py 265): INFO Train: [10/300][520/1562]	eta 0:05:18 lr 0.000003	time 0.2931 (0.3056)	loss 1.9004 (1.9024)	grad_norm 13.7960 (18.1151)	mem 4879MB
[2022-05-30 23:45:21 MetaFG_0] (main.py 265): INFO Train: [10/300][530/1562]	eta 0:05:15 lr 0.000003	time 0.2946 (0.3058)	loss 1.8691 (1.9030)	grad_norm 16.0139 (18.1868)	mem 4879MB
[2022-05-30 23:45:24 MetaFG_0] (main.py 265): INFO Train: [10/300][540/1562]	eta 0:05:12 lr 0.000003	time 0.2992 (0.3058)	loss 1.9178 (1.9031)	grad_norm 16.8241 (18.1734)	mem 4879MB
[2022-05-30 23:45:27 MetaFG_0] (main.py 265): INFO Train: [10/300][550/1562]	eta 0:05:09 lr 0.000003	time 0.2944 (0.3058)	loss 1.7469 (1.9022)	grad_norm 22.4897 (18.1581)	mem 4879MB
[2022-05-30 23:45:30 MetaFG_0] (main.py 265): INFO Train: [10/300][560/1562]	eta 0:05:06 lr 0.000003	time 0.2930 (0.3057)	loss 1.6979 (1.9007)	grad_norm 15.1638 (18.1374)	mem 4879MB
[2022-05-30 23:45:33 MetaFG_0] (main.py 265): INFO Train: [10/300][570/1562]	eta 0:05:03 lr 0.000003	time 0.2935 (0.3057)	loss 2.0581 (1.9002)	grad_norm 12.1713 (18.1171)	mem 4879MB
[2022-05-30 23:45:36 MetaFG_0] (main.py 265): INFO Train: [10/300][580/1562]	eta 0:05:00 lr 0.000003	time 0.2933 (0.3057)	loss 2.0847 (1.9007)	grad_norm 27.5314 (18.1219)	mem 4879MB
[2022-05-30 23:45:39 MetaFG_0] (main.py 265): INFO Train: [10/300][590/1562]	eta 0:04:57 lr 0.000003	time 0.2949 (0.3057)	loss 2.0661 (1.9020)	grad_norm 12.4145 (18.0993)	mem 4879MB
[2022-05-30 23:45:42 MetaFG_0] (main.py 265): INFO Train: [10/300][600/1562]	eta 0:04:54 lr 0.000003	time 0.2982 (0.3057)	loss 1.8278 (1.9007)	grad_norm 23.4873 (18.1084)	mem 4879MB
[2022-05-30 23:45:45 MetaFG_0] (main.py 265): INFO Train: [10/300][610/1562]	eta 0:04:50 lr 0.000003	time 0.2947 (0.3056)	loss 1.9419 (1.9014)	grad_norm 17.8870 (18.1322)	mem 4879MB
[2022-05-30 23:45:48 MetaFG_0] (main.py 265): INFO Train: [10/300][620/1562]	eta 0:04:47 lr 0.000003	time 0.2931 (0.3056)	loss 1.8900 (1.9000)	grad_norm 15.6360 (18.2457)	mem 4879MB
[2022-05-30 23:45:51 MetaFG_0] (main.py 265): INFO Train: [10/300][630/1562]	eta 0:04:44 lr 0.000003	time 0.2939 (0.3056)	loss 1.9521 (1.9000)	grad_norm 13.6139 (18.2716)	mem 4879MB
[2022-05-30 23:45:54 MetaFG_0] (main.py 265): INFO Train: [10/300][640/1562]	eta 0:04:41 lr 0.000003	time 0.2963 (0.3056)	loss 2.0589 (1.9000)	grad_norm 15.2566 (18.2459)	mem 4879MB
[2022-05-30 23:45:58 MetaFG_0] (main.py 265): INFO Train: [10/300][650/1562]	eta 0:04:38 lr 0.000003	time 0.2985 (0.3056)	loss 1.8653 (1.8995)	grad_norm 25.7178 (18.2485)	mem 4879MB
[2022-05-30 23:46:01 MetaFG_0] (main.py 265): INFO Train: [10/300][660/1562]	eta 0:04:35 lr 0.000003	time 0.2935 (0.3056)	loss 1.9891 (1.8986)	grad_norm 17.0758 (18.2627)	mem 4879MB
[2022-05-30 23:46:04 MetaFG_0] (main.py 265): INFO Train: [10/300][670/1562]	eta 0:04:32 lr 0.000003	time 0.2986 (0.3055)	loss 1.6606 (1.8983)	grad_norm 14.3604 (18.2560)	mem 4879MB
[2022-05-30 23:46:07 MetaFG_0] (main.py 265): INFO Train: [10/300][680/1562]	eta 0:04:29 lr 0.000003	time 0.3000 (0.3055)	loss 1.9531 (1.8974)	grad_norm 18.6079 (18.2419)	mem 4879MB
[2022-05-30 23:46:10 MetaFG_0] (main.py 265): INFO Train: [10/300][690/1562]	eta 0:04:26 lr 0.000003	time 0.2928 (0.3055)	loss 1.6018 (1.8968)	grad_norm 18.8826 (18.2599)	mem 4879MB
[2022-05-30 23:46:13 MetaFG_0] (main.py 265): INFO Train: [10/300][700/1562]	eta 0:04:23 lr 0.000003	time 0.2925 (0.3055)	loss 1.9583 (1.8970)	grad_norm 11.4251 (18.2263)	mem 4879MB
[2022-05-30 23:46:16 MetaFG_0] (main.py 265): INFO Train: [10/300][710/1562]	eta 0:04:20 lr 0.000003	time 0.2914 (0.3055)	loss 2.0364 (1.8970)	grad_norm 13.2236 (18.1876)	mem 4879MB
[2022-05-30 23:46:19 MetaFG_0] (main.py 265): INFO Train: [10/300][720/1562]	eta 0:04:17 lr 0.000003	time 0.2976 (0.3055)	loss 2.0058 (1.8973)	grad_norm 15.4330 (18.1930)	mem 4879MB
[2022-05-30 23:46:22 MetaFG_0] (main.py 265): INFO Train: [10/300][730/1562]	eta 0:04:14 lr 0.000003	time 0.2997 (0.3055)	loss 2.1465 (1.8971)	grad_norm 16.7465 (18.1991)	mem 4879MB
[2022-05-30 23:46:25 MetaFG_0] (main.py 265): INFO Train: [10/300][740/1562]	eta 0:04:11 lr 0.000003	time 0.2928 (0.3054)	loss 1.8278 (1.8965)	grad_norm 24.7260 (18.2541)	mem 4879MB
[2022-05-30 23:46:28 MetaFG_0] (main.py 265): INFO Train: [10/300][750/1562]	eta 0:04:07 lr 0.000003	time 0.2933 (0.3054)	loss 1.6665 (1.8965)	grad_norm 23.0805 (18.2741)	mem 4879MB
[2022-05-30 23:46:31 MetaFG_0] (main.py 265): INFO Train: [10/300][760/1562]	eta 0:04:04 lr 0.000003	time 0.2986 (0.3054)	loss 1.6743 (1.8968)	grad_norm 26.6190 (18.2833)	mem 4879MB
[2022-05-30 23:46:34 MetaFG_0] (main.py 265): INFO Train: [10/300][770/1562]	eta 0:04:01 lr 0.000003	time 0.2921 (0.3054)	loss 2.1023 (1.8965)	grad_norm 11.5736 (18.2561)	mem 4879MB
[2022-05-30 23:46:37 MetaFG_0] (main.py 265): INFO Train: [10/300][780/1562]	eta 0:03:58 lr 0.000003	time 0.2922 (0.3054)	loss 1.7730 (1.8962)	grad_norm 14.2468 (18.2370)	mem 4879MB
[2022-05-30 23:46:40 MetaFG_0] (main.py 265): INFO Train: [10/300][790/1562]	eta 0:03:55 lr 0.000003	time 0.2919 (0.3053)	loss 1.9484 (1.8962)	grad_norm 14.4630 (18.2339)	mem 4879MB
[2022-05-30 23:46:43 MetaFG_0] (main.py 265): INFO Train: [10/300][800/1562]	eta 0:03:52 lr 0.000003	time 0.2976 (0.3053)	loss 1.7887 (1.8958)	grad_norm 25.8382 (18.2407)	mem 4879MB
[2022-05-30 23:46:46 MetaFG_0] (main.py 265): INFO Train: [10/300][810/1562]	eta 0:03:49 lr 0.000003	time 0.2917 (0.3053)	loss 2.0341 (1.8960)	grad_norm 17.9636 (18.2584)	mem 4879MB
[2022-05-30 23:46:49 MetaFG_0] (main.py 265): INFO Train: [10/300][820/1562]	eta 0:03:46 lr 0.000003	time 0.3000 (0.3053)	loss 1.8574 (1.8964)	grad_norm 13.9483 (18.2280)	mem 4879MB
[2022-05-30 23:46:52 MetaFG_0] (main.py 265): INFO Train: [10/300][830/1562]	eta 0:03:43 lr 0.000003	time 0.2978 (0.3053)	loss 1.8874 (1.8962)	grad_norm 20.3556 (18.2159)	mem 4879MB
[2022-05-30 23:46:55 MetaFG_0] (main.py 265): INFO Train: [10/300][840/1562]	eta 0:03:40 lr 0.000003	time 0.2934 (0.3052)	loss 2.0674 (1.8957)	grad_norm 12.5005 (18.2137)	mem 4879MB
[2022-05-30 23:46:58 MetaFG_0] (main.py 265): INFO Train: [10/300][850/1562]	eta 0:03:37 lr 0.000003	time 0.2983 (0.3052)	loss 1.9669 (1.8955)	grad_norm 16.7471 (18.2039)	mem 4879MB
[2022-05-30 23:47:01 MetaFG_0] (main.py 265): INFO Train: [10/300][860/1562]	eta 0:03:34 lr 0.000003	time 0.2983 (0.3052)	loss 1.7815 (1.8951)	grad_norm 13.4429 (18.2178)	mem 4879MB
[2022-05-30 23:47:04 MetaFG_0] (main.py 265): INFO Train: [10/300][870/1562]	eta 0:03:31 lr 0.000003	time 0.2928 (0.3052)	loss 1.9383 (1.8950)	grad_norm 12.8143 (18.1967)	mem 4879MB
[2022-05-30 23:47:07 MetaFG_0] (main.py 265): INFO Train: [10/300][880/1562]	eta 0:03:28 lr 0.000003	time 0.3002 (0.3052)	loss 1.9459 (1.8955)	grad_norm 18.9038 (18.1912)	mem 4879MB
[2022-05-30 23:47:11 MetaFG_0] (main.py 265): INFO Train: [10/300][890/1562]	eta 0:03:25 lr 0.000003	time 0.3002 (0.3052)	loss 1.9472 (1.8961)	grad_norm 13.8646 (18.1859)	mem 4879MB
[2022-05-30 23:47:14 MetaFG_0] (main.py 265): INFO Train: [10/300][900/1562]	eta 0:03:22 lr 0.000003	time 0.2931 (0.3052)	loss 1.8809 (1.8960)	grad_norm 13.0747 (18.1745)	mem 4879MB
[2022-05-30 23:47:17 MetaFG_0] (main.py 265): INFO Train: [10/300][910/1562]	eta 0:03:18 lr 0.000003	time 0.2923 (0.3051)	loss 1.7977 (1.8957)	grad_norm 17.4416 (18.1716)	mem 4879MB
[2022-05-30 23:47:20 MetaFG_0] (main.py 265): INFO Train: [10/300][920/1562]	eta 0:03:15 lr 0.000003	time 0.2967 (0.3051)	loss 2.0077 (1.8949)	grad_norm 18.5033 (18.1613)	mem 4879MB
[2022-05-30 23:47:23 MetaFG_0] (main.py 265): INFO Train: [10/300][930/1562]	eta 0:03:12 lr 0.000003	time 0.2989 (0.3051)	loss 1.8222 (1.8943)	grad_norm 17.6144 (18.1483)	mem 4879MB
[2022-05-30 23:47:26 MetaFG_0] (main.py 265): INFO Train: [10/300][940/1562]	eta 0:03:09 lr 0.000003	time 0.2948 (0.3051)	loss 1.9632 (1.8934)	grad_norm 22.9730 (18.1712)	mem 4879MB
[2022-05-30 23:47:29 MetaFG_0] (main.py 265): INFO Train: [10/300][950/1562]	eta 0:03:06 lr 0.000003	time 0.3105 (0.3051)	loss 1.9865 (1.8933)	grad_norm 18.3597 (18.1763)	mem 4879MB
[2022-05-30 23:47:32 MetaFG_0] (main.py 265): INFO Train: [10/300][960/1562]	eta 0:03:03 lr 0.000003	time 0.2933 (0.3051)	loss 2.0232 (1.8936)	grad_norm 12.5321 (18.1341)	mem 4879MB
[2022-05-30 23:47:35 MetaFG_0] (main.py 265): INFO Train: [10/300][970/1562]	eta 0:03:00 lr 0.000003	time 0.2984 (0.3051)	loss 1.6928 (1.8935)	grad_norm 22.3567 (18.1453)	mem 4879MB
[2022-05-30 23:47:38 MetaFG_0] (main.py 265): INFO Train: [10/300][980/1562]	eta 0:02:57 lr 0.000003	time 0.3142 (0.3051)	loss 1.8645 (1.8931)	grad_norm 13.6252 (18.1447)	mem 4879MB
[2022-05-30 23:47:41 MetaFG_0] (main.py 265): INFO Train: [10/300][990/1562]	eta 0:02:54 lr 0.000003	time 0.2995 (0.3051)	loss 1.7306 (1.8928)	grad_norm 13.7148 (18.1366)	mem 4879MB
[2022-05-30 23:47:44 MetaFG_0] (main.py 265): INFO Train: [10/300][1000/1562]	eta 0:02:51 lr 0.000003	time 0.2932 (0.3051)	loss 2.0204 (1.8927)	grad_norm 19.0783 (18.1275)	mem 4879MB
[2022-05-30 23:47:47 MetaFG_0] (main.py 265): INFO Train: [10/300][1010/1562]	eta 0:02:48 lr 0.000003	time 0.2933 (0.3050)	loss 1.9111 (1.8928)	grad_norm 24.0418 (18.1320)	mem 4879MB
[2022-05-30 23:47:50 MetaFG_0] (main.py 265): INFO Train: [10/300][1020/1562]	eta 0:02:45 lr 0.000003	time 0.2918 (0.3050)	loss 1.9539 (1.8926)	grad_norm 11.2989 (18.1294)	mem 4879MB
[2022-05-30 23:47:53 MetaFG_0] (main.py 265): INFO Train: [10/300][1030/1562]	eta 0:02:42 lr 0.000003	time 0.2930 (0.3050)	loss 1.8678 (1.8923)	grad_norm 12.8631 (18.1203)	mem 4879MB
[2022-05-30 23:47:56 MetaFG_0] (main.py 265): INFO Train: [10/300][1040/1562]	eta 0:02:39 lr 0.000003	time 0.2918 (0.3050)	loss 1.9529 (1.8920)	grad_norm 12.4756 (18.1128)	mem 4879MB
[2022-05-30 23:47:59 MetaFG_0] (main.py 265): INFO Train: [10/300][1050/1562]	eta 0:02:36 lr 0.000003	time 0.2935 (0.3050)	loss 1.7858 (1.8916)	grad_norm 12.2590 (18.1227)	mem 4879MB
[2022-05-30 23:48:02 MetaFG_0] (main.py 265): INFO Train: [10/300][1060/1562]	eta 0:02:33 lr 0.000003	time 0.2923 (0.3049)	loss 1.8966 (1.8914)	grad_norm 21.1500 (18.1350)	mem 4879MB
[2022-05-30 23:48:05 MetaFG_0] (main.py 265): INFO Train: [10/300][1070/1562]	eta 0:02:30 lr 0.000003	time 0.2920 (0.3049)	loss 1.6457 (1.8914)	grad_norm 13.0037 (18.1767)	mem 4879MB
[2022-05-30 23:48:08 MetaFG_0] (main.py 265): INFO Train: [10/300][1080/1562]	eta 0:02:26 lr 0.000003	time 0.2975 (0.3049)	loss 1.8986 (1.8917)	grad_norm 14.0222 (18.1717)	mem 4879MB
[2022-05-30 23:48:11 MetaFG_0] (main.py 265): INFO Train: [10/300][1090/1562]	eta 0:02:23 lr 0.000003	time 0.2920 (0.3049)	loss 2.0337 (1.8918)	grad_norm 20.0915 (18.1822)	mem 4879MB
[2022-05-30 23:48:14 MetaFG_0] (main.py 265): INFO Train: [10/300][1100/1562]	eta 0:02:20 lr 0.000003	time 0.2942 (0.3049)	loss 1.9746 (1.8909)	grad_norm 11.6904 (18.1871)	mem 4879MB
[2022-05-30 23:48:17 MetaFG_0] (main.py 265): INFO Train: [10/300][1110/1562]	eta 0:02:17 lr 0.000003	time 0.2921 (0.3049)	loss 1.8643 (1.8900)	grad_norm 26.0775 (18.2033)	mem 4879MB
[2022-05-30 23:48:20 MetaFG_0] (main.py 265): INFO Train: [10/300][1120/1562]	eta 0:02:14 lr 0.000003	time 0.2937 (0.3049)	loss 1.9589 (1.8899)	grad_norm 24.9316 (18.2027)	mem 4879MB
[2022-05-30 23:48:23 MetaFG_0] (main.py 265): INFO Train: [10/300][1130/1562]	eta 0:02:11 lr 0.000003	time 0.2931 (0.3049)	loss 1.9537 (1.8906)	grad_norm 15.7604 (18.1896)	mem 4879MB
[2022-05-30 23:48:26 MetaFG_0] (main.py 265): INFO Train: [10/300][1140/1562]	eta 0:02:08 lr 0.000003	time 0.2924 (0.3048)	loss 2.1353 (1.8913)	grad_norm 14.4265 (18.1819)	mem 4879MB
[2022-05-30 23:48:29 MetaFG_0] (main.py 265): INFO Train: [10/300][1150/1562]	eta 0:02:05 lr 0.000003	time 0.2915 (0.3048)	loss 1.7956 (1.8913)	grad_norm 11.8796 (18.1918)	mem 4879MB
[2022-05-30 23:48:32 MetaFG_0] (main.py 265): INFO Train: [10/300][1160/1562]	eta 0:02:02 lr 0.000003	time 0.2934 (0.3048)	loss 1.9497 (1.8916)	grad_norm 21.9150 (18.1920)	mem 4879MB
[2022-05-30 23:48:36 MetaFG_0] (main.py 265): INFO Train: [10/300][1170/1562]	eta 0:01:59 lr 0.000003	time 0.2919 (0.3048)	loss 1.7355 (1.8909)	grad_norm 14.8281 (18.2006)	mem 4879MB
[2022-05-30 23:48:39 MetaFG_0] (main.py 265): INFO Train: [10/300][1180/1562]	eta 0:01:56 lr 0.000003	time 0.2933 (0.3048)	loss 1.6522 (1.8904)	grad_norm 23.6015 (18.1951)	mem 4879MB
[2022-05-30 23:48:42 MetaFG_0] (main.py 265): INFO Train: [10/300][1190/1562]	eta 0:01:53 lr 0.000003	time 0.2991 (0.3048)	loss 1.8234 (1.8900)	grad_norm 14.3205 (18.2310)	mem 4879MB
[2022-05-30 23:48:45 MetaFG_0] (main.py 265): INFO Train: [10/300][1200/1562]	eta 0:01:50 lr 0.000003	time 0.2985 (0.3048)	loss 1.6333 (1.8895)	grad_norm 23.0272 (18.2362)	mem 4879MB
[2022-05-30 23:48:48 MetaFG_0] (main.py 265): INFO Train: [10/300][1210/1562]	eta 0:01:47 lr 0.000003	time 0.2934 (0.3048)	loss 2.0252 (1.8897)	grad_norm 13.1273 (18.2192)	mem 4879MB
[2022-05-30 23:48:51 MetaFG_0] (main.py 265): INFO Train: [10/300][1220/1562]	eta 0:01:44 lr 0.000003	time 0.2984 (0.3047)	loss 1.7895 (1.8898)	grad_norm 16.1666 (18.2183)	mem 4879MB
[2022-05-30 23:48:54 MetaFG_0] (main.py 265): INFO Train: [10/300][1230/1562]	eta 0:01:41 lr 0.000003	time 0.2913 (0.3047)	loss 1.7629 (1.8902)	grad_norm 20.9023 (18.2162)	mem 4879MB
[2022-05-30 23:48:57 MetaFG_0] (main.py 265): INFO Train: [10/300][1240/1562]	eta 0:01:38 lr 0.000003	time 0.2918 (0.3047)	loss 2.0452 (1.8895)	grad_norm 21.8881 (18.2512)	mem 4879MB
[2022-05-30 23:49:00 MetaFG_0] (main.py 265): INFO Train: [10/300][1250/1562]	eta 0:01:35 lr 0.000003	time 0.2919 (0.3047)	loss 1.6756 (1.8896)	grad_norm 16.1610 (18.2496)	mem 4879MB
[2022-05-30 23:49:03 MetaFG_0] (main.py 265): INFO Train: [10/300][1260/1562]	eta 0:01:32 lr 0.000003	time 0.2920 (0.3047)	loss 1.8613 (1.8893)	grad_norm 15.8442 (18.2807)	mem 4879MB
[2022-05-30 23:49:06 MetaFG_0] (main.py 265): INFO Train: [10/300][1270/1562]	eta 0:01:28 lr 0.000003	time 0.2949 (0.3047)	loss 1.7318 (1.8890)	grad_norm 14.7016 (18.2945)	mem 4879MB
[2022-05-30 23:49:09 MetaFG_0] (main.py 265): INFO Train: [10/300][1280/1562]	eta 0:01:25 lr 0.000003	time 0.2926 (0.3047)	loss 1.6631 (1.8887)	grad_norm 13.6515 (18.3154)	mem 4879MB
[2022-05-30 23:49:12 MetaFG_0] (main.py 265): INFO Train: [10/300][1290/1562]	eta 0:01:22 lr 0.000003	time 0.3004 (0.3047)	loss 1.9360 (1.8885)	grad_norm 19.7945 (18.3135)	mem 4879MB
[2022-05-30 23:49:15 MetaFG_0] (main.py 265): INFO Train: [10/300][1300/1562]	eta 0:01:19 lr 0.000003	time 0.2921 (0.3047)	loss 1.9605 (1.8887)	grad_norm 19.9409 (18.3053)	mem 4879MB
[2022-05-30 23:49:18 MetaFG_0] (main.py 265): INFO Train: [10/300][1310/1562]	eta 0:01:16 lr 0.000003	time 0.2922 (0.3046)	loss 1.9711 (1.8881)	grad_norm 11.9290 (18.2937)	mem 4879MB
[2022-05-30 23:49:21 MetaFG_0] (main.py 265): INFO Train: [10/300][1320/1562]	eta 0:01:13 lr 0.000003	time 0.2918 (0.3046)	loss 1.9365 (1.8876)	grad_norm 16.1210 (18.2953)	mem 4879MB
[2022-05-30 23:49:24 MetaFG_0] (main.py 265): INFO Train: [10/300][1330/1562]	eta 0:01:10 lr 0.000003	time 0.2961 (0.3046)	loss 2.1200 (1.8883)	grad_norm 17.4631 (18.2859)	mem 4879MB
[2022-05-30 23:49:27 MetaFG_0] (main.py 265): INFO Train: [10/300][1340/1562]	eta 0:01:07 lr 0.000003	time 0.2918 (0.3046)	loss 2.0313 (1.8882)	grad_norm 16.5171 (18.2865)	mem 4879MB
[2022-05-30 23:49:30 MetaFG_0] (main.py 265): INFO Train: [10/300][1350/1562]	eta 0:01:04 lr 0.000003	time 0.2935 (0.3046)	loss 2.0114 (1.8878)	grad_norm 17.4852 (18.2935)	mem 4879MB
[2022-05-30 23:49:33 MetaFG_0] (main.py 265): INFO Train: [10/300][1360/1562]	eta 0:01:01 lr 0.000003	time 0.2923 (0.3046)	loss 1.6665 (1.8878)	grad_norm 17.2577 (18.3095)	mem 4879MB
[2022-05-30 23:49:36 MetaFG_0] (main.py 265): INFO Train: [10/300][1370/1562]	eta 0:00:58 lr 0.000003	time 0.2990 (0.3046)	loss 1.9059 (1.8876)	grad_norm 16.0916 (18.2965)	mem 4879MB
[2022-05-30 23:49:39 MetaFG_0] (main.py 265): INFO Train: [10/300][1380/1562]	eta 0:00:55 lr 0.000003	time 0.2933 (0.3046)	loss 1.8965 (1.8878)	grad_norm 15.0498 (18.2975)	mem 4879MB
[2022-05-30 23:49:42 MetaFG_0] (main.py 265): INFO Train: [10/300][1390/1562]	eta 0:00:52 lr 0.000003	time 0.2930 (0.3046)	loss 1.9416 (1.8873)	grad_norm 16.4867 (18.3181)	mem 4879MB
[2022-05-30 23:49:45 MetaFG_0] (main.py 265): INFO Train: [10/300][1400/1562]	eta 0:00:49 lr 0.000003	time 0.2920 (0.3045)	loss 1.7698 (1.8864)	grad_norm 42.3059 (18.3601)	mem 4879MB
[2022-05-30 23:49:48 MetaFG_0] (main.py 265): INFO Train: [10/300][1410/1562]	eta 0:00:46 lr 0.000003	time 0.2978 (0.3045)	loss 2.0175 (1.8861)	grad_norm 13.5007 (18.3766)	mem 4879MB
[2022-05-30 23:49:51 MetaFG_0] (main.py 265): INFO Train: [10/300][1420/1562]	eta 0:00:43 lr 0.000003	time 0.2983 (0.3045)	loss 2.0228 (1.8858)	grad_norm 17.4597 (18.3702)	mem 4879MB
[2022-05-30 23:49:54 MetaFG_0] (main.py 265): INFO Train: [10/300][1430/1562]	eta 0:00:40 lr 0.000003	time 0.2995 (0.3045)	loss 1.7453 (1.8853)	grad_norm 15.1539 (18.3888)	mem 4879MB
[2022-05-30 23:49:57 MetaFG_0] (main.py 265): INFO Train: [10/300][1440/1562]	eta 0:00:37 lr 0.000003	time 0.2917 (0.3046)	loss 1.9981 (1.8853)	grad_norm 29.7174 (18.4122)	mem 4879MB
[2022-05-30 23:50:01 MetaFG_0] (main.py 265): INFO Train: [10/300][1450/1562]	eta 0:00:34 lr 0.000003	time 0.2979 (0.3046)	loss 1.9316 (1.8851)	grad_norm 10.1768 (18.4048)	mem 4879MB
[2022-05-30 23:50:04 MetaFG_0] (main.py 265): INFO Train: [10/300][1460/1562]	eta 0:00:31 lr 0.000003	time 0.2939 (0.3046)	loss 1.9578 (1.8852)	grad_norm 13.7409 (18.4162)	mem 4879MB
[2022-05-30 23:50:07 MetaFG_0] (main.py 265): INFO Train: [10/300][1470/1562]	eta 0:00:28 lr 0.000003	time 0.2924 (0.3046)	loss 1.7127 (1.8850)	grad_norm 22.4792 (18.4131)	mem 4879MB
[2022-05-30 23:50:10 MetaFG_0] (main.py 265): INFO Train: [10/300][1480/1562]	eta 0:00:24 lr 0.000003	time 0.2984 (0.3046)	loss 2.0050 (1.8852)	grad_norm 17.9242 (18.3954)	mem 4879MB
[2022-05-30 23:50:13 MetaFG_0] (main.py 265): INFO Train: [10/300][1490/1562]	eta 0:00:21 lr 0.000003	time 0.2930 (0.3045)	loss 1.8041 (1.8847)	grad_norm 21.4814 (18.3996)	mem 4879MB
[2022-05-30 23:50:16 MetaFG_0] (main.py 265): INFO Train: [10/300][1500/1562]	eta 0:00:18 lr 0.000003	time 0.2940 (0.3045)	loss 1.7832 (1.8847)	grad_norm 24.8928 (18.3918)	mem 4879MB
[2022-05-30 23:50:19 MetaFG_0] (main.py 265): INFO Train: [10/300][1510/1562]	eta 0:00:15 lr 0.000003	time 0.2977 (0.3045)	loss 1.9833 (1.8842)	grad_norm 17.8411 (18.3977)	mem 4879MB
[2022-05-30 23:50:22 MetaFG_0] (main.py 265): INFO Train: [10/300][1520/1562]	eta 0:00:12 lr 0.000003	time 0.2974 (0.3045)	loss 2.0506 (1.8845)	grad_norm 9.6035 (18.4019)	mem 4879MB
[2022-05-30 23:50:25 MetaFG_0] (main.py 265): INFO Train: [10/300][1530/1562]	eta 0:00:09 lr 0.000003	time 0.2992 (0.3046)	loss 1.7315 (1.8843)	grad_norm 19.3253 (18.3929)	mem 4879MB
[2022-05-30 23:50:28 MetaFG_0] (main.py 265): INFO Train: [10/300][1540/1562]	eta 0:00:06 lr 0.000003	time 0.2931 (0.3046)	loss 2.0357 (1.8847)	grad_norm 27.0191 (18.4164)	mem 4879MB
[2022-05-30 23:50:31 MetaFG_0] (main.py 265): INFO Train: [10/300][1550/1562]	eta 0:00:03 lr 0.000003	time 0.2926 (0.3045)	loss 1.7813 (1.8843)	grad_norm 22.9001 (18.4079)	mem 4879MB
[2022-05-30 23:50:34 MetaFG_0] (main.py 265): INFO Train: [10/300][1560/1562]	eta 0:00:00 lr 0.000003	time 0.2922 (0.3045)	loss 1.5163 (1.8840)	grad_norm 29.2975 (18.4079)	mem 4879MB
[2022-05-30 23:50:34 MetaFG_0] (main.py 272): INFO EPOCH 10 training takes 0:07:55
[2022-05-30 23:50:34 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_10.pth saving......
[2022-05-30 23:50:35 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_10.pth saved !!!
[2022-05-30 23:50:35 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:50:37 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:50:37 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:50:37 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.540 (0.540)	Loss 2.2358 (2.2358)	Acc@1 59.375 (59.375)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-30 23:50:38 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.138)	Loss 2.3720 (2.3455)	Acc@1 59.375 (58.523)	Acc@5 84.375 (87.784)	Mem 4879MB
[2022-05-30 23:50:39 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.116)	Loss 2.3492 (2.3552)	Acc@1 53.125 (57.292)	Acc@5 87.500 (88.393)	Mem 4879MB
[2022-05-30 23:50:40 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.107 (0.109)	Loss 2.2518 (2.3656)	Acc@1 56.250 (55.343)	Acc@5 84.375 (88.004)	Mem 4879MB
[2022-05-30 23:50:41 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.095 (0.105)	Loss 2.4519 (2.3752)	Acc@1 43.750 (54.268)	Acc@5 90.625 (87.271)	Mem 4879MB
[2022-05-30 23:50:42 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.095 (0.103)	Loss 2.1491 (2.3676)	Acc@1 62.500 (54.534)	Acc@5 96.875 (87.377)	Mem 4879MB
[2022-05-30 23:50:43 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.099 (0.101)	Loss 2.3180 (2.3623)	Acc@1 50.000 (54.713)	Acc@5 84.375 (87.244)	Mem 4879MB
[2022-05-30 23:50:44 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.100)	Loss 2.3229 (2.3755)	Acc@1 65.625 (54.533)	Acc@5 100.000 (87.060)	Mem 4879MB
[2022-05-30 23:50:45 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.099)	Loss 2.7159 (2.3783)	Acc@1 40.625 (54.205)	Acc@5 84.375 (87.037)	Mem 4879MB
[2022-05-30 23:50:46 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.088 (0.099)	Loss 2.2469 (2.3872)	Acc@1 59.375 (53.640)	Acc@5 87.500 (86.641)	Mem 4879MB
[2022-05-30 23:50:47 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.098)	Loss 2.5465 (2.3873)	Acc@1 50.000 (53.775)	Acc@5 81.250 (86.572)	Mem 4879MB
[2022-05-30 23:50:48 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.093 (0.098)	Loss 2.2826 (2.3919)	Acc@1 50.000 (53.378)	Acc@5 93.750 (86.008)	Mem 4879MB
[2022-05-30 23:50:49 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.092 (0.097)	Loss 2.3440 (2.3904)	Acc@1 50.000 (53.435)	Acc@5 93.750 (86.338)	Mem 4879MB
[2022-05-30 23:50:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.097)	Loss 2.5573 (2.3956)	Acc@1 40.625 (52.886)	Acc@5 78.125 (86.140)	Mem 4879MB
[2022-05-30 23:50:50 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.097)	Loss 2.2062 (2.3910)	Acc@1 68.750 (53.036)	Acc@5 90.625 (86.126)	Mem 4879MB
[2022-05-30 23:50:51 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.097)	Loss 2.2116 (2.3862)	Acc@1 62.500 (53.291)	Acc@5 93.750 (86.238)	Mem 4879MB
[2022-05-30 23:50:52 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.091 (0.096)	Loss 2.2171 (2.3826)	Acc@1 62.500 (53.339)	Acc@5 87.500 (86.355)	Mem 4879MB
[2022-05-30 23:50:53 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.096)	Loss 2.6202 (2.3854)	Acc@1 46.875 (53.289)	Acc@5 81.250 (86.312)	Mem 4879MB
[2022-05-30 23:50:54 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.096)	Loss 2.0507 (2.3859)	Acc@1 65.625 (53.246)	Acc@5 93.750 (86.395)	Mem 4879MB
[2022-05-30 23:50:55 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.096)	Loss 2.4842 (2.3848)	Acc@1 50.000 (53.256)	Acc@5 81.250 (86.600)	Mem 4879MB
[2022-05-30 23:50:56 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.096)	Loss 2.3620 (2.3818)	Acc@1 65.625 (53.343)	Acc@5 84.375 (86.489)	Mem 4879MB
[2022-05-30 23:50:57 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.100 (0.096)	Loss 2.4057 (2.3824)	Acc@1 62.500 (53.495)	Acc@5 84.375 (86.493)	Mem 4879MB
[2022-05-30 23:50:58 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.096)	Loss 2.2861 (2.3836)	Acc@1 53.125 (53.507)	Acc@5 84.375 (86.482)	Mem 4879MB
[2022-05-30 23:50:59 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.096)	Loss 2.3531 (2.3836)	Acc@1 56.250 (53.423)	Acc@5 87.500 (86.472)	Mem 4879MB
[2022-05-30 23:51:00 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.095)	Loss 2.5043 (2.3876)	Acc@1 46.875 (53.384)	Acc@5 87.500 (86.463)	Mem 4879MB
[2022-05-30 23:51:01 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.089 (0.095)	Loss 2.0490 (2.3854)	Acc@1 71.875 (53.474)	Acc@5 93.750 (86.467)	Mem 4879MB
[2022-05-30 23:51:02 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.095)	Loss 2.5862 (2.3904)	Acc@1 50.000 (53.352)	Acc@5 75.000 (86.279)	Mem 4879MB
[2022-05-30 23:51:03 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.095)	Loss 2.4120 (2.3895)	Acc@1 50.000 (53.275)	Acc@5 87.500 (86.255)	Mem 4879MB
[2022-05-30 23:51:04 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.095)	Loss 2.6154 (2.3921)	Acc@1 31.250 (53.125)	Acc@5 87.500 (86.154)	Mem 4879MB
[2022-05-30 23:51:05 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.095)	Loss 2.2316 (2.3914)	Acc@1 62.500 (53.265)	Acc@5 87.500 (86.136)	Mem 4879MB
[2022-05-30 23:51:05 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.095)	Loss 2.2607 (2.3887)	Acc@1 46.875 (53.250)	Acc@5 96.875 (86.244)	Mem 4879MB
[2022-05-30 23:51:06 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 2.3561 (2.3901)	Acc@1 62.500 (53.195)	Acc@5 84.375 (86.204)	Mem 4879MB
[2022-05-30 23:51:07 MetaFG_0] (main.py 330): INFO  * Acc@1 53.220 Acc@5 86.220
[2022-05-30 23:51:07 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 53.2%
[2022-05-30 23:51:07 MetaFG_0] (main.py 171): INFO Max accuracy: 53.22%
[2022-05-30 23:51:08 MetaFG_0] (main.py 265): INFO Train: [11/300][0/1562]	eta 0:25:39 lr 0.000003	time 0.9853 (0.9853)	loss 1.7972 (1.7972)	grad_norm 12.7634 (12.7634)	mem 4879MB
[2022-05-30 23:51:11 MetaFG_0] (main.py 265): INFO Train: [11/300][10/1562]	eta 0:09:34 lr 0.000003	time 0.2994 (0.3704)	loss 1.6632 (1.8022)	grad_norm 15.4142 (16.7884)	mem 4879MB
[2022-05-30 23:51:14 MetaFG_0] (main.py 265): INFO Train: [11/300][20/1562]	eta 0:08:42 lr 0.000003	time 0.2920 (0.3386)	loss 1.7892 (1.8432)	grad_norm 11.6539 (17.4236)	mem 4879MB
[2022-05-30 23:51:17 MetaFG_0] (main.py 265): INFO Train: [11/300][30/1562]	eta 0:08:20 lr 0.000003	time 0.2951 (0.3266)	loss 1.8239 (1.8416)	grad_norm 23.5911 (17.6403)	mem 4879MB
[2022-05-30 23:51:20 MetaFG_0] (main.py 265): INFO Train: [11/300][40/1562]	eta 0:08:08 lr 0.000003	time 0.2918 (0.3210)	loss 1.8916 (1.8444)	grad_norm 18.4744 (18.0195)	mem 4879MB
[2022-05-30 23:51:23 MetaFG_0] (main.py 265): INFO Train: [11/300][50/1562]	eta 0:08:00 lr 0.000003	time 0.2932 (0.3178)	loss 1.8347 (1.8387)	grad_norm 18.8343 (18.1998)	mem 4879MB
[2022-05-30 23:51:26 MetaFG_0] (main.py 265): INFO Train: [11/300][60/1562]	eta 0:07:53 lr 0.000003	time 0.2980 (0.3156)	loss 1.9932 (1.8463)	grad_norm 10.9009 (18.1968)	mem 4879MB
[2022-05-30 23:51:29 MetaFG_0] (main.py 265): INFO Train: [11/300][70/1562]	eta 0:07:48 lr 0.000003	time 0.2975 (0.3137)	loss 1.9324 (1.8546)	grad_norm 23.9157 (18.1581)	mem 4879MB
[2022-05-30 23:51:32 MetaFG_0] (main.py 265): INFO Train: [11/300][80/1562]	eta 0:07:42 lr 0.000003	time 0.2949 (0.3124)	loss 1.8044 (1.8572)	grad_norm 16.0897 (18.0744)	mem 4879MB
[2022-05-30 23:51:35 MetaFG_0] (main.py 265): INFO Train: [11/300][90/1562]	eta 0:07:38 lr 0.000003	time 0.2946 (0.3113)	loss 2.0539 (1.8508)	grad_norm 17.4459 (18.0642)	mem 4879MB
[2022-05-30 23:51:38 MetaFG_0] (main.py 265): INFO Train: [11/300][100/1562]	eta 0:07:33 lr 0.000003	time 0.3005 (0.3105)	loss 2.0051 (1.8510)	grad_norm 14.8521 (18.2372)	mem 4879MB
[2022-05-30 23:51:41 MetaFG_0] (main.py 265): INFO Train: [11/300][110/1562]	eta 0:07:29 lr 0.000003	time 0.2926 (0.3098)	loss 1.9419 (1.8537)	grad_norm 14.9798 (18.0764)	mem 4879MB
[2022-05-30 23:51:44 MetaFG_0] (main.py 265): INFO Train: [11/300][120/1562]	eta 0:07:26 lr 0.000003	time 0.2974 (0.3094)	loss 2.0305 (1.8552)	grad_norm 26.8003 (18.1720)	mem 4879MB
[2022-05-30 23:51:47 MetaFG_0] (main.py 265): INFO Train: [11/300][130/1562]	eta 0:07:22 lr 0.000003	time 0.2931 (0.3089)	loss 2.1106 (1.8584)	grad_norm 20.6522 (18.0433)	mem 4879MB
[2022-05-30 23:51:50 MetaFG_0] (main.py 265): INFO Train: [11/300][140/1562]	eta 0:07:18 lr 0.000003	time 0.2974 (0.3085)	loss 1.9876 (1.8569)	grad_norm 22.2382 (18.0698)	mem 4879MB
[2022-05-30 23:51:53 MetaFG_0] (main.py 265): INFO Train: [11/300][150/1562]	eta 0:07:15 lr 0.000003	time 0.2943 (0.3083)	loss 1.9443 (1.8632)	grad_norm 15.1315 (17.8895)	mem 4879MB
[2022-05-30 23:51:56 MetaFG_0] (main.py 265): INFO Train: [11/300][160/1562]	eta 0:07:11 lr 0.000003	time 0.2918 (0.3079)	loss 1.5059 (1.8584)	grad_norm 17.3285 (17.8953)	mem 4879MB
[2022-05-30 23:51:59 MetaFG_0] (main.py 265): INFO Train: [11/300][170/1562]	eta 0:07:08 lr 0.000003	time 0.2919 (0.3076)	loss 1.9822 (1.8582)	grad_norm 17.0252 (17.9142)	mem 4879MB
[2022-05-30 23:52:02 MetaFG_0] (main.py 265): INFO Train: [11/300][180/1562]	eta 0:07:04 lr 0.000003	time 0.2917 (0.3073)	loss 1.6128 (1.8544)	grad_norm 21.8687 (17.8640)	mem 4879MB
[2022-05-30 23:52:05 MetaFG_0] (main.py 265): INFO Train: [11/300][190/1562]	eta 0:07:01 lr 0.000003	time 0.2918 (0.3071)	loss 1.6505 (1.8536)	grad_norm 19.7891 (17.9370)	mem 4879MB
[2022-05-30 23:52:08 MetaFG_0] (main.py 265): INFO Train: [11/300][200/1562]	eta 0:06:58 lr 0.000003	time 0.2942 (0.3074)	loss 1.8192 (1.8498)	grad_norm 24.8868 (18.0896)	mem 4879MB
[2022-05-30 23:52:11 MetaFG_0] (main.py 265): INFO Train: [11/300][210/1562]	eta 0:06:55 lr 0.000003	time 0.2923 (0.3072)	loss 1.7120 (1.8484)	grad_norm 22.6861 (18.2474)	mem 4879MB
[2022-05-30 23:52:14 MetaFG_0] (main.py 265): INFO Train: [11/300][220/1562]	eta 0:06:52 lr 0.000003	time 0.2987 (0.3070)	loss 2.1139 (1.8519)	grad_norm 16.5009 (18.1876)	mem 4879MB
[2022-05-30 23:52:18 MetaFG_0] (main.py 265): INFO Train: [11/300][230/1562]	eta 0:06:48 lr 0.000003	time 0.2919 (0.3069)	loss 1.7010 (1.8506)	grad_norm 15.3894 (18.1791)	mem 4879MB
[2022-05-30 23:52:21 MetaFG_0] (main.py 265): INFO Train: [11/300][240/1562]	eta 0:06:45 lr 0.000003	time 0.2927 (0.3067)	loss 1.9638 (1.8535)	grad_norm 12.4219 (18.0614)	mem 4879MB
[2022-05-30 23:52:24 MetaFG_0] (main.py 265): INFO Train: [11/300][250/1562]	eta 0:06:42 lr 0.000003	time 0.2931 (0.3065)	loss 1.9488 (1.8538)	grad_norm 21.9395 (18.1038)	mem 4879MB
[2022-05-30 23:52:27 MetaFG_0] (main.py 265): INFO Train: [11/300][260/1562]	eta 0:06:38 lr 0.000003	time 0.2944 (0.3064)	loss 1.6893 (1.8523)	grad_norm 32.1541 (18.1188)	mem 4879MB
[2022-05-30 23:52:30 MetaFG_0] (main.py 265): INFO Train: [11/300][270/1562]	eta 0:06:35 lr 0.000003	time 0.2929 (0.3062)	loss 1.8693 (1.8509)	grad_norm 20.6047 (18.1991)	mem 4879MB
[2022-05-30 23:52:33 MetaFG_0] (main.py 265): INFO Train: [11/300][280/1562]	eta 0:06:32 lr 0.000003	time 0.2982 (0.3061)	loss 1.9124 (1.8496)	grad_norm 13.4874 (18.2560)	mem 4879MB
[2022-05-30 23:52:36 MetaFG_0] (main.py 265): INFO Train: [11/300][290/1562]	eta 0:06:29 lr 0.000003	time 0.2920 (0.3060)	loss 1.8338 (1.8508)	grad_norm 12.9338 (18.2553)	mem 4879MB
[2022-05-30 23:52:39 MetaFG_0] (main.py 265): INFO Train: [11/300][300/1562]	eta 0:06:26 lr 0.000004	time 0.2934 (0.3059)	loss 1.8125 (1.8505)	grad_norm 11.5679 (18.2508)	mem 4879MB
[2022-05-30 23:52:42 MetaFG_0] (main.py 265): INFO Train: [11/300][310/1562]	eta 0:06:22 lr 0.000004	time 0.2979 (0.3058)	loss 1.6889 (1.8509)	grad_norm 22.0577 (18.2810)	mem 4879MB
[2022-05-30 23:52:45 MetaFG_0] (main.py 265): INFO Train: [11/300][320/1562]	eta 0:06:19 lr 0.000004	time 0.2914 (0.3058)	loss 1.5793 (1.8485)	grad_norm 14.1241 (18.2467)	mem 4879MB
[2022-05-30 23:52:48 MetaFG_0] (main.py 265): INFO Train: [11/300][330/1562]	eta 0:06:16 lr 0.000004	time 0.2947 (0.3057)	loss 1.7780 (1.8507)	grad_norm 24.4607 (18.2543)	mem 4879MB
[2022-05-30 23:52:51 MetaFG_0] (main.py 265): INFO Train: [11/300][340/1562]	eta 0:06:13 lr 0.000004	time 0.2945 (0.3057)	loss 2.0070 (1.8525)	grad_norm 24.1835 (18.2464)	mem 4879MB
[2022-05-30 23:52:54 MetaFG_0] (main.py 265): INFO Train: [11/300][350/1562]	eta 0:06:10 lr 0.000004	time 0.2974 (0.3055)	loss 1.5514 (1.8517)	grad_norm 19.4607 (18.2502)	mem 4879MB
[2022-05-30 23:52:57 MetaFG_0] (main.py 265): INFO Train: [11/300][360/1562]	eta 0:06:07 lr 0.000004	time 0.2981 (0.3055)	loss 1.9850 (1.8536)	grad_norm 15.1373 (18.2567)	mem 4879MB
[2022-05-30 23:53:00 MetaFG_0] (main.py 265): INFO Train: [11/300][370/1562]	eta 0:06:04 lr 0.000004	time 0.2976 (0.3055)	loss 2.0522 (1.8523)	grad_norm 20.6163 (18.3119)	mem 4879MB
[2022-05-30 23:53:03 MetaFG_0] (main.py 265): INFO Train: [11/300][380/1562]	eta 0:06:01 lr 0.000004	time 0.2926 (0.3054)	loss 1.5941 (1.8504)	grad_norm 15.3270 (18.2976)	mem 4879MB
[2022-05-30 23:53:06 MetaFG_0] (main.py 265): INFO Train: [11/300][390/1562]	eta 0:05:57 lr 0.000004	time 0.2926 (0.3054)	loss 1.9414 (1.8512)	grad_norm 14.6331 (18.2219)	mem 4879MB
[2022-05-30 23:53:09 MetaFG_0] (main.py 265): INFO Train: [11/300][400/1562]	eta 0:05:54 lr 0.000004	time 0.2993 (0.3053)	loss 1.8646 (1.8504)	grad_norm 13.3983 (18.2536)	mem 4879MB
[2022-05-30 23:53:12 MetaFG_0] (main.py 265): INFO Train: [11/300][410/1562]	eta 0:05:51 lr 0.000004	time 0.2920 (0.3053)	loss 1.9155 (1.8508)	grad_norm 11.6551 (18.2086)	mem 4879MB
[2022-05-30 23:53:15 MetaFG_0] (main.py 265): INFO Train: [11/300][420/1562]	eta 0:05:48 lr 0.000004	time 0.2972 (0.3053)	loss 1.8277 (1.8506)	grad_norm 14.0051 (18.2110)	mem 4879MB
[2022-05-30 23:53:18 MetaFG_0] (main.py 265): INFO Train: [11/300][430/1562]	eta 0:05:45 lr 0.000004	time 0.2950 (0.3052)	loss 1.8873 (1.8510)	grad_norm 16.8184 (18.2031)	mem 4879MB
[2022-05-30 23:53:21 MetaFG_0] (main.py 265): INFO Train: [11/300][440/1562]	eta 0:05:42 lr 0.000004	time 0.2978 (0.3052)	loss 2.0092 (1.8501)	grad_norm 14.8099 (18.2148)	mem 4879MB
[2022-05-30 23:53:24 MetaFG_0] (main.py 265): INFO Train: [11/300][450/1562]	eta 0:05:39 lr 0.000004	time 0.2921 (0.3052)	loss 1.7184 (1.8488)	grad_norm 18.0188 (18.1977)	mem 4879MB
[2022-05-30 23:53:27 MetaFG_0] (main.py 265): INFO Train: [11/300][460/1562]	eta 0:05:36 lr 0.000004	time 0.2922 (0.3052)	loss 1.9046 (1.8490)	grad_norm 11.4393 (18.2136)	mem 4879MB
[2022-05-30 23:53:30 MetaFG_0] (main.py 265): INFO Train: [11/300][470/1562]	eta 0:05:33 lr 0.000004	time 0.2934 (0.3051)	loss 1.9655 (1.8484)	grad_norm 12.0923 (18.2819)	mem 4879MB
[2022-05-30 23:53:33 MetaFG_0] (main.py 265): INFO Train: [11/300][480/1562]	eta 0:05:30 lr 0.000004	time 0.2999 (0.3051)	loss 1.9482 (1.8488)	grad_norm 25.7601 (18.3173)	mem 4879MB
[2022-05-30 23:53:36 MetaFG_0] (main.py 265): INFO Train: [11/300][490/1562]	eta 0:05:26 lr 0.000004	time 0.2987 (0.3050)	loss 1.9066 (1.8492)	grad_norm 16.8583 (18.3166)	mem 4879MB
[2022-05-30 23:53:39 MetaFG_0] (main.py 265): INFO Train: [11/300][500/1562]	eta 0:05:23 lr 0.000004	time 0.2985 (0.3050)	loss 2.0194 (1.8481)	grad_norm 19.8151 (18.3262)	mem 4879MB
[2022-05-30 23:53:42 MetaFG_0] (main.py 265): INFO Train: [11/300][510/1562]	eta 0:05:20 lr 0.000004	time 0.2980 (0.3050)	loss 1.6494 (1.8460)	grad_norm 22.8104 (18.3041)	mem 4879MB
[2022-05-30 23:53:45 MetaFG_0] (main.py 265): INFO Train: [11/300][520/1562]	eta 0:05:17 lr 0.000004	time 0.2915 (0.3049)	loss 1.8875 (1.8465)	grad_norm 25.5590 (18.3197)	mem 4879MB
[2022-05-30 23:53:48 MetaFG_0] (main.py 265): INFO Train: [11/300][530/1562]	eta 0:05:14 lr 0.000004	time 0.2978 (0.3049)	loss 2.0180 (1.8463)	grad_norm 22.8953 (18.3417)	mem 4879MB
[2022-05-30 23:53:52 MetaFG_0] (main.py 265): INFO Train: [11/300][540/1562]	eta 0:05:11 lr 0.000004	time 0.3010 (0.3048)	loss 1.8408 (1.8454)	grad_norm 17.1095 (18.3158)	mem 4879MB
[2022-05-30 23:53:55 MetaFG_0] (main.py 265): INFO Train: [11/300][550/1562]	eta 0:05:08 lr 0.000004	time 0.2931 (0.3048)	loss 1.6762 (1.8446)	grad_norm 19.9646 (18.3657)	mem 4879MB
[2022-05-30 23:53:58 MetaFG_0] (main.py 265): INFO Train: [11/300][560/1562]	eta 0:05:05 lr 0.000004	time 0.2930 (0.3047)	loss 2.0933 (1.8447)	grad_norm 19.7690 (18.4214)	mem 4879MB
[2022-05-30 23:54:01 MetaFG_0] (main.py 265): INFO Train: [11/300][570/1562]	eta 0:05:02 lr 0.000004	time 0.2982 (0.3047)	loss 1.9747 (1.8456)	grad_norm 16.5878 (18.3922)	mem 4879MB
[2022-05-30 23:54:04 MetaFG_0] (main.py 265): INFO Train: [11/300][580/1562]	eta 0:04:59 lr 0.000004	time 0.2997 (0.3047)	loss 1.8997 (1.8457)	grad_norm 11.8284 (18.4084)	mem 4879MB
[2022-05-30 23:54:07 MetaFG_0] (main.py 265): INFO Train: [11/300][590/1562]	eta 0:04:56 lr 0.000004	time 0.2988 (0.3047)	loss 1.8965 (1.8460)	grad_norm 15.5320 (18.4180)	mem 4879MB
[2022-05-30 23:54:10 MetaFG_0] (main.py 265): INFO Train: [11/300][600/1562]	eta 0:04:53 lr 0.000004	time 0.2917 (0.3047)	loss 1.8375 (1.8459)	grad_norm 14.8601 (18.3865)	mem 4879MB
[2022-05-30 23:54:13 MetaFG_0] (main.py 265): INFO Train: [11/300][610/1562]	eta 0:04:50 lr 0.000004	time 0.2938 (0.3047)	loss 1.8582 (1.8447)	grad_norm 10.6279 (18.3492)	mem 4879MB
[2022-05-30 23:54:16 MetaFG_0] (main.py 265): INFO Train: [11/300][620/1562]	eta 0:04:46 lr 0.000004	time 0.2920 (0.3046)	loss 1.8561 (1.8434)	grad_norm 13.4559 (18.3239)	mem 4879MB
[2022-05-30 23:54:19 MetaFG_0] (main.py 265): INFO Train: [11/300][630/1562]	eta 0:04:43 lr 0.000004	time 0.3016 (0.3047)	loss 1.6110 (1.8437)	grad_norm 20.5737 (18.3217)	mem 4879MB
[2022-05-30 23:54:22 MetaFG_0] (main.py 265): INFO Train: [11/300][640/1562]	eta 0:04:40 lr 0.000004	time 0.2932 (0.3048)	loss 1.6618 (1.8422)	grad_norm 19.0244 (18.3076)	mem 4879MB
[2022-05-30 23:54:25 MetaFG_0] (main.py 265): INFO Train: [11/300][650/1562]	eta 0:04:37 lr 0.000004	time 0.2919 (0.3047)	loss 1.7358 (1.8428)	grad_norm 14.9904 (nan)	mem 4879MB
[2022-05-30 23:54:28 MetaFG_0] (main.py 265): INFO Train: [11/300][660/1562]	eta 0:04:34 lr 0.000004	time 0.2922 (0.3047)	loss 2.0439 (1.8430)	grad_norm 17.3213 (nan)	mem 4879MB
[2022-05-30 23:54:31 MetaFG_0] (main.py 265): INFO Train: [11/300][670/1562]	eta 0:04:31 lr 0.000004	time 0.3008 (0.3047)	loss 1.8237 (1.8427)	grad_norm 21.4356 (nan)	mem 4879MB
[2022-05-30 23:54:34 MetaFG_0] (main.py 265): INFO Train: [11/300][680/1562]	eta 0:04:28 lr 0.000004	time 0.2919 (0.3047)	loss 1.9226 (1.8425)	grad_norm 19.2556 (nan)	mem 4879MB
[2022-05-30 23:54:37 MetaFG_0] (main.py 265): INFO Train: [11/300][690/1562]	eta 0:04:25 lr 0.000004	time 0.2991 (0.3047)	loss 1.9613 (1.8425)	grad_norm 14.7809 (nan)	mem 4879MB
[2022-05-30 23:54:40 MetaFG_0] (main.py 265): INFO Train: [11/300][700/1562]	eta 0:04:22 lr 0.000004	time 0.2918 (0.3047)	loss 1.6694 (1.8427)	grad_norm 15.2754 (nan)	mem 4879MB
[2022-05-30 23:54:43 MetaFG_0] (main.py 265): INFO Train: [11/300][710/1562]	eta 0:04:19 lr 0.000004	time 0.2933 (0.3047)	loss 2.1115 (1.8432)	grad_norm 23.9992 (nan)	mem 4879MB
[2022-05-30 23:54:46 MetaFG_0] (main.py 265): INFO Train: [11/300][720/1562]	eta 0:04:16 lr 0.000004	time 0.2920 (0.3047)	loss 1.9080 (1.8435)	grad_norm 14.3602 (nan)	mem 4879MB
[2022-05-30 23:54:49 MetaFG_0] (main.py 265): INFO Train: [11/300][730/1562]	eta 0:04:13 lr 0.000004	time 0.2930 (0.3047)	loss 1.9565 (1.8438)	grad_norm 31.6766 (nan)	mem 4879MB
[2022-05-30 23:54:52 MetaFG_0] (main.py 265): INFO Train: [11/300][740/1562]	eta 0:04:10 lr 0.000004	time 0.2974 (0.3047)	loss 2.1124 (1.8437)	grad_norm 16.1623 (nan)	mem 4879MB
[2022-05-30 23:54:55 MetaFG_0] (main.py 265): INFO Train: [11/300][750/1562]	eta 0:04:07 lr 0.000004	time 0.2927 (0.3047)	loss 1.8738 (1.8437)	grad_norm 11.1125 (nan)	mem 4879MB
[2022-05-30 23:54:58 MetaFG_0] (main.py 265): INFO Train: [11/300][760/1562]	eta 0:04:04 lr 0.000004	time 0.2930 (0.3047)	loss 1.7925 (1.8434)	grad_norm 15.6579 (nan)	mem 4879MB
[2022-05-30 23:55:02 MetaFG_0] (main.py 265): INFO Train: [11/300][770/1562]	eta 0:04:01 lr 0.000004	time 0.2993 (0.3047)	loss 1.7572 (1.8427)	grad_norm 30.2139 (nan)	mem 4879MB
[2022-05-30 23:55:05 MetaFG_0] (main.py 265): INFO Train: [11/300][780/1562]	eta 0:03:58 lr 0.000004	time 0.2915 (0.3047)	loss 1.8504 (1.8423)	grad_norm 13.1244 (nan)	mem 4879MB
[2022-05-30 23:55:08 MetaFG_0] (main.py 265): INFO Train: [11/300][790/1562]	eta 0:03:55 lr 0.000004	time 0.2928 (0.3047)	loss 1.8343 (1.8427)	grad_norm 11.5072 (nan)	mem 4879MB
[2022-05-30 23:55:11 MetaFG_0] (main.py 265): INFO Train: [11/300][800/1562]	eta 0:03:52 lr 0.000004	time 0.2927 (0.3047)	loss 1.5802 (1.8413)	grad_norm 18.5335 (nan)	mem 4879MB
[2022-05-30 23:55:14 MetaFG_0] (main.py 265): INFO Train: [11/300][810/1562]	eta 0:03:49 lr 0.000004	time 0.2990 (0.3047)	loss 1.9496 (1.8413)	grad_norm 20.8188 (nan)	mem 4879MB
[2022-05-30 23:55:17 MetaFG_0] (main.py 265): INFO Train: [11/300][820/1562]	eta 0:03:46 lr 0.000004	time 0.2962 (0.3047)	loss 1.8466 (1.8410)	grad_norm 20.7698 (nan)	mem 4879MB
[2022-05-30 23:55:20 MetaFG_0] (main.py 265): INFO Train: [11/300][830/1562]	eta 0:03:43 lr 0.000004	time 0.2935 (0.3046)	loss 1.8843 (1.8410)	grad_norm 11.8195 (nan)	mem 4879MB
[2022-05-30 23:55:23 MetaFG_0] (main.py 265): INFO Train: [11/300][840/1562]	eta 0:03:39 lr 0.000004	time 0.2985 (0.3046)	loss 1.9112 (1.8411)	grad_norm 33.2101 (nan)	mem 4879MB
[2022-05-30 23:55:26 MetaFG_0] (main.py 265): INFO Train: [11/300][850/1562]	eta 0:03:36 lr 0.000004	time 0.2932 (0.3046)	loss 1.7328 (1.8402)	grad_norm 13.7806 (nan)	mem 4879MB
[2022-05-30 23:55:29 MetaFG_0] (main.py 265): INFO Train: [11/300][860/1562]	eta 0:03:33 lr 0.000004	time 0.2920 (0.3046)	loss 1.8507 (1.8401)	grad_norm 15.0965 (nan)	mem 4879MB
[2022-05-30 23:55:32 MetaFG_0] (main.py 265): INFO Train: [11/300][870/1562]	eta 0:03:30 lr 0.000004	time 0.2930 (0.3046)	loss 1.8629 (1.8398)	grad_norm 19.2912 (nan)	mem 4879MB
[2022-05-30 23:55:35 MetaFG_0] (main.py 265): INFO Train: [11/300][880/1562]	eta 0:03:27 lr 0.000004	time 0.2932 (0.3046)	loss 1.7843 (1.8389)	grad_norm 15.7738 (nan)	mem 4879MB
[2022-05-30 23:55:38 MetaFG_0] (main.py 265): INFO Train: [11/300][890/1562]	eta 0:03:24 lr 0.000004	time 0.2974 (0.3046)	loss 1.9758 (1.8391)	grad_norm 20.5303 (nan)	mem 4879MB
[2022-05-30 23:55:41 MetaFG_0] (main.py 265): INFO Train: [11/300][900/1562]	eta 0:03:21 lr 0.000004	time 0.2922 (0.3046)	loss 1.8765 (1.8396)	grad_norm 21.4516 (nan)	mem 4879MB
[2022-05-30 23:55:44 MetaFG_0] (main.py 265): INFO Train: [11/300][910/1562]	eta 0:03:18 lr 0.000004	time 0.2991 (0.3046)	loss 1.9318 (1.8396)	grad_norm 17.5812 (nan)	mem 4879MB
[2022-05-30 23:55:47 MetaFG_0] (main.py 265): INFO Train: [11/300][920/1562]	eta 0:03:15 lr 0.000004	time 0.2938 (0.3046)	loss 2.0308 (1.8410)	grad_norm 17.6705 (nan)	mem 4879MB
[2022-05-30 23:55:50 MetaFG_0] (main.py 265): INFO Train: [11/300][930/1562]	eta 0:03:12 lr 0.000004	time 0.2982 (0.3046)	loss 1.9592 (1.8407)	grad_norm 20.6090 (nan)	mem 4879MB
[2022-05-30 23:55:53 MetaFG_0] (main.py 265): INFO Train: [11/300][940/1562]	eta 0:03:09 lr 0.000004	time 0.2978 (0.3046)	loss 1.7564 (1.8397)	grad_norm 15.1283 (nan)	mem 4879MB
[2022-05-30 23:55:56 MetaFG_0] (main.py 265): INFO Train: [11/300][950/1562]	eta 0:03:06 lr 0.000004	time 0.2985 (0.3045)	loss 1.7991 (1.8395)	grad_norm 32.9248 (nan)	mem 4879MB
[2022-05-30 23:55:59 MetaFG_0] (main.py 265): INFO Train: [11/300][960/1562]	eta 0:03:03 lr 0.000004	time 0.2971 (0.3045)	loss 1.9970 (1.8391)	grad_norm 16.8250 (nan)	mem 4879MB
[2022-05-30 23:56:02 MetaFG_0] (main.py 265): INFO Train: [11/300][970/1562]	eta 0:03:00 lr 0.000004	time 0.2990 (0.3045)	loss 1.9445 (1.8392)	grad_norm 11.0008 (nan)	mem 4879MB
[2022-05-30 23:56:05 MetaFG_0] (main.py 265): INFO Train: [11/300][980/1562]	eta 0:02:57 lr 0.000004	time 0.2976 (0.3045)	loss 1.9653 (1.8387)	grad_norm 15.6532 (nan)	mem 4879MB
[2022-05-30 23:56:08 MetaFG_0] (main.py 265): INFO Train: [11/300][990/1562]	eta 0:02:54 lr 0.000004	time 0.2977 (0.3045)	loss 1.8911 (1.8390)	grad_norm 11.3414 (nan)	mem 4879MB
[2022-05-30 23:56:11 MetaFG_0] (main.py 265): INFO Train: [11/300][1000/1562]	eta 0:02:51 lr 0.000004	time 0.2955 (0.3045)	loss 1.7238 (1.8388)	grad_norm 20.7383 (nan)	mem 4879MB
[2022-05-30 23:56:14 MetaFG_0] (main.py 265): INFO Train: [11/300][1010/1562]	eta 0:02:48 lr 0.000004	time 0.2941 (0.3045)	loss 1.8149 (1.8385)	grad_norm 18.9966 (nan)	mem 4879MB
[2022-05-30 23:56:17 MetaFG_0] (main.py 265): INFO Train: [11/300][1020/1562]	eta 0:02:45 lr 0.000004	time 0.3049 (0.3045)	loss 1.7200 (1.8385)	grad_norm 20.4382 (nan)	mem 4879MB
[2022-05-30 23:56:21 MetaFG_0] (main.py 265): INFO Train: [11/300][1030/1562]	eta 0:02:41 lr 0.000004	time 0.2934 (0.3045)	loss 1.7515 (1.8387)	grad_norm 17.3234 (nan)	mem 4879MB
[2022-05-30 23:56:24 MetaFG_0] (main.py 265): INFO Train: [11/300][1040/1562]	eta 0:02:38 lr 0.000004	time 0.2916 (0.3045)	loss 1.9520 (1.8386)	grad_norm 23.5872 (nan)	mem 4879MB
[2022-05-30 23:56:27 MetaFG_0] (main.py 265): INFO Train: [11/300][1050/1562]	eta 0:02:35 lr 0.000004	time 0.2915 (0.3044)	loss 2.0254 (1.8390)	grad_norm 25.8423 (nan)	mem 4879MB
[2022-05-30 23:56:30 MetaFG_0] (main.py 265): INFO Train: [11/300][1060/1562]	eta 0:02:32 lr 0.000004	time 0.2936 (0.3044)	loss 1.9515 (1.8394)	grad_norm 12.3966 (nan)	mem 4879MB
[2022-05-30 23:56:33 MetaFG_0] (main.py 265): INFO Train: [11/300][1070/1562]	eta 0:02:29 lr 0.000004	time 0.2981 (0.3044)	loss 1.9395 (1.8398)	grad_norm 18.9881 (nan)	mem 4879MB
[2022-05-30 23:56:36 MetaFG_0] (main.py 265): INFO Train: [11/300][1080/1562]	eta 0:02:26 lr 0.000004	time 0.2978 (0.3045)	loss 1.7506 (1.8402)	grad_norm 14.4804 (nan)	mem 4879MB
[2022-05-30 23:56:39 MetaFG_0] (main.py 265): INFO Train: [11/300][1090/1562]	eta 0:02:23 lr 0.000004	time 0.2918 (0.3045)	loss 1.8622 (1.8407)	grad_norm 15.0365 (nan)	mem 4879MB
[2022-05-30 23:56:42 MetaFG_0] (main.py 265): INFO Train: [11/300][1100/1562]	eta 0:02:20 lr 0.000004	time 0.2947 (0.3045)	loss 1.9212 (1.8410)	grad_norm 14.3383 (nan)	mem 4879MB
[2022-05-30 23:56:45 MetaFG_0] (main.py 265): INFO Train: [11/300][1110/1562]	eta 0:02:17 lr 0.000004	time 0.3040 (0.3045)	loss 1.5539 (1.8410)	grad_norm 20.4928 (nan)	mem 4879MB
[2022-05-30 23:56:48 MetaFG_0] (main.py 265): INFO Train: [11/300][1120/1562]	eta 0:02:14 lr 0.000004	time 0.2917 (0.3044)	loss 1.6614 (1.8408)	grad_norm 22.3013 (nan)	mem 4879MB
[2022-05-30 23:56:51 MetaFG_0] (main.py 265): INFO Train: [11/300][1130/1562]	eta 0:02:11 lr 0.000004	time 0.2924 (0.3045)	loss 1.6532 (1.8403)	grad_norm 16.9750 (nan)	mem 4879MB
[2022-05-30 23:56:54 MetaFG_0] (main.py 265): INFO Train: [11/300][1140/1562]	eta 0:02:08 lr 0.000004	time 0.2929 (0.3044)	loss 1.9733 (1.8407)	grad_norm 14.3894 (nan)	mem 4879MB
[2022-05-30 23:56:57 MetaFG_0] (main.py 265): INFO Train: [11/300][1150/1562]	eta 0:02:05 lr 0.000004	time 0.2932 (0.3044)	loss 1.6174 (1.8405)	grad_norm 40.2395 (nan)	mem 4879MB
[2022-05-30 23:57:00 MetaFG_0] (main.py 265): INFO Train: [11/300][1160/1562]	eta 0:02:02 lr 0.000004	time 0.2920 (0.3044)	loss 2.0307 (1.8405)	grad_norm 15.4754 (nan)	mem 4879MB
[2022-05-30 23:57:03 MetaFG_0] (main.py 265): INFO Train: [11/300][1170/1562]	eta 0:01:59 lr 0.000004	time 0.2998 (0.3044)	loss 1.5929 (1.8402)	grad_norm 38.6538 (nan)	mem 4879MB
[2022-05-30 23:57:06 MetaFG_0] (main.py 265): INFO Train: [11/300][1180/1562]	eta 0:01:56 lr 0.000004	time 0.2994 (0.3044)	loss 1.6278 (1.8398)	grad_norm 33.3521 (nan)	mem 4879MB
[2022-05-30 23:57:09 MetaFG_0] (main.py 265): INFO Train: [11/300][1190/1562]	eta 0:01:53 lr 0.000004	time 0.2931 (0.3044)	loss 1.9161 (1.8394)	grad_norm 14.4290 (nan)	mem 4879MB
[2022-05-30 23:57:12 MetaFG_0] (main.py 265): INFO Train: [11/300][1200/1562]	eta 0:01:50 lr 0.000004	time 0.2977 (0.3044)	loss 1.6097 (1.8391)	grad_norm 42.0307 (nan)	mem 4879MB
[2022-05-30 23:57:15 MetaFG_0] (main.py 265): INFO Train: [11/300][1210/1562]	eta 0:01:47 lr 0.000004	time 0.2931 (0.3044)	loss 2.0009 (1.8396)	grad_norm 15.7169 (nan)	mem 4879MB
[2022-05-30 23:57:18 MetaFG_0] (main.py 265): INFO Train: [11/300][1220/1562]	eta 0:01:44 lr 0.000004	time 0.2946 (0.3044)	loss 2.0211 (1.8395)	grad_norm 19.6969 (nan)	mem 4879MB
[2022-05-30 23:57:21 MetaFG_0] (main.py 265): INFO Train: [11/300][1230/1562]	eta 0:01:41 lr 0.000004	time 0.2931 (0.3043)	loss 1.8355 (1.8397)	grad_norm 21.6733 (nan)	mem 4879MB
[2022-05-30 23:57:24 MetaFG_0] (main.py 265): INFO Train: [11/300][1240/1562]	eta 0:01:37 lr 0.000004	time 0.2981 (0.3043)	loss 1.8985 (1.8396)	grad_norm 12.1834 (nan)	mem 4879MB
[2022-05-30 23:57:27 MetaFG_0] (main.py 265): INFO Train: [11/300][1250/1562]	eta 0:01:34 lr 0.000004	time 0.2980 (0.3043)	loss 2.0284 (1.8389)	grad_norm 18.2446 (nan)	mem 4879MB
[2022-05-30 23:57:30 MetaFG_0] (main.py 265): INFO Train: [11/300][1260/1562]	eta 0:01:31 lr 0.000004	time 0.2922 (0.3043)	loss 1.6647 (1.8380)	grad_norm 24.9890 (nan)	mem 4879MB
[2022-05-30 23:57:33 MetaFG_0] (main.py 265): INFO Train: [11/300][1270/1562]	eta 0:01:28 lr 0.000004	time 0.2930 (0.3043)	loss 1.4500 (1.8374)	grad_norm 18.9352 (nan)	mem 4879MB
[2022-05-30 23:57:36 MetaFG_0] (main.py 265): INFO Train: [11/300][1280/1562]	eta 0:01:25 lr 0.000004	time 0.2994 (0.3043)	loss 1.6497 (1.8369)	grad_norm 35.4591 (nan)	mem 4879MB
[2022-05-30 23:57:39 MetaFG_0] (main.py 265): INFO Train: [11/300][1290/1562]	eta 0:01:22 lr 0.000004	time 0.2948 (0.3043)	loss 1.8695 (1.8367)	grad_norm 14.1392 (nan)	mem 4879MB
[2022-05-30 23:57:43 MetaFG_0] (main.py 265): INFO Train: [11/300][1300/1562]	eta 0:01:19 lr 0.000004	time 0.2921 (0.3043)	loss 1.6696 (1.8365)	grad_norm 23.3192 (nan)	mem 4879MB
[2022-05-30 23:57:46 MetaFG_0] (main.py 265): INFO Train: [11/300][1310/1562]	eta 0:01:16 lr 0.000004	time 0.2918 (0.3043)	loss 1.8081 (1.8365)	grad_norm 28.3592 (nan)	mem 4879MB
[2022-05-30 23:57:49 MetaFG_0] (main.py 265): INFO Train: [11/300][1320/1562]	eta 0:01:13 lr 0.000004	time 0.2931 (0.3043)	loss 1.8988 (1.8370)	grad_norm 12.5930 (nan)	mem 4879MB
[2022-05-30 23:57:52 MetaFG_0] (main.py 265): INFO Train: [11/300][1330/1562]	eta 0:01:10 lr 0.000004	time 0.2927 (0.3043)	loss 1.7825 (1.8368)	grad_norm 13.7498 (nan)	mem 4879MB
[2022-05-30 23:57:55 MetaFG_0] (main.py 265): INFO Train: [11/300][1340/1562]	eta 0:01:07 lr 0.000004	time 0.2917 (0.3043)	loss 1.6987 (1.8369)	grad_norm 28.9420 (nan)	mem 4879MB
[2022-05-30 23:57:58 MetaFG_0] (main.py 265): INFO Train: [11/300][1350/1562]	eta 0:01:04 lr 0.000004	time 0.2983 (0.3043)	loss 1.5563 (1.8367)	grad_norm 15.5233 (nan)	mem 4879MB
[2022-05-30 23:58:01 MetaFG_0] (main.py 265): INFO Train: [11/300][1360/1562]	eta 0:01:01 lr 0.000004	time 0.2934 (0.3043)	loss 1.7393 (1.8357)	grad_norm 14.9626 (nan)	mem 4879MB
[2022-05-30 23:58:04 MetaFG_0] (main.py 265): INFO Train: [11/300][1370/1562]	eta 0:00:58 lr 0.000004	time 0.2925 (0.3043)	loss 2.0454 (1.8355)	grad_norm 21.0144 (nan)	mem 4879MB
[2022-05-30 23:58:07 MetaFG_0] (main.py 265): INFO Train: [11/300][1380/1562]	eta 0:00:55 lr 0.000004	time 0.2981 (0.3043)	loss 1.9391 (1.8355)	grad_norm 18.1487 (nan)	mem 4879MB
[2022-05-30 23:58:10 MetaFG_0] (main.py 265): INFO Train: [11/300][1390/1562]	eta 0:00:52 lr 0.000004	time 0.2985 (0.3043)	loss 1.7159 (1.8355)	grad_norm 22.5575 (nan)	mem 4879MB
[2022-05-30 23:58:13 MetaFG_0] (main.py 265): INFO Train: [11/300][1400/1562]	eta 0:00:49 lr 0.000004	time 0.2932 (0.3043)	loss 1.7419 (1.8349)	grad_norm 11.9073 (nan)	mem 4879MB
[2022-05-30 23:58:16 MetaFG_0] (main.py 265): INFO Train: [11/300][1410/1562]	eta 0:00:46 lr 0.000004	time 0.2932 (0.3043)	loss 1.8280 (1.8345)	grad_norm 16.8868 (nan)	mem 4879MB
[2022-05-30 23:58:19 MetaFG_0] (main.py 265): INFO Train: [11/300][1420/1562]	eta 0:00:43 lr 0.000004	time 0.2990 (0.3043)	loss 1.8879 (1.8345)	grad_norm 17.0493 (nan)	mem 4879MB
[2022-05-30 23:58:22 MetaFG_0] (main.py 265): INFO Train: [11/300][1430/1562]	eta 0:00:40 lr 0.000004	time 0.2979 (0.3043)	loss 1.3967 (1.8341)	grad_norm 13.7049 (nan)	mem 4879MB
[2022-05-30 23:58:25 MetaFG_0] (main.py 265): INFO Train: [11/300][1440/1562]	eta 0:00:37 lr 0.000004	time 0.2932 (0.3043)	loss 1.6295 (1.8334)	grad_norm 22.4203 (nan)	mem 4879MB
[2022-05-30 23:58:28 MetaFG_0] (main.py 265): INFO Train: [11/300][1450/1562]	eta 0:00:34 lr 0.000004	time 0.2916 (0.3043)	loss 1.9403 (1.8329)	grad_norm 19.9706 (nan)	mem 4879MB
[2022-05-30 23:58:31 MetaFG_0] (main.py 265): INFO Train: [11/300][1460/1562]	eta 0:00:31 lr 0.000004	time 0.2919 (0.3043)	loss 1.8778 (1.8332)	grad_norm 22.5001 (nan)	mem 4879MB
[2022-05-30 23:58:34 MetaFG_0] (main.py 265): INFO Train: [11/300][1470/1562]	eta 0:00:27 lr 0.000004	time 0.2955 (0.3043)	loss 1.8686 (1.8329)	grad_norm 25.3884 (nan)	mem 4879MB
[2022-05-30 23:58:37 MetaFG_0] (main.py 265): INFO Train: [11/300][1480/1562]	eta 0:00:24 lr 0.000004	time 0.2992 (0.3043)	loss 1.8190 (1.8329)	grad_norm 19.0705 (nan)	mem 4879MB
[2022-05-30 23:58:40 MetaFG_0] (main.py 265): INFO Train: [11/300][1490/1562]	eta 0:00:21 lr 0.000004	time 0.2924 (0.3043)	loss 1.9212 (1.8330)	grad_norm 17.3583 (nan)	mem 4879MB
[2022-05-30 23:58:43 MetaFG_0] (main.py 265): INFO Train: [11/300][1500/1562]	eta 0:00:18 lr 0.000004	time 0.2990 (0.3043)	loss 1.9174 (1.8329)	grad_norm 21.0030 (nan)	mem 4879MB
[2022-05-30 23:58:46 MetaFG_0] (main.py 265): INFO Train: [11/300][1510/1562]	eta 0:00:15 lr 0.000004	time 0.2930 (0.3043)	loss 1.8381 (1.8330)	grad_norm 17.2182 (nan)	mem 4879MB
[2022-05-30 23:58:49 MetaFG_0] (main.py 265): INFO Train: [11/300][1520/1562]	eta 0:00:12 lr 0.000004	time 0.3337 (0.3043)	loss 1.5932 (1.8329)	grad_norm 19.3578 (nan)	mem 4879MB
[2022-05-30 23:58:53 MetaFG_0] (main.py 265): INFO Train: [11/300][1530/1562]	eta 0:00:09 lr 0.000004	time 0.2978 (0.3043)	loss 1.6561 (1.8322)	grad_norm 11.5849 (nan)	mem 4879MB
[2022-05-30 23:58:56 MetaFG_0] (main.py 265): INFO Train: [11/300][1540/1562]	eta 0:00:06 lr 0.000004	time 0.2974 (0.3043)	loss 1.9961 (1.8320)	grad_norm 19.7243 (nan)	mem 4879MB
[2022-05-30 23:58:59 MetaFG_0] (main.py 265): INFO Train: [11/300][1550/1562]	eta 0:00:03 lr 0.000004	time 0.2943 (0.3043)	loss 1.4929 (1.8315)	grad_norm 21.3127 (nan)	mem 4879MB
[2022-05-30 23:59:02 MetaFG_0] (main.py 265): INFO Train: [11/300][1560/1562]	eta 0:00:00 lr 0.000004	time 0.2915 (0.3043)	loss 1.6378 (1.8313)	grad_norm 17.9857 (nan)	mem 4879MB
[2022-05-30 23:59:02 MetaFG_0] (main.py 272): INFO EPOCH 11 training takes 0:07:55
[2022-05-30 23:59:02 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_11.pth saving......
[2022-05-30 23:59:03 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_11.pth saved !!!
[2022-05-30 23:59:03 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-30 23:59:04 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-30 23:59:04 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-30 23:59:05 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.729 (0.729)	Loss 2.4272 (2.4272)	Acc@1 40.625 (40.625)	Acc@5 81.250 (81.250)	Mem 4879MB
[2022-05-30 23:59:06 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.158)	Loss 2.0489 (2.1562)	Acc@1 56.250 (57.102)	Acc@5 96.875 (88.068)	Mem 4879MB
[2022-05-30 23:59:07 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.089 (0.127)	Loss 2.1018 (2.1444)	Acc@1 62.500 (58.036)	Acc@5 81.250 (88.095)	Mem 4879MB
[2022-05-30 23:59:08 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.097 (0.116)	Loss 2.3767 (2.1659)	Acc@1 53.125 (57.964)	Acc@5 87.500 (87.702)	Mem 4879MB
[2022-05-30 23:59:09 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.111)	Loss 2.3243 (2.1463)	Acc@1 43.750 (58.384)	Acc@5 81.250 (88.034)	Mem 4879MB
[2022-05-30 23:59:10 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.091 (0.108)	Loss 2.0580 (2.1474)	Acc@1 62.500 (58.456)	Acc@5 93.750 (88.174)	Mem 4879MB
[2022-05-30 23:59:11 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.097 (0.106)	Loss 2.2153 (2.1455)	Acc@1 59.375 (58.094)	Acc@5 84.375 (88.064)	Mem 4879MB
[2022-05-30 23:59:12 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.104)	Loss 2.2487 (2.1415)	Acc@1 56.250 (58.055)	Acc@5 84.375 (88.116)	Mem 4879MB
[2022-05-30 23:59:13 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.103)	Loss 2.1889 (2.1546)	Acc@1 53.125 (57.793)	Acc@5 90.625 (88.117)	Mem 4879MB
[2022-05-30 23:59:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.102)	Loss 2.1414 (2.1514)	Acc@1 65.625 (58.688)	Acc@5 87.500 (88.187)	Mem 4879MB
[2022-05-30 23:59:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.101)	Loss 2.2681 (2.1423)	Acc@1 40.625 (58.323)	Acc@5 68.750 (88.212)	Mem 4879MB
[2022-05-30 23:59:15 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 1.8847 (2.1362)	Acc@1 65.625 (58.446)	Acc@5 96.875 (88.598)	Mem 4879MB
[2022-05-30 23:59:16 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.099)	Loss 2.2607 (2.1468)	Acc@1 43.750 (57.825)	Acc@5 87.500 (88.559)	Mem 4879MB
[2022-05-30 23:59:17 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 2.1873 (2.1449)	Acc@1 62.500 (58.039)	Acc@5 93.750 (88.550)	Mem 4879MB
[2022-05-30 23:59:18 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 1.9379 (2.1430)	Acc@1 65.625 (57.934)	Acc@5 87.500 (88.564)	Mem 4879MB
[2022-05-30 23:59:19 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.099)	Loss 2.1784 (2.1442)	Acc@1 56.250 (57.657)	Acc@5 90.625 (88.535)	Mem 4879MB
[2022-05-30 23:59:20 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.102 (0.098)	Loss 2.0453 (2.1404)	Acc@1 62.500 (57.803)	Acc@5 87.500 (88.568)	Mem 4879MB
[2022-05-30 23:59:21 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 1.7558 (2.1403)	Acc@1 81.250 (57.803)	Acc@5 96.875 (88.724)	Mem 4879MB
[2022-05-30 23:59:22 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 1.9764 (2.1388)	Acc@1 75.000 (58.011)	Acc@5 100.000 (88.760)	Mem 4879MB
[2022-05-30 23:59:23 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 2.2287 (2.1365)	Acc@1 50.000 (58.033)	Acc@5 84.375 (88.825)	Mem 4879MB
[2022-05-30 23:59:24 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 2.3427 (2.1369)	Acc@1 50.000 (58.053)	Acc@5 93.750 (88.868)	Mem 4879MB
[2022-05-30 23:59:25 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.097)	Loss 2.2626 (2.1406)	Acc@1 62.500 (58.012)	Acc@5 90.625 (88.848)	Mem 4879MB
[2022-05-30 23:59:26 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 2.1572 (2.1405)	Acc@1 59.375 (58.003)	Acc@5 84.375 (88.843)	Mem 4879MB
[2022-05-30 23:59:27 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.090 (0.097)	Loss 2.1119 (2.1417)	Acc@1 59.375 (57.955)	Acc@5 96.875 (88.893)	Mem 4879MB
[2022-05-30 23:59:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 2.1066 (2.1417)	Acc@1 65.625 (58.065)	Acc@5 93.750 (89.043)	Mem 4879MB
[2022-05-30 23:59:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 2.3202 (2.1418)	Acc@1 56.250 (58.030)	Acc@5 84.375 (88.969)	Mem 4879MB
[2022-05-30 23:59:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 2.2119 (2.1438)	Acc@1 53.125 (57.890)	Acc@5 90.625 (88.901)	Mem 4879MB
[2022-05-30 23:59:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.100 (0.096)	Loss 2.5856 (2.1451)	Acc@1 34.375 (57.784)	Acc@5 81.250 (88.849)	Mem 4879MB
[2022-05-30 23:59:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 2.2485 (2.1462)	Acc@1 65.625 (57.829)	Acc@5 90.625 (88.757)	Mem 4879MB
[2022-05-30 23:59:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.093 (0.096)	Loss 1.9806 (2.1444)	Acc@1 59.375 (57.882)	Acc@5 87.500 (88.832)	Mem 4879MB
[2022-05-30 23:59:33 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 1.9040 (2.1427)	Acc@1 65.625 (58.046)	Acc@5 96.875 (88.860)	Mem 4879MB
[2022-05-30 23:59:34 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 2.0909 (2.1425)	Acc@1 50.000 (58.049)	Acc@5 90.625 (88.907)	Mem 4879MB
[2022-05-30 23:59:34 MetaFG_0] (main.py 330): INFO  * Acc@1 58.110 Acc@5 88.900
[2022-05-30 23:59:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 58.1%
[2022-05-30 23:59:34 MetaFG_0] (main.py 171): INFO Max accuracy: 58.11%
[2022-05-30 23:59:35 MetaFG_0] (main.py 265): INFO Train: [12/300][0/1562]	eta 0:26:33 lr 0.000004	time 1.0199 (1.0199)	loss 1.7284 (1.7284)	grad_norm 14.8433 (14.8433)	mem 4879MB
[2022-05-30 23:59:38 MetaFG_0] (main.py 265): INFO Train: [12/300][10/1562]	eta 0:09:41 lr 0.000004	time 0.2944 (0.3746)	loss 1.9037 (1.7626)	grad_norm 28.1695 (21.0773)	mem 4879MB
[2022-05-30 23:59:41 MetaFG_0] (main.py 265): INFO Train: [12/300][20/1562]	eta 0:08:44 lr 0.000004	time 0.2975 (0.3404)	loss 1.8296 (1.7718)	grad_norm 13.5540 (20.0418)	mem 4879MB
[2022-05-30 23:59:44 MetaFG_0] (main.py 265): INFO Train: [12/300][30/1562]	eta 0:08:23 lr 0.000004	time 0.2929 (0.3285)	loss 1.8602 (1.8031)	grad_norm 17.1666 (18.4667)	mem 4879MB
[2022-05-30 23:59:47 MetaFG_0] (main.py 265): INFO Train: [12/300][40/1562]	eta 0:08:09 lr 0.000004	time 0.2919 (0.3219)	loss 1.9195 (1.8093)	grad_norm 11.3738 (18.2470)	mem 4879MB
[2022-05-30 23:59:50 MetaFG_0] (main.py 265): INFO Train: [12/300][50/1562]	eta 0:08:01 lr 0.000004	time 0.2917 (0.3184)	loss 1.8109 (1.8059)	grad_norm 19.8860 (18.1459)	mem 4879MB
[2022-05-30 23:59:54 MetaFG_0] (main.py 265): INFO Train: [12/300][60/1562]	eta 0:07:54 lr 0.000004	time 0.2916 (0.3157)	loss 1.7340 (1.8046)	grad_norm 15.2353 (18.6903)	mem 4879MB
[2022-05-30 23:59:57 MetaFG_0] (main.py 265): INFO Train: [12/300][70/1562]	eta 0:07:48 lr 0.000004	time 0.2970 (0.3139)	loss 1.8738 (1.8080)	grad_norm 20.5000 (18.6540)	mem 4879MB
[2022-05-31 00:00:00 MetaFG_0] (main.py 265): INFO Train: [12/300][80/1562]	eta 0:07:43 lr 0.000004	time 0.2934 (0.3124)	loss 1.6904 (1.8009)	grad_norm 20.8230 (18.7373)	mem 4879MB
[2022-05-31 00:00:03 MetaFG_0] (main.py 265): INFO Train: [12/300][90/1562]	eta 0:07:38 lr 0.000004	time 0.2970 (0.3115)	loss 2.0315 (1.8000)	grad_norm 16.1524 (19.0044)	mem 4879MB
[2022-05-31 00:00:06 MetaFG_0] (main.py 265): INFO Train: [12/300][100/1562]	eta 0:07:34 lr 0.000004	time 0.2925 (0.3106)	loss 1.5485 (1.7917)	grad_norm 21.5214 (19.0455)	mem 4879MB
[2022-05-31 00:00:09 MetaFG_0] (main.py 265): INFO Train: [12/300][110/1562]	eta 0:07:29 lr 0.000004	time 0.2924 (0.3098)	loss 1.8504 (1.7902)	grad_norm 17.5324 (19.4593)	mem 4879MB
[2022-05-31 00:00:12 MetaFG_0] (main.py 265): INFO Train: [12/300][120/1562]	eta 0:07:25 lr 0.000004	time 0.2958 (0.3092)	loss 1.7792 (1.7967)	grad_norm 14.0260 (19.5352)	mem 4879MB
[2022-05-31 00:00:15 MetaFG_0] (main.py 265): INFO Train: [12/300][130/1562]	eta 0:07:22 lr 0.000004	time 0.2978 (0.3087)	loss 1.9220 (1.7956)	grad_norm 19.8258 (19.6457)	mem 4879MB
[2022-05-31 00:00:18 MetaFG_0] (main.py 265): INFO Train: [12/300][140/1562]	eta 0:07:18 lr 0.000004	time 0.2921 (0.3083)	loss 1.8738 (1.7971)	grad_norm 18.7403 (19.5923)	mem 4879MB
[2022-05-31 00:00:21 MetaFG_0] (main.py 265): INFO Train: [12/300][150/1562]	eta 0:07:14 lr 0.000004	time 0.2917 (0.3081)	loss 1.9316 (1.8019)	grad_norm 17.1706 (19.4610)	mem 4879MB
[2022-05-31 00:00:24 MetaFG_0] (main.py 265): INFO Train: [12/300][160/1562]	eta 0:07:11 lr 0.000004	time 0.2969 (0.3077)	loss 1.8222 (1.8027)	grad_norm 27.7650 (19.5066)	mem 4879MB
[2022-05-31 00:00:27 MetaFG_0] (main.py 265): INFO Train: [12/300][170/1562]	eta 0:07:07 lr 0.000004	time 0.2930 (0.3074)	loss 2.0904 (1.8005)	grad_norm 19.9427 (19.4057)	mem 4879MB
[2022-05-31 00:00:30 MetaFG_0] (main.py 265): INFO Train: [12/300][180/1562]	eta 0:07:04 lr 0.000004	time 0.2935 (0.3072)	loss 1.9365 (1.8007)	grad_norm 20.4485 (19.3203)	mem 4879MB
[2022-05-31 00:00:33 MetaFG_0] (main.py 265): INFO Train: [12/300][190/1562]	eta 0:07:01 lr 0.000004	time 0.2931 (0.3069)	loss 1.7537 (1.8002)	grad_norm 25.3951 (19.2011)	mem 4879MB
[2022-05-31 00:00:36 MetaFG_0] (main.py 265): INFO Train: [12/300][200/1562]	eta 0:06:57 lr 0.000004	time 0.2986 (0.3069)	loss 1.8194 (1.7994)	grad_norm 24.6775 (19.4123)	mem 4879MB
[2022-05-31 00:00:39 MetaFG_0] (main.py 265): INFO Train: [12/300][210/1562]	eta 0:06:54 lr 0.000004	time 0.2974 (0.3066)	loss 1.6103 (1.7963)	grad_norm 12.7151 (19.3835)	mem 4879MB
[2022-05-31 00:00:42 MetaFG_0] (main.py 265): INFO Train: [12/300][220/1562]	eta 0:06:51 lr 0.000004	time 0.3041 (0.3065)	loss 1.8559 (1.7947)	grad_norm 23.0656 (19.5207)	mem 4879MB
[2022-05-31 00:00:45 MetaFG_0] (main.py 265): INFO Train: [12/300][230/1562]	eta 0:06:48 lr 0.000004	time 0.2981 (0.3064)	loss 1.9397 (1.7972)	grad_norm 13.7048 (19.4569)	mem 4879MB
[2022-05-31 00:00:48 MetaFG_0] (main.py 265): INFO Train: [12/300][240/1562]	eta 0:06:44 lr 0.000004	time 0.2993 (0.3063)	loss 1.8875 (1.7982)	grad_norm 20.9299 (19.5423)	mem 4879MB
[2022-05-31 00:00:51 MetaFG_0] (main.py 265): INFO Train: [12/300][250/1562]	eta 0:06:41 lr 0.000004	time 0.2915 (0.3061)	loss 1.8801 (1.7969)	grad_norm 12.5403 (19.5748)	mem 4879MB
[2022-05-31 00:00:54 MetaFG_0] (main.py 265): INFO Train: [12/300][260/1562]	eta 0:06:38 lr 0.000004	time 0.2968 (0.3060)	loss 2.0416 (1.7971)	grad_norm 34.6912 (19.6249)	mem 4879MB
[2022-05-31 00:00:57 MetaFG_0] (main.py 265): INFO Train: [12/300][270/1562]	eta 0:06:35 lr 0.000004	time 0.2942 (0.3059)	loss 1.7360 (1.7975)	grad_norm 15.0068 (19.6262)	mem 4879MB
[2022-05-31 00:01:00 MetaFG_0] (main.py 265): INFO Train: [12/300][280/1562]	eta 0:06:32 lr 0.000004	time 0.3132 (0.3060)	loss 1.7412 (1.7980)	grad_norm 23.2833 (19.5814)	mem 4879MB
[2022-05-31 00:01:03 MetaFG_0] (main.py 265): INFO Train: [12/300][290/1562]	eta 0:06:29 lr 0.000004	time 0.3026 (0.3061)	loss 1.7977 (1.7964)	grad_norm 12.5699 (19.7526)	mem 4879MB
[2022-05-31 00:01:06 MetaFG_0] (main.py 265): INFO Train: [12/300][300/1562]	eta 0:06:26 lr 0.000004	time 0.2986 (0.3060)	loss 1.7073 (1.7940)	grad_norm 14.0309 (19.7828)	mem 4879MB
[2022-05-31 00:01:09 MetaFG_0] (main.py 265): INFO Train: [12/300][310/1562]	eta 0:06:23 lr 0.000004	time 0.2936 (0.3059)	loss 2.0402 (1.7959)	grad_norm 11.4162 (19.7329)	mem 4879MB
[2022-05-31 00:01:12 MetaFG_0] (main.py 265): INFO Train: [12/300][320/1562]	eta 0:06:19 lr 0.000004	time 0.2994 (0.3058)	loss 1.8987 (1.7946)	grad_norm 10.6304 (19.7590)	mem 4879MB
[2022-05-31 00:01:15 MetaFG_0] (main.py 265): INFO Train: [12/300][330/1562]	eta 0:06:16 lr 0.000004	time 0.2968 (0.3058)	loss 1.8825 (1.7947)	grad_norm 23.9455 (19.8096)	mem 4879MB
[2022-05-31 00:01:19 MetaFG_0] (main.py 265): INFO Train: [12/300][340/1562]	eta 0:06:13 lr 0.000004	time 0.2938 (0.3057)	loss 1.9793 (1.7980)	grad_norm 20.3626 (19.9598)	mem 4879MB
[2022-05-31 00:01:22 MetaFG_0] (main.py 265): INFO Train: [12/300][350/1562]	eta 0:06:10 lr 0.000004	time 0.2943 (0.3057)	loss 1.9833 (1.7959)	grad_norm 21.9554 (19.9883)	mem 4879MB
[2022-05-31 00:01:25 MetaFG_0] (main.py 265): INFO Train: [12/300][360/1562]	eta 0:06:07 lr 0.000004	time 0.2975 (0.3057)	loss 2.0168 (1.7994)	grad_norm 16.6038 (19.9382)	mem 4879MB
[2022-05-31 00:01:28 MetaFG_0] (main.py 265): INFO Train: [12/300][370/1562]	eta 0:06:04 lr 0.000004	time 0.2981 (0.3056)	loss 1.9078 (1.8009)	grad_norm 20.0015 (19.8826)	mem 4879MB
[2022-05-31 00:01:31 MetaFG_0] (main.py 265): INFO Train: [12/300][380/1562]	eta 0:06:01 lr 0.000004	time 0.2919 (0.3055)	loss 1.5309 (1.7997)	grad_norm 26.3078 (19.8646)	mem 4879MB
[2022-05-31 00:01:34 MetaFG_0] (main.py 265): INFO Train: [12/300][390/1562]	eta 0:05:58 lr 0.000004	time 0.2975 (0.3055)	loss 1.7911 (1.7962)	grad_norm 21.9788 (19.9408)	mem 4879MB
[2022-05-31 00:01:37 MetaFG_0] (main.py 265): INFO Train: [12/300][400/1562]	eta 0:05:54 lr 0.000004	time 0.2933 (0.3055)	loss 1.8897 (1.7961)	grad_norm 14.7111 (19.9366)	mem 4879MB
[2022-05-31 00:01:40 MetaFG_0] (main.py 265): INFO Train: [12/300][410/1562]	eta 0:05:51 lr 0.000004	time 0.2978 (0.3054)	loss 1.7613 (1.7961)	grad_norm 29.4493 (19.9044)	mem 4879MB
[2022-05-31 00:01:43 MetaFG_0] (main.py 265): INFO Train: [12/300][420/1562]	eta 0:05:48 lr 0.000004	time 0.2993 (0.3054)	loss 1.9256 (1.7951)	grad_norm 11.6808 (19.9179)	mem 4879MB
[2022-05-31 00:01:46 MetaFG_0] (main.py 265): INFO Train: [12/300][430/1562]	eta 0:05:45 lr 0.000004	time 0.2927 (0.3054)	loss 1.9157 (1.7965)	grad_norm 19.9028 (19.8918)	mem 4879MB
[2022-05-31 00:01:49 MetaFG_0] (main.py 265): INFO Train: [12/300][440/1562]	eta 0:05:42 lr 0.000004	time 0.2921 (0.3053)	loss 2.0020 (1.7979)	grad_norm 21.8584 (19.8866)	mem 4879MB
[2022-05-31 00:01:52 MetaFG_0] (main.py 265): INFO Train: [12/300][450/1562]	eta 0:05:39 lr 0.000004	time 0.2921 (0.3053)	loss 1.6793 (1.7973)	grad_norm 14.3436 (19.8969)	mem 4879MB
[2022-05-31 00:01:55 MetaFG_0] (main.py 265): INFO Train: [12/300][460/1562]	eta 0:05:36 lr 0.000004	time 0.2994 (0.3053)	loss 1.7975 (1.7974)	grad_norm 20.2476 (19.8652)	mem 4879MB
[2022-05-31 00:01:58 MetaFG_0] (main.py 265): INFO Train: [12/300][470/1562]	eta 0:05:33 lr 0.000004	time 0.2924 (0.3052)	loss 1.8691 (1.7980)	grad_norm 13.8040 (19.8441)	mem 4879MB
[2022-05-31 00:02:01 MetaFG_0] (main.py 265): INFO Train: [12/300][480/1562]	eta 0:05:30 lr 0.000004	time 0.3005 (0.3052)	loss 1.9649 (1.7989)	grad_norm 14.0568 (19.8967)	mem 4879MB
[2022-05-31 00:02:04 MetaFG_0] (main.py 265): INFO Train: [12/300][490/1562]	eta 0:05:27 lr 0.000004	time 0.2918 (0.3052)	loss 1.6736 (1.7999)	grad_norm 22.6082 (19.8878)	mem 4879MB
[2022-05-31 00:02:07 MetaFG_0] (main.py 265): INFO Train: [12/300][500/1562]	eta 0:05:24 lr 0.000004	time 0.2925 (0.3051)	loss 1.7220 (1.7993)	grad_norm 16.5029 (19.9355)	mem 4879MB
[2022-05-31 00:02:10 MetaFG_0] (main.py 265): INFO Train: [12/300][510/1562]	eta 0:05:20 lr 0.000004	time 0.2928 (0.3051)	loss 1.9189 (1.7988)	grad_norm 19.0082 (19.9665)	mem 4879MB
[2022-05-31 00:02:13 MetaFG_0] (main.py 265): INFO Train: [12/300][520/1562]	eta 0:05:17 lr 0.000004	time 0.2950 (0.3051)	loss 1.8823 (1.7980)	grad_norm 31.4732 (20.0437)	mem 4879MB
[2022-05-31 00:02:16 MetaFG_0] (main.py 265): INFO Train: [12/300][530/1562]	eta 0:05:14 lr 0.000004	time 0.2939 (0.3051)	loss 2.0982 (1.7974)	grad_norm 29.2291 (20.3283)	mem 4879MB
[2022-05-31 00:02:19 MetaFG_0] (main.py 265): INFO Train: [12/300][540/1562]	eta 0:05:11 lr 0.000004	time 0.2929 (0.3050)	loss 1.6521 (1.7962)	grad_norm 19.0456 (20.3838)	mem 4879MB
[2022-05-31 00:02:22 MetaFG_0] (main.py 265): INFO Train: [12/300][550/1562]	eta 0:05:08 lr 0.000004	time 0.2929 (0.3050)	loss 1.9406 (1.7965)	grad_norm 25.4929 (20.3963)	mem 4879MB
[2022-05-31 00:02:25 MetaFG_0] (main.py 265): INFO Train: [12/300][560/1562]	eta 0:05:05 lr 0.000004	time 0.2932 (0.3050)	loss 1.8160 (1.7968)	grad_norm 22.6139 (20.3849)	mem 4879MB
[2022-05-31 00:02:28 MetaFG_0] (main.py 265): INFO Train: [12/300][570/1562]	eta 0:05:02 lr 0.000004	time 0.2943 (0.3050)	loss 1.6666 (1.7969)	grad_norm 18.7040 (20.3208)	mem 4879MB
[2022-05-31 00:02:31 MetaFG_0] (main.py 265): INFO Train: [12/300][580/1562]	eta 0:04:59 lr 0.000004	time 0.2943 (0.3049)	loss 1.8028 (1.7962)	grad_norm 18.2526 (20.3183)	mem 4879MB
[2022-05-31 00:02:34 MetaFG_0] (main.py 265): INFO Train: [12/300][590/1562]	eta 0:04:56 lr 0.000004	time 0.2928 (0.3049)	loss 1.6587 (1.7959)	grad_norm 25.4458 (20.3620)	mem 4879MB
[2022-05-31 00:02:37 MetaFG_0] (main.py 265): INFO Train: [12/300][600/1562]	eta 0:04:53 lr 0.000004	time 0.2971 (0.3049)	loss 1.9208 (1.7976)	grad_norm 18.6744 (20.3957)	mem 4879MB
[2022-05-31 00:02:41 MetaFG_0] (main.py 265): INFO Train: [12/300][610/1562]	eta 0:04:50 lr 0.000004	time 0.2917 (0.3049)	loss 1.9092 (1.7976)	grad_norm 18.8226 (20.4444)	mem 4879MB
[2022-05-31 00:02:44 MetaFG_0] (main.py 265): INFO Train: [12/300][620/1562]	eta 0:04:47 lr 0.000004	time 0.2933 (0.3048)	loss 1.8138 (1.7984)	grad_norm 13.9850 (20.3823)	mem 4879MB
[2022-05-31 00:02:47 MetaFG_0] (main.py 265): INFO Train: [12/300][630/1562]	eta 0:04:44 lr 0.000004	time 0.2920 (0.3048)	loss 1.9692 (1.7974)	grad_norm 19.5147 (20.4032)	mem 4879MB
[2022-05-31 00:02:50 MetaFG_0] (main.py 265): INFO Train: [12/300][640/1562]	eta 0:04:40 lr 0.000004	time 0.2932 (0.3047)	loss 1.8177 (1.7958)	grad_norm 25.5585 (20.4088)	mem 4879MB
[2022-05-31 00:02:53 MetaFG_0] (main.py 265): INFO Train: [12/300][650/1562]	eta 0:04:37 lr 0.000004	time 0.2976 (0.3047)	loss 1.4989 (1.7943)	grad_norm 20.0820 (20.5005)	mem 4879MB
[2022-05-31 00:02:56 MetaFG_0] (main.py 265): INFO Train: [12/300][660/1562]	eta 0:04:34 lr 0.000004	time 0.2921 (0.3047)	loss 1.8548 (1.7945)	grad_norm 21.0122 (20.5028)	mem 4879MB
[2022-05-31 00:02:59 MetaFG_0] (main.py 265): INFO Train: [12/300][670/1562]	eta 0:04:31 lr 0.000004	time 0.2931 (0.3047)	loss 1.8923 (1.7945)	grad_norm 27.2363 (20.5259)	mem 4879MB
[2022-05-31 00:03:02 MetaFG_0] (main.py 265): INFO Train: [12/300][680/1562]	eta 0:04:28 lr 0.000004	time 0.2930 (0.3047)	loss 1.9588 (1.7956)	grad_norm 17.8071 (20.4958)	mem 4879MB
[2022-05-31 00:03:05 MetaFG_0] (main.py 265): INFO Train: [12/300][690/1562]	eta 0:04:25 lr 0.000004	time 0.2931 (0.3047)	loss 1.6734 (1.7952)	grad_norm 25.7039 (20.5125)	mem 4879MB
[2022-05-31 00:03:08 MetaFG_0] (main.py 265): INFO Train: [12/300][700/1562]	eta 0:04:22 lr 0.000004	time 0.2977 (0.3047)	loss 1.6118 (1.7965)	grad_norm 24.5561 (20.5077)	mem 4879MB
[2022-05-31 00:03:11 MetaFG_0] (main.py 265): INFO Train: [12/300][710/1562]	eta 0:04:19 lr 0.000004	time 0.2933 (0.3047)	loss 1.8196 (1.7963)	grad_norm 25.1447 (20.4977)	mem 4879MB
[2022-05-31 00:03:14 MetaFG_0] (main.py 265): INFO Train: [12/300][720/1562]	eta 0:04:16 lr 0.000004	time 0.2984 (0.3047)	loss 1.5504 (1.7958)	grad_norm 18.5767 (20.4540)	mem 4879MB
[2022-05-31 00:03:17 MetaFG_0] (main.py 265): INFO Train: [12/300][730/1562]	eta 0:04:13 lr 0.000004	time 0.3041 (0.3048)	loss 1.8051 (1.7959)	grad_norm 19.0589 (20.4589)	mem 4879MB
[2022-05-31 00:03:20 MetaFG_0] (main.py 265): INFO Train: [12/300][740/1562]	eta 0:04:10 lr 0.000004	time 0.2930 (0.3048)	loss 1.6566 (1.7963)	grad_norm 13.4313 (20.4119)	mem 4879MB
[2022-05-31 00:03:23 MetaFG_0] (main.py 265): INFO Train: [12/300][750/1562]	eta 0:04:07 lr 0.000004	time 0.3023 (0.3048)	loss 1.8912 (1.7962)	grad_norm 19.2270 (20.4339)	mem 4879MB
[2022-05-31 00:03:26 MetaFG_0] (main.py 265): INFO Train: [12/300][760/1562]	eta 0:04:04 lr 0.000004	time 0.2926 (0.3048)	loss 1.5449 (1.7954)	grad_norm 26.7032 (20.4187)	mem 4879MB
[2022-05-31 00:03:29 MetaFG_0] (main.py 265): INFO Train: [12/300][770/1562]	eta 0:04:01 lr 0.000004	time 0.2954 (0.3047)	loss 1.8480 (1.7947)	grad_norm 13.1305 (20.4131)	mem 4879MB
[2022-05-31 00:03:32 MetaFG_0] (main.py 265): INFO Train: [12/300][780/1562]	eta 0:03:58 lr 0.000004	time 0.2921 (0.3047)	loss 1.9409 (1.7953)	grad_norm 24.4839 (20.4401)	mem 4879MB
[2022-05-31 00:03:35 MetaFG_0] (main.py 265): INFO Train: [12/300][790/1562]	eta 0:03:55 lr 0.000004	time 0.2920 (0.3047)	loss 1.9181 (1.7952)	grad_norm 18.6112 (20.4406)	mem 4879MB
[2022-05-31 00:03:38 MetaFG_0] (main.py 265): INFO Train: [12/300][800/1562]	eta 0:03:52 lr 0.000004	time 0.2994 (0.3047)	loss 1.7005 (1.7954)	grad_norm 19.1232 (20.4490)	mem 4879MB
[2022-05-31 00:03:41 MetaFG_0] (main.py 265): INFO Train: [12/300][810/1562]	eta 0:03:49 lr 0.000004	time 0.3004 (0.3047)	loss 1.8935 (1.7951)	grad_norm 17.7100 (20.4343)	mem 4879MB
[2022-05-31 00:03:44 MetaFG_0] (main.py 265): INFO Train: [12/300][820/1562]	eta 0:03:46 lr 0.000004	time 0.3053 (0.3047)	loss 2.0546 (1.7952)	grad_norm 20.8939 (20.4055)	mem 4879MB
[2022-05-31 00:03:47 MetaFG_0] (main.py 265): INFO Train: [12/300][830/1562]	eta 0:03:42 lr 0.000004	time 0.2983 (0.3046)	loss 1.8183 (1.7958)	grad_norm 16.8063 (20.3848)	mem 4879MB
[2022-05-31 00:03:50 MetaFG_0] (main.py 265): INFO Train: [12/300][840/1562]	eta 0:03:39 lr 0.000004	time 0.2937 (0.3046)	loss 1.5280 (1.7956)	grad_norm 23.6654 (20.3999)	mem 4879MB
[2022-05-31 00:03:54 MetaFG_0] (main.py 265): INFO Train: [12/300][850/1562]	eta 0:03:36 lr 0.000004	time 0.2919 (0.3046)	loss 1.8138 (1.7955)	grad_norm 17.0921 (20.3778)	mem 4879MB
[2022-05-31 00:03:57 MetaFG_0] (main.py 265): INFO Train: [12/300][860/1562]	eta 0:03:33 lr 0.000004	time 0.2987 (0.3046)	loss 1.6933 (1.7955)	grad_norm 15.7849 (20.3925)	mem 4879MB
[2022-05-31 00:04:00 MetaFG_0] (main.py 265): INFO Train: [12/300][870/1562]	eta 0:03:30 lr 0.000004	time 0.2921 (0.3046)	loss 1.7581 (1.7952)	grad_norm 19.7036 (20.3631)	mem 4879MB
[2022-05-31 00:04:03 MetaFG_0] (main.py 265): INFO Train: [12/300][880/1562]	eta 0:03:27 lr 0.000004	time 0.2917 (0.3046)	loss 1.9482 (1.7957)	grad_norm 17.3495 (20.3603)	mem 4879MB
[2022-05-31 00:04:06 MetaFG_0] (main.py 265): INFO Train: [12/300][890/1562]	eta 0:03:24 lr 0.000004	time 0.3012 (0.3046)	loss 1.9263 (1.7956)	grad_norm 13.6774 (20.3701)	mem 4879MB
[2022-05-31 00:04:09 MetaFG_0] (main.py 265): INFO Train: [12/300][900/1562]	eta 0:03:21 lr 0.000004	time 0.2991 (0.3046)	loss 1.7639 (1.7949)	grad_norm 18.0951 (20.3831)	mem 4879MB
[2022-05-31 00:04:12 MetaFG_0] (main.py 265): INFO Train: [12/300][910/1562]	eta 0:03:18 lr 0.000004	time 0.2981 (0.3046)	loss 2.0479 (1.7946)	grad_norm 18.3324 (20.4159)	mem 4879MB
[2022-05-31 00:04:15 MetaFG_0] (main.py 265): INFO Train: [12/300][920/1562]	eta 0:03:15 lr 0.000004	time 0.2931 (0.3046)	loss 1.5082 (1.7943)	grad_norm 30.5877 (20.4024)	mem 4879MB
[2022-05-31 00:04:18 MetaFG_0] (main.py 265): INFO Train: [12/300][930/1562]	eta 0:03:12 lr 0.000004	time 0.2989 (0.3046)	loss 1.8007 (1.7941)	grad_norm 13.7591 (20.4033)	mem 4879MB
[2022-05-31 00:04:21 MetaFG_0] (main.py 265): INFO Train: [12/300][940/1562]	eta 0:03:09 lr 0.000004	time 0.2928 (0.3046)	loss 2.0090 (1.7938)	grad_norm 20.1493 (20.4090)	mem 4879MB
[2022-05-31 00:04:24 MetaFG_0] (main.py 265): INFO Train: [12/300][950/1562]	eta 0:03:06 lr 0.000004	time 0.2920 (0.3046)	loss 1.9410 (1.7929)	grad_norm 22.2122 (20.4069)	mem 4879MB
[2022-05-31 00:04:27 MetaFG_0] (main.py 265): INFO Train: [12/300][960/1562]	eta 0:03:03 lr 0.000004	time 0.2918 (0.3046)	loss 1.9077 (1.7934)	grad_norm 17.2023 (20.4067)	mem 4879MB
[2022-05-31 00:04:30 MetaFG_0] (main.py 265): INFO Train: [12/300][970/1562]	eta 0:03:00 lr 0.000004	time 0.2978 (0.3046)	loss 1.7134 (1.7925)	grad_norm 22.0820 (20.4140)	mem 4879MB
[2022-05-31 00:04:33 MetaFG_0] (main.py 265): INFO Train: [12/300][980/1562]	eta 0:02:57 lr 0.000004	time 0.2923 (0.3046)	loss 2.0421 (1.7930)	grad_norm 25.9858 (20.4040)	mem 4879MB
[2022-05-31 00:04:36 MetaFG_0] (main.py 265): INFO Train: [12/300][990/1562]	eta 0:02:54 lr 0.000004	time 0.2954 (0.3046)	loss 1.7007 (1.7932)	grad_norm 17.2125 (20.3869)	mem 4879MB
[2022-05-31 00:04:39 MetaFG_0] (main.py 265): INFO Train: [12/300][1000/1562]	eta 0:02:51 lr 0.000004	time 0.2920 (0.3045)	loss 1.7809 (1.7935)	grad_norm 12.9033 (20.3958)	mem 4879MB
[2022-05-31 00:04:42 MetaFG_0] (main.py 265): INFO Train: [12/300][1010/1562]	eta 0:02:48 lr 0.000004	time 0.2927 (0.3046)	loss 1.8261 (1.7934)	grad_norm 23.1087 (20.3957)	mem 4879MB
[2022-05-31 00:04:45 MetaFG_0] (main.py 265): INFO Train: [12/300][1020/1562]	eta 0:02:45 lr 0.000004	time 0.2998 (0.3046)	loss 2.0100 (1.7936)	grad_norm 17.0429 (20.3615)	mem 4879MB
[2022-05-31 00:04:48 MetaFG_0] (main.py 265): INFO Train: [12/300][1030/1562]	eta 0:02:42 lr 0.000004	time 0.2988 (0.3046)	loss 1.5150 (1.7930)	grad_norm 19.1165 (20.3535)	mem 4879MB
[2022-05-31 00:04:51 MetaFG_0] (main.py 265): INFO Train: [12/300][1040/1562]	eta 0:02:38 lr 0.000004	time 0.2918 (0.3046)	loss 1.5153 (1.7927)	grad_norm 23.7042 (20.3316)	mem 4879MB
[2022-05-31 00:04:54 MetaFG_0] (main.py 265): INFO Train: [12/300][1050/1562]	eta 0:02:35 lr 0.000004	time 0.2916 (0.3046)	loss 1.4907 (1.7919)	grad_norm 27.6923 (20.3400)	mem 4879MB
[2022-05-31 00:04:57 MetaFG_0] (main.py 265): INFO Train: [12/300][1060/1562]	eta 0:02:32 lr 0.000004	time 0.2970 (0.3045)	loss 1.7979 (1.7921)	grad_norm 17.6531 (20.3210)	mem 4879MB
[2022-05-31 00:05:00 MetaFG_0] (main.py 265): INFO Train: [12/300][1070/1562]	eta 0:02:29 lr 0.000004	time 0.2938 (0.3045)	loss 1.9219 (1.7908)	grad_norm 19.5085 (20.3134)	mem 4879MB
[2022-05-31 00:05:03 MetaFG_0] (main.py 265): INFO Train: [12/300][1080/1562]	eta 0:02:26 lr 0.000004	time 0.2917 (0.3045)	loss 1.5362 (1.7907)	grad_norm 18.7727 (20.3405)	mem 4879MB
[2022-05-31 00:05:06 MetaFG_0] (main.py 265): INFO Train: [12/300][1090/1562]	eta 0:02:23 lr 0.000004	time 0.2945 (0.3045)	loss 1.9553 (1.7907)	grad_norm 22.4894 (20.3453)	mem 4879MB
[2022-05-31 00:05:10 MetaFG_0] (main.py 265): INFO Train: [12/300][1100/1562]	eta 0:02:20 lr 0.000004	time 0.2991 (0.3045)	loss 1.9424 (1.7909)	grad_norm 23.6215 (20.3461)	mem 4879MB
[2022-05-31 00:05:13 MetaFG_0] (main.py 265): INFO Train: [12/300][1110/1562]	eta 0:02:17 lr 0.000004	time 0.2946 (0.3045)	loss 2.0836 (1.7902)	grad_norm 14.5120 (20.3910)	mem 4879MB
[2022-05-31 00:05:16 MetaFG_0] (main.py 265): INFO Train: [12/300][1120/1562]	eta 0:02:14 lr 0.000004	time 0.2918 (0.3045)	loss 1.5539 (1.7905)	grad_norm 15.8467 (20.3810)	mem 4879MB
[2022-05-31 00:05:19 MetaFG_0] (main.py 265): INFO Train: [12/300][1130/1562]	eta 0:02:11 lr 0.000004	time 0.2938 (0.3045)	loss 1.9947 (1.7905)	grad_norm 25.6031 (20.3847)	mem 4879MB
[2022-05-31 00:05:22 MetaFG_0] (main.py 265): INFO Train: [12/300][1140/1562]	eta 0:02:08 lr 0.000004	time 0.2989 (0.3045)	loss 1.5678 (1.7906)	grad_norm 11.4725 (20.3800)	mem 4879MB
[2022-05-31 00:05:25 MetaFG_0] (main.py 265): INFO Train: [12/300][1150/1562]	eta 0:02:05 lr 0.000004	time 0.2914 (0.3045)	loss 1.5912 (1.7904)	grad_norm 19.3544 (20.3706)	mem 4879MB
[2022-05-31 00:05:28 MetaFG_0] (main.py 265): INFO Train: [12/300][1160/1562]	eta 0:02:02 lr 0.000004	time 0.2933 (0.3045)	loss 1.6002 (1.7905)	grad_norm 24.7076 (20.3940)	mem 4879MB
[2022-05-31 00:05:31 MetaFG_0] (main.py 265): INFO Train: [12/300][1170/1562]	eta 0:01:59 lr 0.000004	time 0.2975 (0.3045)	loss 2.0431 (1.7902)	grad_norm 16.0390 (20.3884)	mem 4879MB
[2022-05-31 00:05:34 MetaFG_0] (main.py 265): INFO Train: [12/300][1180/1562]	eta 0:01:56 lr 0.000004	time 0.2978 (0.3045)	loss 1.9708 (1.7904)	grad_norm 21.1434 (20.4126)	mem 4879MB
[2022-05-31 00:05:37 MetaFG_0] (main.py 265): INFO Train: [12/300][1190/1562]	eta 0:01:53 lr 0.000004	time 0.2929 (0.3045)	loss 1.7836 (1.7906)	grad_norm 19.6617 (20.4266)	mem 4879MB
[2022-05-31 00:05:40 MetaFG_0] (main.py 265): INFO Train: [12/300][1200/1562]	eta 0:01:50 lr 0.000004	time 0.2974 (0.3045)	loss 1.6142 (1.7906)	grad_norm 19.3583 (20.4479)	mem 4879MB
[2022-05-31 00:05:43 MetaFG_0] (main.py 265): INFO Train: [12/300][1210/1562]	eta 0:01:47 lr 0.000004	time 0.2935 (0.3045)	loss 1.9366 (1.7906)	grad_norm 23.0503 (20.4525)	mem 4879MB
[2022-05-31 00:05:46 MetaFG_0] (main.py 265): INFO Train: [12/300][1220/1562]	eta 0:01:44 lr 0.000004	time 0.2997 (0.3045)	loss 1.8495 (1.7907)	grad_norm 20.1796 (20.4408)	mem 4879MB
[2022-05-31 00:05:49 MetaFG_0] (main.py 265): INFO Train: [12/300][1230/1562]	eta 0:01:41 lr 0.000004	time 0.2967 (0.3045)	loss 1.9222 (1.7902)	grad_norm 37.3519 (20.4362)	mem 4879MB
[2022-05-31 00:05:52 MetaFG_0] (main.py 265): INFO Train: [12/300][1240/1562]	eta 0:01:38 lr 0.000004	time 0.3013 (0.3045)	loss 1.6286 (1.7903)	grad_norm 29.4916 (20.4767)	mem 4879MB
[2022-05-31 00:05:55 MetaFG_0] (main.py 265): INFO Train: [12/300][1250/1562]	eta 0:01:35 lr 0.000004	time 0.2989 (0.3045)	loss 1.7682 (1.7902)	grad_norm 9.1402 (20.4867)	mem 4879MB
[2022-05-31 00:05:58 MetaFG_0] (main.py 265): INFO Train: [12/300][1260/1562]	eta 0:01:31 lr 0.000004	time 0.2949 (0.3045)	loss 1.8261 (1.7900)	grad_norm 19.1337 (20.4701)	mem 4879MB
[2022-05-31 00:06:01 MetaFG_0] (main.py 265): INFO Train: [12/300][1270/1562]	eta 0:01:28 lr 0.000004	time 0.3020 (0.3045)	loss 1.7820 (1.7894)	grad_norm 19.4410 (20.4950)	mem 4879MB
[2022-05-31 00:06:04 MetaFG_0] (main.py 265): INFO Train: [12/300][1280/1562]	eta 0:01:25 lr 0.000004	time 0.2920 (0.3045)	loss 1.9190 (1.7895)	grad_norm 16.7022 (20.5301)	mem 4879MB
[2022-05-31 00:06:07 MetaFG_0] (main.py 265): INFO Train: [12/300][1290/1562]	eta 0:01:22 lr 0.000004	time 0.2985 (0.3045)	loss 2.0197 (1.7904)	grad_norm 24.2139 (20.5121)	mem 4879MB
[2022-05-31 00:06:10 MetaFG_0] (main.py 265): INFO Train: [12/300][1300/1562]	eta 0:01:19 lr 0.000004	time 0.2978 (0.3045)	loss 1.8473 (1.7903)	grad_norm 19.0926 (20.5097)	mem 4879MB
[2022-05-31 00:06:13 MetaFG_0] (main.py 265): INFO Train: [12/300][1310/1562]	eta 0:01:16 lr 0.000004	time 0.2950 (0.3045)	loss 1.7152 (1.7904)	grad_norm 24.3048 (20.5099)	mem 4879MB
[2022-05-31 00:06:16 MetaFG_0] (main.py 265): INFO Train: [12/300][1320/1562]	eta 0:01:13 lr 0.000004	time 0.2939 (0.3045)	loss 1.8548 (1.7906)	grad_norm 10.7126 (20.5117)	mem 4879MB
[2022-05-31 00:06:20 MetaFG_0] (main.py 265): INFO Train: [12/300][1330/1562]	eta 0:01:10 lr 0.000004	time 0.2982 (0.3045)	loss 1.5758 (1.7902)	grad_norm 21.4226 (20.5368)	mem 4879MB
[2022-05-31 00:06:23 MetaFG_0] (main.py 265): INFO Train: [12/300][1340/1562]	eta 0:01:07 lr 0.000004	time 0.2979 (0.3045)	loss 1.7054 (1.7901)	grad_norm 22.8230 (20.5433)	mem 4879MB
[2022-05-31 00:06:26 MetaFG_0] (main.py 265): INFO Train: [12/300][1350/1562]	eta 0:01:04 lr 0.000004	time 0.3003 (0.3045)	loss 1.7497 (1.7902)	grad_norm 12.1627 (20.5460)	mem 4879MB
[2022-05-31 00:06:29 MetaFG_0] (main.py 265): INFO Train: [12/300][1360/1562]	eta 0:01:01 lr 0.000004	time 0.2996 (0.3045)	loss 1.6318 (1.7900)	grad_norm 17.8631 (20.5665)	mem 4879MB
[2022-05-31 00:06:32 MetaFG_0] (main.py 265): INFO Train: [12/300][1370/1562]	eta 0:00:58 lr 0.000004	time 0.2977 (0.3045)	loss 1.7405 (1.7894)	grad_norm 20.0122 (20.5647)	mem 4879MB
[2022-05-31 00:06:35 MetaFG_0] (main.py 265): INFO Train: [12/300][1380/1562]	eta 0:00:55 lr 0.000004	time 0.2979 (0.3045)	loss 1.9391 (1.7887)	grad_norm 17.9336 (20.5632)	mem 4879MB
[2022-05-31 00:06:38 MetaFG_0] (main.py 265): INFO Train: [12/300][1390/1562]	eta 0:00:52 lr 0.000004	time 0.2935 (0.3045)	loss 1.7494 (1.7884)	grad_norm 27.2351 (20.5880)	mem 4879MB
[2022-05-31 00:06:41 MetaFG_0] (main.py 265): INFO Train: [12/300][1400/1562]	eta 0:00:49 lr 0.000004	time 0.3018 (0.3045)	loss 1.5240 (1.7880)	grad_norm 23.4150 (20.5870)	mem 4879MB
[2022-05-31 00:06:44 MetaFG_0] (main.py 265): INFO Train: [12/300][1410/1562]	eta 0:00:46 lr 0.000004	time 0.2918 (0.3045)	loss 1.7649 (1.7877)	grad_norm 12.6611 (20.5717)	mem 4879MB
[2022-05-31 00:06:47 MetaFG_0] (main.py 265): INFO Train: [12/300][1420/1562]	eta 0:00:43 lr 0.000004	time 0.2930 (0.3045)	loss 1.5242 (1.7874)	grad_norm 27.3470 (20.5799)	mem 4879MB
[2022-05-31 00:06:50 MetaFG_0] (main.py 265): INFO Train: [12/300][1430/1562]	eta 0:00:40 lr 0.000004	time 0.2974 (0.3045)	loss 1.6611 (1.7873)	grad_norm 14.2490 (20.5755)	mem 4879MB
[2022-05-31 00:06:53 MetaFG_0] (main.py 265): INFO Train: [12/300][1440/1562]	eta 0:00:37 lr 0.000004	time 0.2934 (0.3045)	loss 1.8754 (1.7865)	grad_norm 19.7377 (20.5859)	mem 4879MB
[2022-05-31 00:06:56 MetaFG_0] (main.py 265): INFO Train: [12/300][1450/1562]	eta 0:00:34 lr 0.000004	time 0.2999 (0.3045)	loss 1.6218 (1.7863)	grad_norm 36.5214 (20.6085)	mem 4879MB
[2022-05-31 00:06:59 MetaFG_0] (main.py 265): INFO Train: [12/300][1460/1562]	eta 0:00:31 lr 0.000004	time 0.2995 (0.3045)	loss 1.8376 (1.7859)	grad_norm 13.3997 (20.6000)	mem 4879MB
[2022-05-31 00:07:02 MetaFG_0] (main.py 265): INFO Train: [12/300][1470/1562]	eta 0:00:28 lr 0.000004	time 0.2919 (0.3045)	loss 1.7844 (1.7857)	grad_norm 15.9818 (20.6170)	mem 4879MB
[2022-05-31 00:07:05 MetaFG_0] (main.py 265): INFO Train: [12/300][1480/1562]	eta 0:00:24 lr 0.000004	time 0.2974 (0.3045)	loss 1.7900 (1.7859)	grad_norm 19.7106 (20.6317)	mem 4879MB
[2022-05-31 00:07:08 MetaFG_0] (main.py 265): INFO Train: [12/300][1490/1562]	eta 0:00:21 lr 0.000004	time 0.2998 (0.3045)	loss 1.9350 (1.7856)	grad_norm 19.5149 (20.6230)	mem 4879MB
[2022-05-31 00:07:11 MetaFG_0] (main.py 265): INFO Train: [12/300][1500/1562]	eta 0:00:18 lr 0.000004	time 0.2997 (0.3045)	loss 1.8183 (1.7856)	grad_norm 14.0012 (20.6272)	mem 4879MB
[2022-05-31 00:07:14 MetaFG_0] (main.py 265): INFO Train: [12/300][1510/1562]	eta 0:00:15 lr 0.000004	time 0.2989 (0.3045)	loss 1.5155 (1.7848)	grad_norm 17.2686 (20.6440)	mem 4879MB
[2022-05-31 00:07:17 MetaFG_0] (main.py 265): INFO Train: [12/300][1520/1562]	eta 0:00:12 lr 0.000004	time 0.3008 (0.3045)	loss 1.7269 (1.7847)	grad_norm 18.1640 (20.6338)	mem 4879MB
[2022-05-31 00:07:20 MetaFG_0] (main.py 265): INFO Train: [12/300][1530/1562]	eta 0:00:09 lr 0.000004	time 0.2979 (0.3045)	loss 1.5923 (1.7848)	grad_norm 16.8243 (20.6195)	mem 4879MB
[2022-05-31 00:07:23 MetaFG_0] (main.py 265): INFO Train: [12/300][1540/1562]	eta 0:00:06 lr 0.000004	time 0.2923 (0.3045)	loss 1.8241 (1.7850)	grad_norm 18.5200 (20.6173)	mem 4879MB
[2022-05-31 00:07:26 MetaFG_0] (main.py 265): INFO Train: [12/300][1550/1562]	eta 0:00:03 lr 0.000004	time 0.2937 (0.3045)	loss 1.8783 (1.7852)	grad_norm 14.6735 (20.6172)	mem 4879MB
[2022-05-31 00:07:30 MetaFG_0] (main.py 265): INFO Train: [12/300][1560/1562]	eta 0:00:00 lr 0.000004	time 0.2921 (0.3045)	loss 1.7201 (1.7850)	grad_norm 13.1937 (20.6002)	mem 4879MB
[2022-05-31 00:07:30 MetaFG_0] (main.py 272): INFO EPOCH 12 training takes 0:07:55
[2022-05-31 00:07:30 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_12.pth saving......
[2022-05-31 00:07:31 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_12.pth saved !!!
[2022-05-31 00:07:31 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:07:32 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:07:32 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:07:33 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.634 (0.634)	Loss 1.8355 (1.8355)	Acc@1 68.750 (68.750)	Acc@5 84.375 (84.375)	Mem 4879MB
[2022-05-31 00:07:34 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.143)	Loss 2.1903 (1.8838)	Acc@1 56.250 (64.205)	Acc@5 87.500 (92.045)	Mem 4879MB
[2022-05-31 00:07:35 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.099 (0.120)	Loss 1.9353 (1.8586)	Acc@1 62.500 (64.435)	Acc@5 96.875 (92.560)	Mem 4879MB
[2022-05-31 00:07:36 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.111)	Loss 2.1821 (1.8613)	Acc@1 56.250 (63.810)	Acc@5 81.250 (92.440)	Mem 4879MB
[2022-05-31 00:07:37 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.107)	Loss 1.8025 (1.8889)	Acc@1 71.875 (62.729)	Acc@5 96.875 (91.692)	Mem 4879MB
[2022-05-31 00:07:38 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.097 (0.104)	Loss 1.7361 (1.8952)	Acc@1 71.875 (62.316)	Acc@5 93.750 (91.728)	Mem 4879MB
[2022-05-31 00:07:39 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.102)	Loss 2.0038 (1.8992)	Acc@1 65.625 (62.807)	Acc@5 87.500 (91.957)	Mem 4879MB
[2022-05-31 00:07:40 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.102 (0.101)	Loss 1.9761 (1.9018)	Acc@1 62.500 (63.072)	Acc@5 93.750 (92.033)	Mem 4879MB
[2022-05-31 00:07:41 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.100)	Loss 1.8297 (1.9024)	Acc@1 68.750 (63.002)	Acc@5 87.500 (92.052)	Mem 4879MB
[2022-05-31 00:07:42 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.100)	Loss 1.8611 (1.9060)	Acc@1 75.000 (62.843)	Acc@5 96.875 (91.930)	Mem 4879MB
[2022-05-31 00:07:42 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.103 (0.099)	Loss 2.2142 (1.9093)	Acc@1 43.750 (62.531)	Acc@5 84.375 (91.986)	Mem 4879MB
[2022-05-31 00:07:43 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.098)	Loss 1.8739 (1.9170)	Acc@1 68.750 (61.937)	Acc@5 90.625 (91.864)	Mem 4879MB
[2022-05-31 00:07:44 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.099 (0.098)	Loss 2.1686 (1.9184)	Acc@1 56.250 (61.777)	Acc@5 87.500 (91.761)	Mem 4879MB
[2022-05-31 00:07:45 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.103 (0.098)	Loss 1.9382 (1.9250)	Acc@1 65.625 (61.689)	Acc@5 87.500 (91.436)	Mem 4879MB
[2022-05-31 00:07:46 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.097)	Loss 1.9123 (1.9344)	Acc@1 56.250 (61.281)	Acc@5 87.500 (91.157)	Mem 4879MB
[2022-05-31 00:07:47 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.089 (0.097)	Loss 2.1222 (1.9386)	Acc@1 50.000 (61.258)	Acc@5 96.875 (91.204)	Mem 4879MB
[2022-05-31 00:07:48 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.106 (0.098)	Loss 1.9963 (1.9400)	Acc@1 65.625 (61.471)	Acc@5 84.375 (91.168)	Mem 4879MB
[2022-05-31 00:07:49 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 2.0895 (1.9425)	Acc@1 65.625 (61.458)	Acc@5 87.500 (91.173)	Mem 4879MB
[2022-05-31 00:07:50 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.092 (0.098)	Loss 1.9245 (1.9421)	Acc@1 62.500 (61.481)	Acc@5 93.750 (91.126)	Mem 4879MB
[2022-05-31 00:07:51 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.092 (0.097)	Loss 2.0420 (1.9408)	Acc@1 68.750 (61.682)	Acc@5 84.375 (91.034)	Mem 4879MB
[2022-05-31 00:07:52 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.097)	Loss 1.9241 (1.9388)	Acc@1 62.500 (61.583)	Acc@5 96.875 (91.076)	Mem 4879MB
[2022-05-31 00:07:53 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 2.2017 (1.9363)	Acc@1 62.500 (61.804)	Acc@5 93.750 (91.188)	Mem 4879MB
[2022-05-31 00:07:54 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 1.6065 (1.9345)	Acc@1 78.125 (61.821)	Acc@5 93.750 (91.134)	Mem 4879MB
[2022-05-31 00:07:55 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 2.0585 (1.9346)	Acc@1 62.500 (61.797)	Acc@5 87.500 (91.044)	Mem 4879MB
[2022-05-31 00:07:56 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.092 (0.097)	Loss 1.9197 (1.9349)	Acc@1 65.625 (61.865)	Acc@5 90.625 (91.040)	Mem 4879MB
[2022-05-31 00:07:57 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.095 (0.097)	Loss 2.0166 (1.9381)	Acc@1 56.250 (61.803)	Acc@5 96.875 (90.961)	Mem 4879MB
[2022-05-31 00:07:58 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 2.2111 (1.9413)	Acc@1 56.250 (61.710)	Acc@5 84.375 (90.829)	Mem 4879MB
[2022-05-31 00:07:59 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 1.9122 (1.9448)	Acc@1 56.250 (61.612)	Acc@5 93.750 (90.763)	Mem 4879MB
[2022-05-31 00:08:00 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 1.8746 (1.9455)	Acc@1 71.875 (61.555)	Acc@5 96.875 (90.814)	Mem 4879MB
[2022-05-31 00:08:00 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.095 (0.096)	Loss 1.6894 (1.9433)	Acc@1 78.125 (61.684)	Acc@5 100.000 (90.883)	Mem 4879MB
[2022-05-31 00:08:01 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.096)	Loss 2.3493 (1.9457)	Acc@1 37.500 (61.576)	Acc@5 87.500 (90.895)	Mem 4879MB
[2022-05-31 00:08:02 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 1.9577 (1.9453)	Acc@1 56.250 (61.535)	Acc@5 87.500 (90.967)	Mem 4879MB
[2022-05-31 00:08:03 MetaFG_0] (main.py 330): INFO  * Acc@1 61.560 Acc@5 90.970
[2022-05-31 00:08:03 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 61.6%
[2022-05-31 00:08:03 MetaFG_0] (main.py 171): INFO Max accuracy: 61.56%
[2022-05-31 00:08:04 MetaFG_0] (main.py 265): INFO Train: [13/300][0/1562]	eta 0:27:21 lr 0.000004	time 1.0512 (1.0512)	loss 1.5533 (1.5533)	grad_norm 19.7121 (19.7121)	mem 4879MB
[2022-05-31 00:08:07 MetaFG_0] (main.py 265): INFO Train: [13/300][10/1562]	eta 0:09:38 lr 0.000004	time 0.2924 (0.3727)	loss 1.5941 (1.7638)	grad_norm 25.1976 (19.5203)	mem 4879MB
[2022-05-31 00:08:10 MetaFG_0] (main.py 265): INFO Train: [13/300][20/1562]	eta 0:08:44 lr 0.000004	time 0.2981 (0.3404)	loss 1.7011 (1.7486)	grad_norm 14.2225 (21.6726)	mem 4879MB
[2022-05-31 00:08:13 MetaFG_0] (main.py 265): INFO Train: [13/300][30/1562]	eta 0:08:23 lr 0.000004	time 0.2994 (0.3287)	loss 1.9560 (1.7780)	grad_norm 19.4247 (20.8836)	mem 4879MB
[2022-05-31 00:08:16 MetaFG_0] (main.py 265): INFO Train: [13/300][40/1562]	eta 0:08:10 lr 0.000004	time 0.2922 (0.3224)	loss 1.6359 (1.7461)	grad_norm 25.5746 (20.9324)	mem 4879MB
[2022-05-31 00:08:19 MetaFG_0] (main.py 265): INFO Train: [13/300][50/1562]	eta 0:08:01 lr 0.000004	time 0.2918 (0.3187)	loss 1.4186 (1.7439)	grad_norm 18.2192 (20.8051)	mem 4879MB
[2022-05-31 00:08:22 MetaFG_0] (main.py 265): INFO Train: [13/300][60/1562]	eta 0:07:55 lr 0.000004	time 0.2919 (0.3165)	loss 1.8449 (1.7371)	grad_norm 19.8420 (21.1852)	mem 4879MB
[2022-05-31 00:08:25 MetaFG_0] (main.py 265): INFO Train: [13/300][70/1562]	eta 0:07:50 lr 0.000004	time 0.2992 (0.3150)	loss 1.6871 (1.7492)	grad_norm 21.8428 (20.9905)	mem 4879MB
[2022-05-31 00:08:28 MetaFG_0] (main.py 265): INFO Train: [13/300][80/1562]	eta 0:07:45 lr 0.000004	time 0.2918 (0.3138)	loss 1.6975 (1.7500)	grad_norm 27.8319 (21.0878)	mem 4879MB
[2022-05-31 00:08:31 MetaFG_0] (main.py 265): INFO Train: [13/300][90/1562]	eta 0:07:40 lr 0.000004	time 0.3002 (0.3128)	loss 1.9001 (1.7589)	grad_norm 18.8832 (21.0396)	mem 4879MB
[2022-05-31 00:08:34 MetaFG_0] (main.py 265): INFO Train: [13/300][100/1562]	eta 0:07:35 lr 0.000004	time 0.2931 (0.3119)	loss 1.8831 (1.7567)	grad_norm 21.7788 (21.1029)	mem 4879MB
[2022-05-31 00:08:37 MetaFG_0] (main.py 265): INFO Train: [13/300][110/1562]	eta 0:07:32 lr 0.000004	time 0.3018 (0.3114)	loss 1.9043 (1.7570)	grad_norm 20.2965 (21.2655)	mem 4879MB
[2022-05-31 00:08:40 MetaFG_0] (main.py 265): INFO Train: [13/300][120/1562]	eta 0:07:28 lr 0.000004	time 0.2933 (0.3108)	loss 1.5872 (1.7622)	grad_norm 11.2710 (20.8659)	mem 4879MB
[2022-05-31 00:08:43 MetaFG_0] (main.py 265): INFO Train: [13/300][130/1562]	eta 0:07:24 lr 0.000004	time 0.2919 (0.3102)	loss 1.6940 (1.7661)	grad_norm 28.4637 (20.9050)	mem 4879MB
[2022-05-31 00:08:46 MetaFG_0] (main.py 265): INFO Train: [13/300][140/1562]	eta 0:07:20 lr 0.000004	time 0.3002 (0.3098)	loss 1.7557 (1.7591)	grad_norm 23.3171 (20.8272)	mem 4879MB
[2022-05-31 00:08:49 MetaFG_0] (main.py 265): INFO Train: [13/300][150/1562]	eta 0:07:16 lr 0.000004	time 0.2922 (0.3093)	loss 2.0551 (1.7609)	grad_norm 26.4876 (20.9542)	mem 4879MB
[2022-05-31 00:08:52 MetaFG_0] (main.py 265): INFO Train: [13/300][160/1562]	eta 0:07:13 lr 0.000004	time 0.2953 (0.3090)	loss 1.7426 (1.7610)	grad_norm 23.7988 (21.2555)	mem 4879MB
[2022-05-31 00:08:55 MetaFG_0] (main.py 265): INFO Train: [13/300][170/1562]	eta 0:07:09 lr 0.000004	time 0.2929 (0.3087)	loss 1.8607 (1.7634)	grad_norm 20.3421 (21.3396)	mem 4879MB
[2022-05-31 00:08:58 MetaFG_0] (main.py 265): INFO Train: [13/300][180/1562]	eta 0:07:06 lr 0.000004	time 0.2918 (0.3084)	loss 1.6242 (1.7632)	grad_norm 14.4793 (21.4404)	mem 4879MB
[2022-05-31 00:09:01 MetaFG_0] (main.py 265): INFO Train: [13/300][190/1562]	eta 0:07:02 lr 0.000004	time 0.2923 (0.3081)	loss 1.9346 (1.7607)	grad_norm 53.4653 (21.7990)	mem 4879MB
[2022-05-31 00:09:04 MetaFG_0] (main.py 265): INFO Train: [13/300][200/1562]	eta 0:06:59 lr 0.000004	time 0.2935 (0.3078)	loss 1.8751 (1.7601)	grad_norm 20.7092 (22.0240)	mem 4879MB
[2022-05-31 00:09:08 MetaFG_0] (main.py 265): INFO Train: [13/300][210/1562]	eta 0:06:55 lr 0.000004	time 0.2989 (0.3076)	loss 1.7983 (1.7613)	grad_norm 19.5375 (22.1422)	mem 4879MB
[2022-05-31 00:09:11 MetaFG_0] (main.py 265): INFO Train: [13/300][220/1562]	eta 0:06:52 lr 0.000004	time 0.2918 (0.3075)	loss 1.9698 (1.7635)	grad_norm 22.5322 (22.0283)	mem 4879MB
[2022-05-31 00:09:14 MetaFG_0] (main.py 265): INFO Train: [13/300][230/1562]	eta 0:06:49 lr 0.000004	time 0.2938 (0.3074)	loss 1.8279 (1.7638)	grad_norm 15.0268 (21.9847)	mem 4879MB
[2022-05-31 00:09:17 MetaFG_0] (main.py 265): INFO Train: [13/300][240/1562]	eta 0:06:46 lr 0.000004	time 0.2919 (0.3072)	loss 1.7609 (1.7656)	grad_norm 14.4872 (21.8374)	mem 4879MB
[2022-05-31 00:09:20 MetaFG_0] (main.py 265): INFO Train: [13/300][250/1562]	eta 0:06:42 lr 0.000004	time 0.2992 (0.3071)	loss 1.6163 (1.7629)	grad_norm 28.3210 (22.1199)	mem 4879MB
[2022-05-31 00:09:23 MetaFG_0] (main.py 265): INFO Train: [13/300][260/1562]	eta 0:06:39 lr 0.000004	time 0.2975 (0.3071)	loss 1.8025 (1.7617)	grad_norm 14.4635 (22.0289)	mem 4879MB
[2022-05-31 00:09:26 MetaFG_0] (main.py 265): INFO Train: [13/300][270/1562]	eta 0:06:36 lr 0.000004	time 0.2930 (0.3070)	loss 1.8602 (1.7631)	grad_norm 33.8583 (22.0886)	mem 4879MB
[2022-05-31 00:09:29 MetaFG_0] (main.py 265): INFO Train: [13/300][280/1562]	eta 0:06:33 lr 0.000004	time 0.2926 (0.3068)	loss 1.7848 (1.7652)	grad_norm 19.1979 (nan)	mem 4879MB
[2022-05-31 00:09:32 MetaFG_0] (main.py 265): INFO Train: [13/300][290/1562]	eta 0:06:30 lr 0.000004	time 0.2934 (0.3066)	loss 1.7712 (1.7648)	grad_norm 16.5052 (nan)	mem 4879MB
[2022-05-31 00:09:35 MetaFG_0] (main.py 265): INFO Train: [13/300][300/1562]	eta 0:06:26 lr 0.000004	time 0.2953 (0.3066)	loss 1.7162 (1.7643)	grad_norm 16.2237 (nan)	mem 4879MB
[2022-05-31 00:09:38 MetaFG_0] (main.py 265): INFO Train: [13/300][310/1562]	eta 0:06:23 lr 0.000004	time 0.2982 (0.3065)	loss 1.7824 (1.7686)	grad_norm 12.3829 (nan)	mem 4879MB
[2022-05-31 00:09:41 MetaFG_0] (main.py 265): INFO Train: [13/300][320/1562]	eta 0:06:20 lr 0.000004	time 0.2990 (0.3065)	loss 1.5755 (1.7681)	grad_norm 29.9106 (nan)	mem 4879MB
[2022-05-31 00:09:44 MetaFG_0] (main.py 265): INFO Train: [13/300][330/1562]	eta 0:06:17 lr 0.000004	time 0.2977 (0.3064)	loss 1.5550 (1.7682)	grad_norm 33.5027 (nan)	mem 4879MB
[2022-05-31 00:09:47 MetaFG_0] (main.py 265): INFO Train: [13/300][340/1562]	eta 0:06:14 lr 0.000004	time 0.2985 (0.3063)	loss 1.8998 (1.7678)	grad_norm 12.3775 (nan)	mem 4879MB
[2022-05-31 00:09:50 MetaFG_0] (main.py 265): INFO Train: [13/300][350/1562]	eta 0:06:11 lr 0.000004	time 0.2979 (0.3063)	loss 1.8386 (1.7687)	grad_norm 12.5481 (nan)	mem 4879MB
[2022-05-31 00:09:53 MetaFG_0] (main.py 265): INFO Train: [13/300][360/1562]	eta 0:06:08 lr 0.000004	time 0.2994 (0.3062)	loss 1.5043 (1.7673)	grad_norm 23.7366 (nan)	mem 4879MB
[2022-05-31 00:09:56 MetaFG_0] (main.py 265): INFO Train: [13/300][370/1562]	eta 0:06:04 lr 0.000004	time 0.2998 (0.3061)	loss 1.9566 (1.7669)	grad_norm 18.6548 (nan)	mem 4879MB
[2022-05-31 00:09:59 MetaFG_0] (main.py 265): INFO Train: [13/300][380/1562]	eta 0:06:01 lr 0.000004	time 0.2962 (0.3061)	loss 1.7569 (1.7679)	grad_norm 16.6010 (nan)	mem 4879MB
[2022-05-31 00:10:02 MetaFG_0] (main.py 265): INFO Train: [13/300][390/1562]	eta 0:05:58 lr 0.000004	time 0.2973 (0.3061)	loss 1.6912 (1.7677)	grad_norm 18.7013 (nan)	mem 4879MB
[2022-05-31 00:10:05 MetaFG_0] (main.py 265): INFO Train: [13/300][400/1562]	eta 0:05:55 lr 0.000004	time 0.2945 (0.3062)	loss 1.6248 (1.7667)	grad_norm 19.8248 (nan)	mem 4879MB
[2022-05-31 00:10:08 MetaFG_0] (main.py 265): INFO Train: [13/300][410/1562]	eta 0:05:52 lr 0.000004	time 0.2990 (0.3061)	loss 1.6355 (1.7660)	grad_norm 18.3886 (nan)	mem 4879MB
[2022-05-31 00:10:11 MetaFG_0] (main.py 265): INFO Train: [13/300][420/1562]	eta 0:05:49 lr 0.000004	time 0.2956 (0.3060)	loss 1.9053 (1.7661)	grad_norm 22.0268 (nan)	mem 4879MB
[2022-05-31 00:10:14 MetaFG_0] (main.py 265): INFO Train: [13/300][430/1562]	eta 0:05:46 lr 0.000004	time 0.2985 (0.3060)	loss 1.7897 (1.7640)	grad_norm 21.4578 (nan)	mem 4879MB
[2022-05-31 00:10:18 MetaFG_0] (main.py 265): INFO Train: [13/300][440/1562]	eta 0:05:43 lr 0.000004	time 0.2976 (0.3059)	loss 1.6662 (1.7654)	grad_norm 19.3514 (nan)	mem 4879MB
[2022-05-31 00:10:21 MetaFG_0] (main.py 265): INFO Train: [13/300][450/1562]	eta 0:05:40 lr 0.000004	time 0.2926 (0.3058)	loss 1.8955 (1.7653)	grad_norm 21.2478 (nan)	mem 4879MB
[2022-05-31 00:10:24 MetaFG_0] (main.py 265): INFO Train: [13/300][460/1562]	eta 0:05:36 lr 0.000004	time 0.2933 (0.3058)	loss 1.5549 (1.7642)	grad_norm 24.6389 (nan)	mem 4879MB
[2022-05-31 00:10:27 MetaFG_0] (main.py 265): INFO Train: [13/300][470/1562]	eta 0:05:33 lr 0.000004	time 0.2919 (0.3058)	loss 1.8516 (1.7633)	grad_norm 12.5190 (nan)	mem 4879MB
[2022-05-31 00:10:30 MetaFG_0] (main.py 265): INFO Train: [13/300][480/1562]	eta 0:05:30 lr 0.000004	time 0.2934 (0.3057)	loss 1.9945 (1.7656)	grad_norm 25.9921 (nan)	mem 4879MB
[2022-05-31 00:10:33 MetaFG_0] (main.py 265): INFO Train: [13/300][490/1562]	eta 0:05:27 lr 0.000004	time 0.2925 (0.3057)	loss 1.3924 (1.7636)	grad_norm 15.7866 (nan)	mem 4879MB
[2022-05-31 00:10:36 MetaFG_0] (main.py 265): INFO Train: [13/300][500/1562]	eta 0:05:24 lr 0.000004	time 0.3016 (0.3056)	loss 1.5743 (1.7629)	grad_norm 22.8266 (nan)	mem 4879MB
[2022-05-31 00:10:39 MetaFG_0] (main.py 265): INFO Train: [13/300][510/1562]	eta 0:05:21 lr 0.000004	time 0.2933 (0.3056)	loss 1.9025 (1.7641)	grad_norm 25.3425 (nan)	mem 4879MB
[2022-05-31 00:10:42 MetaFG_0] (main.py 265): INFO Train: [13/300][520/1562]	eta 0:05:18 lr 0.000004	time 0.2991 (0.3055)	loss 1.5301 (1.7637)	grad_norm 22.5012 (nan)	mem 4879MB
[2022-05-31 00:10:45 MetaFG_0] (main.py 265): INFO Train: [13/300][530/1562]	eta 0:05:15 lr 0.000004	time 0.2985 (0.3055)	loss 1.9176 (1.7643)	grad_norm 21.6217 (nan)	mem 4879MB
[2022-05-31 00:10:48 MetaFG_0] (main.py 265): INFO Train: [13/300][540/1562]	eta 0:05:12 lr 0.000004	time 0.2938 (0.3055)	loss 1.5050 (1.7649)	grad_norm 20.5931 (nan)	mem 4879MB
[2022-05-31 00:10:51 MetaFG_0] (main.py 265): INFO Train: [13/300][550/1562]	eta 0:05:09 lr 0.000004	time 0.2983 (0.3055)	loss 1.7268 (1.7647)	grad_norm 11.7770 (nan)	mem 4879MB
[2022-05-31 00:10:54 MetaFG_0] (main.py 265): INFO Train: [13/300][560/1562]	eta 0:05:06 lr 0.000004	time 0.2998 (0.3055)	loss 1.5331 (1.7630)	grad_norm 39.2890 (nan)	mem 4879MB
[2022-05-31 00:10:57 MetaFG_0] (main.py 265): INFO Train: [13/300][570/1562]	eta 0:05:03 lr 0.000004	time 0.2931 (0.3055)	loss 1.8237 (1.7639)	grad_norm 17.8531 (nan)	mem 4879MB
[2022-05-31 00:11:00 MetaFG_0] (main.py 265): INFO Train: [13/300][580/1562]	eta 0:05:00 lr 0.000004	time 0.2992 (0.3055)	loss 1.7451 (1.7618)	grad_norm 31.2359 (nan)	mem 4879MB
[2022-05-31 00:11:03 MetaFG_0] (main.py 265): INFO Train: [13/300][590/1562]	eta 0:04:56 lr 0.000004	time 0.2923 (0.3055)	loss 1.8761 (1.7620)	grad_norm 24.9804 (nan)	mem 4879MB
[2022-05-31 00:11:06 MetaFG_0] (main.py 265): INFO Train: [13/300][600/1562]	eta 0:04:53 lr 0.000004	time 0.2979 (0.3054)	loss 1.4445 (1.7622)	grad_norm 15.9564 (nan)	mem 4879MB
[2022-05-31 00:11:09 MetaFG_0] (main.py 265): INFO Train: [13/300][610/1562]	eta 0:04:50 lr 0.000004	time 0.2943 (0.3054)	loss 1.8254 (1.7623)	grad_norm 19.5479 (nan)	mem 4879MB
[2022-05-31 00:11:12 MetaFG_0] (main.py 265): INFO Train: [13/300][620/1562]	eta 0:04:47 lr 0.000004	time 0.2930 (0.3054)	loss 1.4771 (1.7621)	grad_norm 41.1361 (nan)	mem 4879MB
[2022-05-31 00:11:15 MetaFG_0] (main.py 265): INFO Train: [13/300][630/1562]	eta 0:04:44 lr 0.000004	time 0.2942 (0.3054)	loss 1.6499 (1.7614)	grad_norm 12.8541 (nan)	mem 4879MB
[2022-05-31 00:11:18 MetaFG_0] (main.py 265): INFO Train: [13/300][640/1562]	eta 0:04:41 lr 0.000004	time 0.2929 (0.3053)	loss 1.4385 (1.7588)	grad_norm 22.3622 (nan)	mem 4879MB
[2022-05-31 00:11:21 MetaFG_0] (main.py 265): INFO Train: [13/300][650/1562]	eta 0:04:38 lr 0.000004	time 0.2992 (0.3053)	loss 1.5790 (1.7595)	grad_norm 31.8933 (nan)	mem 4879MB
[2022-05-31 00:11:24 MetaFG_0] (main.py 265): INFO Train: [13/300][660/1562]	eta 0:04:35 lr 0.000004	time 0.2981 (0.3053)	loss 1.8728 (1.7588)	grad_norm 13.4698 (nan)	mem 4879MB
[2022-05-31 00:11:27 MetaFG_0] (main.py 265): INFO Train: [13/300][670/1562]	eta 0:04:32 lr 0.000004	time 0.2977 (0.3053)	loss 1.5322 (1.7582)	grad_norm 14.7777 (nan)	mem 4879MB
[2022-05-31 00:11:30 MetaFG_0] (main.py 265): INFO Train: [13/300][680/1562]	eta 0:04:29 lr 0.000004	time 0.2949 (0.3052)	loss 1.8816 (1.7580)	grad_norm 19.9269 (nan)	mem 4879MB
[2022-05-31 00:11:34 MetaFG_0] (main.py 265): INFO Train: [13/300][690/1562]	eta 0:04:26 lr 0.000004	time 0.2916 (0.3052)	loss 1.9618 (1.7585)	grad_norm 20.1109 (nan)	mem 4879MB
[2022-05-31 00:11:37 MetaFG_0] (main.py 265): INFO Train: [13/300][700/1562]	eta 0:04:23 lr 0.000004	time 0.2926 (0.3052)	loss 1.7860 (1.7589)	grad_norm 22.0166 (nan)	mem 4879MB
[2022-05-31 00:11:40 MetaFG_0] (main.py 265): INFO Train: [13/300][710/1562]	eta 0:04:20 lr 0.000004	time 0.2938 (0.3052)	loss 1.6503 (1.7580)	grad_norm 28.1127 (nan)	mem 4879MB
[2022-05-31 00:11:43 MetaFG_0] (main.py 265): INFO Train: [13/300][720/1562]	eta 0:04:16 lr 0.000004	time 0.2982 (0.3052)	loss 2.0104 (1.7577)	grad_norm 20.9021 (nan)	mem 4879MB
[2022-05-31 00:11:46 MetaFG_0] (main.py 265): INFO Train: [13/300][730/1562]	eta 0:04:13 lr 0.000004	time 0.3011 (0.3052)	loss 1.8892 (1.7569)	grad_norm 18.9007 (nan)	mem 4879MB
[2022-05-31 00:11:49 MetaFG_0] (main.py 265): INFO Train: [13/300][740/1562]	eta 0:04:10 lr 0.000004	time 0.2922 (0.3051)	loss 1.5969 (1.7568)	grad_norm 31.2715 (nan)	mem 4879MB
[2022-05-31 00:11:52 MetaFG_0] (main.py 265): INFO Train: [13/300][750/1562]	eta 0:04:07 lr 0.000004	time 0.3033 (0.3052)	loss 1.8266 (1.7561)	grad_norm 25.1833 (nan)	mem 4879MB
[2022-05-31 00:11:55 MetaFG_0] (main.py 265): INFO Train: [13/300][760/1562]	eta 0:04:04 lr 0.000004	time 0.2982 (0.3052)	loss 1.8494 (1.7567)	grad_norm 32.1456 (nan)	mem 4879MB
[2022-05-31 00:11:58 MetaFG_0] (main.py 265): INFO Train: [13/300][770/1562]	eta 0:04:01 lr 0.000004	time 0.2988 (0.3052)	loss 1.7851 (1.7560)	grad_norm 25.1275 (nan)	mem 4879MB
[2022-05-31 00:12:01 MetaFG_0] (main.py 265): INFO Train: [13/300][780/1562]	eta 0:03:58 lr 0.000004	time 0.2943 (0.3051)	loss 1.8364 (1.7555)	grad_norm 27.1092 (nan)	mem 4879MB
[2022-05-31 00:12:04 MetaFG_0] (main.py 265): INFO Train: [13/300][790/1562]	eta 0:03:55 lr 0.000004	time 0.2939 (0.3051)	loss 1.8604 (1.7550)	grad_norm 31.9625 (nan)	mem 4879MB
[2022-05-31 00:12:07 MetaFG_0] (main.py 265): INFO Train: [13/300][800/1562]	eta 0:03:52 lr 0.000004	time 0.3002 (0.3051)	loss 1.4844 (1.7544)	grad_norm 20.4829 (nan)	mem 4879MB
[2022-05-31 00:12:10 MetaFG_0] (main.py 265): INFO Train: [13/300][810/1562]	eta 0:03:49 lr 0.000004	time 0.2977 (0.3051)	loss 1.8425 (1.7538)	grad_norm 25.2576 (nan)	mem 4879MB
[2022-05-31 00:12:13 MetaFG_0] (main.py 265): INFO Train: [13/300][820/1562]	eta 0:03:46 lr 0.000004	time 0.2998 (0.3051)	loss 1.7347 (1.7534)	grad_norm 20.2210 (nan)	mem 4879MB
[2022-05-31 00:12:16 MetaFG_0] (main.py 265): INFO Train: [13/300][830/1562]	eta 0:03:43 lr 0.000004	time 0.2928 (0.3051)	loss 1.4546 (1.7521)	grad_norm 17.9770 (nan)	mem 4879MB
[2022-05-31 00:12:19 MetaFG_0] (main.py 265): INFO Train: [13/300][840/1562]	eta 0:03:40 lr 0.000004	time 0.2918 (0.3051)	loss 1.8473 (1.7526)	grad_norm 19.9924 (nan)	mem 4879MB
[2022-05-31 00:12:22 MetaFG_0] (main.py 265): INFO Train: [13/300][850/1562]	eta 0:03:37 lr 0.000004	time 0.2921 (0.3051)	loss 1.6840 (1.7527)	grad_norm 28.2823 (nan)	mem 4879MB
[2022-05-31 00:12:25 MetaFG_0] (main.py 265): INFO Train: [13/300][860/1562]	eta 0:03:34 lr 0.000004	time 0.2978 (0.3051)	loss 1.6614 (1.7525)	grad_norm 15.7240 (nan)	mem 4879MB
[2022-05-31 00:12:28 MetaFG_0] (main.py 265): INFO Train: [13/300][870/1562]	eta 0:03:31 lr 0.000004	time 0.2916 (0.3051)	loss 1.6127 (1.7523)	grad_norm 23.9018 (nan)	mem 4879MB
[2022-05-31 00:12:31 MetaFG_0] (main.py 265): INFO Train: [13/300][880/1562]	eta 0:03:28 lr 0.000004	time 0.2935 (0.3051)	loss 2.0990 (1.7530)	grad_norm 20.4611 (nan)	mem 4879MB
[2022-05-31 00:12:34 MetaFG_0] (main.py 265): INFO Train: [13/300][890/1562]	eta 0:03:25 lr 0.000004	time 0.2918 (0.3051)	loss 1.5890 (1.7534)	grad_norm 17.0933 (nan)	mem 4879MB
[2022-05-31 00:12:37 MetaFG_0] (main.py 265): INFO Train: [13/300][900/1562]	eta 0:03:21 lr 0.000004	time 0.2924 (0.3050)	loss 1.6268 (1.7533)	grad_norm 29.1085 (nan)	mem 4879MB
[2022-05-31 00:12:40 MetaFG_0] (main.py 265): INFO Train: [13/300][910/1562]	eta 0:03:18 lr 0.000004	time 0.2934 (0.3050)	loss 1.7864 (1.7535)	grad_norm 16.5245 (nan)	mem 4879MB
[2022-05-31 00:12:44 MetaFG_0] (main.py 265): INFO Train: [13/300][920/1562]	eta 0:03:15 lr 0.000004	time 0.2988 (0.3050)	loss 1.9314 (1.7535)	grad_norm 20.2239 (nan)	mem 4879MB
[2022-05-31 00:12:47 MetaFG_0] (main.py 265): INFO Train: [13/300][930/1562]	eta 0:03:12 lr 0.000004	time 0.2938 (0.3050)	loss 1.6492 (1.7530)	grad_norm 19.7883 (nan)	mem 4879MB
[2022-05-31 00:12:50 MetaFG_0] (main.py 265): INFO Train: [13/300][940/1562]	eta 0:03:09 lr 0.000004	time 0.2915 (0.3050)	loss 1.8933 (1.7539)	grad_norm 19.0127 (nan)	mem 4879MB
[2022-05-31 00:12:53 MetaFG_0] (main.py 265): INFO Train: [13/300][950/1562]	eta 0:03:06 lr 0.000004	time 0.2985 (0.3050)	loss 1.8990 (1.7539)	grad_norm 16.4992 (nan)	mem 4879MB
[2022-05-31 00:12:56 MetaFG_0] (main.py 265): INFO Train: [13/300][960/1562]	eta 0:03:03 lr 0.000004	time 0.2977 (0.3050)	loss 1.7043 (1.7534)	grad_norm 16.3431 (nan)	mem 4879MB
[2022-05-31 00:12:59 MetaFG_0] (main.py 265): INFO Train: [13/300][970/1562]	eta 0:03:00 lr 0.000004	time 0.2934 (0.3050)	loss 1.5795 (1.7536)	grad_norm 24.2820 (nan)	mem 4879MB
[2022-05-31 00:13:02 MetaFG_0] (main.py 265): INFO Train: [13/300][980/1562]	eta 0:02:57 lr 0.000004	time 0.2965 (0.3050)	loss 1.9750 (1.7539)	grad_norm 28.1544 (nan)	mem 4879MB
[2022-05-31 00:13:05 MetaFG_0] (main.py 265): INFO Train: [13/300][990/1562]	eta 0:02:54 lr 0.000004	time 0.2976 (0.3050)	loss 1.6046 (1.7528)	grad_norm 18.0346 (nan)	mem 4879MB
[2022-05-31 00:13:08 MetaFG_0] (main.py 265): INFO Train: [13/300][1000/1562]	eta 0:02:51 lr 0.000004	time 0.2924 (0.3049)	loss 1.4809 (1.7522)	grad_norm 22.6650 (nan)	mem 4879MB
[2022-05-31 00:13:11 MetaFG_0] (main.py 265): INFO Train: [13/300][1010/1562]	eta 0:02:48 lr 0.000004	time 0.2931 (0.3049)	loss 1.5818 (1.7502)	grad_norm 23.5579 (nan)	mem 4879MB
[2022-05-31 00:13:14 MetaFG_0] (main.py 265): INFO Train: [13/300][1020/1562]	eta 0:02:45 lr 0.000004	time 0.2923 (0.3049)	loss 1.9339 (1.7504)	grad_norm 24.7432 (nan)	mem 4879MB
[2022-05-31 00:13:17 MetaFG_0] (main.py 265): INFO Train: [13/300][1030/1562]	eta 0:02:42 lr 0.000004	time 0.2977 (0.3049)	loss 1.9707 (1.7501)	grad_norm 14.4314 (nan)	mem 4879MB
[2022-05-31 00:13:20 MetaFG_0] (main.py 265): INFO Train: [13/300][1040/1562]	eta 0:02:39 lr 0.000004	time 0.2992 (0.3049)	loss 1.5412 (1.7501)	grad_norm 23.0956 (nan)	mem 4879MB
[2022-05-31 00:13:23 MetaFG_0] (main.py 265): INFO Train: [13/300][1050/1562]	eta 0:02:36 lr 0.000004	time 0.2988 (0.3049)	loss 1.4974 (1.7501)	grad_norm 16.4876 (nan)	mem 4879MB
[2022-05-31 00:13:26 MetaFG_0] (main.py 265): INFO Train: [13/300][1060/1562]	eta 0:02:33 lr 0.000004	time 0.2997 (0.3049)	loss 1.4590 (1.7504)	grad_norm 25.7832 (nan)	mem 4879MB
[2022-05-31 00:13:29 MetaFG_0] (main.py 265): INFO Train: [13/300][1070/1562]	eta 0:02:29 lr 0.000004	time 0.2985 (0.3049)	loss 1.8960 (1.7505)	grad_norm 20.4680 (nan)	mem 4879MB
[2022-05-31 00:13:32 MetaFG_0] (main.py 265): INFO Train: [13/300][1080/1562]	eta 0:02:26 lr 0.000004	time 0.2932 (0.3048)	loss 2.0580 (1.7497)	grad_norm 32.6669 (nan)	mem 4879MB
[2022-05-31 00:13:35 MetaFG_0] (main.py 265): INFO Train: [13/300][1090/1562]	eta 0:02:23 lr 0.000004	time 0.2928 (0.3048)	loss 1.6620 (1.7499)	grad_norm 22.6877 (nan)	mem 4879MB
[2022-05-31 00:13:38 MetaFG_0] (main.py 265): INFO Train: [13/300][1100/1562]	eta 0:02:20 lr 0.000004	time 0.2939 (0.3049)	loss 1.5750 (1.7492)	grad_norm 15.8839 (nan)	mem 4879MB
[2022-05-31 00:13:41 MetaFG_0] (main.py 265): INFO Train: [13/300][1110/1562]	eta 0:02:17 lr 0.000004	time 0.2930 (0.3049)	loss 1.4706 (1.7492)	grad_norm 58.2529 (nan)	mem 4879MB
[2022-05-31 00:13:44 MetaFG_0] (main.py 265): INFO Train: [13/300][1120/1562]	eta 0:02:14 lr 0.000004	time 0.2949 (0.3049)	loss 1.8419 (1.7488)	grad_norm 14.9097 (nan)	mem 4879MB
[2022-05-31 00:13:47 MetaFG_0] (main.py 265): INFO Train: [13/300][1130/1562]	eta 0:02:11 lr 0.000004	time 0.2926 (0.3049)	loss 1.4539 (1.7479)	grad_norm 20.1254 (nan)	mem 4879MB
[2022-05-31 00:13:50 MetaFG_0] (main.py 265): INFO Train: [13/300][1140/1562]	eta 0:02:08 lr 0.000004	time 0.2931 (0.3049)	loss 1.6451 (1.7474)	grad_norm 32.8388 (nan)	mem 4879MB
[2022-05-31 00:13:53 MetaFG_0] (main.py 265): INFO Train: [13/300][1150/1562]	eta 0:02:05 lr 0.000004	time 0.2938 (0.3049)	loss 1.8137 (1.7472)	grad_norm 14.1182 (nan)	mem 4879MB
[2022-05-31 00:13:57 MetaFG_0] (main.py 265): INFO Train: [13/300][1160/1562]	eta 0:02:02 lr 0.000004	time 0.2997 (0.3049)	loss 1.7378 (1.7462)	grad_norm 19.9996 (nan)	mem 4879MB
[2022-05-31 00:14:00 MetaFG_0] (main.py 265): INFO Train: [13/300][1170/1562]	eta 0:01:59 lr 0.000004	time 0.2919 (0.3048)	loss 1.6812 (1.7466)	grad_norm 14.7656 (nan)	mem 4879MB
[2022-05-31 00:14:03 MetaFG_0] (main.py 265): INFO Train: [13/300][1180/1562]	eta 0:01:56 lr 0.000004	time 0.2978 (0.3048)	loss 1.6135 (1.7465)	grad_norm 15.5802 (nan)	mem 4879MB
[2022-05-31 00:14:06 MetaFG_0] (main.py 265): INFO Train: [13/300][1190/1562]	eta 0:01:53 lr 0.000004	time 0.2923 (0.3048)	loss 1.3780 (1.7456)	grad_norm 17.5852 (nan)	mem 4879MB
[2022-05-31 00:14:09 MetaFG_0] (main.py 265): INFO Train: [13/300][1200/1562]	eta 0:01:50 lr 0.000004	time 0.2932 (0.3048)	loss 1.6916 (1.7452)	grad_norm 29.8776 (nan)	mem 4879MB
[2022-05-31 00:14:12 MetaFG_0] (main.py 265): INFO Train: [13/300][1210/1562]	eta 0:01:47 lr 0.000004	time 0.2917 (0.3048)	loss 1.5043 (1.7450)	grad_norm 22.9808 (nan)	mem 4879MB
[2022-05-31 00:14:15 MetaFG_0] (main.py 265): INFO Train: [13/300][1220/1562]	eta 0:01:44 lr 0.000004	time 0.2978 (0.3048)	loss 1.7203 (1.7447)	grad_norm 13.0597 (nan)	mem 4879MB
[2022-05-31 00:14:18 MetaFG_0] (main.py 265): INFO Train: [13/300][1230/1562]	eta 0:01:41 lr 0.000004	time 0.2982 (0.3048)	loss 1.4584 (1.7440)	grad_norm 16.2009 (nan)	mem 4879MB
[2022-05-31 00:14:21 MetaFG_0] (main.py 265): INFO Train: [13/300][1240/1562]	eta 0:01:38 lr 0.000004	time 0.2998 (0.3048)	loss 1.8421 (1.7435)	grad_norm 18.8474 (nan)	mem 4879MB
[2022-05-31 00:14:24 MetaFG_0] (main.py 265): INFO Train: [13/300][1250/1562]	eta 0:01:35 lr 0.000004	time 0.2938 (0.3048)	loss 1.6135 (1.7430)	grad_norm 15.8374 (nan)	mem 4879MB
[2022-05-31 00:14:27 MetaFG_0] (main.py 265): INFO Train: [13/300][1260/1562]	eta 0:01:32 lr 0.000004	time 0.2929 (0.3048)	loss 1.7526 (1.7434)	grad_norm 16.0624 (nan)	mem 4879MB
[2022-05-31 00:14:30 MetaFG_0] (main.py 265): INFO Train: [13/300][1270/1562]	eta 0:01:28 lr 0.000004	time 0.2983 (0.3047)	loss 1.8642 (1.7437)	grad_norm 29.0988 (nan)	mem 4879MB
[2022-05-31 00:14:33 MetaFG_0] (main.py 265): INFO Train: [13/300][1280/1562]	eta 0:01:25 lr 0.000004	time 0.3110 (0.3048)	loss 1.9100 (1.7439)	grad_norm 14.0156 (nan)	mem 4879MB
[2022-05-31 00:14:36 MetaFG_0] (main.py 265): INFO Train: [13/300][1290/1562]	eta 0:01:22 lr 0.000004	time 0.2934 (0.3048)	loss 1.6528 (1.7426)	grad_norm 20.5896 (nan)	mem 4879MB
[2022-05-31 00:14:39 MetaFG_0] (main.py 265): INFO Train: [13/300][1300/1562]	eta 0:01:19 lr 0.000004	time 0.2924 (0.3048)	loss 1.7538 (1.7422)	grad_norm 29.9909 (nan)	mem 4879MB
[2022-05-31 00:14:42 MetaFG_0] (main.py 265): INFO Train: [13/300][1310/1562]	eta 0:01:16 lr 0.000004	time 0.2931 (0.3048)	loss 1.4488 (1.7413)	grad_norm 13.2647 (nan)	mem 4879MB
[2022-05-31 00:14:45 MetaFG_0] (main.py 265): INFO Train: [13/300][1320/1562]	eta 0:01:13 lr 0.000004	time 0.2994 (0.3048)	loss 1.8559 (1.7419)	grad_norm 14.9217 (nan)	mem 4879MB
[2022-05-31 00:14:48 MetaFG_0] (main.py 265): INFO Train: [13/300][1330/1562]	eta 0:01:10 lr 0.000004	time 0.2928 (0.3048)	loss 1.7009 (1.7418)	grad_norm 15.8622 (nan)	mem 4879MB
[2022-05-31 00:14:51 MetaFG_0] (main.py 265): INFO Train: [13/300][1340/1562]	eta 0:01:07 lr 0.000004	time 0.2994 (0.3048)	loss 2.0127 (1.7417)	grad_norm 22.2485 (nan)	mem 4879MB
[2022-05-31 00:14:54 MetaFG_0] (main.py 265): INFO Train: [13/300][1350/1562]	eta 0:01:04 lr 0.000004	time 0.2921 (0.3048)	loss 1.8207 (1.7417)	grad_norm 17.4757 (nan)	mem 4879MB
[2022-05-31 00:14:57 MetaFG_0] (main.py 265): INFO Train: [13/300][1360/1562]	eta 0:01:01 lr 0.000004	time 0.2930 (0.3047)	loss 1.8943 (1.7414)	grad_norm 41.2507 (nan)	mem 4879MB
[2022-05-31 00:15:00 MetaFG_0] (main.py 265): INFO Train: [13/300][1370/1562]	eta 0:00:58 lr 0.000004	time 0.2995 (0.3048)	loss 1.2450 (1.7413)	grad_norm 21.3917 (nan)	mem 4879MB
[2022-05-31 00:15:03 MetaFG_0] (main.py 265): INFO Train: [13/300][1380/1562]	eta 0:00:55 lr 0.000004	time 0.2992 (0.3048)	loss 1.7518 (1.7413)	grad_norm 19.8215 (nan)	mem 4879MB
[2022-05-31 00:15:06 MetaFG_0] (main.py 265): INFO Train: [13/300][1390/1562]	eta 0:00:52 lr 0.000004	time 0.2998 (0.3047)	loss 2.0103 (1.7416)	grad_norm 17.0931 (nan)	mem 4879MB
[2022-05-31 00:15:10 MetaFG_0] (main.py 265): INFO Train: [13/300][1400/1562]	eta 0:00:49 lr 0.000004	time 0.2928 (0.3047)	loss 1.5593 (1.7414)	grad_norm 19.4031 (nan)	mem 4879MB
[2022-05-31 00:15:13 MetaFG_0] (main.py 265): INFO Train: [13/300][1410/1562]	eta 0:00:46 lr 0.000004	time 0.2936 (0.3047)	loss 1.5153 (1.7415)	grad_norm 17.6991 (nan)	mem 4879MB
[2022-05-31 00:15:16 MetaFG_0] (main.py 265): INFO Train: [13/300][1420/1562]	eta 0:00:43 lr 0.000004	time 0.2925 (0.3047)	loss 1.8634 (1.7414)	grad_norm 20.8575 (nan)	mem 4879MB
[2022-05-31 00:15:19 MetaFG_0] (main.py 265): INFO Train: [13/300][1430/1562]	eta 0:00:40 lr 0.000004	time 0.2977 (0.3047)	loss 1.6652 (1.7414)	grad_norm 31.3322 (nan)	mem 4879MB
[2022-05-31 00:15:22 MetaFG_0] (main.py 265): INFO Train: [13/300][1440/1562]	eta 0:00:37 lr 0.000004	time 0.2926 (0.3047)	loss 2.0315 (1.7411)	grad_norm 17.8709 (nan)	mem 4879MB
[2022-05-31 00:15:25 MetaFG_0] (main.py 265): INFO Train: [13/300][1450/1562]	eta 0:00:34 lr 0.000004	time 0.2937 (0.3047)	loss 1.7491 (1.7411)	grad_norm 22.6658 (nan)	mem 4879MB
[2022-05-31 00:15:28 MetaFG_0] (main.py 265): INFO Train: [13/300][1460/1562]	eta 0:00:31 lr 0.000004	time 0.3020 (0.3047)	loss 1.8500 (1.7419)	grad_norm 15.7881 (nan)	mem 4879MB
[2022-05-31 00:15:31 MetaFG_0] (main.py 265): INFO Train: [13/300][1470/1562]	eta 0:00:28 lr 0.000004	time 0.2996 (0.3047)	loss 1.4929 (1.7416)	grad_norm 32.6674 (nan)	mem 4879MB
[2022-05-31 00:15:34 MetaFG_0] (main.py 265): INFO Train: [13/300][1480/1562]	eta 0:00:24 lr 0.000004	time 0.2919 (0.3047)	loss 1.9332 (1.7418)	grad_norm 38.7983 (nan)	mem 4879MB
[2022-05-31 00:15:37 MetaFG_0] (main.py 265): INFO Train: [13/300][1490/1562]	eta 0:00:21 lr 0.000004	time 0.2986 (0.3047)	loss 1.5449 (1.7410)	grad_norm 31.8021 (nan)	mem 4879MB
[2022-05-31 00:15:40 MetaFG_0] (main.py 265): INFO Train: [13/300][1500/1562]	eta 0:00:18 lr 0.000004	time 0.2978 (0.3047)	loss 1.9145 (1.7411)	grad_norm 24.1954 (nan)	mem 4879MB
[2022-05-31 00:15:43 MetaFG_0] (main.py 265): INFO Train: [13/300][1510/1562]	eta 0:00:15 lr 0.000004	time 0.2959 (0.3047)	loss 1.8263 (1.7418)	grad_norm 20.4873 (nan)	mem 4879MB
[2022-05-31 00:15:46 MetaFG_0] (main.py 265): INFO Train: [13/300][1520/1562]	eta 0:00:12 lr 0.000004	time 0.2935 (0.3047)	loss 2.0088 (1.7420)	grad_norm 24.1965 (nan)	mem 4879MB
[2022-05-31 00:15:49 MetaFG_0] (main.py 265): INFO Train: [13/300][1530/1562]	eta 0:00:09 lr 0.000004	time 0.3009 (0.3047)	loss 1.6440 (1.7418)	grad_norm 21.2046 (nan)	mem 4879MB
[2022-05-31 00:15:52 MetaFG_0] (main.py 265): INFO Train: [13/300][1540/1562]	eta 0:00:06 lr 0.000004	time 0.2929 (0.3046)	loss 1.7331 (1.7417)	grad_norm 17.8461 (nan)	mem 4879MB
[2022-05-31 00:15:55 MetaFG_0] (main.py 265): INFO Train: [13/300][1550/1562]	eta 0:00:03 lr 0.000004	time 0.2983 (0.3046)	loss 1.9588 (1.7411)	grad_norm 17.5336 (nan)	mem 4879MB
[2022-05-31 00:15:58 MetaFG_0] (main.py 265): INFO Train: [13/300][1560/1562]	eta 0:00:00 lr 0.000004	time 0.2914 (0.3046)	loss 1.8992 (1.7411)	grad_norm 21.0349 (nan)	mem 4879MB
[2022-05-31 00:15:59 MetaFG_0] (main.py 272): INFO EPOCH 13 training takes 0:07:55
[2022-05-31 00:15:59 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_13.pth saving......
[2022-05-31 00:15:59 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_13.pth saved !!!
[2022-05-31 00:15:59 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:16:01 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:16:01 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:16:02 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.714 (0.714)	Loss 1.9890 (1.9890)	Acc@1 53.125 (53.125)	Acc@5 87.500 (87.500)	Mem 4879MB
[2022-05-31 00:16:02 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.149)	Loss 1.7096 (1.7555)	Acc@1 62.500 (66.761)	Acc@5 93.750 (94.602)	Mem 4879MB
[2022-05-31 00:16:03 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.091 (0.123)	Loss 1.4486 (1.7746)	Acc@1 75.000 (64.583)	Acc@5 100.000 (93.006)	Mem 4879MB
[2022-05-31 00:16:04 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.112)	Loss 1.8945 (1.7740)	Acc@1 56.250 (64.718)	Acc@5 90.625 (92.540)	Mem 4879MB
[2022-05-31 00:16:05 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.097 (0.108)	Loss 1.8803 (1.7792)	Acc@1 65.625 (64.787)	Acc@5 100.000 (92.988)	Mem 4879MB
[2022-05-31 00:16:06 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.105)	Loss 1.6990 (1.7741)	Acc@1 71.875 (65.319)	Acc@5 90.625 (92.586)	Mem 4879MB
[2022-05-31 00:16:07 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.091 (0.103)	Loss 2.0701 (1.7787)	Acc@1 46.875 (65.010)	Acc@5 84.375 (92.469)	Mem 4879MB
[2022-05-31 00:16:08 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.102)	Loss 1.6026 (1.7603)	Acc@1 65.625 (65.537)	Acc@5 93.750 (92.826)	Mem 4879MB
[2022-05-31 00:16:09 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.107 (0.101)	Loss 1.9147 (1.7602)	Acc@1 62.500 (65.625)	Acc@5 81.250 (92.593)	Mem 4879MB
[2022-05-31 00:16:10 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.101)	Loss 1.7833 (1.7695)	Acc@1 65.625 (64.938)	Acc@5 93.750 (92.514)	Mem 4879MB
[2022-05-31 00:16:11 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.100)	Loss 2.0636 (1.7757)	Acc@1 46.875 (64.418)	Acc@5 78.125 (92.234)	Mem 4879MB
[2022-05-31 00:16:12 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.099)	Loss 1.7001 (1.7742)	Acc@1 65.625 (64.358)	Acc@5 96.875 (92.342)	Mem 4879MB
[2022-05-31 00:16:13 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 1.7875 (1.7726)	Acc@1 75.000 (64.411)	Acc@5 84.375 (92.330)	Mem 4879MB
[2022-05-31 00:16:14 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.092 (0.098)	Loss 1.5856 (1.7728)	Acc@1 75.000 (64.408)	Acc@5 96.875 (92.366)	Mem 4879MB
[2022-05-31 00:16:15 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.098)	Loss 1.9276 (1.7754)	Acc@1 53.125 (64.251)	Acc@5 87.500 (92.176)	Mem 4879MB
[2022-05-31 00:16:16 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.097)	Loss 1.9064 (1.7736)	Acc@1 59.375 (64.218)	Acc@5 96.875 (92.198)	Mem 4879MB
[2022-05-31 00:16:16 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.097)	Loss 1.6791 (1.7754)	Acc@1 71.875 (63.975)	Acc@5 93.750 (92.158)	Mem 4879MB
[2022-05-31 00:16:17 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.091 (0.097)	Loss 1.8672 (1.7746)	Acc@1 56.250 (63.999)	Acc@5 90.625 (92.233)	Mem 4879MB
[2022-05-31 00:16:18 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.097)	Loss 1.7524 (1.7761)	Acc@1 68.750 (64.002)	Acc@5 90.625 (92.144)	Mem 4879MB
[2022-05-31 00:16:19 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.097)	Loss 1.9129 (1.7763)	Acc@1 56.250 (63.891)	Acc@5 90.625 (92.098)	Mem 4879MB
[2022-05-31 00:16:20 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 1.7819 (1.7732)	Acc@1 62.500 (64.008)	Acc@5 96.875 (92.226)	Mem 4879MB
[2022-05-31 00:16:21 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.096)	Loss 1.8740 (1.7746)	Acc@1 62.500 (64.040)	Acc@5 87.500 (92.210)	Mem 4879MB
[2022-05-31 00:16:22 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.091 (0.096)	Loss 1.7446 (1.7704)	Acc@1 59.375 (64.211)	Acc@5 90.625 (92.265)	Mem 4879MB
[2022-05-31 00:16:23 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 1.5407 (1.7685)	Acc@1 71.875 (64.191)	Acc@5 93.750 (92.289)	Mem 4879MB
[2022-05-31 00:16:24 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.088 (0.096)	Loss 1.7443 (1.7648)	Acc@1 68.750 (64.380)	Acc@5 93.750 (92.363)	Mem 4879MB
[2022-05-31 00:16:25 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.097 (0.096)	Loss 2.1356 (1.7669)	Acc@1 43.750 (64.392)	Acc@5 93.750 (92.393)	Mem 4879MB
[2022-05-31 00:16:26 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 1.5801 (1.7677)	Acc@1 71.875 (64.392)	Acc@5 96.875 (92.421)	Mem 4879MB
[2022-05-31 00:16:27 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 1.6687 (1.7703)	Acc@1 62.500 (64.333)	Acc@5 87.500 (92.389)	Mem 4879MB
[2022-05-31 00:16:28 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.099 (0.095)	Loss 1.9538 (1.7704)	Acc@1 59.375 (64.335)	Acc@5 84.375 (92.338)	Mem 4879MB
[2022-05-31 00:16:29 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.095)	Loss 1.7593 (1.7703)	Acc@1 62.500 (64.304)	Acc@5 87.500 (92.365)	Mem 4879MB
[2022-05-31 00:16:29 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.095)	Loss 1.7843 (1.7688)	Acc@1 65.625 (64.348)	Acc@5 90.625 (92.390)	Mem 4879MB
[2022-05-31 00:16:30 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 1.7751 (1.7660)	Acc@1 68.750 (64.359)	Acc@5 93.750 (92.434)	Mem 4879MB
[2022-05-31 00:16:31 MetaFG_0] (main.py 330): INFO  * Acc@1 64.370 Acc@5 92.410
[2022-05-31 00:16:31 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 64.4%
[2022-05-31 00:16:31 MetaFG_0] (main.py 171): INFO Max accuracy: 64.37%
[2022-05-31 00:16:32 MetaFG_0] (main.py 265): INFO Train: [14/300][0/1562]	eta 0:28:31 lr 0.000004	time 1.0958 (1.0958)	loss 1.6822 (1.6822)	grad_norm 17.2896 (17.2896)	mem 4879MB
[2022-05-31 00:16:35 MetaFG_0] (main.py 265): INFO Train: [14/300][10/1562]	eta 0:09:47 lr 0.000004	time 0.2920 (0.3784)	loss 1.7944 (1.7791)	grad_norm 17.1787 (24.5828)	mem 4879MB
[2022-05-31 00:16:38 MetaFG_0] (main.py 265): INFO Train: [14/300][20/1562]	eta 0:08:48 lr 0.000004	time 0.2939 (0.3429)	loss 1.9820 (1.7200)	grad_norm 28.8639 (23.8252)	mem 4879MB
[2022-05-31 00:16:41 MetaFG_0] (main.py 265): INFO Train: [14/300][30/1562]	eta 0:08:24 lr 0.000004	time 0.2925 (0.3294)	loss 1.6994 (1.7442)	grad_norm 16.3047 (22.0263)	mem 4879MB
[2022-05-31 00:16:44 MetaFG_0] (main.py 265): INFO Train: [14/300][40/1562]	eta 0:08:10 lr 0.000004	time 0.2919 (0.3225)	loss 1.4439 (1.7357)	grad_norm 35.1152 (21.7368)	mem 4879MB
[2022-05-31 00:16:47 MetaFG_0] (main.py 265): INFO Train: [14/300][50/1562]	eta 0:08:02 lr 0.000004	time 0.3079 (0.3188)	loss 1.8014 (1.7268)	grad_norm 21.6567 (21.6741)	mem 4879MB
[2022-05-31 00:16:50 MetaFG_0] (main.py 265): INFO Train: [14/300][60/1562]	eta 0:07:57 lr 0.000004	time 0.2983 (0.3180)	loss 1.9951 (1.7332)	grad_norm 18.7533 (21.3465)	mem 4879MB
[2022-05-31 00:16:53 MetaFG_0] (main.py 265): INFO Train: [14/300][70/1562]	eta 0:07:51 lr 0.000004	time 0.2947 (0.3159)	loss 1.8912 (1.7240)	grad_norm 15.5322 (21.3219)	mem 4879MB
[2022-05-31 00:16:56 MetaFG_0] (main.py 265): INFO Train: [14/300][80/1562]	eta 0:07:45 lr 0.000004	time 0.2917 (0.3143)	loss 1.5420 (1.7202)	grad_norm 23.8326 (21.4595)	mem 4879MB
[2022-05-31 00:16:59 MetaFG_0] (main.py 265): INFO Train: [14/300][90/1562]	eta 0:07:40 lr 0.000004	time 0.2980 (0.3130)	loss 1.4889 (1.7101)	grad_norm 28.6879 (21.4687)	mem 4879MB
[2022-05-31 00:17:02 MetaFG_0] (main.py 265): INFO Train: [14/300][100/1562]	eta 0:07:36 lr 0.000004	time 0.2985 (0.3122)	loss 1.6661 (1.7078)	grad_norm 14.9509 (21.3206)	mem 4879MB
[2022-05-31 00:17:05 MetaFG_0] (main.py 265): INFO Train: [14/300][110/1562]	eta 0:07:32 lr 0.000004	time 0.2921 (0.3114)	loss 1.7933 (1.7157)	grad_norm 15.5867 (21.2665)	mem 4879MB
[2022-05-31 00:17:08 MetaFG_0] (main.py 265): INFO Train: [14/300][120/1562]	eta 0:07:28 lr 0.000004	time 0.2929 (0.3108)	loss 1.8275 (1.7198)	grad_norm 17.7155 (20.9781)	mem 4879MB
[2022-05-31 00:17:11 MetaFG_0] (main.py 265): INFO Train: [14/300][130/1562]	eta 0:07:24 lr 0.000004	time 0.2921 (0.3102)	loss 1.7864 (1.7152)	grad_norm 22.4225 (21.0243)	mem 4879MB
[2022-05-31 00:17:14 MetaFG_0] (main.py 265): INFO Train: [14/300][140/1562]	eta 0:07:20 lr 0.000004	time 0.2944 (0.3096)	loss 1.4412 (1.7189)	grad_norm 29.1812 (20.9747)	mem 4879MB
[2022-05-31 00:17:17 MetaFG_0] (main.py 265): INFO Train: [14/300][150/1562]	eta 0:07:16 lr 0.000004	time 0.2923 (0.3094)	loss 2.0055 (1.7216)	grad_norm 20.8623 (20.9924)	mem 4879MB
[2022-05-31 00:17:20 MetaFG_0] (main.py 265): INFO Train: [14/300][160/1562]	eta 0:07:13 lr 0.000004	time 0.2975 (0.3090)	loss 1.7450 (1.7205)	grad_norm 19.5246 (20.9856)	mem 4879MB
[2022-05-31 00:17:23 MetaFG_0] (main.py 265): INFO Train: [14/300][170/1562]	eta 0:07:09 lr 0.000004	time 0.2913 (0.3087)	loss 1.8411 (1.7174)	grad_norm 21.1017 (21.0908)	mem 4879MB
[2022-05-31 00:17:26 MetaFG_0] (main.py 265): INFO Train: [14/300][180/1562]	eta 0:07:06 lr 0.000004	time 0.2988 (0.3084)	loss 1.7619 (1.7203)	grad_norm 17.7217 (21.0851)	mem 4879MB
[2022-05-31 00:17:29 MetaFG_0] (main.py 265): INFO Train: [14/300][190/1562]	eta 0:07:02 lr 0.000004	time 0.2934 (0.3080)	loss 1.5804 (1.7215)	grad_norm 16.3140 (21.1555)	mem 4879MB
[2022-05-31 00:17:32 MetaFG_0] (main.py 265): INFO Train: [14/300][200/1562]	eta 0:06:59 lr 0.000004	time 0.2947 (0.3077)	loss 1.8629 (1.7224)	grad_norm 11.0129 (21.1613)	mem 4879MB
[2022-05-31 00:17:36 MetaFG_0] (main.py 265): INFO Train: [14/300][210/1562]	eta 0:06:55 lr 0.000004	time 0.2923 (0.3076)	loss 1.8205 (1.7218)	grad_norm 35.5839 (21.2146)	mem 4879MB
[2022-05-31 00:17:39 MetaFG_0] (main.py 265): INFO Train: [14/300][220/1562]	eta 0:06:52 lr 0.000004	time 0.2921 (0.3074)	loss 1.3655 (1.7195)	grad_norm 15.1293 (21.2499)	mem 4879MB
[2022-05-31 00:17:42 MetaFG_0] (main.py 265): INFO Train: [14/300][230/1562]	eta 0:06:49 lr 0.000004	time 0.2931 (0.3072)	loss 1.5262 (1.7171)	grad_norm 20.6946 (21.4697)	mem 4879MB
[2022-05-31 00:17:45 MetaFG_0] (main.py 265): INFO Train: [14/300][240/1562]	eta 0:06:45 lr 0.000004	time 0.2988 (0.3071)	loss 1.4668 (1.7184)	grad_norm 21.6619 (21.4087)	mem 4879MB
[2022-05-31 00:17:48 MetaFG_0] (main.py 265): INFO Train: [14/300][250/1562]	eta 0:06:42 lr 0.000004	time 0.2977 (0.3069)	loss 1.9014 (1.7161)	grad_norm 22.2642 (21.4520)	mem 4879MB
[2022-05-31 00:17:51 MetaFG_0] (main.py 265): INFO Train: [14/300][260/1562]	eta 0:06:39 lr 0.000004	time 0.2972 (0.3068)	loss 1.8000 (1.7142)	grad_norm 17.8133 (21.3966)	mem 4879MB
[2022-05-31 00:17:54 MetaFG_0] (main.py 265): INFO Train: [14/300][270/1562]	eta 0:06:36 lr 0.000004	time 0.2972 (0.3067)	loss 1.5637 (1.7126)	grad_norm 21.8318 (21.4261)	mem 4879MB
[2022-05-31 00:17:57 MetaFG_0] (main.py 265): INFO Train: [14/300][280/1562]	eta 0:06:33 lr 0.000004	time 0.2982 (0.3066)	loss 1.9964 (1.7151)	grad_norm 22.9663 (21.6289)	mem 4879MB
[2022-05-31 00:18:00 MetaFG_0] (main.py 265): INFO Train: [14/300][290/1562]	eta 0:06:29 lr 0.000004	time 0.2920 (0.3065)	loss 1.4499 (1.7136)	grad_norm 27.3276 (21.6416)	mem 4879MB
[2022-05-31 00:18:03 MetaFG_0] (main.py 265): INFO Train: [14/300][300/1562]	eta 0:06:26 lr 0.000004	time 0.2939 (0.3064)	loss 1.9404 (1.7131)	grad_norm 14.5915 (21.6655)	mem 4879MB
[2022-05-31 00:18:06 MetaFG_0] (main.py 265): INFO Train: [14/300][310/1562]	eta 0:06:23 lr 0.000004	time 0.2995 (0.3063)	loss 1.6565 (1.7134)	grad_norm 16.0482 (21.6932)	mem 4879MB
[2022-05-31 00:18:09 MetaFG_0] (main.py 265): INFO Train: [14/300][320/1562]	eta 0:06:20 lr 0.000004	time 0.2959 (0.3063)	loss 1.4692 (1.7166)	grad_norm 19.2230 (21.8129)	mem 4879MB
[2022-05-31 00:18:12 MetaFG_0] (main.py 265): INFO Train: [14/300][330/1562]	eta 0:06:17 lr 0.000004	time 0.3001 (0.3062)	loss 1.7377 (1.7128)	grad_norm 18.9584 (21.7841)	mem 4879MB
[2022-05-31 00:18:15 MetaFG_0] (main.py 265): INFO Train: [14/300][340/1562]	eta 0:06:14 lr 0.000004	time 0.2939 (0.3061)	loss 1.7893 (1.7134)	grad_norm 20.1485 (21.8216)	mem 4879MB
[2022-05-31 00:18:18 MetaFG_0] (main.py 265): INFO Train: [14/300][350/1562]	eta 0:06:10 lr 0.000004	time 0.2984 (0.3060)	loss 1.8324 (1.7131)	grad_norm 13.5888 (21.9258)	mem 4879MB
[2022-05-31 00:18:21 MetaFG_0] (main.py 265): INFO Train: [14/300][360/1562]	eta 0:06:07 lr 0.000004	time 0.2918 (0.3060)	loss 1.3408 (1.7129)	grad_norm 30.0993 (21.9778)	mem 4879MB
[2022-05-31 00:18:24 MetaFG_0] (main.py 265): INFO Train: [14/300][370/1562]	eta 0:06:04 lr 0.000004	time 0.2933 (0.3059)	loss 1.8018 (1.7131)	grad_norm 25.5982 (22.0201)	mem 4879MB
[2022-05-31 00:18:27 MetaFG_0] (main.py 265): INFO Train: [14/300][380/1562]	eta 0:06:01 lr 0.000004	time 0.2990 (0.3059)	loss 1.7663 (1.7133)	grad_norm 17.2343 (21.9792)	mem 4879MB
[2022-05-31 00:18:30 MetaFG_0] (main.py 265): INFO Train: [14/300][390/1562]	eta 0:05:58 lr 0.000004	time 0.2947 (0.3058)	loss 1.7858 (1.7153)	grad_norm 17.5786 (21.9702)	mem 4879MB
[2022-05-31 00:18:33 MetaFG_0] (main.py 265): INFO Train: [14/300][400/1562]	eta 0:05:55 lr 0.000004	time 0.3031 (0.3058)	loss 1.7451 (1.7164)	grad_norm 25.2755 (21.9422)	mem 4879MB
[2022-05-31 00:18:36 MetaFG_0] (main.py 265): INFO Train: [14/300][410/1562]	eta 0:05:52 lr 0.000004	time 0.2930 (0.3057)	loss 1.7923 (1.7183)	grad_norm 14.8162 (21.9110)	mem 4879MB
[2022-05-31 00:18:39 MetaFG_0] (main.py 265): INFO Train: [14/300][420/1562]	eta 0:05:49 lr 0.000004	time 0.2924 (0.3057)	loss 1.3834 (1.7167)	grad_norm 26.1111 (21.9412)	mem 4879MB
[2022-05-31 00:18:42 MetaFG_0] (main.py 265): INFO Train: [14/300][430/1562]	eta 0:05:45 lr 0.000004	time 0.2918 (0.3056)	loss 1.9971 (1.7160)	grad_norm 19.8108 (21.8783)	mem 4879MB
[2022-05-31 00:18:45 MetaFG_0] (main.py 265): INFO Train: [14/300][440/1562]	eta 0:05:42 lr 0.000004	time 0.3005 (0.3056)	loss 1.6175 (1.7147)	grad_norm 17.3621 (21.9148)	mem 4879MB
[2022-05-31 00:18:48 MetaFG_0] (main.py 265): INFO Train: [14/300][450/1562]	eta 0:05:39 lr 0.000004	time 0.2930 (0.3055)	loss 1.7937 (1.7165)	grad_norm 15.2615 (22.0171)	mem 4879MB
[2022-05-31 00:18:51 MetaFG_0] (main.py 265): INFO Train: [14/300][460/1562]	eta 0:05:36 lr 0.000004	time 0.2992 (0.3055)	loss 1.8100 (1.7177)	grad_norm 26.6651 (22.0712)	mem 4879MB
[2022-05-31 00:18:54 MetaFG_0] (main.py 265): INFO Train: [14/300][470/1562]	eta 0:05:33 lr 0.000004	time 0.2924 (0.3054)	loss 1.8012 (1.7193)	grad_norm 20.0076 (21.9760)	mem 4879MB
[2022-05-31 00:18:58 MetaFG_0] (main.py 265): INFO Train: [14/300][480/1562]	eta 0:05:30 lr 0.000004	time 0.2981 (0.3054)	loss 1.5060 (1.7197)	grad_norm 19.7689 (21.9233)	mem 4879MB
[2022-05-31 00:19:01 MetaFG_0] (main.py 265): INFO Train: [14/300][490/1562]	eta 0:05:27 lr 0.000004	time 0.2935 (0.3054)	loss 1.7102 (1.7194)	grad_norm 14.1468 (21.8725)	mem 4879MB
[2022-05-31 00:19:04 MetaFG_0] (main.py 265): INFO Train: [14/300][500/1562]	eta 0:05:24 lr 0.000004	time 0.3078 (0.3056)	loss 1.5689 (1.7192)	grad_norm 34.1141 (21.9263)	mem 4879MB
[2022-05-31 00:19:07 MetaFG_0] (main.py 265): INFO Train: [14/300][510/1562]	eta 0:05:21 lr 0.000004	time 0.2916 (0.3055)	loss 1.7831 (1.7196)	grad_norm 14.3453 (21.9273)	mem 4879MB
[2022-05-31 00:19:10 MetaFG_0] (main.py 265): INFO Train: [14/300][520/1562]	eta 0:05:18 lr 0.000004	time 0.2944 (0.3055)	loss 1.9196 (1.7204)	grad_norm 24.3853 (21.9603)	mem 4879MB
[2022-05-31 00:19:13 MetaFG_0] (main.py 265): INFO Train: [14/300][530/1562]	eta 0:05:15 lr 0.000004	time 0.2919 (0.3055)	loss 1.7653 (1.7201)	grad_norm 23.7816 (22.0688)	mem 4879MB
[2022-05-31 00:19:16 MetaFG_0] (main.py 265): INFO Train: [14/300][540/1562]	eta 0:05:12 lr 0.000004	time 0.2981 (0.3055)	loss 1.6234 (1.7196)	grad_norm 26.2710 (22.1494)	mem 4879MB
[2022-05-31 00:19:19 MetaFG_0] (main.py 265): INFO Train: [14/300][550/1562]	eta 0:05:09 lr 0.000004	time 0.2986 (0.3054)	loss 1.8982 (1.7207)	grad_norm 15.3202 (22.1157)	mem 4879MB
[2022-05-31 00:19:22 MetaFG_0] (main.py 265): INFO Train: [14/300][560/1562]	eta 0:05:06 lr 0.000004	time 0.2918 (0.3054)	loss 1.6568 (1.7189)	grad_norm 18.7178 (22.1419)	mem 4879MB
[2022-05-31 00:19:25 MetaFG_0] (main.py 265): INFO Train: [14/300][570/1562]	eta 0:05:02 lr 0.000004	time 0.2930 (0.3054)	loss 1.8547 (1.7198)	grad_norm 25.1317 (22.1231)	mem 4879MB
[2022-05-31 00:19:28 MetaFG_0] (main.py 265): INFO Train: [14/300][580/1562]	eta 0:04:59 lr 0.000004	time 0.2927 (0.3054)	loss 1.5163 (1.7214)	grad_norm 36.5484 (22.0856)	mem 4879MB
[2022-05-31 00:19:31 MetaFG_0] (main.py 265): INFO Train: [14/300][590/1562]	eta 0:04:56 lr 0.000004	time 0.3033 (0.3054)	loss 1.7527 (1.7220)	grad_norm 15.7414 (22.0577)	mem 4879MB
[2022-05-31 00:19:34 MetaFG_0] (main.py 265): INFO Train: [14/300][600/1562]	eta 0:04:53 lr 0.000004	time 0.2917 (0.3053)	loss 1.4527 (1.7231)	grad_norm 23.2348 (22.0486)	mem 4879MB
[2022-05-31 00:19:37 MetaFG_0] (main.py 265): INFO Train: [14/300][610/1562]	eta 0:04:50 lr 0.000004	time 0.2921 (0.3053)	loss 1.6335 (1.7213)	grad_norm 33.5778 (22.0786)	mem 4879MB
[2022-05-31 00:19:40 MetaFG_0] (main.py 265): INFO Train: [14/300][620/1562]	eta 0:04:47 lr 0.000005	time 0.2919 (0.3052)	loss 1.4816 (1.7217)	grad_norm 27.7880 (22.1754)	mem 4879MB
[2022-05-31 00:19:43 MetaFG_0] (main.py 265): INFO Train: [14/300][630/1562]	eta 0:04:44 lr 0.000005	time 0.2980 (0.3052)	loss 1.4854 (1.7206)	grad_norm 17.4334 (22.2049)	mem 4879MB
[2022-05-31 00:19:46 MetaFG_0] (main.py 265): INFO Train: [14/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.2942 (0.3052)	loss 1.7545 (1.7219)	grad_norm 21.4636 (22.1855)	mem 4879MB
[2022-05-31 00:19:49 MetaFG_0] (main.py 265): INFO Train: [14/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2920 (0.3052)	loss 1.5465 (1.7232)	grad_norm 21.1823 (22.1896)	mem 4879MB
[2022-05-31 00:19:52 MetaFG_0] (main.py 265): INFO Train: [14/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2938 (0.3052)	loss 1.9096 (1.7229)	grad_norm 27.3696 (22.2225)	mem 4879MB
[2022-05-31 00:19:55 MetaFG_0] (main.py 265): INFO Train: [14/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2920 (0.3052)	loss 1.5641 (1.7225)	grad_norm 37.2772 (22.2478)	mem 4879MB
[2022-05-31 00:19:58 MetaFG_0] (main.py 265): INFO Train: [14/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2926 (0.3051)	loss 1.7780 (1.7221)	grad_norm 15.6010 (22.2188)	mem 4879MB
[2022-05-31 00:20:01 MetaFG_0] (main.py 265): INFO Train: [14/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2978 (0.3051)	loss 1.5453 (1.7223)	grad_norm 17.4494 (22.1980)	mem 4879MB
[2022-05-31 00:20:04 MetaFG_0] (main.py 265): INFO Train: [14/300][700/1562]	eta 0:04:22 lr 0.000005	time 0.2928 (0.3051)	loss 1.6331 (1.7209)	grad_norm 24.7904 (22.2047)	mem 4879MB
[2022-05-31 00:20:08 MetaFG_0] (main.py 265): INFO Train: [14/300][710/1562]	eta 0:04:19 lr 0.000005	time 0.2982 (0.3051)	loss 1.9777 (1.7217)	grad_norm 33.1831 (22.2427)	mem 4879MB
[2022-05-31 00:20:11 MetaFG_0] (main.py 265): INFO Train: [14/300][720/1562]	eta 0:04:16 lr 0.000005	time 0.2921 (0.3050)	loss 1.4239 (1.7209)	grad_norm 21.3406 (22.2363)	mem 4879MB
[2022-05-31 00:20:14 MetaFG_0] (main.py 265): INFO Train: [14/300][730/1562]	eta 0:04:13 lr 0.000005	time 0.2917 (0.3050)	loss 1.9838 (1.7201)	grad_norm 27.6449 (22.2992)	mem 4879MB
[2022-05-31 00:20:17 MetaFG_0] (main.py 265): INFO Train: [14/300][740/1562]	eta 0:04:10 lr 0.000005	time 0.2919 (0.3050)	loss 1.8006 (1.7206)	grad_norm 18.9519 (22.3235)	mem 4879MB
[2022-05-31 00:20:20 MetaFG_0] (main.py 265): INFO Train: [14/300][750/1562]	eta 0:04:07 lr 0.000005	time 0.3001 (0.3050)	loss 1.6327 (1.7199)	grad_norm 21.6958 (22.3104)	mem 4879MB
[2022-05-31 00:20:23 MetaFG_0] (main.py 265): INFO Train: [14/300][760/1562]	eta 0:04:04 lr 0.000005	time 0.2935 (0.3050)	loss 1.7252 (1.7208)	grad_norm 27.0603 (22.2762)	mem 4879MB
[2022-05-31 00:20:26 MetaFG_0] (main.py 265): INFO Train: [14/300][770/1562]	eta 0:04:01 lr 0.000005	time 0.2970 (0.3050)	loss 1.8786 (1.7213)	grad_norm 22.8363 (22.2620)	mem 4879MB
[2022-05-31 00:20:29 MetaFG_0] (main.py 265): INFO Train: [14/300][780/1562]	eta 0:03:58 lr 0.000005	time 0.2978 (0.3050)	loss 1.8098 (1.7208)	grad_norm 19.4131 (22.2462)	mem 4879MB
[2022-05-31 00:20:32 MetaFG_0] (main.py 265): INFO Train: [14/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2935 (0.3050)	loss 1.7083 (1.7218)	grad_norm 27.0574 (22.2508)	mem 4879MB
[2022-05-31 00:20:35 MetaFG_0] (main.py 265): INFO Train: [14/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2935 (0.3049)	loss 1.7449 (1.7216)	grad_norm 19.1122 (22.2598)	mem 4879MB
[2022-05-31 00:20:38 MetaFG_0] (main.py 265): INFO Train: [14/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2934 (0.3049)	loss 1.8355 (1.7203)	grad_norm 22.9666 (22.2819)	mem 4879MB
[2022-05-31 00:20:41 MetaFG_0] (main.py 265): INFO Train: [14/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2985 (0.3049)	loss 1.7754 (1.7193)	grad_norm 24.4058 (22.3115)	mem 4879MB
[2022-05-31 00:20:44 MetaFG_0] (main.py 265): INFO Train: [14/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.3002 (0.3049)	loss 1.6676 (1.7186)	grad_norm 18.8954 (22.3758)	mem 4879MB
[2022-05-31 00:20:47 MetaFG_0] (main.py 265): INFO Train: [14/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.3009 (0.3049)	loss 1.8534 (1.7176)	grad_norm 21.0132 (22.3843)	mem 4879MB
[2022-05-31 00:20:50 MetaFG_0] (main.py 265): INFO Train: [14/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2985 (0.3049)	loss 1.8319 (1.7178)	grad_norm 19.9331 (22.3943)	mem 4879MB
[2022-05-31 00:20:53 MetaFG_0] (main.py 265): INFO Train: [14/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2984 (0.3049)	loss 1.6536 (1.7174)	grad_norm 19.0756 (22.3904)	mem 4879MB
[2022-05-31 00:20:56 MetaFG_0] (main.py 265): INFO Train: [14/300][870/1562]	eta 0:03:30 lr 0.000005	time 0.2915 (0.3049)	loss 1.8159 (1.7172)	grad_norm 28.7199 (22.3723)	mem 4879MB
[2022-05-31 00:20:59 MetaFG_0] (main.py 265): INFO Train: [14/300][880/1562]	eta 0:03:27 lr 0.000005	time 0.2981 (0.3049)	loss 1.8237 (1.7168)	grad_norm 18.8605 (22.3633)	mem 4879MB
[2022-05-31 00:21:02 MetaFG_0] (main.py 265): INFO Train: [14/300][890/1562]	eta 0:03:24 lr 0.000005	time 0.2918 (0.3049)	loss 1.2025 (1.7168)	grad_norm 32.4013 (22.3466)	mem 4879MB
[2022-05-31 00:21:05 MetaFG_0] (main.py 265): INFO Train: [14/300][900/1562]	eta 0:03:21 lr 0.000005	time 0.2918 (0.3049)	loss 1.5531 (1.7174)	grad_norm 27.4402 (22.3304)	mem 4879MB
[2022-05-31 00:21:08 MetaFG_0] (main.py 265): INFO Train: [14/300][910/1562]	eta 0:03:18 lr 0.000005	time 0.2928 (0.3049)	loss 1.7443 (1.7166)	grad_norm 17.6147 (22.3390)	mem 4879MB
[2022-05-31 00:21:11 MetaFG_0] (main.py 265): INFO Train: [14/300][920/1562]	eta 0:03:15 lr 0.000005	time 0.2916 (0.3049)	loss 1.6248 (1.7172)	grad_norm 25.3824 (22.3294)	mem 4879MB
[2022-05-31 00:21:14 MetaFG_0] (main.py 265): INFO Train: [14/300][930/1562]	eta 0:03:12 lr 0.000005	time 0.2931 (0.3049)	loss 2.0595 (1.7176)	grad_norm 22.9066 (22.3339)	mem 4879MB
[2022-05-31 00:21:18 MetaFG_0] (main.py 265): INFO Train: [14/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2981 (0.3050)	loss 1.8033 (1.7179)	grad_norm 23.6484 (22.3129)	mem 4879MB
[2022-05-31 00:21:21 MetaFG_0] (main.py 265): INFO Train: [14/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.2985 (0.3050)	loss 1.8654 (1.7177)	grad_norm 16.9429 (22.3489)	mem 4879MB
[2022-05-31 00:21:24 MetaFG_0] (main.py 265): INFO Train: [14/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2930 (0.3050)	loss 1.6118 (1.7168)	grad_norm 26.6012 (22.3852)	mem 4879MB
[2022-05-31 00:21:27 MetaFG_0] (main.py 265): INFO Train: [14/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2924 (0.3050)	loss 1.3823 (1.7164)	grad_norm 22.2455 (22.3622)	mem 4879MB
[2022-05-31 00:21:30 MetaFG_0] (main.py 265): INFO Train: [14/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.3005 (0.3050)	loss 1.9156 (1.7173)	grad_norm 14.3665 (22.3305)	mem 4879MB
[2022-05-31 00:21:33 MetaFG_0] (main.py 265): INFO Train: [14/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2932 (0.3050)	loss 1.5593 (1.7172)	grad_norm 15.9930 (22.3296)	mem 4879MB
[2022-05-31 00:21:36 MetaFG_0] (main.py 265): INFO Train: [14/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2990 (0.3050)	loss 1.9512 (1.7171)	grad_norm 31.8729 (22.3397)	mem 4879MB
[2022-05-31 00:21:39 MetaFG_0] (main.py 265): INFO Train: [14/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2922 (0.3050)	loss 1.5004 (1.7155)	grad_norm 29.4008 (22.3788)	mem 4879MB
[2022-05-31 00:21:42 MetaFG_0] (main.py 265): INFO Train: [14/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2920 (0.3050)	loss 1.7429 (1.7153)	grad_norm 33.1605 (22.3792)	mem 4879MB
[2022-05-31 00:21:45 MetaFG_0] (main.py 265): INFO Train: [14/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2941 (0.3050)	loss 1.4774 (1.7150)	grad_norm 19.1160 (22.3959)	mem 4879MB
[2022-05-31 00:21:48 MetaFG_0] (main.py 265): INFO Train: [14/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2978 (0.3050)	loss 1.9204 (1.7152)	grad_norm 38.3393 (22.3964)	mem 4879MB
[2022-05-31 00:21:51 MetaFG_0] (main.py 265): INFO Train: [14/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2937 (0.3050)	loss 1.7973 (1.7156)	grad_norm 17.7587 (22.3817)	mem 4879MB
[2022-05-31 00:21:54 MetaFG_0] (main.py 265): INFO Train: [14/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2934 (0.3050)	loss 1.4613 (1.7148)	grad_norm 17.4135 (22.3695)	mem 4879MB
[2022-05-31 00:21:57 MetaFG_0] (main.py 265): INFO Train: [14/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.3028 (0.3050)	loss 1.9215 (1.7149)	grad_norm 27.3282 (22.4086)	mem 4879MB
[2022-05-31 00:22:00 MetaFG_0] (main.py 265): INFO Train: [14/300][1080/1562]	eta 0:02:26 lr 0.000005	time 0.3023 (0.3050)	loss 1.5695 (1.7149)	grad_norm 27.6001 (22.4144)	mem 4879MB
[2022-05-31 00:22:03 MetaFG_0] (main.py 265): INFO Train: [14/300][1090/1562]	eta 0:02:23 lr 0.000005	time 0.3005 (0.3050)	loss 1.8732 (1.7146)	grad_norm 19.6984 (22.4098)	mem 4879MB
[2022-05-31 00:22:06 MetaFG_0] (main.py 265): INFO Train: [14/300][1100/1562]	eta 0:02:20 lr 0.000005	time 0.2961 (0.3050)	loss 1.7139 (1.7145)	grad_norm 29.0988 (22.4286)	mem 4879MB
[2022-05-31 00:22:09 MetaFG_0] (main.py 265): INFO Train: [14/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2984 (0.3050)	loss 1.8068 (1.7138)	grad_norm 19.2324 (22.4511)	mem 4879MB
[2022-05-31 00:22:12 MetaFG_0] (main.py 265): INFO Train: [14/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2948 (0.3050)	loss 1.7192 (1.7137)	grad_norm 22.3701 (22.4378)	mem 4879MB
[2022-05-31 00:22:16 MetaFG_0] (main.py 265): INFO Train: [14/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2930 (0.3049)	loss 1.3842 (1.7128)	grad_norm 21.8496 (22.4041)	mem 4879MB
[2022-05-31 00:22:19 MetaFG_0] (main.py 265): INFO Train: [14/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2937 (0.3049)	loss 1.7320 (1.7131)	grad_norm 18.9458 (22.3897)	mem 4879MB
[2022-05-31 00:22:22 MetaFG_0] (main.py 265): INFO Train: [14/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.3005 (0.3049)	loss 1.7395 (1.7136)	grad_norm 18.4256 (22.3666)	mem 4879MB
[2022-05-31 00:22:25 MetaFG_0] (main.py 265): INFO Train: [14/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2922 (0.3049)	loss 1.8197 (1.7134)	grad_norm 20.0305 (22.3882)	mem 4879MB
[2022-05-31 00:22:28 MetaFG_0] (main.py 265): INFO Train: [14/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2979 (0.3049)	loss 1.8311 (1.7130)	grad_norm 20.7869 (22.4056)	mem 4879MB
[2022-05-31 00:22:31 MetaFG_0] (main.py 265): INFO Train: [14/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2931 (0.3049)	loss 1.3585 (1.7125)	grad_norm 19.0521 (22.4501)	mem 4879MB
[2022-05-31 00:22:34 MetaFG_0] (main.py 265): INFO Train: [14/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2992 (0.3049)	loss 1.7636 (1.7118)	grad_norm 30.4164 (22.4694)	mem 4879MB
[2022-05-31 00:22:37 MetaFG_0] (main.py 265): INFO Train: [14/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2931 (0.3049)	loss 1.9253 (1.7119)	grad_norm 14.6576 (22.4735)	mem 4879MB
[2022-05-31 00:22:40 MetaFG_0] (main.py 265): INFO Train: [14/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2941 (0.3049)	loss 1.9889 (1.7118)	grad_norm 20.8717 (22.4799)	mem 4879MB
[2022-05-31 00:22:43 MetaFG_0] (main.py 265): INFO Train: [14/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2923 (0.3049)	loss 1.5374 (1.7118)	grad_norm 21.3348 (22.4743)	mem 4879MB
[2022-05-31 00:22:46 MetaFG_0] (main.py 265): INFO Train: [14/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2986 (0.3049)	loss 1.9066 (1.7117)	grad_norm 26.0872 (22.4651)	mem 4879MB
[2022-05-31 00:22:49 MetaFG_0] (main.py 265): INFO Train: [14/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2936 (0.3049)	loss 1.8145 (1.7123)	grad_norm 26.7930 (22.4758)	mem 4879MB
[2022-05-31 00:22:52 MetaFG_0] (main.py 265): INFO Train: [14/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2938 (0.3049)	loss 1.7051 (1.7118)	grad_norm 21.1178 (22.4874)	mem 4879MB
[2022-05-31 00:22:55 MetaFG_0] (main.py 265): INFO Train: [14/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2998 (0.3049)	loss 1.6215 (1.7117)	grad_norm 17.4939 (22.5851)	mem 4879MB
[2022-05-31 00:22:58 MetaFG_0] (main.py 265): INFO Train: [14/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2939 (0.3049)	loss 1.9585 (1.7112)	grad_norm 15.7978 (22.5758)	mem 4879MB
[2022-05-31 00:23:01 MetaFG_0] (main.py 265): INFO Train: [14/300][1280/1562]	eta 0:01:25 lr 0.000005	time 0.2989 (0.3049)	loss 1.7312 (1.7109)	grad_norm 26.9977 (22.5951)	mem 4879MB
[2022-05-31 00:23:04 MetaFG_0] (main.py 265): INFO Train: [14/300][1290/1562]	eta 0:01:22 lr 0.000005	time 0.2934 (0.3048)	loss 1.7076 (1.7110)	grad_norm 17.8761 (22.6084)	mem 4879MB
[2022-05-31 00:23:07 MetaFG_0] (main.py 265): INFO Train: [14/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2991 (0.3048)	loss 1.2685 (1.7102)	grad_norm 47.5910 (22.6110)	mem 4879MB
[2022-05-31 00:23:10 MetaFG_0] (main.py 265): INFO Train: [14/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2934 (0.3048)	loss 1.7453 (1.7098)	grad_norm 14.9968 (22.5955)	mem 4879MB
[2022-05-31 00:23:13 MetaFG_0] (main.py 265): INFO Train: [14/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.3013 (0.3049)	loss 1.7364 (1.7093)	grad_norm 15.8574 (22.6157)	mem 4879MB
[2022-05-31 00:23:16 MetaFG_0] (main.py 265): INFO Train: [14/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.3028 (0.3049)	loss 1.9040 (1.7101)	grad_norm 23.7126 (22.6203)	mem 4879MB
[2022-05-31 00:23:19 MetaFG_0] (main.py 265): INFO Train: [14/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2990 (0.3049)	loss 1.9528 (1.7099)	grad_norm 17.4812 (22.6177)	mem 4879MB
[2022-05-31 00:23:23 MetaFG_0] (main.py 265): INFO Train: [14/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2975 (0.3049)	loss 1.8343 (1.7097)	grad_norm 15.1179 (22.6297)	mem 4879MB
[2022-05-31 00:23:26 MetaFG_0] (main.py 265): INFO Train: [14/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2931 (0.3049)	loss 1.8820 (1.7099)	grad_norm 22.0394 (22.6331)	mem 4879MB
[2022-05-31 00:23:29 MetaFG_0] (main.py 265): INFO Train: [14/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2979 (0.3049)	loss 1.6909 (1.7101)	grad_norm 13.7135 (22.6328)	mem 4879MB
[2022-05-31 00:23:32 MetaFG_0] (main.py 265): INFO Train: [14/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2945 (0.3049)	loss 1.7484 (1.7096)	grad_norm 19.0111 (22.6525)	mem 4879MB
[2022-05-31 00:23:35 MetaFG_0] (main.py 265): INFO Train: [14/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2945 (0.3049)	loss 1.7126 (1.7097)	grad_norm 22.9036 (22.6263)	mem 4879MB
[2022-05-31 00:23:38 MetaFG_0] (main.py 265): INFO Train: [14/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2921 (0.3049)	loss 1.5897 (1.7101)	grad_norm 17.7051 (22.6135)	mem 4879MB
[2022-05-31 00:23:41 MetaFG_0] (main.py 265): INFO Train: [14/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2927 (0.3049)	loss 1.9936 (1.7099)	grad_norm 22.1750 (22.6229)	mem 4879MB
[2022-05-31 00:23:44 MetaFG_0] (main.py 265): INFO Train: [14/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2936 (0.3049)	loss 1.3925 (1.7090)	grad_norm 16.3011 (22.6191)	mem 4879MB
[2022-05-31 00:23:47 MetaFG_0] (main.py 265): INFO Train: [14/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2921 (0.3049)	loss 1.3607 (1.7089)	grad_norm 13.4231 (22.6106)	mem 4879MB
[2022-05-31 00:23:50 MetaFG_0] (main.py 265): INFO Train: [14/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2934 (0.3049)	loss 1.8455 (1.7090)	grad_norm 14.4097 (22.5928)	mem 4879MB
[2022-05-31 00:23:53 MetaFG_0] (main.py 265): INFO Train: [14/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2926 (0.3049)	loss 1.7220 (1.7095)	grad_norm 28.1231 (22.6018)	mem 4879MB
[2022-05-31 00:23:56 MetaFG_0] (main.py 265): INFO Train: [14/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2992 (0.3049)	loss 1.8067 (1.7096)	grad_norm 17.8304 (22.5877)	mem 4879MB
[2022-05-31 00:23:59 MetaFG_0] (main.py 265): INFO Train: [14/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2930 (0.3049)	loss 1.3912 (1.7091)	grad_norm 18.5277 (22.6200)	mem 4879MB
[2022-05-31 00:24:02 MetaFG_0] (main.py 265): INFO Train: [14/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2932 (0.3049)	loss 1.8720 (1.7094)	grad_norm 16.0149 (22.6229)	mem 4879MB
[2022-05-31 00:24:05 MetaFG_0] (main.py 265): INFO Train: [14/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2993 (0.3049)	loss 1.5197 (1.7093)	grad_norm 17.6819 (22.6191)	mem 4879MB
[2022-05-31 00:24:08 MetaFG_0] (main.py 265): INFO Train: [14/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2934 (0.3049)	loss 1.7800 (1.7092)	grad_norm 36.4413 (22.6306)	mem 4879MB
[2022-05-31 00:24:11 MetaFG_0] (main.py 265): INFO Train: [14/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2927 (0.3049)	loss 1.6388 (1.7089)	grad_norm 14.6023 (22.6385)	mem 4879MB
[2022-05-31 00:24:14 MetaFG_0] (main.py 265): INFO Train: [14/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2995 (0.3049)	loss 1.5127 (1.7086)	grad_norm 35.1339 (22.6645)	mem 4879MB
[2022-05-31 00:24:17 MetaFG_0] (main.py 265): INFO Train: [14/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2947 (0.3049)	loss 1.8097 (1.7081)	grad_norm 13.8836 (22.6491)	mem 4879MB
[2022-05-31 00:24:20 MetaFG_0] (main.py 265): INFO Train: [14/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2984 (0.3049)	loss 1.6965 (1.7076)	grad_norm 63.4390 (22.6670)	mem 4879MB
[2022-05-31 00:24:23 MetaFG_0] (main.py 265): INFO Train: [14/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2937 (0.3049)	loss 1.8121 (1.7069)	grad_norm 28.4145 (22.6728)	mem 4879MB
[2022-05-31 00:24:26 MetaFG_0] (main.py 265): INFO Train: [14/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2915 (0.3049)	loss 1.8272 (1.7064)	grad_norm 15.8716 (22.6777)	mem 4879MB
[2022-05-31 00:24:27 MetaFG_0] (main.py 272): INFO EPOCH 14 training takes 0:07:56
[2022-05-31 00:24:27 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_14.pth saving......
[2022-05-31 00:24:28 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_14.pth saved !!!
[2022-05-31 00:24:28 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:24:29 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:24:29 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:24:30 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.651 (0.651)	Loss 1.7112 (1.7112)	Acc@1 71.875 (71.875)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 00:24:31 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.146)	Loss 1.1973 (1.6319)	Acc@1 75.000 (67.898)	Acc@5 93.750 (92.898)	Mem 4879MB
[2022-05-31 00:24:32 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.123)	Loss 1.7454 (1.5820)	Acc@1 56.250 (68.006)	Acc@5 100.000 (94.792)	Mem 4879MB
[2022-05-31 00:24:33 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.113)	Loss 1.4462 (1.5802)	Acc@1 59.375 (68.347)	Acc@5 100.000 (94.859)	Mem 4879MB
[2022-05-31 00:24:34 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.109)	Loss 1.5365 (1.5721)	Acc@1 78.125 (68.216)	Acc@5 96.875 (94.817)	Mem 4879MB
[2022-05-31 00:24:35 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.106)	Loss 1.5172 (1.5766)	Acc@1 68.750 (68.015)	Acc@5 100.000 (94.914)	Mem 4879MB
[2022-05-31 00:24:36 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 1.6645 (1.5685)	Acc@1 65.625 (68.238)	Acc@5 84.375 (94.980)	Mem 4879MB
[2022-05-31 00:24:37 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.092 (0.103)	Loss 1.3973 (1.5674)	Acc@1 71.875 (68.354)	Acc@5 100.000 (94.806)	Mem 4879MB
[2022-05-31 00:24:38 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.102)	Loss 1.6759 (1.5750)	Acc@1 59.375 (67.978)	Acc@5 93.750 (94.676)	Mem 4879MB
[2022-05-31 00:24:38 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 1.8530 (1.5779)	Acc@1 62.500 (67.926)	Acc@5 81.250 (94.677)	Mem 4879MB
[2022-05-31 00:24:39 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 1.3929 (1.5753)	Acc@1 68.750 (67.884)	Acc@5 100.000 (94.833)	Mem 4879MB
[2022-05-31 00:24:40 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 1.5885 (1.5834)	Acc@1 68.750 (68.159)	Acc@5 100.000 (94.735)	Mem 4879MB
[2022-05-31 00:24:41 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.099)	Loss 1.4725 (1.5808)	Acc@1 68.750 (68.363)	Acc@5 93.750 (94.706)	Mem 4879MB
[2022-05-31 00:24:42 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.100 (0.099)	Loss 1.7808 (1.5827)	Acc@1 59.375 (68.225)	Acc@5 84.375 (94.752)	Mem 4879MB
[2022-05-31 00:24:43 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.098)	Loss 1.6034 (1.5875)	Acc@1 56.250 (68.085)	Acc@5 96.875 (94.614)	Mem 4879MB
[2022-05-31 00:24:44 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 1.3592 (1.5871)	Acc@1 78.125 (68.108)	Acc@5 100.000 (94.578)	Mem 4879MB
[2022-05-31 00:24:45 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.088 (0.098)	Loss 1.6520 (1.5925)	Acc@1 68.750 (67.954)	Acc@5 90.625 (94.255)	Mem 4879MB
[2022-05-31 00:24:46 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 1.7890 (1.5946)	Acc@1 65.625 (67.964)	Acc@5 93.750 (94.170)	Mem 4879MB
[2022-05-31 00:24:47 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.098)	Loss 1.8701 (1.5979)	Acc@1 50.000 (67.835)	Acc@5 90.625 (93.992)	Mem 4879MB
[2022-05-31 00:24:48 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.110 (0.098)	Loss 1.4112 (1.5950)	Acc@1 68.750 (67.932)	Acc@5 96.875 (94.028)	Mem 4879MB
[2022-05-31 00:24:49 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 1.3170 (1.5970)	Acc@1 84.375 (67.848)	Acc@5 96.875 (94.045)	Mem 4879MB
[2022-05-31 00:24:50 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 1.4885 (1.5953)	Acc@1 68.750 (67.995)	Acc@5 100.000 (94.091)	Mem 4879MB
[2022-05-31 00:24:51 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.089 (0.097)	Loss 1.4778 (1.5990)	Acc@1 84.375 (67.831)	Acc@5 96.875 (94.005)	Mem 4879MB
[2022-05-31 00:24:52 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 1.4606 (1.5979)	Acc@1 65.625 (67.871)	Acc@5 90.625 (93.966)	Mem 4879MB
[2022-05-31 00:24:53 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 1.9084 (1.5980)	Acc@1 59.375 (67.829)	Acc@5 93.750 (93.996)	Mem 4879MB
[2022-05-31 00:24:54 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.097)	Loss 1.4185 (1.5979)	Acc@1 84.375 (67.916)	Acc@5 100.000 (93.999)	Mem 4879MB
[2022-05-31 00:24:54 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 1.5134 (1.5967)	Acc@1 75.000 (68.032)	Acc@5 90.625 (94.085)	Mem 4879MB
[2022-05-31 00:24:55 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 1.8597 (1.5990)	Acc@1 62.500 (67.862)	Acc@5 90.625 (94.107)	Mem 4879MB
[2022-05-31 00:24:56 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.107 (0.096)	Loss 1.6328 (1.5949)	Acc@1 65.625 (68.038)	Acc@5 87.500 (94.117)	Mem 4879MB
[2022-05-31 00:24:57 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.096)	Loss 1.6026 (1.5920)	Acc@1 75.000 (68.127)	Acc@5 93.750 (94.126)	Mem 4879MB
[2022-05-31 00:24:58 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 1.5355 (1.5960)	Acc@1 75.000 (67.982)	Acc@5 93.750 (94.093)	Mem 4879MB
[2022-05-31 00:24:59 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 1.5253 (1.5937)	Acc@1 75.000 (67.986)	Acc@5 96.875 (94.172)	Mem 4879MB
[2022-05-31 00:24:59 MetaFG_0] (main.py 330): INFO  * Acc@1 68.030 Acc@5 94.170
[2022-05-31 00:24:59 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 68.0%
[2022-05-31 00:24:59 MetaFG_0] (main.py 171): INFO Max accuracy: 68.03%
[2022-05-31 00:25:00 MetaFG_0] (main.py 265): INFO Train: [15/300][0/1562]	eta 0:24:25 lr 0.000005	time 0.9380 (0.9380)	loss 1.6740 (1.6740)	grad_norm 20.7438 (20.7438)	mem 4879MB
[2022-05-31 00:25:03 MetaFG_0] (main.py 265): INFO Train: [15/300][10/1562]	eta 0:09:35 lr 0.000005	time 0.2946 (0.3706)	loss 1.3801 (1.6937)	grad_norm 18.6542 (21.7767)	mem 4879MB
[2022-05-31 00:25:06 MetaFG_0] (main.py 265): INFO Train: [15/300][20/1562]	eta 0:08:42 lr 0.000005	time 0.2921 (0.3390)	loss 1.6490 (1.6962)	grad_norm 45.5891 (22.5832)	mem 4879MB
[2022-05-31 00:25:10 MetaFG_0] (main.py 265): INFO Train: [15/300][30/1562]	eta 0:08:22 lr 0.000005	time 0.3002 (0.3277)	loss 1.7469 (1.7144)	grad_norm 21.6907 (22.6417)	mem 4879MB
[2022-05-31 00:25:13 MetaFG_0] (main.py 265): INFO Train: [15/300][40/1562]	eta 0:08:10 lr 0.000005	time 0.2918 (0.3220)	loss 1.5506 (1.7123)	grad_norm 21.1145 (22.3951)	mem 4879MB
[2022-05-31 00:25:16 MetaFG_0] (main.py 265): INFO Train: [15/300][50/1562]	eta 0:08:01 lr 0.000005	time 0.2973 (0.3183)	loss 1.5726 (1.7013)	grad_norm 22.1222 (22.5864)	mem 4879MB
[2022-05-31 00:25:19 MetaFG_0] (main.py 265): INFO Train: [15/300][60/1562]	eta 0:07:55 lr 0.000005	time 0.2980 (0.3164)	loss 1.5994 (1.6980)	grad_norm 19.0075 (21.8815)	mem 4879MB
[2022-05-31 00:25:22 MetaFG_0] (main.py 265): INFO Train: [15/300][70/1562]	eta 0:07:49 lr 0.000005	time 0.2991 (0.3145)	loss 1.6636 (1.6843)	grad_norm 26.7100 (22.0901)	mem 4879MB
[2022-05-31 00:25:25 MetaFG_0] (main.py 265): INFO Train: [15/300][80/1562]	eta 0:07:43 lr 0.000005	time 0.2988 (0.3131)	loss 1.4018 (1.6879)	grad_norm 19.5879 (22.1906)	mem 4879MB
[2022-05-31 00:25:28 MetaFG_0] (main.py 265): INFO Train: [15/300][90/1562]	eta 0:07:39 lr 0.000005	time 0.2985 (0.3124)	loss 1.5817 (1.6872)	grad_norm 24.1778 (22.2264)	mem 4879MB
[2022-05-31 00:25:31 MetaFG_0] (main.py 265): INFO Train: [15/300][100/1562]	eta 0:07:35 lr 0.000005	time 0.2917 (0.3117)	loss 1.7550 (1.6861)	grad_norm 25.4518 (22.4916)	mem 4879MB
[2022-05-31 00:25:34 MetaFG_0] (main.py 265): INFO Train: [15/300][110/1562]	eta 0:07:31 lr 0.000005	time 0.2994 (0.3111)	loss 1.7574 (1.6875)	grad_norm 26.1190 (22.2673)	mem 4879MB
[2022-05-31 00:25:37 MetaFG_0] (main.py 265): INFO Train: [15/300][120/1562]	eta 0:07:27 lr 0.000005	time 0.2923 (0.3105)	loss 1.8326 (1.6857)	grad_norm 23.2350 (22.3967)	mem 4879MB
[2022-05-31 00:25:40 MetaFG_0] (main.py 265): INFO Train: [15/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.3009 (0.3101)	loss 1.7645 (1.6885)	grad_norm 19.3605 (22.2952)	mem 4879MB
[2022-05-31 00:25:43 MetaFG_0] (main.py 265): INFO Train: [15/300][140/1562]	eta 0:07:20 lr 0.000005	time 0.2974 (0.3097)	loss 1.7648 (1.6920)	grad_norm 15.1716 (22.4137)	mem 4879MB
[2022-05-31 00:25:46 MetaFG_0] (main.py 265): INFO Train: [15/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2941 (0.3099)	loss 1.6873 (1.6950)	grad_norm 20.4553 (22.6284)	mem 4879MB
[2022-05-31 00:25:49 MetaFG_0] (main.py 265): INFO Train: [15/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.2979 (0.3096)	loss 1.8991 (1.6946)	grad_norm 19.3760 (23.0121)	mem 4879MB
[2022-05-31 00:25:52 MetaFG_0] (main.py 265): INFO Train: [15/300][170/1562]	eta 0:07:10 lr 0.000005	time 0.2944 (0.3093)	loss 1.6975 (1.6986)	grad_norm 25.9852 (23.0954)	mem 4879MB
[2022-05-31 00:25:55 MetaFG_0] (main.py 265): INFO Train: [15/300][180/1562]	eta 0:07:07 lr 0.000005	time 0.3033 (0.3092)	loss 1.8152 (1.7028)	grad_norm 15.6350 (23.1619)	mem 4879MB
[2022-05-31 00:25:58 MetaFG_0] (main.py 265): INFO Train: [15/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2973 (0.3089)	loss 1.7502 (1.7047)	grad_norm 31.3491 (23.1197)	mem 4879MB
[2022-05-31 00:26:01 MetaFG_0] (main.py 265): INFO Train: [15/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.3001 (0.3087)	loss 1.8140 (1.7022)	grad_norm 21.9365 (23.0241)	mem 4879MB
[2022-05-31 00:26:04 MetaFG_0] (main.py 265): INFO Train: [15/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2983 (0.3085)	loss 1.4697 (1.7036)	grad_norm 21.1458 (22.9708)	mem 4879MB
[2022-05-31 00:26:07 MetaFG_0] (main.py 265): INFO Train: [15/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2927 (0.3083)	loss 1.6253 (1.7015)	grad_norm 21.1852 (23.0143)	mem 4879MB
[2022-05-31 00:26:11 MetaFG_0] (main.py 265): INFO Train: [15/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2935 (0.3081)	loss 1.3003 (1.7004)	grad_norm 18.1768 (22.8762)	mem 4879MB
[2022-05-31 00:26:14 MetaFG_0] (main.py 265): INFO Train: [15/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2938 (0.3080)	loss 1.5535 (1.7003)	grad_norm 27.0146 (22.7852)	mem 4879MB
[2022-05-31 00:26:17 MetaFG_0] (main.py 265): INFO Train: [15/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2934 (0.3078)	loss 1.5470 (1.7018)	grad_norm 19.8588 (22.6485)	mem 4879MB
[2022-05-31 00:26:20 MetaFG_0] (main.py 265): INFO Train: [15/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2922 (0.3078)	loss 2.0262 (1.7009)	grad_norm 40.5955 (22.6486)	mem 4879MB
[2022-05-31 00:26:23 MetaFG_0] (main.py 265): INFO Train: [15/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2984 (0.3076)	loss 1.6861 (1.7010)	grad_norm 19.2504 (22.5846)	mem 4879MB
[2022-05-31 00:26:26 MetaFG_0] (main.py 265): INFO Train: [15/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2931 (0.3074)	loss 1.4569 (1.6992)	grad_norm 29.7254 (22.6184)	mem 4879MB
[2022-05-31 00:26:29 MetaFG_0] (main.py 265): INFO Train: [15/300][290/1562]	eta 0:06:30 lr 0.000005	time 0.2960 (0.3073)	loss 1.8285 (1.6996)	grad_norm 17.3869 (22.5795)	mem 4879MB
[2022-05-31 00:26:32 MetaFG_0] (main.py 265): INFO Train: [15/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2969 (0.3072)	loss 1.6141 (1.6966)	grad_norm 21.9266 (22.7023)	mem 4879MB
[2022-05-31 00:26:35 MetaFG_0] (main.py 265): INFO Train: [15/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2937 (0.3071)	loss 1.9721 (1.6973)	grad_norm 17.1278 (22.6367)	mem 4879MB
[2022-05-31 00:26:38 MetaFG_0] (main.py 265): INFO Train: [15/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2916 (0.3071)	loss 1.6180 (1.6955)	grad_norm 25.5100 (22.6386)	mem 4879MB
[2022-05-31 00:26:41 MetaFG_0] (main.py 265): INFO Train: [15/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2983 (0.3070)	loss 1.6821 (1.6946)	grad_norm 22.1612 (22.7190)	mem 4879MB
[2022-05-31 00:26:44 MetaFG_0] (main.py 265): INFO Train: [15/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2920 (0.3069)	loss 1.9282 (1.6962)	grad_norm 26.0465 (22.6566)	mem 4879MB
[2022-05-31 00:26:47 MetaFG_0] (main.py 265): INFO Train: [15/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2989 (0.3068)	loss 1.3073 (1.6944)	grad_norm 15.8353 (22.6813)	mem 4879MB
[2022-05-31 00:26:50 MetaFG_0] (main.py 265): INFO Train: [15/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2995 (0.3068)	loss 1.7250 (1.6921)	grad_norm 18.5594 (22.6395)	mem 4879MB
[2022-05-31 00:26:53 MetaFG_0] (main.py 265): INFO Train: [15/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2928 (0.3068)	loss 1.9353 (1.6927)	grad_norm 16.6217 (22.7154)	mem 4879MB
[2022-05-31 00:26:56 MetaFG_0] (main.py 265): INFO Train: [15/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2996 (0.3068)	loss 1.8471 (1.6911)	grad_norm inf (inf)	mem 4879MB
[2022-05-31 00:26:59 MetaFG_0] (main.py 265): INFO Train: [15/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2932 (0.3067)	loss 1.6792 (1.6888)	grad_norm 40.0612 (nan)	mem 4879MB
[2022-05-31 00:27:02 MetaFG_0] (main.py 265): INFO Train: [15/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2949 (0.3066)	loss 1.8347 (1.6905)	grad_norm 36.4608 (nan)	mem 4879MB
[2022-05-31 00:27:05 MetaFG_0] (main.py 265): INFO Train: [15/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2948 (0.3065)	loss 1.5212 (1.6898)	grad_norm 19.4854 (nan)	mem 4879MB
[2022-05-31 00:27:08 MetaFG_0] (main.py 265): INFO Train: [15/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2934 (0.3065)	loss 1.5888 (1.6874)	grad_norm 16.3944 (nan)	mem 4879MB
[2022-05-31 00:27:11 MetaFG_0] (main.py 265): INFO Train: [15/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2928 (0.3064)	loss 1.5324 (1.6873)	grad_norm 23.5992 (nan)	mem 4879MB
[2022-05-31 00:27:14 MetaFG_0] (main.py 265): INFO Train: [15/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2928 (0.3063)	loss 1.7908 (1.6899)	grad_norm 21.0357 (nan)	mem 4879MB
[2022-05-31 00:27:18 MetaFG_0] (main.py 265): INFO Train: [15/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2948 (0.3063)	loss 1.8424 (1.6876)	grad_norm 31.6073 (nan)	mem 4879MB
[2022-05-31 00:27:21 MetaFG_0] (main.py 265): INFO Train: [15/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2937 (0.3063)	loss 1.8545 (1.6886)	grad_norm 45.7440 (nan)	mem 4879MB
[2022-05-31 00:27:24 MetaFG_0] (main.py 265): INFO Train: [15/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2919 (0.3062)	loss 1.5490 (1.6873)	grad_norm 24.3701 (nan)	mem 4879MB
[2022-05-31 00:27:27 MetaFG_0] (main.py 265): INFO Train: [15/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2923 (0.3062)	loss 1.2961 (1.6879)	grad_norm 15.3922 (nan)	mem 4879MB
[2022-05-31 00:27:30 MetaFG_0] (main.py 265): INFO Train: [15/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2984 (0.3062)	loss 1.6264 (1.6873)	grad_norm 16.8133 (nan)	mem 4879MB
[2022-05-31 00:27:33 MetaFG_0] (main.py 265): INFO Train: [15/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2933 (0.3061)	loss 1.8787 (1.6868)	grad_norm 20.0796 (nan)	mem 4879MB
[2022-05-31 00:27:36 MetaFG_0] (main.py 265): INFO Train: [15/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2939 (0.3061)	loss 1.5211 (1.6866)	grad_norm 24.4798 (nan)	mem 4879MB
[2022-05-31 00:27:39 MetaFG_0] (main.py 265): INFO Train: [15/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.2921 (0.3061)	loss 1.6226 (1.6863)	grad_norm 25.4410 (nan)	mem 4879MB
[2022-05-31 00:27:42 MetaFG_0] (main.py 265): INFO Train: [15/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2920 (0.3060)	loss 1.8128 (1.6870)	grad_norm 23.5742 (nan)	mem 4879MB
[2022-05-31 00:27:45 MetaFG_0] (main.py 265): INFO Train: [15/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2937 (0.3060)	loss 1.8433 (1.6862)	grad_norm 24.3493 (nan)	mem 4879MB
[2022-05-31 00:27:48 MetaFG_0] (main.py 265): INFO Train: [15/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2928 (0.3060)	loss 1.8880 (1.6857)	grad_norm 22.7381 (nan)	mem 4879MB
[2022-05-31 00:27:51 MetaFG_0] (main.py 265): INFO Train: [15/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2919 (0.3059)	loss 1.4355 (1.6856)	grad_norm 28.3819 (nan)	mem 4879MB
[2022-05-31 00:27:54 MetaFG_0] (main.py 265): INFO Train: [15/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2983 (0.3059)	loss 1.6455 (1.6847)	grad_norm 21.6516 (nan)	mem 4879MB
[2022-05-31 00:27:57 MetaFG_0] (main.py 265): INFO Train: [15/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.3013 (0.3058)	loss 1.8801 (1.6866)	grad_norm 34.6659 (nan)	mem 4879MB
[2022-05-31 00:28:00 MetaFG_0] (main.py 265): INFO Train: [15/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2981 (0.3060)	loss 1.5019 (1.6849)	grad_norm 25.1412 (nan)	mem 4879MB
[2022-05-31 00:28:03 MetaFG_0] (main.py 265): INFO Train: [15/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2920 (0.3060)	loss 1.4707 (1.6849)	grad_norm 22.0296 (nan)	mem 4879MB
[2022-05-31 00:28:06 MetaFG_0] (main.py 265): INFO Train: [15/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2996 (0.3060)	loss 1.5735 (1.6841)	grad_norm 40.0407 (nan)	mem 4879MB
[2022-05-31 00:28:09 MetaFG_0] (main.py 265): INFO Train: [15/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.3005 (0.3060)	loss 1.5110 (1.6846)	grad_norm 17.3584 (nan)	mem 4879MB
[2022-05-31 00:28:12 MetaFG_0] (main.py 265): INFO Train: [15/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2995 (0.3060)	loss 1.5676 (1.6842)	grad_norm 24.4166 (nan)	mem 4879MB
[2022-05-31 00:28:16 MetaFG_0] (main.py 265): INFO Train: [15/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2979 (0.3060)	loss 1.6725 (1.6836)	grad_norm 21.8279 (nan)	mem 4879MB
[2022-05-31 00:28:19 MetaFG_0] (main.py 265): INFO Train: [15/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.3008 (0.3060)	loss 1.3950 (1.6833)	grad_norm 25.5914 (nan)	mem 4879MB
[2022-05-31 00:28:22 MetaFG_0] (main.py 265): INFO Train: [15/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2965 (0.3060)	loss 1.6447 (1.6839)	grad_norm 17.5949 (nan)	mem 4879MB
[2022-05-31 00:28:25 MetaFG_0] (main.py 265): INFO Train: [15/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2985 (0.3060)	loss 1.8291 (1.6853)	grad_norm 16.9620 (nan)	mem 4879MB
[2022-05-31 00:28:28 MetaFG_0] (main.py 265): INFO Train: [15/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2978 (0.3060)	loss 1.6967 (1.6842)	grad_norm 12.8866 (nan)	mem 4879MB
[2022-05-31 00:28:31 MetaFG_0] (main.py 265): INFO Train: [15/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2982 (0.3060)	loss 1.5843 (1.6835)	grad_norm 12.9576 (nan)	mem 4879MB
[2022-05-31 00:28:34 MetaFG_0] (main.py 265): INFO Train: [15/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2989 (0.3059)	loss 1.5952 (1.6816)	grad_norm 20.0668 (nan)	mem 4879MB
[2022-05-31 00:28:37 MetaFG_0] (main.py 265): INFO Train: [15/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2986 (0.3059)	loss 1.8130 (1.6827)	grad_norm 21.6660 (nan)	mem 4879MB
[2022-05-31 00:28:40 MetaFG_0] (main.py 265): INFO Train: [15/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2973 (0.3059)	loss 1.7796 (1.6835)	grad_norm 20.3300 (nan)	mem 4879MB
[2022-05-31 00:28:43 MetaFG_0] (main.py 265): INFO Train: [15/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2955 (0.3059)	loss 1.6715 (1.6850)	grad_norm 20.8069 (nan)	mem 4879MB
[2022-05-31 00:28:46 MetaFG_0] (main.py 265): INFO Train: [15/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2930 (0.3059)	loss 1.2735 (1.6845)	grad_norm 24.3000 (nan)	mem 4879MB
[2022-05-31 00:28:49 MetaFG_0] (main.py 265): INFO Train: [15/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2952 (0.3059)	loss 1.7868 (1.6846)	grad_norm 22.0290 (nan)	mem 4879MB
[2022-05-31 00:28:52 MetaFG_0] (main.py 265): INFO Train: [15/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2980 (0.3059)	loss 1.8113 (1.6850)	grad_norm 16.9548 (nan)	mem 4879MB
[2022-05-31 00:28:55 MetaFG_0] (main.py 265): INFO Train: [15/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2935 (0.3059)	loss 1.8056 (1.6857)	grad_norm 23.4074 (nan)	mem 4879MB
[2022-05-31 00:28:58 MetaFG_0] (main.py 265): INFO Train: [15/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2927 (0.3059)	loss 1.7803 (1.6851)	grad_norm 18.0301 (nan)	mem 4879MB
[2022-05-31 00:29:01 MetaFG_0] (main.py 265): INFO Train: [15/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.2995 (0.3058)	loss 1.5360 (1.6847)	grad_norm 23.5329 (nan)	mem 4879MB
[2022-05-31 00:29:04 MetaFG_0] (main.py 265): INFO Train: [15/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2948 (0.3058)	loss 1.7200 (1.6845)	grad_norm 14.4888 (nan)	mem 4879MB
[2022-05-31 00:29:07 MetaFG_0] (main.py 265): INFO Train: [15/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2938 (0.3058)	loss 1.7927 (1.6840)	grad_norm 17.7239 (nan)	mem 4879MB
[2022-05-31 00:29:10 MetaFG_0] (main.py 265): INFO Train: [15/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2922 (0.3058)	loss 1.4622 (1.6843)	grad_norm 19.8847 (nan)	mem 4879MB
[2022-05-31 00:29:13 MetaFG_0] (main.py 265): INFO Train: [15/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2982 (0.3057)	loss 1.3640 (1.6838)	grad_norm 38.0327 (nan)	mem 4879MB
[2022-05-31 00:29:16 MetaFG_0] (main.py 265): INFO Train: [15/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2982 (0.3057)	loss 1.6307 (1.6843)	grad_norm 18.0687 (nan)	mem 4879MB
[2022-05-31 00:29:20 MetaFG_0] (main.py 265): INFO Train: [15/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2993 (0.3057)	loss 1.4077 (1.6844)	grad_norm 25.7390 (nan)	mem 4879MB
[2022-05-31 00:29:23 MetaFG_0] (main.py 265): INFO Train: [15/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2923 (0.3057)	loss 1.7904 (1.6834)	grad_norm 15.9862 (nan)	mem 4879MB
[2022-05-31 00:29:26 MetaFG_0] (main.py 265): INFO Train: [15/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2987 (0.3057)	loss 1.9024 (1.6837)	grad_norm 24.4774 (nan)	mem 4879MB
[2022-05-31 00:29:29 MetaFG_0] (main.py 265): INFO Train: [15/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2940 (0.3057)	loss 1.7259 (1.6843)	grad_norm 15.8125 (nan)	mem 4879MB
[2022-05-31 00:29:32 MetaFG_0] (main.py 265): INFO Train: [15/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.3032 (0.3057)	loss 1.8781 (1.6846)	grad_norm 20.0132 (nan)	mem 4879MB
[2022-05-31 00:29:35 MetaFG_0] (main.py 265): INFO Train: [15/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.3006 (0.3056)	loss 1.7177 (1.6842)	grad_norm 28.4384 (nan)	mem 4879MB
[2022-05-31 00:29:38 MetaFG_0] (main.py 265): INFO Train: [15/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.3007 (0.3056)	loss 1.7129 (1.6841)	grad_norm 19.9724 (nan)	mem 4879MB
[2022-05-31 00:29:41 MetaFG_0] (main.py 265): INFO Train: [15/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2984 (0.3056)	loss 1.8272 (1.6841)	grad_norm 17.8404 (nan)	mem 4879MB
[2022-05-31 00:29:44 MetaFG_0] (main.py 265): INFO Train: [15/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2924 (0.3056)	loss 1.8884 (1.6840)	grad_norm 21.6995 (nan)	mem 4879MB
[2022-05-31 00:29:47 MetaFG_0] (main.py 265): INFO Train: [15/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.2926 (0.3056)	loss 1.7342 (1.6834)	grad_norm 23.9137 (nan)	mem 4879MB
[2022-05-31 00:29:50 MetaFG_0] (main.py 265): INFO Train: [15/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2979 (0.3056)	loss 1.8450 (1.6830)	grad_norm 18.1365 (nan)	mem 4879MB
[2022-05-31 00:29:53 MetaFG_0] (main.py 265): INFO Train: [15/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2966 (0.3055)	loss 1.3686 (1.6818)	grad_norm 14.7427 (nan)	mem 4879MB
[2022-05-31 00:29:56 MetaFG_0] (main.py 265): INFO Train: [15/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2940 (0.3055)	loss 1.5160 (1.6818)	grad_norm 25.5475 (nan)	mem 4879MB
[2022-05-31 00:29:59 MetaFG_0] (main.py 265): INFO Train: [15/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2992 (0.3055)	loss 1.6564 (1.6821)	grad_norm 20.3839 (nan)	mem 4879MB
[2022-05-31 00:30:02 MetaFG_0] (main.py 265): INFO Train: [15/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2922 (0.3055)	loss 1.2906 (1.6811)	grad_norm 33.6644 (nan)	mem 4879MB
[2022-05-31 00:30:05 MetaFG_0] (main.py 265): INFO Train: [15/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2999 (0.3055)	loss 1.8781 (1.6802)	grad_norm 23.4642 (nan)	mem 4879MB
[2022-05-31 00:30:08 MetaFG_0] (main.py 265): INFO Train: [15/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2921 (0.3055)	loss 1.4907 (1.6796)	grad_norm 30.1302 (nan)	mem 4879MB
[2022-05-31 00:30:11 MetaFG_0] (main.py 265): INFO Train: [15/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2923 (0.3055)	loss 1.3577 (1.6776)	grad_norm 34.5092 (nan)	mem 4879MB
[2022-05-31 00:30:14 MetaFG_0] (main.py 265): INFO Train: [15/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.3014 (0.3056)	loss 1.8243 (1.6779)	grad_norm 27.0391 (nan)	mem 4879MB
[2022-05-31 00:30:18 MetaFG_0] (main.py 265): INFO Train: [15/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2996 (0.3056)	loss 1.9009 (1.6774)	grad_norm 13.6023 (nan)	mem 4879MB
[2022-05-31 00:30:21 MetaFG_0] (main.py 265): INFO Train: [15/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2931 (0.3056)	loss 1.8708 (1.6780)	grad_norm 20.8491 (nan)	mem 4879MB
[2022-05-31 00:30:24 MetaFG_0] (main.py 265): INFO Train: [15/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.3009 (0.3056)	loss 1.5628 (1.6779)	grad_norm 30.2029 (nan)	mem 4879MB
[2022-05-31 00:30:27 MetaFG_0] (main.py 265): INFO Train: [15/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2927 (0.3056)	loss 1.6494 (1.6775)	grad_norm 22.1531 (nan)	mem 4879MB
[2022-05-31 00:30:30 MetaFG_0] (main.py 265): INFO Train: [15/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2995 (0.3056)	loss 1.4394 (1.6776)	grad_norm 23.2990 (nan)	mem 4879MB
[2022-05-31 00:30:33 MetaFG_0] (main.py 265): INFO Train: [15/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2922 (0.3056)	loss 1.8665 (1.6777)	grad_norm 29.7323 (nan)	mem 4879MB
[2022-05-31 00:30:36 MetaFG_0] (main.py 265): INFO Train: [15/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2984 (0.3055)	loss 1.7601 (1.6775)	grad_norm 11.6016 (nan)	mem 4879MB
[2022-05-31 00:30:39 MetaFG_0] (main.py 265): INFO Train: [15/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2933 (0.3055)	loss 1.7793 (1.6785)	grad_norm 23.0152 (nan)	mem 4879MB
[2022-05-31 00:30:42 MetaFG_0] (main.py 265): INFO Train: [15/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2995 (0.3055)	loss 1.7346 (1.6791)	grad_norm 19.5129 (nan)	mem 4879MB
[2022-05-31 00:30:45 MetaFG_0] (main.py 265): INFO Train: [15/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2940 (0.3055)	loss 1.7913 (1.6781)	grad_norm 32.2746 (nan)	mem 4879MB
[2022-05-31 00:30:48 MetaFG_0] (main.py 265): INFO Train: [15/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2970 (0.3055)	loss 1.7239 (1.6781)	grad_norm 15.4658 (nan)	mem 4879MB
[2022-05-31 00:30:51 MetaFG_0] (main.py 265): INFO Train: [15/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2955 (0.3055)	loss 1.2797 (1.6782)	grad_norm 17.2770 (nan)	mem 4879MB
[2022-05-31 00:30:54 MetaFG_0] (main.py 265): INFO Train: [15/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2921 (0.3055)	loss 1.3889 (1.6781)	grad_norm 27.1028 (nan)	mem 4879MB
[2022-05-31 00:30:57 MetaFG_0] (main.py 265): INFO Train: [15/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2935 (0.3055)	loss 1.4802 (1.6781)	grad_norm 22.7883 (nan)	mem 4879MB
[2022-05-31 00:31:00 MetaFG_0] (main.py 265): INFO Train: [15/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2988 (0.3055)	loss 1.8355 (1.6781)	grad_norm 28.6266 (nan)	mem 4879MB
[2022-05-31 00:31:03 MetaFG_0] (main.py 265): INFO Train: [15/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2919 (0.3054)	loss 1.7454 (1.6778)	grad_norm 19.6062 (nan)	mem 4879MB
[2022-05-31 00:31:06 MetaFG_0] (main.py 265): INFO Train: [15/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2938 (0.3054)	loss 1.7607 (1.6783)	grad_norm 14.0250 (nan)	mem 4879MB
[2022-05-31 00:31:09 MetaFG_0] (main.py 265): INFO Train: [15/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2943 (0.3054)	loss 1.6552 (1.6779)	grad_norm 24.9536 (nan)	mem 4879MB
[2022-05-31 00:31:12 MetaFG_0] (main.py 265): INFO Train: [15/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2983 (0.3054)	loss 1.8923 (1.6786)	grad_norm 37.3481 (nan)	mem 4879MB
[2022-05-31 00:31:15 MetaFG_0] (main.py 265): INFO Train: [15/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2932 (0.3054)	loss 1.8106 (1.6787)	grad_norm 25.3544 (nan)	mem 4879MB
[2022-05-31 00:31:18 MetaFG_0] (main.py 265): INFO Train: [15/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.3005 (0.3054)	loss 1.5490 (1.6792)	grad_norm 48.3835 (nan)	mem 4879MB
[2022-05-31 00:31:21 MetaFG_0] (main.py 265): INFO Train: [15/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2930 (0.3054)	loss 1.7825 (1.6795)	grad_norm 24.9795 (nan)	mem 4879MB
[2022-05-31 00:31:24 MetaFG_0] (main.py 265): INFO Train: [15/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2935 (0.3054)	loss 1.7478 (1.6794)	grad_norm 18.6540 (nan)	mem 4879MB
[2022-05-31 00:31:27 MetaFG_0] (main.py 265): INFO Train: [15/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.3014 (0.3054)	loss 2.0346 (1.6797)	grad_norm 26.8132 (nan)	mem 4879MB
[2022-05-31 00:31:31 MetaFG_0] (main.py 265): INFO Train: [15/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2994 (0.3054)	loss 1.7042 (1.6794)	grad_norm 17.3303 (nan)	mem 4879MB
[2022-05-31 00:31:34 MetaFG_0] (main.py 265): INFO Train: [15/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2985 (0.3054)	loss 1.5771 (1.6796)	grad_norm 22.3173 (nan)	mem 4879MB
[2022-05-31 00:31:37 MetaFG_0] (main.py 265): INFO Train: [15/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2986 (0.3054)	loss 1.8512 (1.6797)	grad_norm 25.0256 (nan)	mem 4879MB
[2022-05-31 00:31:40 MetaFG_0] (main.py 265): INFO Train: [15/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2992 (0.3054)	loss 1.8933 (1.6808)	grad_norm 21.5206 (nan)	mem 4879MB
[2022-05-31 00:31:43 MetaFG_0] (main.py 265): INFO Train: [15/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2987 (0.3054)	loss 1.7077 (1.6814)	grad_norm 30.4535 (nan)	mem 4879MB
[2022-05-31 00:31:46 MetaFG_0] (main.py 265): INFO Train: [15/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2947 (0.3054)	loss 1.7769 (1.6814)	grad_norm 17.7201 (nan)	mem 4879MB
[2022-05-31 00:31:49 MetaFG_0] (main.py 265): INFO Train: [15/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2930 (0.3054)	loss 1.5500 (1.6811)	grad_norm 30.7959 (nan)	mem 4879MB
[2022-05-31 00:31:52 MetaFG_0] (main.py 265): INFO Train: [15/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2999 (0.3054)	loss 1.6484 (1.6809)	grad_norm 23.8838 (nan)	mem 4879MB
[2022-05-31 00:31:55 MetaFG_0] (main.py 265): INFO Train: [15/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2920 (0.3054)	loss 1.5681 (1.6796)	grad_norm 18.5285 (nan)	mem 4879MB
[2022-05-31 00:31:58 MetaFG_0] (main.py 265): INFO Train: [15/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2981 (0.3054)	loss 1.6308 (1.6794)	grad_norm 20.0691 (nan)	mem 4879MB
[2022-05-31 00:32:01 MetaFG_0] (main.py 265): INFO Train: [15/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2933 (0.3054)	loss 1.8754 (1.6791)	grad_norm 19.9848 (nan)	mem 4879MB
[2022-05-31 00:32:04 MetaFG_0] (main.py 265): INFO Train: [15/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2979 (0.3054)	loss 1.7476 (1.6793)	grad_norm 35.1815 (nan)	mem 4879MB
[2022-05-31 00:32:07 MetaFG_0] (main.py 265): INFO Train: [15/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2921 (0.3053)	loss 1.3186 (1.6779)	grad_norm 24.1430 (nan)	mem 4879MB
[2022-05-31 00:32:10 MetaFG_0] (main.py 265): INFO Train: [15/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2970 (0.3053)	loss 1.4451 (1.6775)	grad_norm 21.0815 (nan)	mem 4879MB
[2022-05-31 00:32:13 MetaFG_0] (main.py 265): INFO Train: [15/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2986 (0.3053)	loss 1.9234 (1.6779)	grad_norm 18.8866 (nan)	mem 4879MB
[2022-05-31 00:32:16 MetaFG_0] (main.py 265): INFO Train: [15/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2985 (0.3053)	loss 1.3766 (1.6780)	grad_norm 13.7029 (nan)	mem 4879MB
[2022-05-31 00:32:19 MetaFG_0] (main.py 265): INFO Train: [15/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2968 (0.3053)	loss 1.8860 (1.6787)	grad_norm 24.6943 (nan)	mem 4879MB
[2022-05-31 00:32:22 MetaFG_0] (main.py 265): INFO Train: [15/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2929 (0.3053)	loss 1.7033 (1.6781)	grad_norm 24.5046 (nan)	mem 4879MB
[2022-05-31 00:32:25 MetaFG_0] (main.py 265): INFO Train: [15/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.3021 (0.3053)	loss 1.5838 (1.6783)	grad_norm 33.5127 (nan)	mem 4879MB
[2022-05-31 00:32:29 MetaFG_0] (main.py 265): INFO Train: [15/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2930 (0.3054)	loss 1.6520 (1.6779)	grad_norm 16.6000 (nan)	mem 4879MB
[2022-05-31 00:32:32 MetaFG_0] (main.py 265): INFO Train: [15/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2934 (0.3054)	loss 1.3538 (1.6774)	grad_norm 12.4217 (nan)	mem 4879MB
[2022-05-31 00:32:35 MetaFG_0] (main.py 265): INFO Train: [15/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2922 (0.3054)	loss 1.8277 (1.6777)	grad_norm 15.5736 (nan)	mem 4879MB
[2022-05-31 00:32:38 MetaFG_0] (main.py 265): INFO Train: [15/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2961 (0.3054)	loss 1.4300 (1.6774)	grad_norm 23.9483 (nan)	mem 4879MB
[2022-05-31 00:32:41 MetaFG_0] (main.py 265): INFO Train: [15/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.3023 (0.3054)	loss 1.6108 (1.6778)	grad_norm 30.1224 (nan)	mem 4879MB
[2022-05-31 00:32:44 MetaFG_0] (main.py 265): INFO Train: [15/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2923 (0.3054)	loss 1.7559 (1.6776)	grad_norm 19.1622 (nan)	mem 4879MB
[2022-05-31 00:32:47 MetaFG_0] (main.py 265): INFO Train: [15/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2929 (0.3054)	loss 1.7948 (1.6783)	grad_norm 32.6767 (nan)	mem 4879MB
[2022-05-31 00:32:50 MetaFG_0] (main.py 265): INFO Train: [15/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2983 (0.3054)	loss 1.9030 (1.6783)	grad_norm 22.0544 (nan)	mem 4879MB
[2022-05-31 00:32:53 MetaFG_0] (main.py 265): INFO Train: [15/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2931 (0.3054)	loss 1.6993 (1.6788)	grad_norm 12.7917 (nan)	mem 4879MB
[2022-05-31 00:32:56 MetaFG_0] (main.py 265): INFO Train: [15/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2918 (0.3054)	loss 1.4165 (1.6780)	grad_norm 21.4593 (nan)	mem 4879MB
[2022-05-31 00:32:56 MetaFG_0] (main.py 272): INFO EPOCH 15 training takes 0:07:57
[2022-05-31 00:32:56 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_15.pth saving......
[2022-05-31 00:32:57 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_15.pth saved !!!
[2022-05-31 00:32:57 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:32:59 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:32:59 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:32:59 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.741 (0.741)	Loss 1.2164 (1.2164)	Acc@1 84.375 (84.375)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 00:33:00 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.090 (0.154)	Loss 1.4317 (1.3681)	Acc@1 84.375 (75.284)	Acc@5 93.750 (96.307)	Mem 4879MB
[2022-05-31 00:33:01 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.126)	Loss 1.6669 (1.4210)	Acc@1 62.500 (72.917)	Acc@5 87.500 (95.536)	Mem 4879MB
[2022-05-31 00:33:02 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.116)	Loss 1.3693 (1.4480)	Acc@1 78.125 (70.766)	Acc@5 100.000 (95.565)	Mem 4879MB
[2022-05-31 00:33:03 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.110)	Loss 1.5910 (1.4791)	Acc@1 68.750 (69.284)	Acc@5 90.625 (95.046)	Mem 4879MB
[2022-05-31 00:33:04 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.107)	Loss 1.4226 (1.4897)	Acc@1 75.000 (69.363)	Acc@5 96.875 (95.159)	Mem 4879MB
[2022-05-31 00:33:05 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.105)	Loss 1.1770 (1.4837)	Acc@1 71.875 (68.955)	Acc@5 100.000 (95.287)	Mem 4879MB
[2022-05-31 00:33:06 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.100 (0.103)	Loss 1.3803 (1.4761)	Acc@1 71.875 (69.278)	Acc@5 100.000 (95.511)	Mem 4879MB
[2022-05-31 00:33:07 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 1.5197 (1.4792)	Acc@1 71.875 (69.483)	Acc@5 96.875 (95.409)	Mem 4879MB
[2022-05-31 00:33:08 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 1.4292 (1.4739)	Acc@1 71.875 (69.815)	Acc@5 96.875 (95.261)	Mem 4879MB
[2022-05-31 00:33:09 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.088 (0.101)	Loss 1.4167 (1.4689)	Acc@1 71.875 (69.926)	Acc@5 93.750 (95.266)	Mem 4879MB
[2022-05-31 00:33:10 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.100)	Loss 1.5335 (1.4705)	Acc@1 75.000 (69.904)	Acc@5 87.500 (95.073)	Mem 4879MB
[2022-05-31 00:33:11 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 1.2070 (1.4678)	Acc@1 84.375 (70.067)	Acc@5 100.000 (95.274)	Mem 4879MB
[2022-05-31 00:33:12 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 1.3753 (1.4628)	Acc@1 84.375 (70.301)	Acc@5 93.750 (95.348)	Mem 4879MB
[2022-05-31 00:33:13 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 1.6231 (1.4567)	Acc@1 59.375 (70.612)	Acc@5 96.875 (95.412)	Mem 4879MB
[2022-05-31 00:33:14 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.099)	Loss 1.4899 (1.4563)	Acc@1 75.000 (70.757)	Acc@5 90.625 (95.468)	Mem 4879MB
[2022-05-31 00:33:15 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.092 (0.098)	Loss 1.4942 (1.4572)	Acc@1 68.750 (70.497)	Acc@5 96.875 (95.516)	Mem 4879MB
[2022-05-31 00:33:15 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.093 (0.098)	Loss 1.6894 (1.4627)	Acc@1 59.375 (70.577)	Acc@5 87.500 (95.413)	Mem 4879MB
[2022-05-31 00:33:16 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.098)	Loss 1.3180 (1.4586)	Acc@1 71.875 (70.684)	Acc@5 96.875 (95.390)	Mem 4879MB
[2022-05-31 00:33:17 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.091 (0.097)	Loss 1.4932 (1.4591)	Acc@1 65.625 (70.533)	Acc@5 93.750 (95.386)	Mem 4879MB
[2022-05-31 00:33:18 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.091 (0.097)	Loss 1.3730 (1.4578)	Acc@1 68.750 (70.631)	Acc@5 93.750 (95.196)	Mem 4879MB
[2022-05-31 00:33:19 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 1.5386 (1.4559)	Acc@1 71.875 (70.823)	Acc@5 90.625 (95.024)	Mem 4879MB
[2022-05-31 00:33:20 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 1.4550 (1.4565)	Acc@1 71.875 (70.772)	Acc@5 87.500 (94.980)	Mem 4879MB
[2022-05-31 00:33:21 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 1.1912 (1.4567)	Acc@1 78.125 (70.752)	Acc@5 100.000 (95.035)	Mem 4879MB
[2022-05-31 00:33:22 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 1.3700 (1.4552)	Acc@1 68.750 (70.747)	Acc@5 100.000 (95.060)	Mem 4879MB
[2022-05-31 00:33:23 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.097 (0.096)	Loss 1.4139 (1.4558)	Acc@1 68.750 (70.692)	Acc@5 100.000 (95.107)	Mem 4879MB
[2022-05-31 00:33:24 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 1.5668 (1.4610)	Acc@1 65.625 (70.498)	Acc@5 100.000 (95.103)	Mem 4879MB
[2022-05-31 00:33:25 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.099 (0.096)	Loss 1.1949 (1.4579)	Acc@1 78.125 (70.549)	Acc@5 100.000 (95.191)	Mem 4879MB
[2022-05-31 00:33:26 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 1.2433 (1.4559)	Acc@1 71.875 (70.607)	Acc@5 96.875 (95.218)	Mem 4879MB
[2022-05-31 00:33:27 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.093 (0.096)	Loss 1.6954 (1.4591)	Acc@1 62.500 (70.500)	Acc@5 90.625 (95.125)	Mem 4879MB
[2022-05-31 00:33:28 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 1.3785 (1.4560)	Acc@1 81.250 (70.671)	Acc@5 96.875 (95.193)	Mem 4879MB
[2022-05-31 00:33:29 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 1.5417 (1.4556)	Acc@1 81.250 (70.719)	Acc@5 93.750 (95.147)	Mem 4879MB
[2022-05-31 00:33:29 MetaFG_0] (main.py 330): INFO  * Acc@1 70.720 Acc@5 95.110
[2022-05-31 00:33:29 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 70.7%
[2022-05-31 00:33:29 MetaFG_0] (main.py 171): INFO Max accuracy: 70.72%
[2022-05-31 00:33:30 MetaFG_0] (main.py 265): INFO Train: [16/300][0/1562]	eta 0:27:42 lr 0.000005	time 1.0640 (1.0640)	loss 1.7140 (1.7140)	grad_norm 13.6670 (13.6670)	mem 4879MB
[2022-05-31 00:33:33 MetaFG_0] (main.py 265): INFO Train: [16/300][10/1562]	eta 0:09:46 lr 0.000005	time 0.2943 (0.3780)	loss 1.8148 (1.7226)	grad_norm 27.4529 (23.5332)	mem 4879MB
[2022-05-31 00:33:36 MetaFG_0] (main.py 265): INFO Train: [16/300][20/1562]	eta 0:08:48 lr 0.000005	time 0.2949 (0.3426)	loss 1.7236 (1.7032)	grad_norm 13.9228 (24.5467)	mem 4879MB
[2022-05-31 00:33:39 MetaFG_0] (main.py 265): INFO Train: [16/300][30/1562]	eta 0:08:25 lr 0.000005	time 0.2997 (0.3300)	loss 1.8282 (1.6641)	grad_norm 26.3663 (24.9830)	mem 4879MB
[2022-05-31 00:33:42 MetaFG_0] (main.py 265): INFO Train: [16/300][40/1562]	eta 0:08:12 lr 0.000005	time 0.2920 (0.3239)	loss 1.2927 (1.6662)	grad_norm 25.1631 (24.5747)	mem 4879MB
[2022-05-31 00:33:45 MetaFG_0] (main.py 265): INFO Train: [16/300][50/1562]	eta 0:08:03 lr 0.000005	time 0.2923 (0.3199)	loss 1.6636 (1.6609)	grad_norm 18.1767 (24.1439)	mem 4879MB
[2022-05-31 00:33:48 MetaFG_0] (main.py 265): INFO Train: [16/300][60/1562]	eta 0:07:56 lr 0.000005	time 0.2929 (0.3172)	loss 1.9244 (1.6636)	grad_norm 20.4858 (24.1960)	mem 4879MB
[2022-05-31 00:33:51 MetaFG_0] (main.py 265): INFO Train: [16/300][70/1562]	eta 0:07:50 lr 0.000005	time 0.2927 (0.3155)	loss 1.7549 (1.6549)	grad_norm 15.5760 (24.0673)	mem 4879MB
[2022-05-31 00:33:54 MetaFG_0] (main.py 265): INFO Train: [16/300][80/1562]	eta 0:07:45 lr 0.000005	time 0.2979 (0.3141)	loss 1.5708 (1.6401)	grad_norm 16.8696 (24.1256)	mem 4879MB
[2022-05-31 00:33:57 MetaFG_0] (main.py 265): INFO Train: [16/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.2943 (0.3131)	loss 1.7487 (1.6446)	grad_norm 24.9926 (24.1977)	mem 4879MB
[2022-05-31 00:34:00 MetaFG_0] (main.py 265): INFO Train: [16/300][100/1562]	eta 0:07:36 lr 0.000005	time 0.2920 (0.3121)	loss 1.7203 (1.6478)	grad_norm 29.0781 (24.4407)	mem 4879MB
[2022-05-31 00:34:03 MetaFG_0] (main.py 265): INFO Train: [16/300][110/1562]	eta 0:07:32 lr 0.000005	time 0.3037 (0.3113)	loss 1.8453 (1.6473)	grad_norm 28.9532 (24.2028)	mem 4879MB
[2022-05-31 00:34:06 MetaFG_0] (main.py 265): INFO Train: [16/300][120/1562]	eta 0:07:28 lr 0.000005	time 0.2987 (0.3108)	loss 1.5783 (1.6437)	grad_norm 26.6283 (24.0768)	mem 4879MB
[2022-05-31 00:34:09 MetaFG_0] (main.py 265): INFO Train: [16/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.2932 (0.3102)	loss 1.3871 (1.6346)	grad_norm 38.0042 (24.1372)	mem 4879MB
[2022-05-31 00:34:12 MetaFG_0] (main.py 265): INFO Train: [16/300][140/1562]	eta 0:07:20 lr 0.000005	time 0.2926 (0.3098)	loss 1.8029 (1.6360)	grad_norm 22.4838 (24.3660)	mem 4879MB
[2022-05-31 00:34:16 MetaFG_0] (main.py 265): INFO Train: [16/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2983 (0.3096)	loss 1.7164 (1.6378)	grad_norm 19.8308 (24.2533)	mem 4879MB
[2022-05-31 00:34:19 MetaFG_0] (main.py 265): INFO Train: [16/300][160/1562]	eta 0:07:13 lr 0.000005	time 0.2925 (0.3094)	loss 1.2378 (1.6388)	grad_norm 21.9602 (24.3795)	mem 4879MB
[2022-05-31 00:34:22 MetaFG_0] (main.py 265): INFO Train: [16/300][170/1562]	eta 0:07:10 lr 0.000005	time 0.2988 (0.3092)	loss 1.8032 (1.6402)	grad_norm 21.2555 (24.5132)	mem 4879MB
[2022-05-31 00:34:25 MetaFG_0] (main.py 265): INFO Train: [16/300][180/1562]	eta 0:07:06 lr 0.000005	time 0.2979 (0.3090)	loss 1.6821 (1.6452)	grad_norm 24.6861 (24.3618)	mem 4879MB
[2022-05-31 00:34:28 MetaFG_0] (main.py 265): INFO Train: [16/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2986 (0.3088)	loss 1.5405 (1.6462)	grad_norm 23.1027 (24.2573)	mem 4879MB
[2022-05-31 00:34:31 MetaFG_0] (main.py 265): INFO Train: [16/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2985 (0.3086)	loss 1.5234 (1.6415)	grad_norm 18.5211 (24.2094)	mem 4879MB
[2022-05-31 00:34:34 MetaFG_0] (main.py 265): INFO Train: [16/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2984 (0.3086)	loss 1.6119 (1.6425)	grad_norm 18.8205 (23.9962)	mem 4879MB
[2022-05-31 00:34:37 MetaFG_0] (main.py 265): INFO Train: [16/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2942 (0.3084)	loss 1.7738 (1.6466)	grad_norm 12.7192 (23.8833)	mem 4879MB
[2022-05-31 00:34:40 MetaFG_0] (main.py 265): INFO Train: [16/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2934 (0.3083)	loss 1.9611 (1.6514)	grad_norm 25.9313 (23.8009)	mem 4879MB
[2022-05-31 00:34:43 MetaFG_0] (main.py 265): INFO Train: [16/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2988 (0.3081)	loss 1.5033 (1.6547)	grad_norm 18.6599 (23.8502)	mem 4879MB
[2022-05-31 00:34:46 MetaFG_0] (main.py 265): INFO Train: [16/300][250/1562]	eta 0:06:44 lr 0.000005	time 0.2995 (0.3082)	loss 1.7761 (1.6552)	grad_norm 84.6277 (24.0770)	mem 4879MB
[2022-05-31 00:34:49 MetaFG_0] (main.py 265): INFO Train: [16/300][260/1562]	eta 0:06:41 lr 0.000005	time 0.2928 (0.3080)	loss 1.9552 (1.6558)	grad_norm 37.2690 (23.9942)	mem 4879MB
[2022-05-31 00:34:52 MetaFG_0] (main.py 265): INFO Train: [16/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2978 (0.3078)	loss 1.7366 (1.6551)	grad_norm 15.2793 (23.9553)	mem 4879MB
[2022-05-31 00:34:55 MetaFG_0] (main.py 265): INFO Train: [16/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2931 (0.3077)	loss 1.8572 (1.6553)	grad_norm 46.1956 (24.0590)	mem 4879MB
[2022-05-31 00:34:58 MetaFG_0] (main.py 265): INFO Train: [16/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2979 (0.3075)	loss 1.6356 (1.6566)	grad_norm 17.8097 (24.0101)	mem 4879MB
[2022-05-31 00:35:01 MetaFG_0] (main.py 265): INFO Train: [16/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2957 (0.3074)	loss 1.7482 (1.6559)	grad_norm 30.0854 (23.9702)	mem 4879MB
[2022-05-31 00:35:04 MetaFG_0] (main.py 265): INFO Train: [16/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2977 (0.3072)	loss 1.8938 (1.6571)	grad_norm 23.1995 (24.0072)	mem 4879MB
[2022-05-31 00:35:07 MetaFG_0] (main.py 265): INFO Train: [16/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2946 (0.3071)	loss 1.8364 (1.6573)	grad_norm 16.1859 (24.1756)	mem 4879MB
[2022-05-31 00:35:10 MetaFG_0] (main.py 265): INFO Train: [16/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2923 (0.3070)	loss 1.3519 (1.6546)	grad_norm 29.3203 (24.1635)	mem 4879MB
[2022-05-31 00:35:13 MetaFG_0] (main.py 265): INFO Train: [16/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2989 (0.3070)	loss 1.7224 (1.6543)	grad_norm 18.7636 (24.1207)	mem 4879MB
[2022-05-31 00:35:16 MetaFG_0] (main.py 265): INFO Train: [16/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2940 (0.3069)	loss 1.3167 (1.6549)	grad_norm 33.0357 (24.0998)	mem 4879MB
[2022-05-31 00:35:20 MetaFG_0] (main.py 265): INFO Train: [16/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2977 (0.3068)	loss 1.6937 (1.6570)	grad_norm 39.8149 (24.1490)	mem 4879MB
[2022-05-31 00:35:23 MetaFG_0] (main.py 265): INFO Train: [16/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2939 (0.3067)	loss 1.7107 (1.6577)	grad_norm 15.8470 (24.1021)	mem 4879MB
[2022-05-31 00:35:26 MetaFG_0] (main.py 265): INFO Train: [16/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2925 (0.3066)	loss 1.7656 (1.6560)	grad_norm 37.3760 (24.1627)	mem 4879MB
[2022-05-31 00:35:29 MetaFG_0] (main.py 265): INFO Train: [16/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2936 (0.3065)	loss 1.5655 (1.6539)	grad_norm 37.1121 (24.1081)	mem 4879MB
[2022-05-31 00:35:32 MetaFG_0] (main.py 265): INFO Train: [16/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2938 (0.3065)	loss 1.3738 (1.6534)	grad_norm 17.5292 (24.2020)	mem 4879MB
[2022-05-31 00:35:35 MetaFG_0] (main.py 265): INFO Train: [16/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.3001 (0.3065)	loss 1.5102 (1.6537)	grad_norm 22.2105 (24.0881)	mem 4879MB
[2022-05-31 00:35:38 MetaFG_0] (main.py 265): INFO Train: [16/300][420/1562]	eta 0:05:49 lr 0.000005	time 0.2952 (0.3065)	loss 1.6489 (1.6516)	grad_norm 21.8317 (24.0280)	mem 4879MB
[2022-05-31 00:35:41 MetaFG_0] (main.py 265): INFO Train: [16/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2923 (0.3064)	loss 1.2513 (1.6504)	grad_norm 20.1098 (24.0962)	mem 4879MB
[2022-05-31 00:35:44 MetaFG_0] (main.py 265): INFO Train: [16/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2979 (0.3063)	loss 1.7631 (1.6525)	grad_norm 17.2817 (24.1062)	mem 4879MB
[2022-05-31 00:35:47 MetaFG_0] (main.py 265): INFO Train: [16/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2990 (0.3062)	loss 1.7332 (1.6558)	grad_norm 27.6621 (24.2100)	mem 4879MB
[2022-05-31 00:35:50 MetaFG_0] (main.py 265): INFO Train: [16/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2924 (0.3062)	loss 1.3187 (1.6559)	grad_norm 24.6499 (24.2139)	mem 4879MB
[2022-05-31 00:35:53 MetaFG_0] (main.py 265): INFO Train: [16/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2941 (0.3062)	loss 1.8495 (1.6543)	grad_norm 31.2108 (24.1575)	mem 4879MB
[2022-05-31 00:35:56 MetaFG_0] (main.py 265): INFO Train: [16/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2982 (0.3061)	loss 1.7895 (1.6514)	grad_norm 20.9343 (24.2308)	mem 4879MB
[2022-05-31 00:35:59 MetaFG_0] (main.py 265): INFO Train: [16/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2934 (0.3061)	loss 1.6770 (1.6518)	grad_norm 17.8599 (24.2912)	mem 4879MB
[2022-05-31 00:36:02 MetaFG_0] (main.py 265): INFO Train: [16/300][500/1562]	eta 0:05:24 lr 0.000005	time 0.2936 (0.3060)	loss 1.8702 (1.6522)	grad_norm 34.9808 (24.2556)	mem 4879MB
[2022-05-31 00:36:05 MetaFG_0] (main.py 265): INFO Train: [16/300][510/1562]	eta 0:05:21 lr 0.000005	time 0.2939 (0.3060)	loss 1.6746 (1.6497)	grad_norm 24.3622 (24.3674)	mem 4879MB
[2022-05-31 00:36:08 MetaFG_0] (main.py 265): INFO Train: [16/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.2984 (0.3060)	loss 1.6614 (1.6492)	grad_norm 18.7930 (24.3735)	mem 4879MB
[2022-05-31 00:36:11 MetaFG_0] (main.py 265): INFO Train: [16/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2975 (0.3060)	loss 1.8024 (1.6489)	grad_norm 23.1438 (24.4531)	mem 4879MB
[2022-05-31 00:36:14 MetaFG_0] (main.py 265): INFO Train: [16/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2940 (0.3060)	loss 1.3339 (1.6460)	grad_norm 24.5757 (24.4633)	mem 4879MB
[2022-05-31 00:36:17 MetaFG_0] (main.py 265): INFO Train: [16/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2994 (0.3060)	loss 1.6036 (1.6451)	grad_norm 32.7618 (24.4732)	mem 4879MB
[2022-05-31 00:36:20 MetaFG_0] (main.py 265): INFO Train: [16/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2925 (0.3060)	loss 1.3978 (1.6453)	grad_norm 33.4158 (24.4855)	mem 4879MB
[2022-05-31 00:36:23 MetaFG_0] (main.py 265): INFO Train: [16/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.3008 (0.3060)	loss 1.5702 (1.6451)	grad_norm 19.0070 (24.5085)	mem 4879MB
[2022-05-31 00:36:27 MetaFG_0] (main.py 265): INFO Train: [16/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2930 (0.3060)	loss 1.4656 (1.6450)	grad_norm 29.3187 (24.4966)	mem 4879MB
[2022-05-31 00:36:30 MetaFG_0] (main.py 265): INFO Train: [16/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2980 (0.3060)	loss 1.7547 (1.6455)	grad_norm 28.0395 (24.5199)	mem 4879MB
[2022-05-31 00:36:33 MetaFG_0] (main.py 265): INFO Train: [16/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2929 (0.3060)	loss 1.6532 (1.6469)	grad_norm 22.6375 (24.5009)	mem 4879MB
[2022-05-31 00:36:36 MetaFG_0] (main.py 265): INFO Train: [16/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2941 (0.3060)	loss 1.4219 (1.6457)	grad_norm 21.5497 (24.4971)	mem 4879MB
[2022-05-31 00:36:39 MetaFG_0] (main.py 265): INFO Train: [16/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2991 (0.3060)	loss 1.8011 (1.6463)	grad_norm 36.6309 (24.5131)	mem 4879MB
[2022-05-31 00:36:42 MetaFG_0] (main.py 265): INFO Train: [16/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2994 (0.3059)	loss 1.9640 (1.6466)	grad_norm 17.9326 (24.4928)	mem 4879MB
[2022-05-31 00:36:45 MetaFG_0] (main.py 265): INFO Train: [16/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2925 (0.3059)	loss 1.6861 (1.6470)	grad_norm 19.7466 (24.5105)	mem 4879MB
[2022-05-31 00:36:48 MetaFG_0] (main.py 265): INFO Train: [16/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2997 (0.3059)	loss 1.6122 (1.6463)	grad_norm 18.8783 (24.4971)	mem 4879MB
[2022-05-31 00:36:51 MetaFG_0] (main.py 265): INFO Train: [16/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2929 (0.3058)	loss 1.9499 (1.6462)	grad_norm 18.0185 (24.4406)	mem 4879MB
[2022-05-31 00:36:54 MetaFG_0] (main.py 265): INFO Train: [16/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2928 (0.3058)	loss 1.4525 (1.6456)	grad_norm 16.3738 (24.3795)	mem 4879MB
[2022-05-31 00:36:57 MetaFG_0] (main.py 265): INFO Train: [16/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2982 (0.3058)	loss 1.4380 (1.6443)	grad_norm 21.0434 (24.3516)	mem 4879MB
[2022-05-31 00:37:00 MetaFG_0] (main.py 265): INFO Train: [16/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2934 (0.3058)	loss 1.3497 (1.6440)	grad_norm 26.6487 (24.3644)	mem 4879MB
[2022-05-31 00:37:03 MetaFG_0] (main.py 265): INFO Train: [16/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2923 (0.3059)	loss 1.6832 (1.6435)	grad_norm 22.7584 (24.4042)	mem 4879MB
[2022-05-31 00:37:06 MetaFG_0] (main.py 265): INFO Train: [16/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2921 (0.3059)	loss 1.7120 (1.6427)	grad_norm 22.9684 (24.3944)	mem 4879MB
[2022-05-31 00:37:09 MetaFG_0] (main.py 265): INFO Train: [16/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.3010 (0.3058)	loss 1.6729 (1.6429)	grad_norm 26.4683 (24.4797)	mem 4879MB
[2022-05-31 00:37:12 MetaFG_0] (main.py 265): INFO Train: [16/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2923 (0.3058)	loss 1.4517 (1.6434)	grad_norm 27.0197 (24.4596)	mem 4879MB
[2022-05-31 00:37:15 MetaFG_0] (main.py 265): INFO Train: [16/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2925 (0.3058)	loss 1.4194 (1.6435)	grad_norm 24.6647 (24.4424)	mem 4879MB
[2022-05-31 00:37:18 MetaFG_0] (main.py 265): INFO Train: [16/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2933 (0.3057)	loss 1.9033 (1.6442)	grad_norm 22.3559 (24.4323)	mem 4879MB
[2022-05-31 00:37:21 MetaFG_0] (main.py 265): INFO Train: [16/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2932 (0.3057)	loss 1.1996 (1.6432)	grad_norm 28.0563 (24.4139)	mem 4879MB
[2022-05-31 00:37:24 MetaFG_0] (main.py 265): INFO Train: [16/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.3005 (0.3057)	loss 1.4623 (1.6436)	grad_norm 19.2672 (24.4349)	mem 4879MB
[2022-05-31 00:37:27 MetaFG_0] (main.py 265): INFO Train: [16/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2932 (0.3056)	loss 1.2174 (1.6422)	grad_norm 29.4692 (24.4639)	mem 4879MB
[2022-05-31 00:37:31 MetaFG_0] (main.py 265): INFO Train: [16/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2988 (0.3056)	loss 1.4985 (1.6418)	grad_norm 22.1184 (24.4800)	mem 4879MB
[2022-05-31 00:37:34 MetaFG_0] (main.py 265): INFO Train: [16/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2933 (0.3056)	loss 1.4576 (1.6416)	grad_norm 25.1300 (24.5099)	mem 4879MB
[2022-05-31 00:37:37 MetaFG_0] (main.py 265): INFO Train: [16/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.3010 (0.3056)	loss 1.8137 (1.6417)	grad_norm 16.6216 (24.4879)	mem 4879MB
[2022-05-31 00:37:40 MetaFG_0] (main.py 265): INFO Train: [16/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2920 (0.3056)	loss 1.7188 (1.6419)	grad_norm 26.7575 (24.4965)	mem 4879MB
[2022-05-31 00:37:43 MetaFG_0] (main.py 265): INFO Train: [16/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2923 (0.3055)	loss 1.5519 (1.6414)	grad_norm 40.8275 (24.4838)	mem 4879MB
[2022-05-31 00:37:46 MetaFG_0] (main.py 265): INFO Train: [16/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2993 (0.3055)	loss 1.5385 (1.6422)	grad_norm 17.2350 (24.4770)	mem 4879MB
[2022-05-31 00:37:49 MetaFG_0] (main.py 265): INFO Train: [16/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2943 (0.3055)	loss 1.6683 (1.6421)	grad_norm 16.4285 (24.4471)	mem 4879MB
[2022-05-31 00:37:52 MetaFG_0] (main.py 265): INFO Train: [16/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2953 (0.3055)	loss 1.7414 (1.6428)	grad_norm 22.3346 (24.4390)	mem 4879MB
[2022-05-31 00:37:55 MetaFG_0] (main.py 265): INFO Train: [16/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2960 (0.3054)	loss 1.7642 (1.6429)	grad_norm 25.7368 (24.4657)	mem 4879MB
[2022-05-31 00:37:58 MetaFG_0] (main.py 265): INFO Train: [16/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2931 (0.3054)	loss 1.7858 (1.6442)	grad_norm 27.7728 (24.4723)	mem 4879MB
[2022-05-31 00:38:01 MetaFG_0] (main.py 265): INFO Train: [16/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2980 (0.3054)	loss 1.6406 (1.6433)	grad_norm 40.6224 (24.5118)	mem 4879MB
[2022-05-31 00:38:04 MetaFG_0] (main.py 265): INFO Train: [16/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2936 (0.3054)	loss 1.5827 (1.6414)	grad_norm 12.8702 (24.4908)	mem 4879MB
[2022-05-31 00:38:07 MetaFG_0] (main.py 265): INFO Train: [16/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2977 (0.3054)	loss 1.6334 (1.6404)	grad_norm 11.9470 (24.4538)	mem 4879MB
[2022-05-31 00:38:10 MetaFG_0] (main.py 265): INFO Train: [16/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2985 (0.3054)	loss 1.7561 (1.6406)	grad_norm 22.8599 (24.4586)	mem 4879MB
[2022-05-31 00:38:13 MetaFG_0] (main.py 265): INFO Train: [16/300][930/1562]	eta 0:03:12 lr 0.000005	time 0.2925 (0.3054)	loss 1.5431 (1.6406)	grad_norm 17.9374 (24.4253)	mem 4879MB
[2022-05-31 00:38:16 MetaFG_0] (main.py 265): INFO Train: [16/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2979 (0.3053)	loss 1.3608 (1.6406)	grad_norm 44.8222 (24.4643)	mem 4879MB
[2022-05-31 00:38:19 MetaFG_0] (main.py 265): INFO Train: [16/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.3000 (0.3053)	loss 1.8279 (1.6409)	grad_norm 15.8274 (24.4575)	mem 4879MB
[2022-05-31 00:38:22 MetaFG_0] (main.py 265): INFO Train: [16/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2926 (0.3053)	loss 1.6933 (1.6395)	grad_norm 18.7905 (24.4737)	mem 4879MB
[2022-05-31 00:38:25 MetaFG_0] (main.py 265): INFO Train: [16/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.3005 (0.3053)	loss 1.1782 (1.6391)	grad_norm 34.0399 (24.4866)	mem 4879MB
[2022-05-31 00:38:28 MetaFG_0] (main.py 265): INFO Train: [16/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2983 (0.3053)	loss 1.1746 (1.6381)	grad_norm 31.2282 (24.4963)	mem 4879MB
[2022-05-31 00:38:31 MetaFG_0] (main.py 265): INFO Train: [16/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2922 (0.3053)	loss 1.3768 (1.6375)	grad_norm 40.3416 (24.5145)	mem 4879MB
[2022-05-31 00:38:34 MetaFG_0] (main.py 265): INFO Train: [16/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2919 (0.3053)	loss 1.6650 (1.6375)	grad_norm 18.5523 (24.4906)	mem 4879MB
[2022-05-31 00:38:37 MetaFG_0] (main.py 265): INFO Train: [16/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2981 (0.3053)	loss 1.6930 (1.6365)	grad_norm 16.6394 (24.5135)	mem 4879MB
[2022-05-31 00:38:40 MetaFG_0] (main.py 265): INFO Train: [16/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2988 (0.3053)	loss 1.7181 (1.6374)	grad_norm 28.2227 (24.4988)	mem 4879MB
[2022-05-31 00:38:44 MetaFG_0] (main.py 265): INFO Train: [16/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2958 (0.3053)	loss 1.8446 (1.6371)	grad_norm 25.0601 (24.5035)	mem 4879MB
[2022-05-31 00:38:47 MetaFG_0] (main.py 265): INFO Train: [16/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2983 (0.3053)	loss 1.3585 (1.6368)	grad_norm 18.9116 (24.5023)	mem 4879MB
[2022-05-31 00:38:50 MetaFG_0] (main.py 265): INFO Train: [16/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2935 (0.3052)	loss 1.6760 (1.6370)	grad_norm 19.4242 (24.5781)	mem 4879MB
[2022-05-31 00:38:53 MetaFG_0] (main.py 265): INFO Train: [16/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2932 (0.3052)	loss 1.7469 (1.6365)	grad_norm 27.9296 (24.6172)	mem 4879MB
[2022-05-31 00:38:56 MetaFG_0] (main.py 265): INFO Train: [16/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2930 (0.3052)	loss 1.8210 (1.6365)	grad_norm 25.9214 (24.6278)	mem 4879MB
[2022-05-31 00:38:59 MetaFG_0] (main.py 265): INFO Train: [16/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2988 (0.3052)	loss 1.4618 (1.6364)	grad_norm 27.3397 (24.6030)	mem 4879MB
[2022-05-31 00:39:02 MetaFG_0] (main.py 265): INFO Train: [16/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2927 (0.3052)	loss 1.9098 (1.6364)	grad_norm 20.0166 (24.6127)	mem 4879MB
[2022-05-31 00:39:05 MetaFG_0] (main.py 265): INFO Train: [16/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2923 (0.3052)	loss 1.5124 (1.6368)	grad_norm 37.3535 (24.6082)	mem 4879MB
[2022-05-31 00:39:08 MetaFG_0] (main.py 265): INFO Train: [16/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2917 (0.3052)	loss 1.7456 (1.6369)	grad_norm 20.5383 (24.5859)	mem 4879MB
[2022-05-31 00:39:11 MetaFG_0] (main.py 265): INFO Train: [16/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2979 (0.3052)	loss 1.2968 (1.6374)	grad_norm 30.4694 (24.5729)	mem 4879MB
[2022-05-31 00:39:14 MetaFG_0] (main.py 265): INFO Train: [16/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2938 (0.3052)	loss 1.6164 (1.6361)	grad_norm 12.5435 (24.5706)	mem 4879MB
[2022-05-31 00:39:17 MetaFG_0] (main.py 265): INFO Train: [16/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2926 (0.3051)	loss 1.8548 (1.6361)	grad_norm 24.1160 (24.5869)	mem 4879MB
[2022-05-31 00:39:20 MetaFG_0] (main.py 265): INFO Train: [16/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2985 (0.3051)	loss 1.4106 (1.6356)	grad_norm 35.6905 (24.5956)	mem 4879MB
[2022-05-31 00:39:23 MetaFG_0] (main.py 265): INFO Train: [16/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2920 (0.3052)	loss 1.8948 (1.6355)	grad_norm 19.8128 (24.5649)	mem 4879MB
[2022-05-31 00:39:26 MetaFG_0] (main.py 265): INFO Train: [16/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2939 (0.3052)	loss 1.6024 (1.6346)	grad_norm 18.8309 (24.5516)	mem 4879MB
[2022-05-31 00:39:29 MetaFG_0] (main.py 265): INFO Train: [16/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2919 (0.3052)	loss 1.9361 (1.6353)	grad_norm 20.2889 (24.5556)	mem 4879MB
[2022-05-31 00:39:32 MetaFG_0] (main.py 265): INFO Train: [16/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2935 (0.3052)	loss 1.4884 (1.6351)	grad_norm 28.9798 (24.5669)	mem 4879MB
[2022-05-31 00:39:35 MetaFG_0] (main.py 265): INFO Train: [16/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.3001 (0.3052)	loss 1.6886 (1.6355)	grad_norm 17.1474 (24.5659)	mem 4879MB
[2022-05-31 00:39:38 MetaFG_0] (main.py 265): INFO Train: [16/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2989 (0.3052)	loss 1.3406 (1.6350)	grad_norm 26.0173 (24.5470)	mem 4879MB
[2022-05-31 00:39:41 MetaFG_0] (main.py 265): INFO Train: [16/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2938 (0.3052)	loss 1.8122 (1.6349)	grad_norm 19.3705 (24.5683)	mem 4879MB
[2022-05-31 00:39:44 MetaFG_0] (main.py 265): INFO Train: [16/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2924 (0.3052)	loss 1.8298 (1.6348)	grad_norm 21.7922 (24.5909)	mem 4879MB
[2022-05-31 00:39:47 MetaFG_0] (main.py 265): INFO Train: [16/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2930 (0.3052)	loss 1.2666 (1.6341)	grad_norm 21.6603 (24.5899)	mem 4879MB
[2022-05-31 00:39:51 MetaFG_0] (main.py 265): INFO Train: [16/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2919 (0.3051)	loss 1.1340 (1.6342)	grad_norm 32.9036 (24.5848)	mem 4879MB
[2022-05-31 00:39:54 MetaFG_0] (main.py 265): INFO Train: [16/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2918 (0.3051)	loss 1.8343 (1.6350)	grad_norm 24.7672 (24.5734)	mem 4879MB
[2022-05-31 00:39:57 MetaFG_0] (main.py 265): INFO Train: [16/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2982 (0.3051)	loss 1.5136 (1.6355)	grad_norm 34.2395 (24.5783)	mem 4879MB
[2022-05-31 00:40:00 MetaFG_0] (main.py 265): INFO Train: [16/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2949 (0.3051)	loss 1.7775 (1.6356)	grad_norm 18.5878 (24.5714)	mem 4879MB
[2022-05-31 00:40:03 MetaFG_0] (main.py 265): INFO Train: [16/300][1290/1562]	eta 0:01:22 lr 0.000005	time 0.2991 (0.3051)	loss 1.7889 (1.6361)	grad_norm 21.2014 (24.5614)	mem 4879MB
[2022-05-31 00:40:06 MetaFG_0] (main.py 265): INFO Train: [16/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2916 (0.3051)	loss 1.7421 (1.6363)	grad_norm 14.6140 (24.5407)	mem 4879MB
[2022-05-31 00:40:09 MetaFG_0] (main.py 265): INFO Train: [16/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2984 (0.3051)	loss 1.7235 (1.6361)	grad_norm 16.8177 (24.5502)	mem 4879MB
[2022-05-31 00:40:12 MetaFG_0] (main.py 265): INFO Train: [16/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2922 (0.3051)	loss 1.4529 (1.6357)	grad_norm 24.4527 (24.5582)	mem 4879MB
[2022-05-31 00:40:15 MetaFG_0] (main.py 265): INFO Train: [16/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.3018 (0.3051)	loss 1.7265 (1.6361)	grad_norm 14.9042 (24.5904)	mem 4879MB
[2022-05-31 00:40:18 MetaFG_0] (main.py 265): INFO Train: [16/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2933 (0.3051)	loss 1.8540 (1.6357)	grad_norm 15.5142 (24.5846)	mem 4879MB
[2022-05-31 00:40:21 MetaFG_0] (main.py 265): INFO Train: [16/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2929 (0.3051)	loss 1.6046 (1.6360)	grad_norm 18.6210 (24.5701)	mem 4879MB
[2022-05-31 00:40:24 MetaFG_0] (main.py 265): INFO Train: [16/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2918 (0.3051)	loss 1.5420 (1.6357)	grad_norm 14.1281 (24.5530)	mem 4879MB
[2022-05-31 00:40:27 MetaFG_0] (main.py 265): INFO Train: [16/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.3010 (0.3051)	loss 1.3978 (1.6359)	grad_norm 22.5193 (24.5784)	mem 4879MB
[2022-05-31 00:40:30 MetaFG_0] (main.py 265): INFO Train: [16/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2921 (0.3051)	loss 1.8525 (1.6363)	grad_norm 24.8657 (24.5474)	mem 4879MB
[2022-05-31 00:40:33 MetaFG_0] (main.py 265): INFO Train: [16/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2922 (0.3051)	loss 1.9381 (1.6367)	grad_norm 23.7185 (24.5536)	mem 4879MB
[2022-05-31 00:40:36 MetaFG_0] (main.py 265): INFO Train: [16/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2990 (0.3051)	loss 1.3071 (1.6363)	grad_norm 16.3986 (24.5490)	mem 4879MB
[2022-05-31 00:40:39 MetaFG_0] (main.py 265): INFO Train: [16/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2980 (0.3051)	loss 1.2720 (1.6361)	grad_norm 22.8498 (24.5487)	mem 4879MB
[2022-05-31 00:40:42 MetaFG_0] (main.py 265): INFO Train: [16/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2982 (0.3051)	loss 1.5130 (1.6360)	grad_norm 25.1494 (24.5745)	mem 4879MB
[2022-05-31 00:40:45 MetaFG_0] (main.py 265): INFO Train: [16/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2982 (0.3051)	loss 1.4875 (1.6368)	grad_norm 16.8761 (24.5464)	mem 4879MB
[2022-05-31 00:40:48 MetaFG_0] (main.py 265): INFO Train: [16/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2932 (0.3051)	loss 1.6748 (1.6371)	grad_norm 21.2117 (24.5442)	mem 4879MB
[2022-05-31 00:40:51 MetaFG_0] (main.py 265): INFO Train: [16/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2930 (0.3051)	loss 1.3403 (1.6369)	grad_norm 39.0087 (24.5487)	mem 4879MB
[2022-05-31 00:40:54 MetaFG_0] (main.py 265): INFO Train: [16/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2949 (0.3050)	loss 1.6933 (1.6373)	grad_norm 19.0074 (24.5500)	mem 4879MB
[2022-05-31 00:40:58 MetaFG_0] (main.py 265): INFO Train: [16/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.3060 (0.3050)	loss 1.7529 (1.6380)	grad_norm 16.6818 (24.5301)	mem 4879MB
[2022-05-31 00:41:01 MetaFG_0] (main.py 265): INFO Train: [16/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2919 (0.3050)	loss 1.8385 (1.6380)	grad_norm 11.8783 (24.5082)	mem 4879MB
[2022-05-31 00:41:04 MetaFG_0] (main.py 265): INFO Train: [16/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2946 (0.3050)	loss 1.6374 (1.6370)	grad_norm 7.7590 (24.5202)	mem 4879MB
[2022-05-31 00:41:07 MetaFG_0] (main.py 265): INFO Train: [16/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2988 (0.3050)	loss 1.5476 (1.6369)	grad_norm 16.6622 (24.5016)	mem 4879MB
[2022-05-31 00:41:10 MetaFG_0] (main.py 265): INFO Train: [16/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.3001 (0.3050)	loss 1.5791 (1.6370)	grad_norm 24.3149 (24.4989)	mem 4879MB
[2022-05-31 00:41:13 MetaFG_0] (main.py 265): INFO Train: [16/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2932 (0.3050)	loss 1.6476 (1.6368)	grad_norm 18.2887 (24.4979)	mem 4879MB
[2022-05-31 00:41:16 MetaFG_0] (main.py 265): INFO Train: [16/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2925 (0.3050)	loss 1.5626 (1.6369)	grad_norm 17.5191 (24.5210)	mem 4879MB
[2022-05-31 00:41:19 MetaFG_0] (main.py 265): INFO Train: [16/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2923 (0.3050)	loss 1.4621 (1.6366)	grad_norm 22.0177 (24.5029)	mem 4879MB
[2022-05-31 00:41:22 MetaFG_0] (main.py 265): INFO Train: [16/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2956 (0.3050)	loss 1.4510 (1.6371)	grad_norm 27.6726 (24.5051)	mem 4879MB
[2022-05-31 00:41:25 MetaFG_0] (main.py 265): INFO Train: [16/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2923 (0.3050)	loss 1.4070 (1.6373)	grad_norm 21.9356 (24.5099)	mem 4879MB
[2022-05-31 00:41:25 MetaFG_0] (main.py 272): INFO EPOCH 16 training takes 0:07:56
[2022-05-31 00:41:25 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_16.pth saving......
[2022-05-31 00:41:26 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_16.pth saved !!!
[2022-05-31 00:41:26 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:41:28 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:41:28 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:41:28 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.686 (0.686)	Loss 1.5148 (1.5148)	Acc@1 75.000 (75.000)	Acc@5 90.625 (90.625)	Mem 4879MB
[2022-05-31 00:41:29 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.097 (0.149)	Loss 1.2400 (1.3487)	Acc@1 65.625 (71.591)	Acc@5 100.000 (95.170)	Mem 4879MB
[2022-05-31 00:41:30 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.123)	Loss 1.5466 (1.3603)	Acc@1 65.625 (72.321)	Acc@5 93.750 (95.685)	Mem 4879MB
[2022-05-31 00:41:31 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.114)	Loss 1.5293 (1.3762)	Acc@1 59.375 (71.069)	Acc@5 93.750 (94.960)	Mem 4879MB
[2022-05-31 00:41:32 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.090 (0.109)	Loss 1.1025 (1.3515)	Acc@1 78.125 (72.027)	Acc@5 96.875 (95.503)	Mem 4879MB
[2022-05-31 00:41:33 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.100 (0.108)	Loss 1.3090 (1.3515)	Acc@1 75.000 (71.569)	Acc@5 90.625 (95.588)	Mem 4879MB
[2022-05-31 00:41:34 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.101 (0.106)	Loss 1.1590 (1.3358)	Acc@1 84.375 (71.875)	Acc@5 96.875 (95.697)	Mem 4879MB
[2022-05-31 00:41:35 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.104)	Loss 1.3810 (1.3307)	Acc@1 68.750 (72.227)	Acc@5 96.875 (95.819)	Mem 4879MB
[2022-05-31 00:41:36 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.092 (0.103)	Loss 1.3750 (1.3174)	Acc@1 62.500 (72.955)	Acc@5 93.750 (95.949)	Mem 4879MB
[2022-05-31 00:41:37 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.108 (0.102)	Loss 1.4571 (1.3257)	Acc@1 71.875 (72.493)	Acc@5 87.500 (95.673)	Mem 4879MB
[2022-05-31 00:41:38 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.102)	Loss 1.0434 (1.3169)	Acc@1 78.125 (72.587)	Acc@5 100.000 (95.885)	Mem 4879MB
[2022-05-31 00:41:39 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.101)	Loss 1.3606 (1.3275)	Acc@1 71.875 (72.157)	Acc@5 96.875 (95.833)	Mem 4879MB
[2022-05-31 00:41:40 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.103 (0.101)	Loss 1.3058 (1.3311)	Acc@1 81.250 (72.314)	Acc@5 93.750 (95.790)	Mem 4879MB
[2022-05-31 00:41:41 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.100)	Loss 1.0623 (1.3292)	Acc@1 87.500 (72.638)	Acc@5 96.875 (95.802)	Mem 4879MB
[2022-05-31 00:41:42 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.091 (0.099)	Loss 1.2051 (1.3276)	Acc@1 81.250 (72.695)	Acc@5 100.000 (95.900)	Mem 4879MB
[2022-05-31 00:41:43 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.099)	Loss 1.3879 (1.3299)	Acc@1 62.500 (72.351)	Acc@5 93.750 (95.820)	Mem 4879MB
[2022-05-31 00:41:43 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.090 (0.099)	Loss 1.5003 (1.3287)	Acc@1 62.500 (72.360)	Acc@5 93.750 (95.769)	Mem 4879MB
[2022-05-31 00:41:44 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 1.3702 (1.3297)	Acc@1 71.875 (72.423)	Acc@5 93.750 (95.779)	Mem 4879MB
[2022-05-31 00:41:45 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.098)	Loss 1.4935 (1.3292)	Acc@1 65.625 (72.393)	Acc@5 96.875 (95.684)	Mem 4879MB
[2022-05-31 00:41:46 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.093 (0.098)	Loss 1.3177 (1.3307)	Acc@1 68.750 (72.366)	Acc@5 96.875 (95.664)	Mem 4879MB
[2022-05-31 00:41:47 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.098)	Loss 1.1375 (1.3300)	Acc@1 81.250 (72.528)	Acc@5 96.875 (95.678)	Mem 4879MB
[2022-05-31 00:41:48 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 1.1044 (1.3307)	Acc@1 81.250 (72.541)	Acc@5 96.875 (95.631)	Mem 4879MB
[2022-05-31 00:41:49 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.097)	Loss 1.2095 (1.3293)	Acc@1 78.125 (72.554)	Acc@5 96.875 (95.631)	Mem 4879MB
[2022-05-31 00:41:50 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.107 (0.097)	Loss 1.3523 (1.3307)	Acc@1 75.000 (72.551)	Acc@5 93.750 (95.671)	Mem 4879MB
[2022-05-31 00:41:51 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 1.2054 (1.3295)	Acc@1 75.000 (72.692)	Acc@5 93.750 (95.708)	Mem 4879MB
[2022-05-31 00:41:52 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.097)	Loss 1.3615 (1.3333)	Acc@1 62.500 (72.535)	Acc@5 96.875 (95.618)	Mem 4879MB
[2022-05-31 00:41:53 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 1.4331 (1.3300)	Acc@1 65.625 (72.629)	Acc@5 93.750 (95.654)	Mem 4879MB
[2022-05-31 00:41:54 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.097)	Loss 1.6844 (1.3341)	Acc@1 75.000 (72.625)	Acc@5 87.500 (95.595)	Mem 4879MB
[2022-05-31 00:41:55 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 1.4346 (1.3357)	Acc@1 68.750 (72.576)	Acc@5 96.875 (95.596)	Mem 4879MB
[2022-05-31 00:41:56 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.097)	Loss 1.3034 (1.3347)	Acc@1 68.750 (72.562)	Acc@5 96.875 (95.640)	Mem 4879MB
[2022-05-31 00:41:57 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.097 (0.097)	Loss 1.4199 (1.3345)	Acc@1 68.750 (72.571)	Acc@5 96.875 (95.640)	Mem 4879MB
[2022-05-31 00:41:58 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 1.3298 (1.3351)	Acc@1 84.375 (72.609)	Acc@5 93.750 (95.609)	Mem 4879MB
[2022-05-31 00:41:58 MetaFG_0] (main.py 330): INFO  * Acc@1 72.650 Acc@5 95.630
[2022-05-31 00:41:58 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 72.7%
[2022-05-31 00:41:58 MetaFG_0] (main.py 171): INFO Max accuracy: 72.65%
[2022-05-31 00:41:59 MetaFG_0] (main.py 265): INFO Train: [17/300][0/1562]	eta 0:23:39 lr 0.000005	time 0.9089 (0.9089)	loss 1.9466 (1.9466)	grad_norm 40.6011 (40.6011)	mem 4879MB
[2022-05-31 00:42:02 MetaFG_0] (main.py 265): INFO Train: [17/300][10/1562]	eta 0:09:29 lr 0.000005	time 0.2923 (0.3673)	loss 1.6028 (1.7170)	grad_norm 26.8621 (25.2954)	mem 4879MB
[2022-05-31 00:42:05 MetaFG_0] (main.py 265): INFO Train: [17/300][20/1562]	eta 0:08:39 lr 0.000005	time 0.2974 (0.3368)	loss 1.7357 (1.6899)	grad_norm 23.4689 (27.0318)	mem 4879MB
[2022-05-31 00:42:08 MetaFG_0] (main.py 265): INFO Train: [17/300][30/1562]	eta 0:08:19 lr 0.000005	time 0.2980 (0.3263)	loss 1.9272 (1.6894)	grad_norm 27.1707 (26.1680)	mem 4879MB
[2022-05-31 00:42:11 MetaFG_0] (main.py 265): INFO Train: [17/300][40/1562]	eta 0:08:08 lr 0.000005	time 0.2937 (0.3211)	loss 1.7407 (1.6822)	grad_norm 21.9181 (24.6041)	mem 4879MB
[2022-05-31 00:42:14 MetaFG_0] (main.py 265): INFO Train: [17/300][50/1562]	eta 0:08:00 lr 0.000005	time 0.2941 (0.3175)	loss 1.7364 (1.6582)	grad_norm 17.9575 (24.4547)	mem 4879MB
[2022-05-31 00:42:17 MetaFG_0] (main.py 265): INFO Train: [17/300][60/1562]	eta 0:07:53 lr 0.000005	time 0.2930 (0.3153)	loss 1.7469 (1.6530)	grad_norm 13.0678 (23.9796)	mem 4879MB
[2022-05-31 00:42:20 MetaFG_0] (main.py 265): INFO Train: [17/300][70/1562]	eta 0:07:48 lr 0.000005	time 0.2925 (0.3140)	loss 1.6269 (1.6426)	grad_norm 19.5024 (23.8309)	mem 4879MB
[2022-05-31 00:42:23 MetaFG_0] (main.py 265): INFO Train: [17/300][80/1562]	eta 0:07:43 lr 0.000005	time 0.2923 (0.3126)	loss 1.2662 (1.6282)	grad_norm 26.3921 (23.4991)	mem 4879MB
[2022-05-31 00:42:26 MetaFG_0] (main.py 265): INFO Train: [17/300][90/1562]	eta 0:07:38 lr 0.000005	time 0.2962 (0.3114)	loss 1.5933 (1.6199)	grad_norm 24.1265 (24.1383)	mem 4879MB
[2022-05-31 00:42:29 MetaFG_0] (main.py 265): INFO Train: [17/300][100/1562]	eta 0:07:34 lr 0.000005	time 0.2918 (0.3105)	loss 1.9028 (1.6332)	grad_norm 21.4356 (24.2659)	mem 4879MB
[2022-05-31 00:42:32 MetaFG_0] (main.py 265): INFO Train: [17/300][110/1562]	eta 0:07:30 lr 0.000005	time 0.2987 (0.3101)	loss 1.6878 (1.6377)	grad_norm 24.2109 (24.7074)	mem 4879MB
[2022-05-31 00:42:35 MetaFG_0] (main.py 265): INFO Train: [17/300][120/1562]	eta 0:07:26 lr 0.000005	time 0.2975 (0.3095)	loss 1.6710 (1.6427)	grad_norm 21.7494 (24.6818)	mem 4879MB
[2022-05-31 00:42:38 MetaFG_0] (main.py 265): INFO Train: [17/300][130/1562]	eta 0:07:22 lr 0.000005	time 0.2982 (0.3091)	loss 1.2215 (1.6396)	grad_norm 18.3950 (24.4175)	mem 4879MB
[2022-05-31 00:42:41 MetaFG_0] (main.py 265): INFO Train: [17/300][140/1562]	eta 0:07:19 lr 0.000005	time 0.2938 (0.3089)	loss 1.4032 (1.6398)	grad_norm 44.6056 (24.6399)	mem 4879MB
[2022-05-31 00:42:44 MetaFG_0] (main.py 265): INFO Train: [17/300][150/1562]	eta 0:07:15 lr 0.000005	time 0.2919 (0.3086)	loss 1.6584 (1.6428)	grad_norm 32.0918 (24.9520)	mem 4879MB
[2022-05-31 00:42:47 MetaFG_0] (main.py 265): INFO Train: [17/300][160/1562]	eta 0:07:12 lr 0.000005	time 0.2937 (0.3083)	loss 1.6493 (1.6442)	grad_norm 34.6653 (24.9148)	mem 4879MB
[2022-05-31 00:42:50 MetaFG_0] (main.py 265): INFO Train: [17/300][170/1562]	eta 0:07:08 lr 0.000005	time 0.2944 (0.3080)	loss 1.2148 (1.6483)	grad_norm 22.1338 (25.1874)	mem 4879MB
[2022-05-31 00:42:54 MetaFG_0] (main.py 265): INFO Train: [17/300][180/1562]	eta 0:07:05 lr 0.000005	time 0.2928 (0.3079)	loss 1.4019 (1.6479)	grad_norm 21.6911 (25.0443)	mem 4879MB
[2022-05-31 00:42:57 MetaFG_0] (main.py 265): INFO Train: [17/300][190/1562]	eta 0:07:02 lr 0.000005	time 0.2924 (0.3076)	loss 1.6447 (1.6479)	grad_norm 21.6305 (25.0511)	mem 4879MB
[2022-05-31 00:43:00 MetaFG_0] (main.py 265): INFO Train: [17/300][200/1562]	eta 0:06:58 lr 0.000005	time 0.2937 (0.3075)	loss 1.4957 (1.6449)	grad_norm 18.2928 (24.8447)	mem 4879MB
[2022-05-31 00:43:03 MetaFG_0] (main.py 265): INFO Train: [17/300][210/1562]	eta 0:06:55 lr 0.000005	time 0.2989 (0.3074)	loss 1.6454 (1.6407)	grad_norm 15.1002 (24.7814)	mem 4879MB
[2022-05-31 00:43:06 MetaFG_0] (main.py 265): INFO Train: [17/300][220/1562]	eta 0:06:52 lr 0.000005	time 0.2937 (0.3072)	loss 1.7000 (1.6404)	grad_norm 56.7122 (24.9477)	mem 4879MB
[2022-05-31 00:43:09 MetaFG_0] (main.py 265): INFO Train: [17/300][230/1562]	eta 0:06:48 lr 0.000005	time 0.2939 (0.3070)	loss 1.7082 (1.6438)	grad_norm 23.4561 (25.0687)	mem 4879MB
[2022-05-31 00:43:12 MetaFG_0] (main.py 265): INFO Train: [17/300][240/1562]	eta 0:06:45 lr 0.000005	time 0.2931 (0.3069)	loss 1.7307 (1.6443)	grad_norm 23.7540 (25.0568)	mem 4879MB
[2022-05-31 00:43:15 MetaFG_0] (main.py 265): INFO Train: [17/300][250/1562]	eta 0:06:42 lr 0.000005	time 0.2975 (0.3068)	loss 2.0251 (1.6469)	grad_norm 17.5099 (25.1540)	mem 4879MB
[2022-05-31 00:43:18 MetaFG_0] (main.py 265): INFO Train: [17/300][260/1562]	eta 0:06:39 lr 0.000005	time 0.2985 (0.3067)	loss 1.1126 (1.6421)	grad_norm 27.2624 (25.1678)	mem 4879MB
[2022-05-31 00:43:21 MetaFG_0] (main.py 265): INFO Train: [17/300][270/1562]	eta 0:06:36 lr 0.000005	time 0.2922 (0.3067)	loss 1.8269 (1.6454)	grad_norm 16.9734 (25.1715)	mem 4879MB
[2022-05-31 00:43:24 MetaFG_0] (main.py 265): INFO Train: [17/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2942 (0.3066)	loss 1.8540 (1.6418)	grad_norm 15.4006 (25.0813)	mem 4879MB
[2022-05-31 00:43:27 MetaFG_0] (main.py 265): INFO Train: [17/300][290/1562]	eta 0:06:29 lr 0.000005	time 0.2925 (0.3066)	loss 1.5881 (1.6390)	grad_norm 30.7623 (25.2148)	mem 4879MB
[2022-05-31 00:43:30 MetaFG_0] (main.py 265): INFO Train: [17/300][300/1562]	eta 0:06:26 lr 0.000005	time 0.2952 (0.3065)	loss 1.7504 (1.6400)	grad_norm 17.0382 (25.1233)	mem 4879MB
[2022-05-31 00:43:33 MetaFG_0] (main.py 265): INFO Train: [17/300][310/1562]	eta 0:06:23 lr 0.000005	time 0.2923 (0.3064)	loss 1.9268 (1.6421)	grad_norm 56.7719 (25.1783)	mem 4879MB
[2022-05-31 00:43:36 MetaFG_0] (main.py 265): INFO Train: [17/300][320/1562]	eta 0:06:20 lr 0.000005	time 0.2997 (0.3063)	loss 1.8833 (1.6436)	grad_norm 21.5836 (25.1509)	mem 4879MB
[2022-05-31 00:43:39 MetaFG_0] (main.py 265): INFO Train: [17/300][330/1562]	eta 0:06:17 lr 0.000005	time 0.2972 (0.3063)	loss 1.3904 (1.6432)	grad_norm 23.7417 (25.1412)	mem 4879MB
[2022-05-31 00:43:42 MetaFG_0] (main.py 265): INFO Train: [17/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.2919 (0.3062)	loss 1.6421 (1.6408)	grad_norm 29.3840 (25.1258)	mem 4879MB
[2022-05-31 00:43:45 MetaFG_0] (main.py 265): INFO Train: [17/300][350/1562]	eta 0:06:10 lr 0.000005	time 0.2924 (0.3061)	loss 1.7911 (1.6428)	grad_norm 14.8411 (25.1034)	mem 4879MB
[2022-05-31 00:43:48 MetaFG_0] (main.py 265): INFO Train: [17/300][360/1562]	eta 0:06:07 lr 0.000005	time 0.2924 (0.3060)	loss 1.6478 (1.6383)	grad_norm 32.6996 (25.1293)	mem 4879MB
[2022-05-31 00:43:51 MetaFG_0] (main.py 265): INFO Train: [17/300][370/1562]	eta 0:06:04 lr 0.000005	time 0.3009 (0.3060)	loss 1.6399 (1.6398)	grad_norm 22.4860 (25.2492)	mem 4879MB
[2022-05-31 00:43:54 MetaFG_0] (main.py 265): INFO Train: [17/300][380/1562]	eta 0:06:01 lr 0.000005	time 0.2984 (0.3060)	loss 1.8346 (1.6366)	grad_norm 23.2475 (25.2110)	mem 4879MB
[2022-05-31 00:43:57 MetaFG_0] (main.py 265): INFO Train: [17/300][390/1562]	eta 0:05:58 lr 0.000005	time 0.2927 (0.3059)	loss 1.7037 (1.6364)	grad_norm 18.4366 (25.1594)	mem 4879MB
[2022-05-31 00:44:00 MetaFG_0] (main.py 265): INFO Train: [17/300][400/1562]	eta 0:05:55 lr 0.000005	time 0.2963 (0.3059)	loss 1.8643 (1.6377)	grad_norm 16.4776 (25.1312)	mem 4879MB
[2022-05-31 00:44:04 MetaFG_0] (main.py 265): INFO Train: [17/300][410/1562]	eta 0:05:52 lr 0.000005	time 0.2947 (0.3059)	loss 1.6260 (1.6374)	grad_norm 22.2567 (25.1672)	mem 4879MB
[2022-05-31 00:44:07 MetaFG_0] (main.py 265): INFO Train: [17/300][420/1562]	eta 0:05:49 lr 0.000005	time 0.2941 (0.3058)	loss 1.4463 (1.6372)	grad_norm 26.7982 (25.2402)	mem 4879MB
[2022-05-31 00:44:10 MetaFG_0] (main.py 265): INFO Train: [17/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2984 (0.3058)	loss 1.5696 (1.6362)	grad_norm 26.9666 (25.2559)	mem 4879MB
[2022-05-31 00:44:13 MetaFG_0] (main.py 265): INFO Train: [17/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2943 (0.3057)	loss 1.7562 (1.6365)	grad_norm 25.3071 (25.1997)	mem 4879MB
[2022-05-31 00:44:16 MetaFG_0] (main.py 265): INFO Train: [17/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.3018 (0.3058)	loss 1.4713 (1.6354)	grad_norm 23.0178 (25.2528)	mem 4879MB
[2022-05-31 00:44:19 MetaFG_0] (main.py 265): INFO Train: [17/300][460/1562]	eta 0:05:36 lr 0.000005	time 0.2989 (0.3057)	loss 1.4267 (1.6330)	grad_norm 22.8699 (25.1663)	mem 4879MB
[2022-05-31 00:44:22 MetaFG_0] (main.py 265): INFO Train: [17/300][470/1562]	eta 0:05:33 lr 0.000005	time 0.2992 (0.3057)	loss 1.6889 (1.6328)	grad_norm 17.3116 (25.1218)	mem 4879MB
[2022-05-31 00:44:25 MetaFG_0] (main.py 265): INFO Train: [17/300][480/1562]	eta 0:05:30 lr 0.000005	time 0.2926 (0.3057)	loss 1.5738 (1.6329)	grad_norm 26.4864 (25.1663)	mem 4879MB
[2022-05-31 00:44:28 MetaFG_0] (main.py 265): INFO Train: [17/300][490/1562]	eta 0:05:27 lr 0.000005	time 0.2992 (0.3057)	loss 1.5017 (1.6332)	grad_norm 33.2907 (25.3810)	mem 4879MB
[2022-05-31 00:44:31 MetaFG_0] (main.py 265): INFO Train: [17/300][500/1562]	eta 0:05:24 lr 0.000005	time 0.2935 (0.3056)	loss 1.7426 (1.6349)	grad_norm 12.5815 (25.3141)	mem 4879MB
[2022-05-31 00:44:34 MetaFG_0] (main.py 265): INFO Train: [17/300][510/1562]	eta 0:05:21 lr 0.000005	time 0.2999 (0.3056)	loss 1.3377 (1.6338)	grad_norm 39.9085 (25.3917)	mem 4879MB
[2022-05-31 00:44:37 MetaFG_0] (main.py 265): INFO Train: [17/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.3025 (0.3056)	loss 1.6965 (1.6335)	grad_norm 21.6965 (25.3158)	mem 4879MB
[2022-05-31 00:44:40 MetaFG_0] (main.py 265): INFO Train: [17/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.3001 (0.3056)	loss 1.7193 (1.6344)	grad_norm 21.2758 (25.2581)	mem 4879MB
[2022-05-31 00:44:43 MetaFG_0] (main.py 265): INFO Train: [17/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2927 (0.3056)	loss 1.6913 (1.6373)	grad_norm 20.1237 (25.3258)	mem 4879MB
[2022-05-31 00:44:46 MetaFG_0] (main.py 265): INFO Train: [17/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2943 (0.3056)	loss 1.9784 (1.6366)	grad_norm 33.2892 (25.3642)	mem 4879MB
[2022-05-31 00:44:49 MetaFG_0] (main.py 265): INFO Train: [17/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.3021 (0.3056)	loss 1.7532 (1.6371)	grad_norm 14.5992 (25.3492)	mem 4879MB
[2022-05-31 00:44:52 MetaFG_0] (main.py 265): INFO Train: [17/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2937 (0.3056)	loss 1.7189 (1.6380)	grad_norm 42.3701 (25.4593)	mem 4879MB
[2022-05-31 00:44:55 MetaFG_0] (main.py 265): INFO Train: [17/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.3029 (0.3056)	loss 1.7107 (1.6375)	grad_norm 16.0336 (25.4721)	mem 4879MB
[2022-05-31 00:44:58 MetaFG_0] (main.py 265): INFO Train: [17/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.3014 (0.3056)	loss 1.5792 (1.6361)	grad_norm 22.0771 (25.5085)	mem 4879MB
[2022-05-31 00:45:01 MetaFG_0] (main.py 265): INFO Train: [17/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2982 (0.3056)	loss 1.6412 (1.6368)	grad_norm 35.6269 (25.5529)	mem 4879MB
[2022-05-31 00:45:05 MetaFG_0] (main.py 265): INFO Train: [17/300][610/1562]	eta 0:04:50 lr 0.000005	time 0.2951 (0.3056)	loss 1.8114 (1.6383)	grad_norm 16.7869 (25.5549)	mem 4879MB
[2022-05-31 00:45:08 MetaFG_0] (main.py 265): INFO Train: [17/300][620/1562]	eta 0:04:47 lr 0.000005	time 0.2988 (0.3056)	loss 1.5148 (1.6379)	grad_norm 22.6369 (25.5342)	mem 4879MB
[2022-05-31 00:45:11 MetaFG_0] (main.py 265): INFO Train: [17/300][630/1562]	eta 0:04:44 lr 0.000005	time 0.2957 (0.3056)	loss 1.4188 (1.6373)	grad_norm 21.0492 (25.5459)	mem 4879MB
[2022-05-31 00:45:14 MetaFG_0] (main.py 265): INFO Train: [17/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.2942 (0.3056)	loss 1.7879 (1.6377)	grad_norm 23.3828 (25.5220)	mem 4879MB
[2022-05-31 00:45:17 MetaFG_0] (main.py 265): INFO Train: [17/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2942 (0.3055)	loss 1.5951 (1.6376)	grad_norm 29.7175 (25.5626)	mem 4879MB
[2022-05-31 00:45:20 MetaFG_0] (main.py 265): INFO Train: [17/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2935 (0.3055)	loss 1.7839 (1.6365)	grad_norm 19.3020 (25.5851)	mem 4879MB
[2022-05-31 00:45:23 MetaFG_0] (main.py 265): INFO Train: [17/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2941 (0.3055)	loss 1.5365 (1.6358)	grad_norm 15.8904 (25.6396)	mem 4879MB
[2022-05-31 00:45:26 MetaFG_0] (main.py 265): INFO Train: [17/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2925 (0.3055)	loss 2.0769 (1.6343)	grad_norm 16.0256 (25.6241)	mem 4879MB
[2022-05-31 00:45:29 MetaFG_0] (main.py 265): INFO Train: [17/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2929 (0.3055)	loss 1.4510 (1.6331)	grad_norm 34.6149 (25.5907)	mem 4879MB
[2022-05-31 00:45:32 MetaFG_0] (main.py 265): INFO Train: [17/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2985 (0.3055)	loss 1.7990 (1.6344)	grad_norm 23.9542 (25.5890)	mem 4879MB
[2022-05-31 00:45:35 MetaFG_0] (main.py 265): INFO Train: [17/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2923 (0.3055)	loss 1.7482 (1.6353)	grad_norm 14.1692 (25.5132)	mem 4879MB
[2022-05-31 00:45:38 MetaFG_0] (main.py 265): INFO Train: [17/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2983 (0.3055)	loss 1.3757 (1.6358)	grad_norm 20.0516 (25.5179)	mem 4879MB
[2022-05-31 00:45:41 MetaFG_0] (main.py 265): INFO Train: [17/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2976 (0.3055)	loss 1.5501 (1.6347)	grad_norm 14.6513 (25.4514)	mem 4879MB
[2022-05-31 00:45:44 MetaFG_0] (main.py 265): INFO Train: [17/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2936 (0.3054)	loss 1.3525 (1.6346)	grad_norm 29.9560 (25.4836)	mem 4879MB
[2022-05-31 00:45:47 MetaFG_0] (main.py 265): INFO Train: [17/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2924 (0.3054)	loss 1.7616 (1.6340)	grad_norm 23.9113 (25.5160)	mem 4879MB
[2022-05-31 00:45:50 MetaFG_0] (main.py 265): INFO Train: [17/300][760/1562]	eta 0:04:04 lr 0.000005	time 0.2920 (0.3054)	loss 1.3643 (1.6333)	grad_norm 34.8129 (25.5333)	mem 4879MB
[2022-05-31 00:45:53 MetaFG_0] (main.py 265): INFO Train: [17/300][770/1562]	eta 0:04:01 lr 0.000005	time 0.2940 (0.3054)	loss 1.8388 (1.6347)	grad_norm 27.9712 (25.5554)	mem 4879MB
[2022-05-31 00:45:56 MetaFG_0] (main.py 265): INFO Train: [17/300][780/1562]	eta 0:03:58 lr 0.000005	time 0.2938 (0.3054)	loss 1.3366 (1.6344)	grad_norm 45.3520 (25.5879)	mem 4879MB
[2022-05-31 00:45:59 MetaFG_0] (main.py 265): INFO Train: [17/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2924 (0.3054)	loss 1.7631 (1.6334)	grad_norm 23.3097 (25.5824)	mem 4879MB
[2022-05-31 00:46:02 MetaFG_0] (main.py 265): INFO Train: [17/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2987 (0.3054)	loss 1.2600 (1.6323)	grad_norm 27.1540 (25.5704)	mem 4879MB
[2022-05-31 00:46:06 MetaFG_0] (main.py 265): INFO Train: [17/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.3005 (0.3054)	loss 1.6636 (1.6334)	grad_norm 36.7245 (25.5868)	mem 4879MB
[2022-05-31 00:46:09 MetaFG_0] (main.py 265): INFO Train: [17/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2922 (0.3054)	loss 1.7079 (1.6336)	grad_norm 29.6748 (25.6223)	mem 4879MB
[2022-05-31 00:46:12 MetaFG_0] (main.py 265): INFO Train: [17/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2924 (0.3054)	loss 1.8423 (1.6336)	grad_norm 29.8144 (25.6298)	mem 4879MB
[2022-05-31 00:46:15 MetaFG_0] (main.py 265): INFO Train: [17/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2924 (0.3054)	loss 1.5343 (1.6346)	grad_norm 28.5994 (25.6342)	mem 4879MB
[2022-05-31 00:46:18 MetaFG_0] (main.py 265): INFO Train: [17/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2982 (0.3054)	loss 1.8058 (1.6348)	grad_norm 32.5509 (25.6173)	mem 4879MB
[2022-05-31 00:46:21 MetaFG_0] (main.py 265): INFO Train: [17/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2991 (0.3054)	loss 1.7229 (1.6344)	grad_norm 27.2392 (25.6589)	mem 4879MB
[2022-05-31 00:46:24 MetaFG_0] (main.py 265): INFO Train: [17/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2946 (0.3054)	loss 2.0867 (1.6343)	grad_norm 37.9093 (25.6624)	mem 4879MB
[2022-05-31 00:46:27 MetaFG_0] (main.py 265): INFO Train: [17/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.3009 (0.3054)	loss 1.8275 (1.6343)	grad_norm 27.7849 (25.6825)	mem 4879MB
[2022-05-31 00:46:30 MetaFG_0] (main.py 265): INFO Train: [17/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2928 (0.3053)	loss 1.5302 (1.6341)	grad_norm 17.0857 (25.6925)	mem 4879MB
[2022-05-31 00:46:33 MetaFG_0] (main.py 265): INFO Train: [17/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2988 (0.3053)	loss 1.2714 (1.6337)	grad_norm 19.6869 (25.6624)	mem 4879MB
[2022-05-31 00:46:36 MetaFG_0] (main.py 265): INFO Train: [17/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2917 (0.3053)	loss 1.4024 (1.6328)	grad_norm 37.1756 (25.6608)	mem 4879MB
[2022-05-31 00:46:39 MetaFG_0] (main.py 265): INFO Train: [17/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2940 (0.3053)	loss 1.7083 (1.6333)	grad_norm 19.4983 (25.6544)	mem 4879MB
[2022-05-31 00:46:42 MetaFG_0] (main.py 265): INFO Train: [17/300][930/1562]	eta 0:03:12 lr 0.000005	time 0.2926 (0.3053)	loss 1.1315 (1.6335)	grad_norm 17.0271 (25.6621)	mem 4879MB
[2022-05-31 00:46:45 MetaFG_0] (main.py 265): INFO Train: [17/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2933 (0.3053)	loss 1.9187 (1.6336)	grad_norm 20.0156 (25.6357)	mem 4879MB
[2022-05-31 00:46:48 MetaFG_0] (main.py 265): INFO Train: [17/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2924 (0.3053)	loss 1.4345 (1.6326)	grad_norm 30.6451 (25.6001)	mem 4879MB
[2022-05-31 00:46:51 MetaFG_0] (main.py 265): INFO Train: [17/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2920 (0.3053)	loss 1.4532 (1.6325)	grad_norm 25.9866 (25.6063)	mem 4879MB
[2022-05-31 00:46:54 MetaFG_0] (main.py 265): INFO Train: [17/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2998 (0.3053)	loss 1.8451 (1.6324)	grad_norm 19.1258 (25.6547)	mem 4879MB
[2022-05-31 00:46:57 MetaFG_0] (main.py 265): INFO Train: [17/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2926 (0.3053)	loss 1.7004 (1.6328)	grad_norm 30.1912 (25.6426)	mem 4879MB
[2022-05-31 00:47:00 MetaFG_0] (main.py 265): INFO Train: [17/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2990 (0.3053)	loss 1.2632 (1.6330)	grad_norm 32.4445 (25.6438)	mem 4879MB
[2022-05-31 00:47:03 MetaFG_0] (main.py 265): INFO Train: [17/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3015 (0.3053)	loss 1.3403 (1.6329)	grad_norm 21.2594 (25.6490)	mem 4879MB
[2022-05-31 00:47:06 MetaFG_0] (main.py 265): INFO Train: [17/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3011 (0.3053)	loss 1.1441 (1.6326)	grad_norm 15.8783 (25.6406)	mem 4879MB
[2022-05-31 00:47:10 MetaFG_0] (main.py 265): INFO Train: [17/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3002 (0.3053)	loss 1.8900 (1.6322)	grad_norm 43.9565 (25.6521)	mem 4879MB
[2022-05-31 00:47:13 MetaFG_0] (main.py 265): INFO Train: [17/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2988 (0.3053)	loss 1.2871 (1.6316)	grad_norm 29.4716 (25.6496)	mem 4879MB
[2022-05-31 00:47:16 MetaFG_0] (main.py 265): INFO Train: [17/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2960 (0.3053)	loss 1.2048 (1.6315)	grad_norm 35.9680 (25.6604)	mem 4879MB
[2022-05-31 00:47:19 MetaFG_0] (main.py 265): INFO Train: [17/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2945 (0.3053)	loss 1.6004 (1.6315)	grad_norm 24.5950 (25.7194)	mem 4879MB
[2022-05-31 00:47:22 MetaFG_0] (main.py 265): INFO Train: [17/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.3002 (0.3053)	loss 1.6008 (1.6315)	grad_norm 25.5051 (25.7093)	mem 4879MB
[2022-05-31 00:47:25 MetaFG_0] (main.py 265): INFO Train: [17/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2919 (0.3053)	loss 1.7451 (1.6309)	grad_norm 12.6881 (25.6759)	mem 4879MB
[2022-05-31 00:47:28 MetaFG_0] (main.py 265): INFO Train: [17/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2993 (0.3053)	loss 1.7364 (1.6311)	grad_norm 52.4894 (25.6871)	mem 4879MB
[2022-05-31 00:47:31 MetaFG_0] (main.py 265): INFO Train: [17/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2942 (0.3053)	loss 1.8592 (1.6308)	grad_norm 30.4062 (25.6807)	mem 4879MB
[2022-05-31 00:47:34 MetaFG_0] (main.py 265): INFO Train: [17/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2951 (0.3053)	loss 1.8343 (1.6302)	grad_norm 27.0758 (25.7028)	mem 4879MB
[2022-05-31 00:47:37 MetaFG_0] (main.py 265): INFO Train: [17/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2922 (0.3053)	loss 1.4561 (1.6311)	grad_norm 32.6510 (25.7048)	mem 4879MB
[2022-05-31 00:47:40 MetaFG_0] (main.py 265): INFO Train: [17/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2935 (0.3053)	loss 1.2893 (1.6303)	grad_norm 27.8420 (25.6888)	mem 4879MB
[2022-05-31 00:47:43 MetaFG_0] (main.py 265): INFO Train: [17/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2925 (0.3053)	loss 1.2904 (1.6295)	grad_norm 22.8840 (25.6919)	mem 4879MB
[2022-05-31 00:47:46 MetaFG_0] (main.py 265): INFO Train: [17/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2953 (0.3053)	loss 1.6758 (1.6297)	grad_norm 19.8305 (25.6688)	mem 4879MB
[2022-05-31 00:47:49 MetaFG_0] (main.py 265): INFO Train: [17/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2927 (0.3053)	loss 1.4478 (1.6297)	grad_norm 52.0146 (25.7185)	mem 4879MB
[2022-05-31 00:47:52 MetaFG_0] (main.py 265): INFO Train: [17/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2925 (0.3053)	loss 1.4884 (1.6288)	grad_norm 24.7779 (25.7468)	mem 4879MB
[2022-05-31 00:47:55 MetaFG_0] (main.py 265): INFO Train: [17/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2937 (0.3053)	loss 1.6541 (1.6286)	grad_norm 20.7294 (25.7407)	mem 4879MB
[2022-05-31 00:47:58 MetaFG_0] (main.py 265): INFO Train: [17/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2923 (0.3053)	loss 1.5282 (1.6289)	grad_norm 32.5883 (25.7373)	mem 4879MB
[2022-05-31 00:48:01 MetaFG_0] (main.py 265): INFO Train: [17/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2993 (0.3053)	loss 1.9061 (1.6287)	grad_norm 15.4137 (25.7342)	mem 4879MB
[2022-05-31 00:48:04 MetaFG_0] (main.py 265): INFO Train: [17/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2927 (0.3053)	loss 1.8153 (1.6281)	grad_norm 32.4267 (25.7466)	mem 4879MB
[2022-05-31 00:48:08 MetaFG_0] (main.py 265): INFO Train: [17/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2984 (0.3053)	loss 1.6136 (1.6277)	grad_norm 21.9056 (25.7152)	mem 4879MB
[2022-05-31 00:48:11 MetaFG_0] (main.py 265): INFO Train: [17/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2995 (0.3053)	loss 1.7571 (1.6278)	grad_norm 22.6254 (25.6920)	mem 4879MB
[2022-05-31 00:48:14 MetaFG_0] (main.py 265): INFO Train: [17/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2982 (0.3053)	loss 1.4818 (1.6273)	grad_norm 18.6369 (25.7030)	mem 4879MB
[2022-05-31 00:48:17 MetaFG_0] (main.py 265): INFO Train: [17/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2938 (0.3053)	loss 1.8183 (1.6264)	grad_norm 23.6758 (25.7002)	mem 4879MB
[2022-05-31 00:48:20 MetaFG_0] (main.py 265): INFO Train: [17/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2938 (0.3053)	loss 1.6110 (1.6270)	grad_norm 24.3700 (25.6713)	mem 4879MB
[2022-05-31 00:48:23 MetaFG_0] (main.py 265): INFO Train: [17/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2982 (0.3053)	loss 1.6269 (1.6272)	grad_norm 18.0558 (25.6707)	mem 4879MB
[2022-05-31 00:48:26 MetaFG_0] (main.py 265): INFO Train: [17/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2995 (0.3053)	loss 1.5667 (1.6270)	grad_norm 33.1613 (25.7048)	mem 4879MB
[2022-05-31 00:48:29 MetaFG_0] (main.py 265): INFO Train: [17/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2923 (0.3053)	loss 1.6425 (1.6269)	grad_norm 35.0465 (25.7247)	mem 4879MB
[2022-05-31 00:48:32 MetaFG_0] (main.py 265): INFO Train: [17/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2986 (0.3053)	loss 1.7932 (1.6269)	grad_norm 20.7058 (25.6973)	mem 4879MB
[2022-05-31 00:48:35 MetaFG_0] (main.py 265): INFO Train: [17/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2927 (0.3053)	loss 1.4224 (1.6273)	grad_norm 17.7777 (25.6736)	mem 4879MB
[2022-05-31 00:48:38 MetaFG_0] (main.py 265): INFO Train: [17/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2923 (0.3053)	loss 1.8048 (1.6276)	grad_norm 22.9030 (25.6878)	mem 4879MB
[2022-05-31 00:48:41 MetaFG_0] (main.py 265): INFO Train: [17/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2996 (0.3053)	loss 1.5045 (1.6281)	grad_norm 31.5972 (25.7301)	mem 4879MB
[2022-05-31 00:48:44 MetaFG_0] (main.py 265): INFO Train: [17/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2959 (0.3053)	loss 1.3691 (1.6279)	grad_norm 31.0071 (25.7806)	mem 4879MB
[2022-05-31 00:48:47 MetaFG_0] (main.py 265): INFO Train: [17/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2931 (0.3053)	loss 1.8184 (1.6276)	grad_norm 18.0437 (25.7957)	mem 4879MB
[2022-05-31 00:48:50 MetaFG_0] (main.py 265): INFO Train: [17/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2985 (0.3053)	loss 1.5979 (1.6269)	grad_norm 29.4216 (25.7746)	mem 4879MB
[2022-05-31 00:48:53 MetaFG_0] (main.py 265): INFO Train: [17/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2978 (0.3053)	loss 1.7330 (1.6264)	grad_norm 21.4187 (25.7778)	mem 4879MB
[2022-05-31 00:48:56 MetaFG_0] (main.py 265): INFO Train: [17/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2988 (0.3053)	loss 1.8417 (1.6267)	grad_norm 22.2227 (25.7748)	mem 4879MB
[2022-05-31 00:48:59 MetaFG_0] (main.py 265): INFO Train: [17/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2942 (0.3053)	loss 1.4820 (1.6261)	grad_norm 21.1989 (25.7427)	mem 4879MB
[2022-05-31 00:49:02 MetaFG_0] (main.py 265): INFO Train: [17/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2922 (0.3053)	loss 1.5830 (1.6257)	grad_norm 17.6707 (25.7349)	mem 4879MB
[2022-05-31 00:49:05 MetaFG_0] (main.py 265): INFO Train: [17/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.3012 (0.3053)	loss 1.5680 (1.6251)	grad_norm 21.6537 (25.7417)	mem 4879MB
[2022-05-31 00:49:09 MetaFG_0] (main.py 265): INFO Train: [17/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2928 (0.3053)	loss 1.7654 (1.6253)	grad_norm 24.7016 (25.7223)	mem 4879MB
[2022-05-31 00:49:12 MetaFG_0] (main.py 265): INFO Train: [17/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3008 (0.3053)	loss 1.4538 (1.6254)	grad_norm 24.4086 (25.6923)	mem 4879MB
[2022-05-31 00:49:15 MetaFG_0] (main.py 265): INFO Train: [17/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2994 (0.3053)	loss 1.6252 (1.6246)	grad_norm 21.7622 (25.6810)	mem 4879MB
[2022-05-31 00:49:18 MetaFG_0] (main.py 265): INFO Train: [17/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2989 (0.3052)	loss 1.6850 (1.6245)	grad_norm 27.2472 (25.6637)	mem 4879MB
[2022-05-31 00:49:21 MetaFG_0] (main.py 265): INFO Train: [17/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2984 (0.3052)	loss 1.5732 (1.6242)	grad_norm 21.7984 (25.6561)	mem 4879MB
[2022-05-31 00:49:24 MetaFG_0] (main.py 265): INFO Train: [17/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2955 (0.3052)	loss 1.7839 (1.6240)	grad_norm 26.5041 (25.6842)	mem 4879MB
[2022-05-31 00:49:27 MetaFG_0] (main.py 265): INFO Train: [17/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2972 (0.3052)	loss 1.6894 (1.6237)	grad_norm 20.5316 (25.7160)	mem 4879MB
[2022-05-31 00:49:30 MetaFG_0] (main.py 265): INFO Train: [17/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2993 (0.3052)	loss 1.7095 (1.6236)	grad_norm 23.9371 (25.7286)	mem 4879MB
[2022-05-31 00:49:33 MetaFG_0] (main.py 265): INFO Train: [17/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3000 (0.3052)	loss 1.2033 (1.6234)	grad_norm 38.8188 (25.7400)	mem 4879MB
[2022-05-31 00:49:36 MetaFG_0] (main.py 265): INFO Train: [17/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2924 (0.3052)	loss 1.5349 (1.6233)	grad_norm 20.4956 (25.7382)	mem 4879MB
[2022-05-31 00:49:39 MetaFG_0] (main.py 265): INFO Train: [17/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2922 (0.3052)	loss 1.6340 (1.6232)	grad_norm 23.5615 (25.7406)	mem 4879MB
[2022-05-31 00:49:42 MetaFG_0] (main.py 265): INFO Train: [17/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2935 (0.3052)	loss 1.7995 (1.6229)	grad_norm 23.1903 (25.7687)	mem 4879MB
[2022-05-31 00:49:45 MetaFG_0] (main.py 265): INFO Train: [17/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2929 (0.3052)	loss 1.6654 (1.6234)	grad_norm 16.4635 (25.7779)	mem 4879MB
[2022-05-31 00:49:48 MetaFG_0] (main.py 265): INFO Train: [17/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2972 (0.3052)	loss 1.2095 (1.6235)	grad_norm 28.9401 (25.7659)	mem 4879MB
[2022-05-31 00:49:51 MetaFG_0] (main.py 265): INFO Train: [17/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3000 (0.3052)	loss 1.6905 (1.6223)	grad_norm 27.8483 (25.7853)	mem 4879MB
[2022-05-31 00:49:54 MetaFG_0] (main.py 265): INFO Train: [17/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2922 (0.3051)	loss 1.3334 (1.6226)	grad_norm 32.8574 (25.7692)	mem 4879MB
[2022-05-31 00:49:55 MetaFG_0] (main.py 272): INFO EPOCH 17 training takes 0:07:56
[2022-05-31 00:49:55 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_17.pth saving......
[2022-05-31 00:49:55 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_17.pth saved !!!
[2022-05-31 00:49:55 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:49:57 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:49:57 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:49:57 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.536 (0.536)	Loss 1.0661 (1.0661)	Acc@1 84.375 (84.375)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 00:49:58 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.097 (0.140)	Loss 1.6044 (1.1845)	Acc@1 59.375 (74.716)	Acc@5 93.750 (96.591)	Mem 4879MB
[2022-05-31 00:49:59 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.118)	Loss 1.2300 (1.1932)	Acc@1 78.125 (75.298)	Acc@5 96.875 (96.577)	Mem 4879MB
[2022-05-31 00:50:00 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.097 (0.111)	Loss 1.3438 (1.2043)	Acc@1 62.500 (74.093)	Acc@5 96.875 (96.472)	Mem 4879MB
[2022-05-31 00:50:01 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.107)	Loss 1.5867 (1.2213)	Acc@1 50.000 (73.704)	Acc@5 93.750 (96.113)	Mem 4879MB
[2022-05-31 00:50:02 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.104)	Loss 1.2849 (1.2099)	Acc@1 62.500 (73.529)	Acc@5 100.000 (96.507)	Mem 4879MB
[2022-05-31 00:50:03 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.103)	Loss 1.2939 (1.2141)	Acc@1 84.375 (73.514)	Acc@5 96.875 (96.363)	Mem 4879MB
[2022-05-31 00:50:04 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.091 (0.102)	Loss 1.1951 (1.2231)	Acc@1 81.250 (73.636)	Acc@5 96.875 (96.391)	Mem 4879MB
[2022-05-31 00:50:05 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 1.2137 (1.2201)	Acc@1 75.000 (73.881)	Acc@5 100.000 (96.451)	Mem 4879MB
[2022-05-31 00:50:06 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.100)	Loss 1.3241 (1.2131)	Acc@1 68.750 (74.348)	Acc@5 90.625 (96.429)	Mem 4879MB
[2022-05-31 00:50:07 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.099)	Loss 1.3176 (1.2208)	Acc@1 59.375 (73.855)	Acc@5 96.875 (96.411)	Mem 4879MB
[2022-05-31 00:50:08 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.099)	Loss 1.1909 (1.2175)	Acc@1 75.000 (73.733)	Acc@5 100.000 (96.650)	Mem 4879MB
[2022-05-31 00:50:09 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.098)	Loss 1.2839 (1.2189)	Acc@1 62.500 (73.735)	Acc@5 96.875 (96.617)	Mem 4879MB
[2022-05-31 00:50:10 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.098)	Loss 1.4689 (1.2249)	Acc@1 65.625 (73.616)	Acc@5 96.875 (96.469)	Mem 4879MB
[2022-05-31 00:50:11 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.088 (0.098)	Loss 1.0710 (1.2280)	Acc@1 75.000 (73.604)	Acc@5 96.875 (96.565)	Mem 4879MB
[2022-05-31 00:50:12 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.088 (0.097)	Loss 0.8502 (1.2222)	Acc@1 84.375 (73.820)	Acc@5 100.000 (96.627)	Mem 4879MB
[2022-05-31 00:50:13 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.088 (0.097)	Loss 1.2786 (1.2207)	Acc@1 71.875 (74.068)	Acc@5 93.750 (96.526)	Mem 4879MB
[2022-05-31 00:50:13 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.097)	Loss 1.1614 (1.2215)	Acc@1 68.750 (73.977)	Acc@5 100.000 (96.583)	Mem 4879MB
[2022-05-31 00:50:14 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.090 (0.097)	Loss 1.1368 (1.2204)	Acc@1 81.250 (74.189)	Acc@5 93.750 (96.581)	Mem 4879MB
[2022-05-31 00:50:15 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 1.4828 (1.2256)	Acc@1 65.625 (74.051)	Acc@5 90.625 (96.466)	Mem 4879MB
[2022-05-31 00:50:16 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.097)	Loss 1.3750 (1.2247)	Acc@1 68.750 (74.238)	Acc@5 96.875 (96.409)	Mem 4879MB
[2022-05-31 00:50:17 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.096)	Loss 1.1705 (1.2215)	Acc@1 78.125 (74.526)	Acc@5 100.000 (96.445)	Mem 4879MB
[2022-05-31 00:50:18 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.096)	Loss 1.1290 (1.2192)	Acc@1 75.000 (74.576)	Acc@5 93.750 (96.408)	Mem 4879MB
[2022-05-31 00:50:19 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.096)	Loss 1.5026 (1.2222)	Acc@1 62.500 (74.445)	Acc@5 93.750 (96.388)	Mem 4879MB
[2022-05-31 00:50:20 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 1.2412 (1.2233)	Acc@1 71.875 (74.442)	Acc@5 96.875 (96.395)	Mem 4879MB
[2022-05-31 00:50:21 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.096)	Loss 1.1705 (1.2216)	Acc@1 84.375 (74.440)	Acc@5 93.750 (96.464)	Mem 4879MB
[2022-05-31 00:50:22 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 1.3263 (1.2210)	Acc@1 62.500 (74.461)	Acc@5 93.750 (96.468)	Mem 4879MB
[2022-05-31 00:50:23 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 1.2514 (1.2216)	Acc@1 68.750 (74.412)	Acc@5 100.000 (96.506)	Mem 4879MB
[2022-05-31 00:50:24 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 1.1722 (1.2217)	Acc@1 75.000 (74.399)	Acc@5 96.875 (96.419)	Mem 4879MB
[2022-05-31 00:50:25 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 1.4620 (1.2229)	Acc@1 65.625 (74.356)	Acc@5 96.875 (96.402)	Mem 4879MB
[2022-05-31 00:50:26 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 1.4739 (1.2220)	Acc@1 59.375 (74.387)	Acc@5 96.875 (96.418)	Mem 4879MB
[2022-05-31 00:50:27 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 1.3289 (1.2203)	Acc@1 71.875 (74.437)	Acc@5 96.875 (96.403)	Mem 4879MB
[2022-05-31 00:50:27 MetaFG_0] (main.py 330): INFO  * Acc@1 74.390 Acc@5 96.380
[2022-05-31 00:50:27 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 74.4%
[2022-05-31 00:50:27 MetaFG_0] (main.py 171): INFO Max accuracy: 74.39%
[2022-05-31 00:50:28 MetaFG_0] (main.py 265): INFO Train: [18/300][0/1562]	eta 0:28:11 lr 0.000006	time 1.0831 (1.0831)	loss 1.7643 (1.7643)	grad_norm 32.3724 (32.3724)	mem 4879MB
[2022-05-31 00:50:31 MetaFG_0] (main.py 265): INFO Train: [18/300][10/1562]	eta 0:09:44 lr 0.000006	time 0.2933 (0.3767)	loss 1.6212 (1.6438)	grad_norm 25.4610 (26.8708)	mem 4879MB
[2022-05-31 00:50:34 MetaFG_0] (main.py 265): INFO Train: [18/300][20/1562]	eta 0:08:49 lr 0.000006	time 0.2981 (0.3437)	loss 1.3652 (1.5568)	grad_norm 17.0203 (27.1923)	mem 4879MB
[2022-05-31 00:50:37 MetaFG_0] (main.py 265): INFO Train: [18/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.2923 (0.3308)	loss 1.5015 (1.5726)	grad_norm 20.5797 (24.8338)	mem 4879MB
[2022-05-31 00:50:40 MetaFG_0] (main.py 265): INFO Train: [18/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.2933 (0.3244)	loss 1.7458 (1.5963)	grad_norm 32.8567 (24.6986)	mem 4879MB
[2022-05-31 00:50:43 MetaFG_0] (main.py 265): INFO Train: [18/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2936 (0.3206)	loss 1.7135 (1.5974)	grad_norm 25.3782 (24.7116)	mem 4879MB
[2022-05-31 00:50:46 MetaFG_0] (main.py 265): INFO Train: [18/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2919 (0.3180)	loss 1.5573 (1.6014)	grad_norm 23.4643 (24.6880)	mem 4879MB
[2022-05-31 00:50:49 MetaFG_0] (main.py 265): INFO Train: [18/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.3001 (0.3162)	loss 1.4515 (1.5950)	grad_norm 32.8126 (25.5955)	mem 4879MB
[2022-05-31 00:50:52 MetaFG_0] (main.py 265): INFO Train: [18/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2920 (0.3149)	loss 1.9740 (1.5937)	grad_norm 23.8746 (25.7751)	mem 4879MB
[2022-05-31 00:50:55 MetaFG_0] (main.py 265): INFO Train: [18/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2944 (0.3137)	loss 1.3736 (1.5925)	grad_norm 22.4489 (25.4475)	mem 4879MB
[2022-05-31 00:50:58 MetaFG_0] (main.py 265): INFO Train: [18/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2920 (0.3130)	loss 1.6438 (1.5885)	grad_norm 28.7387 (25.1231)	mem 4879MB
[2022-05-31 00:51:01 MetaFG_0] (main.py 265): INFO Train: [18/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2988 (0.3121)	loss 1.7294 (1.5962)	grad_norm 18.7413 (25.1776)	mem 4879MB
[2022-05-31 00:51:05 MetaFG_0] (main.py 265): INFO Train: [18/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2987 (0.3115)	loss 1.8013 (1.5980)	grad_norm 25.6873 (25.2320)	mem 4879MB
[2022-05-31 00:51:08 MetaFG_0] (main.py 265): INFO Train: [18/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2926 (0.3110)	loss 1.4695 (1.5902)	grad_norm 24.6360 (25.2971)	mem 4879MB
[2022-05-31 00:51:11 MetaFG_0] (main.py 265): INFO Train: [18/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2938 (0.3104)	loss 1.6745 (1.5934)	grad_norm 21.2129 (25.1941)	mem 4879MB
[2022-05-31 00:51:14 MetaFG_0] (main.py 265): INFO Train: [18/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2932 (0.3100)	loss 1.8005 (1.6007)	grad_norm 16.5468 (25.1218)	mem 4879MB
[2022-05-31 00:51:17 MetaFG_0] (main.py 265): INFO Train: [18/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.3010 (0.3097)	loss 1.8600 (1.6033)	grad_norm 20.6884 (24.9805)	mem 4879MB
[2022-05-31 00:51:20 MetaFG_0] (main.py 265): INFO Train: [18/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2978 (0.3095)	loss 1.4158 (1.5971)	grad_norm 19.7913 (25.0439)	mem 4879MB
[2022-05-31 00:51:23 MetaFG_0] (main.py 265): INFO Train: [18/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2927 (0.3092)	loss 2.0491 (1.5951)	grad_norm inf (inf)	mem 4879MB
[2022-05-31 00:51:26 MetaFG_0] (main.py 265): INFO Train: [18/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.3001 (0.3090)	loss 1.5676 (1.5955)	grad_norm 12.3159 (nan)	mem 4879MB
[2022-05-31 00:51:29 MetaFG_0] (main.py 265): INFO Train: [18/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2926 (0.3087)	loss 1.2584 (1.5947)	grad_norm 33.0420 (nan)	mem 4879MB
[2022-05-31 00:51:32 MetaFG_0] (main.py 265): INFO Train: [18/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2995 (0.3085)	loss 1.5450 (1.5964)	grad_norm 35.0976 (nan)	mem 4879MB
[2022-05-31 00:51:35 MetaFG_0] (main.py 265): INFO Train: [18/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2986 (0.3082)	loss 1.7472 (1.5966)	grad_norm 45.9227 (nan)	mem 4879MB
[2022-05-31 00:51:38 MetaFG_0] (main.py 265): INFO Train: [18/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2934 (0.3082)	loss 1.5193 (1.5940)	grad_norm 18.7505 (nan)	mem 4879MB
[2022-05-31 00:51:41 MetaFG_0] (main.py 265): INFO Train: [18/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2926 (0.3080)	loss 1.7729 (1.5929)	grad_norm 28.6384 (nan)	mem 4879MB
[2022-05-31 00:51:44 MetaFG_0] (main.py 265): INFO Train: [18/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2983 (0.3080)	loss 1.8757 (1.5964)	grad_norm 26.0454 (nan)	mem 4879MB
[2022-05-31 00:51:47 MetaFG_0] (main.py 265): INFO Train: [18/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.3009 (0.3079)	loss 1.7535 (1.5968)	grad_norm 20.3711 (nan)	mem 4879MB
[2022-05-31 00:51:50 MetaFG_0] (main.py 265): INFO Train: [18/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2931 (0.3078)	loss 1.7379 (1.5941)	grad_norm 27.3514 (nan)	mem 4879MB
[2022-05-31 00:51:53 MetaFG_0] (main.py 265): INFO Train: [18/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2924 (0.3077)	loss 1.5384 (1.5933)	grad_norm 27.2060 (nan)	mem 4879MB
[2022-05-31 00:51:56 MetaFG_0] (main.py 265): INFO Train: [18/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2924 (0.3076)	loss 1.5078 (1.5942)	grad_norm 18.6992 (nan)	mem 4879MB
[2022-05-31 00:51:59 MetaFG_0] (main.py 265): INFO Train: [18/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2996 (0.3075)	loss 1.7261 (1.5947)	grad_norm 23.8855 (nan)	mem 4879MB
[2022-05-31 00:52:02 MetaFG_0] (main.py 265): INFO Train: [18/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2926 (0.3074)	loss 1.5310 (1.5925)	grad_norm 14.3946 (nan)	mem 4879MB
[2022-05-31 00:52:05 MetaFG_0] (main.py 265): INFO Train: [18/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2980 (0.3073)	loss 1.5974 (1.5908)	grad_norm 31.8706 (nan)	mem 4879MB
[2022-05-31 00:52:08 MetaFG_0] (main.py 265): INFO Train: [18/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2982 (0.3072)	loss 1.7483 (1.5907)	grad_norm 28.3278 (nan)	mem 4879MB
[2022-05-31 00:52:12 MetaFG_0] (main.py 265): INFO Train: [18/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2986 (0.3071)	loss 1.6983 (1.5898)	grad_norm 27.7521 (nan)	mem 4879MB
[2022-05-31 00:52:15 MetaFG_0] (main.py 265): INFO Train: [18/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2988 (0.3070)	loss 1.7141 (1.5906)	grad_norm 19.8866 (nan)	mem 4879MB
[2022-05-31 00:52:18 MetaFG_0] (main.py 265): INFO Train: [18/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2983 (0.3069)	loss 1.6353 (1.5902)	grad_norm 23.2718 (nan)	mem 4879MB
[2022-05-31 00:52:21 MetaFG_0] (main.py 265): INFO Train: [18/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2920 (0.3068)	loss 2.0585 (1.5907)	grad_norm 24.4370 (nan)	mem 4879MB
[2022-05-31 00:52:24 MetaFG_0] (main.py 265): INFO Train: [18/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2988 (0.3068)	loss 1.6600 (1.5905)	grad_norm 33.8983 (nan)	mem 4879MB
[2022-05-31 00:52:27 MetaFG_0] (main.py 265): INFO Train: [18/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2923 (0.3067)	loss 1.3750 (1.5909)	grad_norm 21.6757 (nan)	mem 4879MB
[2022-05-31 00:52:30 MetaFG_0] (main.py 265): INFO Train: [18/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2938 (0.3067)	loss 1.5823 (1.5920)	grad_norm 24.7604 (nan)	mem 4879MB
[2022-05-31 00:52:33 MetaFG_0] (main.py 265): INFO Train: [18/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2997 (0.3067)	loss 1.5476 (1.5906)	grad_norm 23.5794 (nan)	mem 4879MB
[2022-05-31 00:52:36 MetaFG_0] (main.py 265): INFO Train: [18/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2990 (0.3066)	loss 1.7109 (1.5915)	grad_norm 19.7568 (nan)	mem 4879MB
[2022-05-31 00:52:39 MetaFG_0] (main.py 265): INFO Train: [18/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2993 (0.3066)	loss 1.6768 (1.5936)	grad_norm 19.5883 (nan)	mem 4879MB
[2022-05-31 00:52:42 MetaFG_0] (main.py 265): INFO Train: [18/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2922 (0.3066)	loss 1.5849 (1.5933)	grad_norm 36.2698 (nan)	mem 4879MB
[2022-05-31 00:52:45 MetaFG_0] (main.py 265): INFO Train: [18/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2927 (0.3065)	loss 1.7491 (1.5932)	grad_norm 32.2396 (nan)	mem 4879MB
[2022-05-31 00:52:48 MetaFG_0] (main.py 265): INFO Train: [18/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2921 (0.3065)	loss 1.7587 (1.5942)	grad_norm 26.5688 (nan)	mem 4879MB
[2022-05-31 00:52:51 MetaFG_0] (main.py 265): INFO Train: [18/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2921 (0.3064)	loss 1.7292 (1.5945)	grad_norm 17.9798 (nan)	mem 4879MB
[2022-05-31 00:52:54 MetaFG_0] (main.py 265): INFO Train: [18/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2926 (0.3064)	loss 1.8313 (1.5954)	grad_norm 32.5441 (nan)	mem 4879MB
[2022-05-31 00:52:57 MetaFG_0] (main.py 265): INFO Train: [18/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2945 (0.3063)	loss 1.6424 (1.5951)	grad_norm 24.0584 (nan)	mem 4879MB
[2022-05-31 00:53:00 MetaFG_0] (main.py 265): INFO Train: [18/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2926 (0.3063)	loss 1.4400 (1.5969)	grad_norm 30.4570 (nan)	mem 4879MB
[2022-05-31 00:53:03 MetaFG_0] (main.py 265): INFO Train: [18/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2938 (0.3062)	loss 1.5118 (1.5977)	grad_norm 20.2327 (nan)	mem 4879MB
[2022-05-31 00:53:06 MetaFG_0] (main.py 265): INFO Train: [18/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2990 (0.3062)	loss 1.4475 (1.5982)	grad_norm 34.3141 (nan)	mem 4879MB
[2022-05-31 00:53:09 MetaFG_0] (main.py 265): INFO Train: [18/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.3009 (0.3061)	loss 1.5386 (1.5985)	grad_norm 55.4395 (nan)	mem 4879MB
[2022-05-31 00:53:12 MetaFG_0] (main.py 265): INFO Train: [18/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2930 (0.3061)	loss 1.6374 (1.5998)	grad_norm 27.7127 (nan)	mem 4879MB
[2022-05-31 00:53:15 MetaFG_0] (main.py 265): INFO Train: [18/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2936 (0.3060)	loss 1.6813 (1.5996)	grad_norm 21.2392 (nan)	mem 4879MB
[2022-05-31 00:53:18 MetaFG_0] (main.py 265): INFO Train: [18/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.3015 (0.3060)	loss 1.6715 (1.5990)	grad_norm 20.7895 (nan)	mem 4879MB
[2022-05-31 00:53:22 MetaFG_0] (main.py 265): INFO Train: [18/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2936 (0.3060)	loss 1.6677 (1.5993)	grad_norm 27.2584 (nan)	mem 4879MB
[2022-05-31 00:53:25 MetaFG_0] (main.py 265): INFO Train: [18/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2984 (0.3059)	loss 1.6968 (1.6008)	grad_norm 20.8792 (nan)	mem 4879MB
[2022-05-31 00:53:28 MetaFG_0] (main.py 265): INFO Train: [18/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2982 (0.3059)	loss 1.1251 (1.6012)	grad_norm 23.5026 (nan)	mem 4879MB
[2022-05-31 00:53:31 MetaFG_0] (main.py 265): INFO Train: [18/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2965 (0.3059)	loss 1.6791 (1.6014)	grad_norm 25.4410 (nan)	mem 4879MB
[2022-05-31 00:53:34 MetaFG_0] (main.py 265): INFO Train: [18/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2985 (0.3059)	loss 1.6268 (1.6028)	grad_norm 33.0468 (nan)	mem 4879MB
[2022-05-31 00:53:37 MetaFG_0] (main.py 265): INFO Train: [18/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3008 (0.3059)	loss 2.0413 (1.6030)	grad_norm 14.4399 (nan)	mem 4879MB
[2022-05-31 00:53:40 MetaFG_0] (main.py 265): INFO Train: [18/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3011 (0.3059)	loss 1.5628 (1.6028)	grad_norm 19.2142 (nan)	mem 4879MB
[2022-05-31 00:53:43 MetaFG_0] (main.py 265): INFO Train: [18/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2923 (0.3059)	loss 1.6997 (1.6025)	grad_norm 16.6623 (nan)	mem 4879MB
[2022-05-31 00:53:46 MetaFG_0] (main.py 265): INFO Train: [18/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2984 (0.3059)	loss 1.7144 (1.6008)	grad_norm 15.4624 (nan)	mem 4879MB
[2022-05-31 00:53:49 MetaFG_0] (main.py 265): INFO Train: [18/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2939 (0.3058)	loss 1.3985 (1.6000)	grad_norm 21.7350 (nan)	mem 4879MB
[2022-05-31 00:53:52 MetaFG_0] (main.py 265): INFO Train: [18/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2993 (0.3058)	loss 1.7806 (1.5992)	grad_norm 22.4423 (nan)	mem 4879MB
[2022-05-31 00:53:55 MetaFG_0] (main.py 265): INFO Train: [18/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2984 (0.3058)	loss 1.5228 (1.5988)	grad_norm 41.3918 (nan)	mem 4879MB
[2022-05-31 00:53:58 MetaFG_0] (main.py 265): INFO Train: [18/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2999 (0.3058)	loss 1.7653 (1.5998)	grad_norm 23.0668 (nan)	mem 4879MB
[2022-05-31 00:54:01 MetaFG_0] (main.py 265): INFO Train: [18/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2921 (0.3058)	loss 1.7341 (1.6006)	grad_norm 21.8599 (nan)	mem 4879MB
[2022-05-31 00:54:04 MetaFG_0] (main.py 265): INFO Train: [18/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2981 (0.3058)	loss 1.9303 (1.6004)	grad_norm 19.5500 (nan)	mem 4879MB
[2022-05-31 00:54:07 MetaFG_0] (main.py 265): INFO Train: [18/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2996 (0.3058)	loss 1.5477 (1.6001)	grad_norm 43.6711 (nan)	mem 4879MB
[2022-05-31 00:54:10 MetaFG_0] (main.py 265): INFO Train: [18/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2924 (0.3058)	loss 1.7340 (1.5999)	grad_norm 61.1612 (nan)	mem 4879MB
[2022-05-31 00:54:13 MetaFG_0] (main.py 265): INFO Train: [18/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2932 (0.3058)	loss 1.8865 (1.5999)	grad_norm 39.2530 (nan)	mem 4879MB
[2022-05-31 00:54:16 MetaFG_0] (main.py 265): INFO Train: [18/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2920 (0.3058)	loss 1.8131 (1.6013)	grad_norm 20.7868 (nan)	mem 4879MB
[2022-05-31 00:54:19 MetaFG_0] (main.py 265): INFO Train: [18/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.3018 (0.3057)	loss 1.9834 (1.6020)	grad_norm 28.3828 (nan)	mem 4879MB
[2022-05-31 00:54:23 MetaFG_0] (main.py 265): INFO Train: [18/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2965 (0.3057)	loss 1.0883 (1.6015)	grad_norm 21.5170 (nan)	mem 4879MB
[2022-05-31 00:54:26 MetaFG_0] (main.py 265): INFO Train: [18/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2992 (0.3057)	loss 1.4563 (1.6004)	grad_norm 12.1236 (nan)	mem 4879MB
[2022-05-31 00:54:29 MetaFG_0] (main.py 265): INFO Train: [18/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2928 (0.3058)	loss 1.7766 (1.5997)	grad_norm 35.7368 (nan)	mem 4879MB
[2022-05-31 00:54:32 MetaFG_0] (main.py 265): INFO Train: [18/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2917 (0.3058)	loss 1.4612 (1.5991)	grad_norm 35.6668 (nan)	mem 4879MB
[2022-05-31 00:54:35 MetaFG_0] (main.py 265): INFO Train: [18/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2999 (0.3058)	loss 1.6650 (1.5976)	grad_norm 24.2478 (nan)	mem 4879MB
[2022-05-31 00:54:38 MetaFG_0] (main.py 265): INFO Train: [18/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2931 (0.3057)	loss 1.1448 (1.5970)	grad_norm 14.8907 (nan)	mem 4879MB
[2022-05-31 00:54:41 MetaFG_0] (main.py 265): INFO Train: [18/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2988 (0.3057)	loss 1.4284 (1.5967)	grad_norm 19.6934 (nan)	mem 4879MB
[2022-05-31 00:54:44 MetaFG_0] (main.py 265): INFO Train: [18/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2987 (0.3057)	loss 1.7153 (1.5961)	grad_norm 24.0712 (nan)	mem 4879MB
[2022-05-31 00:54:47 MetaFG_0] (main.py 265): INFO Train: [18/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2951 (0.3057)	loss 1.5055 (1.5971)	grad_norm 39.2578 (nan)	mem 4879MB
[2022-05-31 00:54:50 MetaFG_0] (main.py 265): INFO Train: [18/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2980 (0.3057)	loss 1.9011 (1.5969)	grad_norm 12.4798 (nan)	mem 4879MB
[2022-05-31 00:54:53 MetaFG_0] (main.py 265): INFO Train: [18/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2977 (0.3057)	loss 1.3504 (1.5972)	grad_norm 29.2387 (nan)	mem 4879MB
[2022-05-31 00:54:56 MetaFG_0] (main.py 265): INFO Train: [18/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2962 (0.3057)	loss 1.1243 (1.5970)	grad_norm 42.5676 (nan)	mem 4879MB
[2022-05-31 00:54:59 MetaFG_0] (main.py 265): INFO Train: [18/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2941 (0.3057)	loss 1.6316 (1.5968)	grad_norm 22.3923 (nan)	mem 4879MB
[2022-05-31 00:55:02 MetaFG_0] (main.py 265): INFO Train: [18/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2925 (0.3057)	loss 1.6046 (1.5961)	grad_norm 19.6390 (nan)	mem 4879MB
[2022-05-31 00:55:05 MetaFG_0] (main.py 265): INFO Train: [18/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3003 (0.3057)	loss 1.7270 (1.5965)	grad_norm 24.9411 (nan)	mem 4879MB
[2022-05-31 00:55:08 MetaFG_0] (main.py 265): INFO Train: [18/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2923 (0.3056)	loss 1.4579 (1.5968)	grad_norm 21.0533 (nan)	mem 4879MB
[2022-05-31 00:55:11 MetaFG_0] (main.py 265): INFO Train: [18/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2928 (0.3056)	loss 1.5697 (1.5964)	grad_norm 22.4675 (nan)	mem 4879MB
[2022-05-31 00:55:14 MetaFG_0] (main.py 265): INFO Train: [18/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2995 (0.3056)	loss 1.2683 (1.5965)	grad_norm 40.1672 (nan)	mem 4879MB
[2022-05-31 00:55:17 MetaFG_0] (main.py 265): INFO Train: [18/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2996 (0.3056)	loss 1.1941 (1.5973)	grad_norm 37.2830 (nan)	mem 4879MB
[2022-05-31 00:55:20 MetaFG_0] (main.py 265): INFO Train: [18/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2915 (0.3056)	loss 1.5857 (1.5973)	grad_norm 23.5731 (nan)	mem 4879MB
[2022-05-31 00:55:24 MetaFG_0] (main.py 265): INFO Train: [18/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2939 (0.3055)	loss 1.7888 (1.5980)	grad_norm 30.5415 (nan)	mem 4879MB
[2022-05-31 00:55:27 MetaFG_0] (main.py 265): INFO Train: [18/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2932 (0.3055)	loss 1.4903 (1.5973)	grad_norm 42.0074 (nan)	mem 4879MB
[2022-05-31 00:55:30 MetaFG_0] (main.py 265): INFO Train: [18/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2925 (0.3055)	loss 1.2416 (1.5968)	grad_norm 34.3102 (nan)	mem 4879MB
[2022-05-31 00:55:33 MetaFG_0] (main.py 265): INFO Train: [18/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2958 (0.3055)	loss 1.2719 (1.5963)	grad_norm 32.1777 (nan)	mem 4879MB
[2022-05-31 00:55:36 MetaFG_0] (main.py 265): INFO Train: [18/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2933 (0.3055)	loss 1.4105 (1.5952)	grad_norm 18.0416 (nan)	mem 4879MB
[2022-05-31 00:55:39 MetaFG_0] (main.py 265): INFO Train: [18/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2988 (0.3055)	loss 1.5292 (1.5952)	grad_norm 26.6706 (nan)	mem 4879MB
[2022-05-31 00:55:42 MetaFG_0] (main.py 265): INFO Train: [18/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2983 (0.3055)	loss 1.5941 (1.5953)	grad_norm 13.2613 (nan)	mem 4879MB
[2022-05-31 00:55:45 MetaFG_0] (main.py 265): INFO Train: [18/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2934 (0.3055)	loss 1.7103 (1.5956)	grad_norm 40.0620 (nan)	mem 4879MB
[2022-05-31 00:55:48 MetaFG_0] (main.py 265): INFO Train: [18/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2982 (0.3055)	loss 1.6341 (1.5953)	grad_norm 33.8498 (nan)	mem 4879MB
[2022-05-31 00:55:51 MetaFG_0] (main.py 265): INFO Train: [18/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2936 (0.3055)	loss 1.6238 (1.5950)	grad_norm 26.9330 (nan)	mem 4879MB
[2022-05-31 00:55:54 MetaFG_0] (main.py 265): INFO Train: [18/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2917 (0.3055)	loss 1.6810 (1.5948)	grad_norm 24.4619 (nan)	mem 4879MB
[2022-05-31 00:55:57 MetaFG_0] (main.py 265): INFO Train: [18/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2948 (0.3055)	loss 1.8608 (1.5944)	grad_norm 19.6953 (nan)	mem 4879MB
[2022-05-31 00:56:00 MetaFG_0] (main.py 265): INFO Train: [18/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2928 (0.3055)	loss 1.6813 (1.5948)	grad_norm 14.3238 (nan)	mem 4879MB
[2022-05-31 00:56:03 MetaFG_0] (main.py 265): INFO Train: [18/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2984 (0.3055)	loss 1.6892 (1.5943)	grad_norm 34.1466 (nan)	mem 4879MB
[2022-05-31 00:56:06 MetaFG_0] (main.py 265): INFO Train: [18/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2997 (0.3055)	loss 1.6897 (1.5937)	grad_norm 42.1395 (nan)	mem 4879MB
[2022-05-31 00:56:09 MetaFG_0] (main.py 265): INFO Train: [18/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2979 (0.3055)	loss 1.6917 (1.5939)	grad_norm 17.8059 (nan)	mem 4879MB
[2022-05-31 00:56:12 MetaFG_0] (main.py 265): INFO Train: [18/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2979 (0.3055)	loss 1.7727 (1.5932)	grad_norm 42.3024 (nan)	mem 4879MB
[2022-05-31 00:56:15 MetaFG_0] (main.py 265): INFO Train: [18/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2986 (0.3054)	loss 1.6000 (1.5937)	grad_norm 18.9004 (nan)	mem 4879MB
[2022-05-31 00:56:18 MetaFG_0] (main.py 265): INFO Train: [18/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3054)	loss 1.6235 (1.5926)	grad_norm 17.5628 (nan)	mem 4879MB
[2022-05-31 00:56:21 MetaFG_0] (main.py 265): INFO Train: [18/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2991 (0.3054)	loss 1.8491 (1.5934)	grad_norm 23.5248 (nan)	mem 4879MB
[2022-05-31 00:56:24 MetaFG_0] (main.py 265): INFO Train: [18/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2948 (0.3054)	loss 1.6404 (1.5939)	grad_norm 23.7098 (nan)	mem 4879MB
[2022-05-31 00:56:28 MetaFG_0] (main.py 265): INFO Train: [18/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2982 (0.3054)	loss 1.7551 (1.5946)	grad_norm 25.4388 (nan)	mem 4879MB
[2022-05-31 00:56:31 MetaFG_0] (main.py 265): INFO Train: [18/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2997 (0.3054)	loss 1.6119 (1.5944)	grad_norm 23.6277 (nan)	mem 4879MB
[2022-05-31 00:56:34 MetaFG_0] (main.py 265): INFO Train: [18/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2980 (0.3054)	loss 1.3774 (1.5946)	grad_norm 21.2500 (nan)	mem 4879MB
[2022-05-31 00:56:37 MetaFG_0] (main.py 265): INFO Train: [18/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2991 (0.3054)	loss 1.8365 (1.5943)	grad_norm 19.6303 (nan)	mem 4879MB
[2022-05-31 00:56:40 MetaFG_0] (main.py 265): INFO Train: [18/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2989 (0.3054)	loss 1.8272 (1.5948)	grad_norm 24.2034 (nan)	mem 4879MB
[2022-05-31 00:56:43 MetaFG_0] (main.py 265): INFO Train: [18/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2918 (0.3054)	loss 1.7445 (1.5956)	grad_norm 25.0405 (nan)	mem 4879MB
[2022-05-31 00:56:46 MetaFG_0] (main.py 265): INFO Train: [18/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2929 (0.3054)	loss 1.5549 (1.5952)	grad_norm 26.6748 (nan)	mem 4879MB
[2022-05-31 00:56:49 MetaFG_0] (main.py 265): INFO Train: [18/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2977 (0.3054)	loss 1.7241 (1.5952)	grad_norm 14.6848 (nan)	mem 4879MB
[2022-05-31 00:56:52 MetaFG_0] (main.py 265): INFO Train: [18/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2983 (0.3054)	loss 1.3866 (1.5951)	grad_norm 48.9444 (nan)	mem 4879MB
[2022-05-31 00:56:55 MetaFG_0] (main.py 265): INFO Train: [18/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2936 (0.3054)	loss 1.4636 (1.5941)	grad_norm 18.0571 (nan)	mem 4879MB
[2022-05-31 00:56:58 MetaFG_0] (main.py 265): INFO Train: [18/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2932 (0.3054)	loss 1.7652 (1.5949)	grad_norm 19.9022 (nan)	mem 4879MB
[2022-05-31 00:57:01 MetaFG_0] (main.py 265): INFO Train: [18/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2947 (0.3053)	loss 1.4338 (1.5944)	grad_norm 20.8150 (nan)	mem 4879MB
[2022-05-31 00:57:04 MetaFG_0] (main.py 265): INFO Train: [18/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2991 (0.3053)	loss 1.6875 (1.5950)	grad_norm 21.4764 (nan)	mem 4879MB
[2022-05-31 00:57:07 MetaFG_0] (main.py 265): INFO Train: [18/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2926 (0.3053)	loss 1.1350 (1.5950)	grad_norm 24.9457 (nan)	mem 4879MB
[2022-05-31 00:57:10 MetaFG_0] (main.py 265): INFO Train: [18/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2946 (0.3053)	loss 1.1218 (1.5947)	grad_norm 29.9796 (nan)	mem 4879MB
[2022-05-31 00:57:13 MetaFG_0] (main.py 265): INFO Train: [18/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2930 (0.3053)	loss 1.4484 (1.5945)	grad_norm 22.6721 (nan)	mem 4879MB
[2022-05-31 00:57:16 MetaFG_0] (main.py 265): INFO Train: [18/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2921 (0.3053)	loss 1.1711 (1.5940)	grad_norm 20.6177 (nan)	mem 4879MB
[2022-05-31 00:57:19 MetaFG_0] (main.py 265): INFO Train: [18/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2920 (0.3053)	loss 1.8386 (1.5944)	grad_norm 20.8158 (nan)	mem 4879MB
[2022-05-31 00:57:22 MetaFG_0] (main.py 265): INFO Train: [18/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2925 (0.3053)	loss 1.7623 (1.5945)	grad_norm 22.3526 (nan)	mem 4879MB
[2022-05-31 00:57:25 MetaFG_0] (main.py 265): INFO Train: [18/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.3066 (0.3053)	loss 1.0431 (1.5945)	grad_norm 26.9294 (nan)	mem 4879MB
[2022-05-31 00:57:28 MetaFG_0] (main.py 265): INFO Train: [18/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3016 (0.3053)	loss 1.6745 (1.5954)	grad_norm 26.2987 (nan)	mem 4879MB
[2022-05-31 00:57:31 MetaFG_0] (main.py 265): INFO Train: [18/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2941 (0.3053)	loss 1.6379 (1.5954)	grad_norm 21.0552 (nan)	mem 4879MB
[2022-05-31 00:57:35 MetaFG_0] (main.py 265): INFO Train: [18/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2990 (0.3053)	loss 1.1292 (1.5951)	grad_norm 25.2248 (nan)	mem 4879MB
[2022-05-31 00:57:38 MetaFG_0] (main.py 265): INFO Train: [18/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2939 (0.3053)	loss 1.4500 (1.5948)	grad_norm 33.9414 (nan)	mem 4879MB
[2022-05-31 00:57:41 MetaFG_0] (main.py 265): INFO Train: [18/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2985 (0.3053)	loss 1.7493 (1.5944)	grad_norm 23.6117 (nan)	mem 4879MB
[2022-05-31 00:57:44 MetaFG_0] (main.py 265): INFO Train: [18/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2938 (0.3052)	loss 1.2790 (1.5945)	grad_norm 18.8983 (nan)	mem 4879MB
[2022-05-31 00:57:47 MetaFG_0] (main.py 265): INFO Train: [18/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2920 (0.3052)	loss 1.8857 (1.5943)	grad_norm 15.5565 (nan)	mem 4879MB
[2022-05-31 00:57:50 MetaFG_0] (main.py 265): INFO Train: [18/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2934 (0.3052)	loss 1.6620 (1.5944)	grad_norm 28.9586 (nan)	mem 4879MB
[2022-05-31 00:57:53 MetaFG_0] (main.py 265): INFO Train: [18/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2926 (0.3052)	loss 1.7175 (1.5944)	grad_norm 23.3643 (nan)	mem 4879MB
[2022-05-31 00:57:56 MetaFG_0] (main.py 265): INFO Train: [18/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2921 (0.3052)	loss 1.6333 (1.5941)	grad_norm 23.3671 (nan)	mem 4879MB
[2022-05-31 00:57:59 MetaFG_0] (main.py 265): INFO Train: [18/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2992 (0.3052)	loss 1.4167 (1.5935)	grad_norm 19.4216 (nan)	mem 4879MB
[2022-05-31 00:58:02 MetaFG_0] (main.py 265): INFO Train: [18/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2978 (0.3052)	loss 1.7517 (1.5932)	grad_norm 16.9283 (nan)	mem 4879MB
[2022-05-31 00:58:05 MetaFG_0] (main.py 265): INFO Train: [18/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2932 (0.3052)	loss 1.5901 (1.5930)	grad_norm 29.3999 (nan)	mem 4879MB
[2022-05-31 00:58:08 MetaFG_0] (main.py 265): INFO Train: [18/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2946 (0.3052)	loss 1.7806 (1.5920)	grad_norm 25.8713 (nan)	mem 4879MB
[2022-05-31 00:58:11 MetaFG_0] (main.py 265): INFO Train: [18/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2977 (0.3052)	loss 1.7195 (1.5927)	grad_norm 30.7261 (nan)	mem 4879MB
[2022-05-31 00:58:14 MetaFG_0] (main.py 265): INFO Train: [18/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2925 (0.3052)	loss 1.8123 (1.5928)	grad_norm 18.4411 (nan)	mem 4879MB
[2022-05-31 00:58:17 MetaFG_0] (main.py 265): INFO Train: [18/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2935 (0.3052)	loss 1.3794 (1.5926)	grad_norm 19.4110 (nan)	mem 4879MB
[2022-05-31 00:58:20 MetaFG_0] (main.py 265): INFO Train: [18/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2937 (0.3052)	loss 1.6431 (1.5929)	grad_norm 28.0720 (nan)	mem 4879MB
[2022-05-31 00:58:23 MetaFG_0] (main.py 265): INFO Train: [18/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2928 (0.3051)	loss 1.4435 (1.5927)	grad_norm 20.2031 (nan)	mem 4879MB
[2022-05-31 00:58:24 MetaFG_0] (main.py 272): INFO EPOCH 18 training takes 0:07:56
[2022-05-31 00:58:24 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_18.pth saving......
[2022-05-31 00:58:24 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_18.pth saved !!!
[2022-05-31 00:58:24 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 00:58:26 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 00:58:26 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 00:58:27 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.600 (0.600)	Loss 0.9475 (0.9475)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 00:58:28 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.156)	Loss 1.0912 (1.0878)	Acc@1 81.250 (75.852)	Acc@5 96.875 (98.011)	Mem 4879MB
[2022-05-31 00:58:29 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.126)	Loss 0.8902 (1.0930)	Acc@1 84.375 (76.042)	Acc@5 96.875 (97.470)	Mem 4879MB
[2022-05-31 00:58:30 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.117)	Loss 1.3094 (1.1382)	Acc@1 65.625 (74.496)	Acc@5 100.000 (96.774)	Mem 4879MB
[2022-05-31 00:58:31 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.111)	Loss 1.3532 (1.1321)	Acc@1 62.500 (74.390)	Acc@5 96.875 (97.332)	Mem 4879MB
[2022-05-31 00:58:32 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.108)	Loss 0.8486 (1.1252)	Acc@1 90.625 (75.368)	Acc@5 96.875 (97.181)	Mem 4879MB
[2022-05-31 00:58:33 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.105)	Loss 0.9941 (1.1236)	Acc@1 84.375 (75.461)	Acc@5 100.000 (96.875)	Mem 4879MB
[2022-05-31 00:58:34 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.103)	Loss 1.1132 (1.1247)	Acc@1 68.750 (75.528)	Acc@5 100.000 (96.655)	Mem 4879MB
[2022-05-31 00:58:35 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 1.3476 (1.1334)	Acc@1 68.750 (75.231)	Acc@5 100.000 (96.682)	Mem 4879MB
[2022-05-31 00:58:35 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.9972 (1.1417)	Acc@1 75.000 (74.966)	Acc@5 100.000 (96.532)	Mem 4879MB
[2022-05-31 00:58:36 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.102 (0.101)	Loss 1.0519 (1.1477)	Acc@1 81.250 (74.814)	Acc@5 100.000 (96.473)	Mem 4879MB
[2022-05-31 00:58:37 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.100)	Loss 1.0151 (1.1412)	Acc@1 84.375 (75.338)	Acc@5 100.000 (96.622)	Mem 4879MB
[2022-05-31 00:58:38 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 1.2151 (1.1398)	Acc@1 68.750 (75.362)	Acc@5 96.875 (96.720)	Mem 4879MB
[2022-05-31 00:58:39 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 0.8832 (1.1423)	Acc@1 87.500 (75.453)	Acc@5 96.875 (96.732)	Mem 4879MB
[2022-05-31 00:58:40 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 1.2439 (1.1418)	Acc@1 62.500 (75.532)	Acc@5 100.000 (96.853)	Mem 4879MB
[2022-05-31 00:58:41 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 1.1263 (1.1377)	Acc@1 78.125 (75.704)	Acc@5 96.875 (96.854)	Mem 4879MB
[2022-05-31 00:58:42 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.093 (0.098)	Loss 1.0121 (1.1371)	Acc@1 87.500 (75.893)	Acc@5 96.875 (96.778)	Mem 4879MB
[2022-05-31 00:58:43 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.098)	Loss 1.0836 (1.1390)	Acc@1 71.875 (75.859)	Acc@5 96.875 (96.729)	Mem 4879MB
[2022-05-31 00:58:44 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.097)	Loss 1.0980 (1.1384)	Acc@1 71.875 (75.777)	Acc@5 96.875 (96.823)	Mem 4879MB
[2022-05-31 00:58:45 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.108 (0.097)	Loss 1.1263 (1.1385)	Acc@1 75.000 (75.916)	Acc@5 90.625 (96.777)	Mem 4879MB
[2022-05-31 00:58:46 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 1.2293 (1.1435)	Acc@1 78.125 (75.793)	Acc@5 100.000 (96.704)	Mem 4879MB
[2022-05-31 00:58:47 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.097)	Loss 1.0264 (1.1455)	Acc@1 81.250 (75.741)	Acc@5 100.000 (96.594)	Mem 4879MB
[2022-05-31 00:58:48 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 0.7895 (1.1421)	Acc@1 87.500 (75.933)	Acc@5 100.000 (96.649)	Mem 4879MB
[2022-05-31 00:58:49 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.095 (0.097)	Loss 1.2155 (1.1419)	Acc@1 71.875 (75.879)	Acc@5 100.000 (96.645)	Mem 4879MB
[2022-05-31 00:58:50 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 1.3866 (1.1418)	Acc@1 68.750 (75.882)	Acc@5 93.750 (96.655)	Mem 4879MB
[2022-05-31 00:58:50 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.096)	Loss 1.2015 (1.1391)	Acc@1 75.000 (75.959)	Acc@5 93.750 (96.676)	Mem 4879MB
[2022-05-31 00:58:51 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 1.0663 (1.1398)	Acc@1 71.875 (75.850)	Acc@5 100.000 (96.707)	Mem 4879MB
[2022-05-31 00:58:52 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.106 (0.096)	Loss 1.1759 (1.1388)	Acc@1 68.750 (75.888)	Acc@5 96.875 (96.679)	Mem 4879MB
[2022-05-31 00:58:53 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 1.2502 (1.1397)	Acc@1 75.000 (75.945)	Acc@5 100.000 (96.664)	Mem 4879MB
[2022-05-31 00:58:54 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 1.1635 (1.1415)	Acc@1 71.875 (75.891)	Acc@5 96.875 (96.692)	Mem 4879MB
[2022-05-31 00:58:55 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 1.0840 (1.1419)	Acc@1 84.375 (75.799)	Acc@5 96.875 (96.657)	Mem 4879MB
[2022-05-31 00:58:56 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 1.1747 (1.1418)	Acc@1 78.125 (75.804)	Acc@5 93.750 (96.634)	Mem 4879MB
[2022-05-31 00:58:56 MetaFG_0] (main.py 330): INFO  * Acc@1 75.800 Acc@5 96.630
[2022-05-31 00:58:56 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 75.8%
[2022-05-31 00:58:56 MetaFG_0] (main.py 171): INFO Max accuracy: 75.80%
[2022-05-31 00:58:57 MetaFG_0] (main.py 265): INFO Train: [19/300][0/1562]	eta 0:26:43 lr 0.000006	time 1.0266 (1.0266)	loss 1.4331 (1.4331)	grad_norm 20.0474 (20.0474)	mem 4879MB
[2022-05-31 00:59:00 MetaFG_0] (main.py 265): INFO Train: [19/300][10/1562]	eta 0:09:43 lr 0.000006	time 0.2938 (0.3760)	loss 1.1475 (1.6114)	grad_norm 17.3504 (24.4324)	mem 4879MB
[2022-05-31 00:59:03 MetaFG_0] (main.py 265): INFO Train: [19/300][20/1562]	eta 0:08:46 lr 0.000006	time 0.2926 (0.3416)	loss 1.4264 (1.5746)	grad_norm 44.7126 (25.4735)	mem 4879MB
[2022-05-31 00:59:06 MetaFG_0] (main.py 265): INFO Train: [19/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2986 (0.3292)	loss 1.5686 (1.5812)	grad_norm 20.1346 (25.7558)	mem 4879MB
[2022-05-31 00:59:10 MetaFG_0] (main.py 265): INFO Train: [19/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2941 (0.3234)	loss 1.6328 (1.5786)	grad_norm 32.0254 (26.5500)	mem 4879MB
[2022-05-31 00:59:13 MetaFG_0] (main.py 265): INFO Train: [19/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2986 (0.3198)	loss 1.3302 (1.5790)	grad_norm 28.9673 (26.2412)	mem 4879MB
[2022-05-31 00:59:16 MetaFG_0] (main.py 265): INFO Train: [19/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2920 (0.3176)	loss 1.2713 (1.5732)	grad_norm 52.3939 (26.6111)	mem 4879MB
[2022-05-31 00:59:19 MetaFG_0] (main.py 265): INFO Train: [19/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2987 (0.3162)	loss 1.7976 (1.5830)	grad_norm 16.4307 (25.9470)	mem 4879MB
[2022-05-31 00:59:22 MetaFG_0] (main.py 265): INFO Train: [19/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2932 (0.3147)	loss 1.6980 (1.5836)	grad_norm 11.5728 (25.5982)	mem 4879MB
[2022-05-31 00:59:25 MetaFG_0] (main.py 265): INFO Train: [19/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2947 (0.3135)	loss 1.6356 (1.5804)	grad_norm 39.4609 (25.5427)	mem 4879MB
[2022-05-31 00:59:28 MetaFG_0] (main.py 265): INFO Train: [19/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2948 (0.3125)	loss 1.3248 (1.5627)	grad_norm 19.7299 (25.4245)	mem 4879MB
[2022-05-31 00:59:31 MetaFG_0] (main.py 265): INFO Train: [19/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2934 (0.3117)	loss 1.5916 (1.5638)	grad_norm 30.6430 (25.4803)	mem 4879MB
[2022-05-31 00:59:34 MetaFG_0] (main.py 265): INFO Train: [19/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2997 (0.3112)	loss 1.7293 (1.5694)	grad_norm 26.6248 (25.4555)	mem 4879MB
[2022-05-31 00:59:37 MetaFG_0] (main.py 265): INFO Train: [19/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2937 (0.3107)	loss 1.7863 (1.5811)	grad_norm 34.1145 (25.5473)	mem 4879MB
[2022-05-31 00:59:40 MetaFG_0] (main.py 265): INFO Train: [19/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2922 (0.3102)	loss 1.6509 (1.5809)	grad_norm 18.6142 (25.5419)	mem 4879MB
[2022-05-31 00:59:43 MetaFG_0] (main.py 265): INFO Train: [19/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2926 (0.3098)	loss 1.4887 (1.5832)	grad_norm 35.8293 (25.7442)	mem 4879MB
[2022-05-31 00:59:46 MetaFG_0] (main.py 265): INFO Train: [19/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2925 (0.3095)	loss 1.7971 (1.5833)	grad_norm 27.2838 (25.6430)	mem 4879MB
[2022-05-31 00:59:49 MetaFG_0] (main.py 265): INFO Train: [19/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2923 (0.3091)	loss 1.6509 (1.5861)	grad_norm 22.6645 (25.8717)	mem 4879MB
[2022-05-31 00:59:52 MetaFG_0] (main.py 265): INFO Train: [19/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2919 (0.3088)	loss 1.6764 (1.5861)	grad_norm 25.7891 (25.8148)	mem 4879MB
[2022-05-31 00:59:55 MetaFG_0] (main.py 265): INFO Train: [19/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2919 (0.3085)	loss 1.3995 (1.5860)	grad_norm 26.6447 (25.8715)	mem 4879MB
[2022-05-31 00:59:58 MetaFG_0] (main.py 265): INFO Train: [19/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2921 (0.3083)	loss 1.8865 (1.5858)	grad_norm 41.8354 (25.9780)	mem 4879MB
[2022-05-31 01:00:01 MetaFG_0] (main.py 265): INFO Train: [19/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2985 (0.3080)	loss 1.5782 (1.5829)	grad_norm 26.1927 (25.9295)	mem 4879MB
[2022-05-31 01:00:04 MetaFG_0] (main.py 265): INFO Train: [19/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2964 (0.3079)	loss 1.7548 (1.5812)	grad_norm 30.0721 (25.9410)	mem 4879MB
[2022-05-31 01:00:07 MetaFG_0] (main.py 265): INFO Train: [19/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2982 (0.3077)	loss 1.3590 (1.5779)	grad_norm 14.4045 (25.8180)	mem 4879MB
[2022-05-31 01:00:10 MetaFG_0] (main.py 265): INFO Train: [19/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2988 (0.3075)	loss 1.2650 (1.5745)	grad_norm 33.9695 (26.1575)	mem 4879MB
[2022-05-31 01:00:13 MetaFG_0] (main.py 265): INFO Train: [19/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.3006 (0.3074)	loss 1.4134 (1.5759)	grad_norm 55.0865 (26.2741)	mem 4879MB
[2022-05-31 01:00:16 MetaFG_0] (main.py 265): INFO Train: [19/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2923 (0.3073)	loss 1.5903 (1.5768)	grad_norm 19.4546 (26.2130)	mem 4879MB
[2022-05-31 01:00:19 MetaFG_0] (main.py 265): INFO Train: [19/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2917 (0.3072)	loss 1.5791 (1.5789)	grad_norm 44.4500 (26.3706)	mem 4879MB
[2022-05-31 01:00:23 MetaFG_0] (main.py 265): INFO Train: [19/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2923 (0.3071)	loss 1.7666 (1.5820)	grad_norm 24.5541 (26.3282)	mem 4879MB
[2022-05-31 01:00:26 MetaFG_0] (main.py 265): INFO Train: [19/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2994 (0.3071)	loss 1.8218 (1.5850)	grad_norm 21.3189 (26.2383)	mem 4879MB
[2022-05-31 01:00:29 MetaFG_0] (main.py 265): INFO Train: [19/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2924 (0.3070)	loss 1.4796 (1.5845)	grad_norm 20.1586 (26.2092)	mem 4879MB
[2022-05-31 01:00:32 MetaFG_0] (main.py 265): INFO Train: [19/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.3000 (0.3069)	loss 1.7701 (1.5844)	grad_norm 30.6196 (26.1945)	mem 4879MB
[2022-05-31 01:00:35 MetaFG_0] (main.py 265): INFO Train: [19/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2932 (0.3069)	loss 1.8580 (1.5858)	grad_norm 31.2723 (26.2784)	mem 4879MB
[2022-05-31 01:00:38 MetaFG_0] (main.py 265): INFO Train: [19/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2921 (0.3067)	loss 1.7931 (1.5860)	grad_norm 42.3773 (26.2827)	mem 4879MB
[2022-05-31 01:00:41 MetaFG_0] (main.py 265): INFO Train: [19/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2982 (0.3066)	loss 1.1362 (1.5868)	grad_norm 28.5801 (26.6414)	mem 4879MB
[2022-05-31 01:00:44 MetaFG_0] (main.py 265): INFO Train: [19/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2934 (0.3065)	loss 1.4592 (1.5848)	grad_norm 29.2652 (26.6108)	mem 4879MB
[2022-05-31 01:00:47 MetaFG_0] (main.py 265): INFO Train: [19/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2923 (0.3065)	loss 1.8163 (1.5851)	grad_norm 39.1414 (26.6634)	mem 4879MB
[2022-05-31 01:00:50 MetaFG_0] (main.py 265): INFO Train: [19/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2929 (0.3064)	loss 1.3584 (1.5846)	grad_norm 33.9526 (26.6690)	mem 4879MB
[2022-05-31 01:00:53 MetaFG_0] (main.py 265): INFO Train: [19/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2985 (0.3064)	loss 1.8239 (1.5851)	grad_norm 20.7553 (26.6355)	mem 4879MB
[2022-05-31 01:00:56 MetaFG_0] (main.py 265): INFO Train: [19/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2921 (0.3064)	loss 1.7395 (1.5840)	grad_norm 24.9550 (26.5613)	mem 4879MB
[2022-05-31 01:00:59 MetaFG_0] (main.py 265): INFO Train: [19/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2931 (0.3063)	loss 1.5936 (1.5835)	grad_norm 41.2051 (26.5993)	mem 4879MB
[2022-05-31 01:01:02 MetaFG_0] (main.py 265): INFO Train: [19/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2990 (0.3062)	loss 1.7041 (1.5832)	grad_norm 19.7080 (26.6163)	mem 4879MB
[2022-05-31 01:01:05 MetaFG_0] (main.py 265): INFO Train: [19/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2922 (0.3061)	loss 1.8324 (1.5830)	grad_norm 28.4904 (26.6099)	mem 4879MB
[2022-05-31 01:01:08 MetaFG_0] (main.py 265): INFO Train: [19/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.3008 (0.3061)	loss 1.7571 (1.5829)	grad_norm 26.3822 (26.5644)	mem 4879MB
[2022-05-31 01:01:11 MetaFG_0] (main.py 265): INFO Train: [19/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2979 (0.3061)	loss 1.6143 (1.5844)	grad_norm 17.7465 (26.5272)	mem 4879MB
[2022-05-31 01:01:14 MetaFG_0] (main.py 265): INFO Train: [19/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.3018 (0.3060)	loss 1.6887 (1.5854)	grad_norm 14.0619 (26.5045)	mem 4879MB
[2022-05-31 01:01:17 MetaFG_0] (main.py 265): INFO Train: [19/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2948 (0.3060)	loss 1.5464 (1.5859)	grad_norm 24.4232 (26.4390)	mem 4879MB
[2022-05-31 01:01:20 MetaFG_0] (main.py 265): INFO Train: [19/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2972 (0.3059)	loss 1.6234 (1.5851)	grad_norm 20.0636 (26.4141)	mem 4879MB
[2022-05-31 01:01:23 MetaFG_0] (main.py 265): INFO Train: [19/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2948 (0.3059)	loss 1.8248 (1.5852)	grad_norm 22.6259 (26.3640)	mem 4879MB
[2022-05-31 01:01:26 MetaFG_0] (main.py 265): INFO Train: [19/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2924 (0.3058)	loss 1.5817 (1.5851)	grad_norm 21.3205 (26.3799)	mem 4879MB
[2022-05-31 01:01:29 MetaFG_0] (main.py 265): INFO Train: [19/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2920 (0.3058)	loss 1.7283 (1.5865)	grad_norm 36.8437 (26.3578)	mem 4879MB
[2022-05-31 01:01:32 MetaFG_0] (main.py 265): INFO Train: [19/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2943 (0.3058)	loss 1.8235 (1.5865)	grad_norm 26.6547 (26.3627)	mem 4879MB
[2022-05-31 01:01:36 MetaFG_0] (main.py 265): INFO Train: [19/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2937 (0.3057)	loss 1.5990 (1.5874)	grad_norm 32.6566 (26.4443)	mem 4879MB
[2022-05-31 01:01:39 MetaFG_0] (main.py 265): INFO Train: [19/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.3018 (0.3057)	loss 1.9078 (1.5898)	grad_norm 34.4190 (26.4596)	mem 4879MB
[2022-05-31 01:01:42 MetaFG_0] (main.py 265): INFO Train: [19/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2940 (0.3057)	loss 1.6662 (1.5888)	grad_norm 18.1101 (26.3921)	mem 4879MB
[2022-05-31 01:01:45 MetaFG_0] (main.py 265): INFO Train: [19/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2935 (0.3056)	loss 1.3649 (1.5852)	grad_norm 26.4708 (26.4045)	mem 4879MB
[2022-05-31 01:01:48 MetaFG_0] (main.py 265): INFO Train: [19/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2928 (0.3056)	loss 1.8046 (1.5855)	grad_norm 30.5769 (26.7652)	mem 4879MB
[2022-05-31 01:01:51 MetaFG_0] (main.py 265): INFO Train: [19/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2920 (0.3056)	loss 1.4197 (1.5861)	grad_norm 35.6825 (26.8192)	mem 4879MB
[2022-05-31 01:01:54 MetaFG_0] (main.py 265): INFO Train: [19/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2993 (0.3056)	loss 1.8234 (1.5881)	grad_norm 28.0503 (26.8704)	mem 4879MB
[2022-05-31 01:01:57 MetaFG_0] (main.py 265): INFO Train: [19/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2937 (0.3056)	loss 1.6818 (1.5885)	grad_norm 28.4401 (26.8151)	mem 4879MB
[2022-05-31 01:02:00 MetaFG_0] (main.py 265): INFO Train: [19/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2949 (0.3055)	loss 1.7554 (1.5889)	grad_norm 24.6043 (26.8138)	mem 4879MB
[2022-05-31 01:02:03 MetaFG_0] (main.py 265): INFO Train: [19/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2980 (0.3055)	loss 1.9096 (1.5881)	grad_norm 22.4000 (26.8907)	mem 4879MB
[2022-05-31 01:02:06 MetaFG_0] (main.py 265): INFO Train: [19/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2922 (0.3055)	loss 1.9639 (1.5873)	grad_norm 31.4325 (26.9058)	mem 4879MB
[2022-05-31 01:02:09 MetaFG_0] (main.py 265): INFO Train: [19/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2926 (0.3054)	loss 1.6373 (1.5869)	grad_norm 40.4963 (26.9854)	mem 4879MB
[2022-05-31 01:02:12 MetaFG_0] (main.py 265): INFO Train: [19/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2932 (0.3054)	loss 1.6943 (1.5870)	grad_norm 29.1093 (26.9942)	mem 4879MB
[2022-05-31 01:02:15 MetaFG_0] (main.py 265): INFO Train: [19/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2923 (0.3053)	loss 2.0020 (1.5882)	grad_norm 50.1780 (27.0161)	mem 4879MB
[2022-05-31 01:02:18 MetaFG_0] (main.py 265): INFO Train: [19/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2923 (0.3053)	loss 1.6457 (1.5879)	grad_norm 21.8998 (27.0185)	mem 4879MB
[2022-05-31 01:02:21 MetaFG_0] (main.py 265): INFO Train: [19/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2995 (0.3053)	loss 1.7563 (1.5878)	grad_norm 25.0057 (27.0291)	mem 4879MB
[2022-05-31 01:02:24 MetaFG_0] (main.py 265): INFO Train: [19/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2938 (0.3053)	loss 1.4167 (1.5882)	grad_norm 26.0536 (27.0585)	mem 4879MB
[2022-05-31 01:02:27 MetaFG_0] (main.py 265): INFO Train: [19/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2982 (0.3052)	loss 1.5022 (1.5877)	grad_norm 17.9598 (27.0553)	mem 4879MB
[2022-05-31 01:02:30 MetaFG_0] (main.py 265): INFO Train: [19/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2936 (0.3052)	loss 1.5554 (1.5873)	grad_norm 45.3976 (27.0380)	mem 4879MB
[2022-05-31 01:02:33 MetaFG_0] (main.py 265): INFO Train: [19/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2917 (0.3052)	loss 1.4798 (1.5861)	grad_norm 38.3018 (27.0421)	mem 4879MB
[2022-05-31 01:02:36 MetaFG_0] (main.py 265): INFO Train: [19/300][720/1562]	eta 0:04:16 lr 0.000006	time 0.2930 (0.3052)	loss 1.1782 (1.5848)	grad_norm 30.6312 (27.0297)	mem 4879MB
[2022-05-31 01:02:39 MetaFG_0] (main.py 265): INFO Train: [19/300][730/1562]	eta 0:04:13 lr 0.000006	time 0.2920 (0.3051)	loss 1.3685 (1.5836)	grad_norm 27.0124 (27.0174)	mem 4879MB
[2022-05-31 01:02:42 MetaFG_0] (main.py 265): INFO Train: [19/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2931 (0.3051)	loss 1.7878 (1.5840)	grad_norm 23.5688 (27.0600)	mem 4879MB
[2022-05-31 01:02:45 MetaFG_0] (main.py 265): INFO Train: [19/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2987 (0.3051)	loss 1.3727 (1.5845)	grad_norm 28.6875 (27.0612)	mem 4879MB
[2022-05-31 01:02:48 MetaFG_0] (main.py 265): INFO Train: [19/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2983 (0.3051)	loss 1.8082 (1.5838)	grad_norm 29.6603 (27.1103)	mem 4879MB
[2022-05-31 01:02:51 MetaFG_0] (main.py 265): INFO Train: [19/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2966 (0.3051)	loss 1.4914 (1.5824)	grad_norm 23.9058 (27.0534)	mem 4879MB
[2022-05-31 01:02:54 MetaFG_0] (main.py 265): INFO Train: [19/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.3023 (0.3051)	loss 1.4808 (1.5822)	grad_norm 39.7213 (27.1265)	mem 4879MB
[2022-05-31 01:02:58 MetaFG_0] (main.py 265): INFO Train: [19/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2996 (0.3050)	loss 1.6216 (1.5825)	grad_norm 22.2050 (27.1608)	mem 4879MB
[2022-05-31 01:03:01 MetaFG_0] (main.py 265): INFO Train: [19/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2994 (0.3050)	loss 1.6606 (1.5823)	grad_norm 21.4181 (27.1736)	mem 4879MB
[2022-05-31 01:03:04 MetaFG_0] (main.py 265): INFO Train: [19/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2988 (0.3050)	loss 1.6568 (1.5836)	grad_norm 15.2243 (27.1449)	mem 4879MB
[2022-05-31 01:03:07 MetaFG_0] (main.py 265): INFO Train: [19/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2987 (0.3050)	loss 1.3596 (1.5831)	grad_norm 27.8069 (27.1425)	mem 4879MB
[2022-05-31 01:03:10 MetaFG_0] (main.py 265): INFO Train: [19/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2975 (0.3050)	loss 1.7654 (1.5824)	grad_norm 15.2316 (27.1938)	mem 4879MB
[2022-05-31 01:03:13 MetaFG_0] (main.py 265): INFO Train: [19/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2973 (0.3050)	loss 1.2376 (1.5810)	grad_norm 32.9560 (27.1869)	mem 4879MB
[2022-05-31 01:03:16 MetaFG_0] (main.py 265): INFO Train: [19/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2955 (0.3049)	loss 1.4935 (1.5803)	grad_norm 64.9093 (27.2649)	mem 4879MB
[2022-05-31 01:03:19 MetaFG_0] (main.py 265): INFO Train: [19/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2991 (0.3049)	loss 1.8078 (1.5799)	grad_norm 30.1660 (27.2737)	mem 4879MB
[2022-05-31 01:03:22 MetaFG_0] (main.py 265): INFO Train: [19/300][870/1562]	eta 0:03:30 lr 0.000006	time 0.2956 (0.3049)	loss 1.9018 (1.5800)	grad_norm 31.5172 (27.3078)	mem 4879MB
[2022-05-31 01:03:25 MetaFG_0] (main.py 265): INFO Train: [19/300][880/1562]	eta 0:03:27 lr 0.000006	time 0.2930 (0.3049)	loss 1.2532 (1.5789)	grad_norm 24.8039 (27.2400)	mem 4879MB
[2022-05-31 01:03:28 MetaFG_0] (main.py 265): INFO Train: [19/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2928 (0.3049)	loss 1.4771 (1.5783)	grad_norm 71.9299 (27.2939)	mem 4879MB
[2022-05-31 01:03:31 MetaFG_0] (main.py 265): INFO Train: [19/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2979 (0.3048)	loss 1.2322 (1.5774)	grad_norm 23.9917 (27.3465)	mem 4879MB
[2022-05-31 01:03:34 MetaFG_0] (main.py 265): INFO Train: [19/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2921 (0.3048)	loss 1.5660 (1.5776)	grad_norm 26.0006 (27.3437)	mem 4879MB
[2022-05-31 01:03:37 MetaFG_0] (main.py 265): INFO Train: [19/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2967 (0.3048)	loss 1.6286 (1.5778)	grad_norm 19.6756 (27.3096)	mem 4879MB
[2022-05-31 01:03:40 MetaFG_0] (main.py 265): INFO Train: [19/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2923 (0.3048)	loss 1.8475 (1.5780)	grad_norm 16.0117 (27.2717)	mem 4879MB
[2022-05-31 01:03:43 MetaFG_0] (main.py 265): INFO Train: [19/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2928 (0.3048)	loss 1.7340 (1.5775)	grad_norm 18.9073 (27.2487)	mem 4879MB
[2022-05-31 01:03:46 MetaFG_0] (main.py 265): INFO Train: [19/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2929 (0.3047)	loss 1.7935 (1.5783)	grad_norm 27.8859 (27.2298)	mem 4879MB
[2022-05-31 01:03:49 MetaFG_0] (main.py 265): INFO Train: [19/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2923 (0.3047)	loss 1.6313 (1.5781)	grad_norm 25.8554 (27.2451)	mem 4879MB
[2022-05-31 01:03:52 MetaFG_0] (main.py 265): INFO Train: [19/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2932 (0.3048)	loss 1.7955 (1.5779)	grad_norm 18.7872 (27.1967)	mem 4879MB
[2022-05-31 01:03:55 MetaFG_0] (main.py 265): INFO Train: [19/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2923 (0.3048)	loss 1.8672 (1.5786)	grad_norm 29.5959 (27.1766)	mem 4879MB
[2022-05-31 01:03:58 MetaFG_0] (main.py 265): INFO Train: [19/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2973 (0.3048)	loss 1.6771 (1.5787)	grad_norm 31.3595 (27.1448)	mem 4879MB
[2022-05-31 01:04:01 MetaFG_0] (main.py 265): INFO Train: [19/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2984 (0.3048)	loss 1.7770 (1.5782)	grad_norm 38.4785 (27.1555)	mem 4879MB
[2022-05-31 01:04:04 MetaFG_0] (main.py 265): INFO Train: [19/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2933 (0.3047)	loss 1.2364 (1.5757)	grad_norm 43.8747 (27.1936)	mem 4879MB
[2022-05-31 01:04:07 MetaFG_0] (main.py 265): INFO Train: [19/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2994 (0.3047)	loss 1.6094 (1.5758)	grad_norm 17.8242 (27.1541)	mem 4879MB
[2022-05-31 01:04:10 MetaFG_0] (main.py 265): INFO Train: [19/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2929 (0.3047)	loss 1.8870 (1.5759)	grad_norm 26.1093 (27.1631)	mem 4879MB
[2022-05-31 01:04:13 MetaFG_0] (main.py 265): INFO Train: [19/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2926 (0.3047)	loss 1.5162 (1.5753)	grad_norm 34.7524 (27.1478)	mem 4879MB
[2022-05-31 01:04:16 MetaFG_0] (main.py 265): INFO Train: [19/300][1050/1562]	eta 0:02:35 lr 0.000006	time 0.2975 (0.3047)	loss 1.6095 (1.5754)	grad_norm 20.9497 (27.1580)	mem 4879MB
[2022-05-31 01:04:20 MetaFG_0] (main.py 265): INFO Train: [19/300][1060/1562]	eta 0:02:32 lr 0.000006	time 0.2975 (0.3047)	loss 1.6619 (1.5756)	grad_norm 14.5174 (27.1678)	mem 4879MB
[2022-05-31 01:04:23 MetaFG_0] (main.py 265): INFO Train: [19/300][1070/1562]	eta 0:02:29 lr 0.000006	time 0.2919 (0.3047)	loss 1.6925 (1.5754)	grad_norm 22.1823 (27.1691)	mem 4879MB
[2022-05-31 01:04:26 MetaFG_0] (main.py 265): INFO Train: [19/300][1080/1562]	eta 0:02:26 lr 0.000006	time 0.2983 (0.3047)	loss 1.5058 (1.5758)	grad_norm 29.4688 (27.1807)	mem 4879MB
[2022-05-31 01:04:29 MetaFG_0] (main.py 265): INFO Train: [19/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2978 (0.3047)	loss 1.6014 (1.5765)	grad_norm 21.6584 (27.1636)	mem 4879MB
[2022-05-31 01:04:32 MetaFG_0] (main.py 265): INFO Train: [19/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2935 (0.3047)	loss 1.6845 (1.5768)	grad_norm 30.2239 (27.1532)	mem 4879MB
[2022-05-31 01:04:35 MetaFG_0] (main.py 265): INFO Train: [19/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2934 (0.3047)	loss 1.7106 (1.5769)	grad_norm 23.2550 (27.1151)	mem 4879MB
[2022-05-31 01:04:38 MetaFG_0] (main.py 265): INFO Train: [19/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2991 (0.3047)	loss 1.6201 (1.5775)	grad_norm 16.6332 (27.1063)	mem 4879MB
[2022-05-31 01:04:41 MetaFG_0] (main.py 265): INFO Train: [19/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2929 (0.3046)	loss 1.9057 (1.5785)	grad_norm 26.5976 (27.1051)	mem 4879MB
[2022-05-31 01:04:44 MetaFG_0] (main.py 265): INFO Train: [19/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2925 (0.3046)	loss 1.6395 (1.5792)	grad_norm 20.3058 (27.0637)	mem 4879MB
[2022-05-31 01:04:47 MetaFG_0] (main.py 265): INFO Train: [19/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2925 (0.3046)	loss 1.9379 (1.5807)	grad_norm 33.8592 (27.0584)	mem 4879MB
[2022-05-31 01:04:50 MetaFG_0] (main.py 265): INFO Train: [19/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2941 (0.3047)	loss 1.6835 (1.5805)	grad_norm 29.9501 (27.0662)	mem 4879MB
[2022-05-31 01:04:53 MetaFG_0] (main.py 265): INFO Train: [19/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2985 (0.3047)	loss 1.7199 (1.5802)	grad_norm 35.5749 (27.0775)	mem 4879MB
[2022-05-31 01:04:56 MetaFG_0] (main.py 265): INFO Train: [19/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2923 (0.3046)	loss 1.8729 (1.5813)	grad_norm 25.1501 (27.0735)	mem 4879MB
[2022-05-31 01:04:59 MetaFG_0] (main.py 265): INFO Train: [19/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2921 (0.3046)	loss 1.8109 (1.5816)	grad_norm 37.1238 (27.0784)	mem 4879MB
[2022-05-31 01:05:02 MetaFG_0] (main.py 265): INFO Train: [19/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2929 (0.3046)	loss 1.6577 (1.5823)	grad_norm 20.4605 (27.0784)	mem 4879MB
[2022-05-31 01:05:05 MetaFG_0] (main.py 265): INFO Train: [19/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2985 (0.3046)	loss 1.7094 (1.5822)	grad_norm 24.9588 (27.0639)	mem 4879MB
[2022-05-31 01:05:08 MetaFG_0] (main.py 265): INFO Train: [19/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2982 (0.3046)	loss 1.6576 (1.5817)	grad_norm 17.3099 (27.0551)	mem 4879MB
[2022-05-31 01:05:11 MetaFG_0] (main.py 265): INFO Train: [19/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2977 (0.3046)	loss 1.6478 (1.5818)	grad_norm 23.4670 (27.0975)	mem 4879MB
[2022-05-31 01:05:14 MetaFG_0] (main.py 265): INFO Train: [19/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2977 (0.3046)	loss 1.3284 (1.5821)	grad_norm 41.6086 (27.1499)	mem 4879MB
[2022-05-31 01:05:17 MetaFG_0] (main.py 265): INFO Train: [19/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2979 (0.3046)	loss 1.7087 (1.5818)	grad_norm 16.2596 (27.2012)	mem 4879MB
[2022-05-31 01:05:20 MetaFG_0] (main.py 265): INFO Train: [19/300][1260/1562]	eta 0:01:31 lr 0.000006	time 0.2984 (0.3046)	loss 1.1484 (1.5818)	grad_norm 34.7843 (27.2309)	mem 4879MB
[2022-05-31 01:05:23 MetaFG_0] (main.py 265): INFO Train: [19/300][1270/1562]	eta 0:01:28 lr 0.000006	time 0.2987 (0.3045)	loss 1.5844 (1.5817)	grad_norm 29.1556 (27.2298)	mem 4879MB
[2022-05-31 01:05:26 MetaFG_0] (main.py 265): INFO Train: [19/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2978 (0.3045)	loss 1.7630 (1.5812)	grad_norm 20.3937 (27.2330)	mem 4879MB
[2022-05-31 01:05:29 MetaFG_0] (main.py 265): INFO Train: [19/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2957 (0.3045)	loss 1.3202 (1.5807)	grad_norm 38.8482 (27.2629)	mem 4879MB
[2022-05-31 01:05:32 MetaFG_0] (main.py 265): INFO Train: [19/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2965 (0.3045)	loss 1.6466 (1.5807)	grad_norm 22.6243 (27.2608)	mem 4879MB
[2022-05-31 01:05:35 MetaFG_0] (main.py 265): INFO Train: [19/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2974 (0.3045)	loss 1.6148 (1.5805)	grad_norm 12.5351 (27.2464)	mem 4879MB
[2022-05-31 01:05:38 MetaFG_0] (main.py 265): INFO Train: [19/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2935 (0.3045)	loss 1.6525 (1.5800)	grad_norm 24.0683 (27.2354)	mem 4879MB
[2022-05-31 01:05:42 MetaFG_0] (main.py 265): INFO Train: [19/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2935 (0.3045)	loss 1.2478 (1.5798)	grad_norm 25.9623 (27.2434)	mem 4879MB
[2022-05-31 01:05:45 MetaFG_0] (main.py 265): INFO Train: [19/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2924 (0.3045)	loss 1.8077 (1.5806)	grad_norm 30.3722 (27.2415)	mem 4879MB
[2022-05-31 01:05:48 MetaFG_0] (main.py 265): INFO Train: [19/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2974 (0.3045)	loss 1.7353 (1.5803)	grad_norm 35.4813 (27.2299)	mem 4879MB
[2022-05-31 01:05:51 MetaFG_0] (main.py 265): INFO Train: [19/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3023 (0.3045)	loss 1.6631 (1.5801)	grad_norm 19.8790 (27.2324)	mem 4879MB
[2022-05-31 01:05:54 MetaFG_0] (main.py 265): INFO Train: [19/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2979 (0.3045)	loss 1.8231 (1.5810)	grad_norm 41.0058 (27.2853)	mem 4879MB
[2022-05-31 01:05:57 MetaFG_0] (main.py 265): INFO Train: [19/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2927 (0.3045)	loss 1.7545 (1.5814)	grad_norm 25.1775 (27.2848)	mem 4879MB
[2022-05-31 01:06:00 MetaFG_0] (main.py 265): INFO Train: [19/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2990 (0.3045)	loss 1.9157 (1.5820)	grad_norm 30.0206 (27.3068)	mem 4879MB
[2022-05-31 01:06:03 MetaFG_0] (main.py 265): INFO Train: [19/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2971 (0.3045)	loss 1.6091 (1.5820)	grad_norm 44.6880 (27.3135)	mem 4879MB
[2022-05-31 01:06:06 MetaFG_0] (main.py 265): INFO Train: [19/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2915 (0.3044)	loss 1.8140 (1.5826)	grad_norm 28.6565 (27.2922)	mem 4879MB
[2022-05-31 01:06:09 MetaFG_0] (main.py 265): INFO Train: [19/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2991 (0.3045)	loss 1.4681 (1.5827)	grad_norm 18.7596 (27.3239)	mem 4879MB
[2022-05-31 01:06:12 MetaFG_0] (main.py 265): INFO Train: [19/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2920 (0.3045)	loss 1.3603 (1.5831)	grad_norm 17.9104 (27.3109)	mem 4879MB
[2022-05-31 01:06:15 MetaFG_0] (main.py 265): INFO Train: [19/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2969 (0.3045)	loss 1.6242 (1.5830)	grad_norm 17.7478 (27.3216)	mem 4879MB
[2022-05-31 01:06:18 MetaFG_0] (main.py 265): INFO Train: [19/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2961 (0.3044)	loss 1.7857 (1.5834)	grad_norm 46.8803 (27.3238)	mem 4879MB
[2022-05-31 01:06:21 MetaFG_0] (main.py 265): INFO Train: [19/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2937 (0.3044)	loss 1.5731 (1.5841)	grad_norm 22.8704 (27.3317)	mem 4879MB
[2022-05-31 01:06:24 MetaFG_0] (main.py 265): INFO Train: [19/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2929 (0.3045)	loss 1.5582 (1.5840)	grad_norm 28.3142 (27.3160)	mem 4879MB
[2022-05-31 01:06:27 MetaFG_0] (main.py 265): INFO Train: [19/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2955 (0.3044)	loss 1.2892 (1.5838)	grad_norm 23.6341 (27.3115)	mem 4879MB
[2022-05-31 01:06:30 MetaFG_0] (main.py 265): INFO Train: [19/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2992 (0.3044)	loss 1.9322 (1.5835)	grad_norm 26.6424 (27.3002)	mem 4879MB
[2022-05-31 01:06:33 MetaFG_0] (main.py 265): INFO Train: [19/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2969 (0.3044)	loss 1.3467 (1.5832)	grad_norm 60.3809 (27.3177)	mem 4879MB
[2022-05-31 01:06:36 MetaFG_0] (main.py 265): INFO Train: [19/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2988 (0.3044)	loss 1.2653 (1.5829)	grad_norm 24.5082 (27.3151)	mem 4879MB
[2022-05-31 01:06:39 MetaFG_0] (main.py 265): INFO Train: [19/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2941 (0.3044)	loss 1.6852 (1.5826)	grad_norm 19.0471 (27.3067)	mem 4879MB
[2022-05-31 01:06:42 MetaFG_0] (main.py 265): INFO Train: [19/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2991 (0.3044)	loss 1.4317 (1.5824)	grad_norm 22.6199 (27.3150)	mem 4879MB
[2022-05-31 01:06:45 MetaFG_0] (main.py 265): INFO Train: [19/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2921 (0.3044)	loss 1.4762 (1.5820)	grad_norm 30.3473 (27.3294)	mem 4879MB
[2022-05-31 01:06:48 MetaFG_0] (main.py 265): INFO Train: [19/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2937 (0.3044)	loss 1.7182 (1.5819)	grad_norm 34.6519 (inf)	mem 4879MB
[2022-05-31 01:06:51 MetaFG_0] (main.py 265): INFO Train: [19/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2916 (0.3044)	loss 1.6433 (1.5822)	grad_norm 15.6941 (inf)	mem 4879MB
[2022-05-31 01:06:52 MetaFG_0] (main.py 272): INFO EPOCH 19 training takes 0:07:55
[2022-05-31 01:06:52 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_19.pth saving......
[2022-05-31 01:06:53 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_19.pth saved !!!
[2022-05-31 01:06:53 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:06:54 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:06:54 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:06:55 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.697 (0.697)	Loss 0.9516 (0.9516)	Acc@1 87.500 (87.500)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 01:06:56 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.150)	Loss 1.0781 (1.0973)	Acc@1 81.250 (77.841)	Acc@5 96.875 (96.591)	Mem 4879MB
[2022-05-31 01:06:57 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.124)	Loss 0.9540 (1.1391)	Acc@1 71.875 (74.256)	Acc@5 96.875 (96.280)	Mem 4879MB
[2022-05-31 01:06:58 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.099 (0.115)	Loss 1.4622 (1.0957)	Acc@1 53.125 (75.504)	Acc@5 93.750 (97.077)	Mem 4879MB
[2022-05-31 01:06:59 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.109)	Loss 1.1880 (1.0899)	Acc@1 71.875 (75.838)	Acc@5 93.750 (97.256)	Mem 4879MB
[2022-05-31 01:07:00 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.106)	Loss 1.0081 (1.0728)	Acc@1 78.125 (76.654)	Acc@5 96.875 (97.304)	Mem 4879MB
[2022-05-31 01:07:01 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.102 (0.105)	Loss 1.3182 (1.0676)	Acc@1 75.000 (76.588)	Acc@5 93.750 (97.439)	Mem 4879MB
[2022-05-31 01:07:02 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.092 (0.103)	Loss 1.0848 (1.0672)	Acc@1 87.500 (77.201)	Acc@5 93.750 (97.315)	Mem 4879MB
[2022-05-31 01:07:02 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.101 (0.102)	Loss 1.0167 (1.0570)	Acc@1 78.125 (77.238)	Acc@5 96.875 (97.338)	Mem 4879MB
[2022-05-31 01:07:03 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 1.1913 (1.0608)	Acc@1 78.125 (77.370)	Acc@5 90.625 (97.150)	Mem 4879MB
[2022-05-31 01:07:04 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.102 (0.101)	Loss 1.3444 (1.0582)	Acc@1 62.500 (77.290)	Acc@5 93.750 (97.123)	Mem 4879MB
[2022-05-31 01:07:05 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.100)	Loss 0.9832 (1.0589)	Acc@1 87.500 (77.337)	Acc@5 96.875 (96.959)	Mem 4879MB
[2022-05-31 01:07:06 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 1.2018 (1.0569)	Acc@1 68.750 (77.634)	Acc@5 100.000 (96.978)	Mem 4879MB
[2022-05-31 01:07:07 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.093 (0.099)	Loss 1.0576 (1.0618)	Acc@1 84.375 (77.624)	Acc@5 96.875 (96.851)	Mem 4879MB
[2022-05-31 01:07:08 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.092 (0.099)	Loss 1.1795 (1.0573)	Acc@1 78.125 (77.726)	Acc@5 93.750 (96.964)	Mem 4879MB
[2022-05-31 01:07:09 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 1.2527 (1.0592)	Acc@1 71.875 (77.690)	Acc@5 93.750 (96.937)	Mem 4879MB
[2022-05-31 01:07:10 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.9631 (1.0610)	Acc@1 81.250 (77.562)	Acc@5 100.000 (96.933)	Mem 4879MB
[2022-05-31 01:07:11 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.101 (0.098)	Loss 1.0563 (1.0611)	Acc@1 71.875 (77.449)	Acc@5 96.875 (96.948)	Mem 4879MB
[2022-05-31 01:07:12 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 1.3196 (1.0598)	Acc@1 68.750 (77.676)	Acc@5 90.625 (96.910)	Mem 4879MB
[2022-05-31 01:07:13 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 1.2558 (1.0582)	Acc@1 75.000 (77.618)	Acc@5 96.875 (96.908)	Mem 4879MB
[2022-05-31 01:07:14 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 1.2592 (1.0653)	Acc@1 71.875 (77.192)	Acc@5 96.875 (96.844)	Mem 4879MB
[2022-05-31 01:07:15 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 1.0924 (1.0634)	Acc@1 87.500 (77.266)	Acc@5 93.750 (96.831)	Mem 4879MB
[2022-05-31 01:07:16 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.099 (0.097)	Loss 1.2144 (1.0673)	Acc@1 68.750 (77.121)	Acc@5 93.750 (96.861)	Mem 4879MB
[2022-05-31 01:07:17 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 1.2640 (1.0662)	Acc@1 62.500 (77.124)	Acc@5 93.750 (96.929)	Mem 4879MB
[2022-05-31 01:07:17 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 1.0087 (1.0665)	Acc@1 75.000 (77.036)	Acc@5 100.000 (96.966)	Mem 4879MB
[2022-05-31 01:07:18 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 1.4921 (1.0699)	Acc@1 56.250 (76.980)	Acc@5 93.750 (96.925)	Mem 4879MB
[2022-05-31 01:07:19 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.097 (0.096)	Loss 0.9467 (1.0679)	Acc@1 81.250 (77.023)	Acc@5 100.000 (96.971)	Mem 4879MB
[2022-05-31 01:07:20 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.091 (0.096)	Loss 1.0874 (1.0672)	Acc@1 71.875 (77.064)	Acc@5 93.750 (96.944)	Mem 4879MB
[2022-05-31 01:07:21 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 1.2107 (1.0679)	Acc@1 71.875 (77.013)	Acc@5 96.875 (96.997)	Mem 4879MB
[2022-05-31 01:07:22 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.9652 (1.0673)	Acc@1 87.500 (77.083)	Acc@5 93.750 (97.004)	Mem 4879MB
[2022-05-31 01:07:23 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.103 (0.096)	Loss 1.0074 (1.0657)	Acc@1 71.875 (77.149)	Acc@5 100.000 (96.979)	Mem 4879MB
[2022-05-31 01:07:24 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8293 (1.0665)	Acc@1 87.500 (77.150)	Acc@5 96.875 (96.955)	Mem 4879MB
[2022-05-31 01:07:24 MetaFG_0] (main.py 330): INFO  * Acc@1 77.150 Acc@5 96.960
[2022-05-31 01:07:24 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 77.2%
[2022-05-31 01:07:24 MetaFG_0] (main.py 171): INFO Max accuracy: 77.15%
[2022-05-31 01:07:25 MetaFG_0] (main.py 265): INFO Train: [20/300][0/1562]	eta 0:27:27 lr 0.000006	time 1.0550 (1.0550)	loss 1.4554 (1.4554)	grad_norm 19.7372 (19.7372)	mem 4879MB
[2022-05-31 01:07:28 MetaFG_0] (main.py 265): INFO Train: [20/300][10/1562]	eta 0:09:52 lr 0.000006	time 0.2993 (0.3820)	loss 1.5985 (1.5581)	grad_norm 28.7259 (25.7532)	mem 4879MB
[2022-05-31 01:07:31 MetaFG_0] (main.py 265): INFO Train: [20/300][20/1562]	eta 0:08:51 lr 0.000006	time 0.2944 (0.3446)	loss 1.2289 (1.5616)	grad_norm 37.8962 (25.2519)	mem 4879MB
[2022-05-31 01:07:34 MetaFG_0] (main.py 265): INFO Train: [20/300][30/1562]	eta 0:08:27 lr 0.000006	time 0.2926 (0.3311)	loss 1.4613 (1.5577)	grad_norm 22.4932 (27.7253)	mem 4879MB
[2022-05-31 01:07:38 MetaFG_0] (main.py 265): INFO Train: [20/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.2943 (0.3243)	loss 1.7512 (1.5761)	grad_norm 24.9631 (27.2357)	mem 4879MB
[2022-05-31 01:07:41 MetaFG_0] (main.py 265): INFO Train: [20/300][50/1562]	eta 0:08:05 lr 0.000006	time 0.2972 (0.3209)	loss 1.4189 (1.5636)	grad_norm 28.2721 (26.5268)	mem 4879MB
[2022-05-31 01:07:44 MetaFG_0] (main.py 265): INFO Train: [20/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2991 (0.3182)	loss 1.6357 (1.5405)	grad_norm 31.5278 (27.0247)	mem 4879MB
[2022-05-31 01:07:47 MetaFG_0] (main.py 265): INFO Train: [20/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2934 (0.3160)	loss 1.5912 (1.5400)	grad_norm 21.0364 (27.6985)	mem 4879MB
[2022-05-31 01:07:50 MetaFG_0] (main.py 265): INFO Train: [20/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2976 (0.3148)	loss 1.3143 (1.5292)	grad_norm 33.7045 (27.5887)	mem 4879MB
[2022-05-31 01:07:53 MetaFG_0] (main.py 265): INFO Train: [20/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2975 (0.3135)	loss 1.8366 (1.5430)	grad_norm 30.4898 (27.6700)	mem 4879MB
[2022-05-31 01:07:56 MetaFG_0] (main.py 265): INFO Train: [20/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2920 (0.3126)	loss 1.8791 (1.5602)	grad_norm 30.3398 (27.3907)	mem 4879MB
[2022-05-31 01:07:59 MetaFG_0] (main.py 265): INFO Train: [20/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2991 (0.3120)	loss 1.6214 (1.5531)	grad_norm 39.2124 (27.2002)	mem 4879MB
[2022-05-31 01:08:02 MetaFG_0] (main.py 265): INFO Train: [20/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2993 (0.3113)	loss 1.6787 (1.5565)	grad_norm 25.1338 (27.2982)	mem 4879MB
[2022-05-31 01:08:05 MetaFG_0] (main.py 265): INFO Train: [20/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2988 (0.3107)	loss 1.6582 (1.5576)	grad_norm 17.6837 (27.2705)	mem 4879MB
[2022-05-31 01:08:08 MetaFG_0] (main.py 265): INFO Train: [20/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2933 (0.3103)	loss 1.4521 (1.5561)	grad_norm 33.7334 (27.3701)	mem 4879MB
[2022-05-31 01:08:11 MetaFG_0] (main.py 265): INFO Train: [20/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2925 (0.3098)	loss 1.6743 (1.5653)	grad_norm 32.0610 (28.4466)	mem 4879MB
[2022-05-31 01:08:14 MetaFG_0] (main.py 265): INFO Train: [20/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2993 (0.3094)	loss 1.4084 (1.5602)	grad_norm 26.0493 (28.1595)	mem 4879MB
[2022-05-31 01:08:17 MetaFG_0] (main.py 265): INFO Train: [20/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2991 (0.3092)	loss 1.3191 (1.5593)	grad_norm 42.0547 (28.3538)	mem 4879MB
[2022-05-31 01:08:20 MetaFG_0] (main.py 265): INFO Train: [20/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2922 (0.3089)	loss 1.8038 (1.5585)	grad_norm 39.6361 (28.1642)	mem 4879MB
[2022-05-31 01:08:23 MetaFG_0] (main.py 265): INFO Train: [20/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2917 (0.3086)	loss 1.2514 (1.5615)	grad_norm 25.0818 (28.2039)	mem 4879MB
[2022-05-31 01:08:26 MetaFG_0] (main.py 265): INFO Train: [20/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2934 (0.3083)	loss 1.5981 (1.5648)	grad_norm 16.2208 (28.0878)	mem 4879MB
[2022-05-31 01:08:29 MetaFG_0] (main.py 265): INFO Train: [20/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2934 (0.3081)	loss 1.6078 (1.5627)	grad_norm 25.8595 (27.9933)	mem 4879MB
[2022-05-31 01:08:32 MetaFG_0] (main.py 265): INFO Train: [20/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2990 (0.3079)	loss 1.8098 (1.5633)	grad_norm 26.0370 (28.0387)	mem 4879MB
[2022-05-31 01:08:35 MetaFG_0] (main.py 265): INFO Train: [20/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2986 (0.3077)	loss 1.4041 (1.5632)	grad_norm 31.9494 (28.1198)	mem 4879MB
[2022-05-31 01:08:38 MetaFG_0] (main.py 265): INFO Train: [20/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2998 (0.3076)	loss 1.7468 (1.5651)	grad_norm 13.3157 (27.9323)	mem 4879MB
[2022-05-31 01:08:41 MetaFG_0] (main.py 265): INFO Train: [20/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2995 (0.3074)	loss 1.6992 (1.5651)	grad_norm 28.9051 (28.0092)	mem 4879MB
[2022-05-31 01:08:44 MetaFG_0] (main.py 265): INFO Train: [20/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2923 (0.3074)	loss 1.7048 (1.5671)	grad_norm 22.1333 (27.8601)	mem 4879MB
[2022-05-31 01:08:47 MetaFG_0] (main.py 265): INFO Train: [20/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2934 (0.3073)	loss 1.6899 (1.5672)	grad_norm 24.1478 (27.9387)	mem 4879MB
[2022-05-31 01:08:51 MetaFG_0] (main.py 265): INFO Train: [20/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2923 (0.3072)	loss 1.8062 (1.5690)	grad_norm 23.3648 (27.8288)	mem 4879MB
[2022-05-31 01:08:54 MetaFG_0] (main.py 265): INFO Train: [20/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2995 (0.3071)	loss 1.3747 (1.5702)	grad_norm 41.6383 (27.7845)	mem 4879MB
[2022-05-31 01:08:57 MetaFG_0] (main.py 265): INFO Train: [20/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2953 (0.3070)	loss 1.6947 (1.5691)	grad_norm 37.1967 (28.1382)	mem 4879MB
[2022-05-31 01:09:00 MetaFG_0] (main.py 265): INFO Train: [20/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.3040 (0.3069)	loss 1.1049 (1.5646)	grad_norm 26.9407 (28.1202)	mem 4879MB
[2022-05-31 01:09:03 MetaFG_0] (main.py 265): INFO Train: [20/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2972 (0.3069)	loss 1.5844 (1.5627)	grad_norm 24.6333 (28.0675)	mem 4879MB
[2022-05-31 01:09:06 MetaFG_0] (main.py 265): INFO Train: [20/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2915 (0.3067)	loss 1.6487 (1.5636)	grad_norm 18.6017 (27.9859)	mem 4879MB
[2022-05-31 01:09:09 MetaFG_0] (main.py 265): INFO Train: [20/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2977 (0.3067)	loss 1.3715 (1.5678)	grad_norm 41.0831 (28.0169)	mem 4879MB
[2022-05-31 01:09:12 MetaFG_0] (main.py 265): INFO Train: [20/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2933 (0.3066)	loss 1.8385 (1.5679)	grad_norm 50.9270 (28.1088)	mem 4879MB
[2022-05-31 01:09:15 MetaFG_0] (main.py 265): INFO Train: [20/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2918 (0.3065)	loss 1.6334 (1.5687)	grad_norm 21.2238 (28.1215)	mem 4879MB
[2022-05-31 01:09:18 MetaFG_0] (main.py 265): INFO Train: [20/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2989 (0.3065)	loss 1.7904 (1.5662)	grad_norm 40.3480 (28.2210)	mem 4879MB
[2022-05-31 01:09:21 MetaFG_0] (main.py 265): INFO Train: [20/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2923 (0.3064)	loss 1.6300 (1.5671)	grad_norm 22.0339 (28.1106)	mem 4879MB
[2022-05-31 01:09:24 MetaFG_0] (main.py 265): INFO Train: [20/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2930 (0.3064)	loss 1.7334 (1.5659)	grad_norm 23.1970 (28.1821)	mem 4879MB
[2022-05-31 01:09:27 MetaFG_0] (main.py 265): INFO Train: [20/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2986 (0.3064)	loss 1.6683 (1.5660)	grad_norm 30.4052 (28.0758)	mem 4879MB
[2022-05-31 01:09:30 MetaFG_0] (main.py 265): INFO Train: [20/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2922 (0.3063)	loss 1.7361 (1.5671)	grad_norm 28.5063 (28.1514)	mem 4879MB
[2022-05-31 01:09:33 MetaFG_0] (main.py 265): INFO Train: [20/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2934 (0.3062)	loss 1.8285 (1.5654)	grad_norm 34.0016 (28.1401)	mem 4879MB
[2022-05-31 01:09:36 MetaFG_0] (main.py 265): INFO Train: [20/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2933 (0.3062)	loss 1.2769 (1.5652)	grad_norm 19.5560 (28.0269)	mem 4879MB
[2022-05-31 01:09:39 MetaFG_0] (main.py 265): INFO Train: [20/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2921 (0.3062)	loss 1.7430 (1.5675)	grad_norm 34.3461 (28.0484)	mem 4879MB
[2022-05-31 01:09:42 MetaFG_0] (main.py 265): INFO Train: [20/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2918 (0.3061)	loss 1.2776 (1.5666)	grad_norm 31.7023 (28.0758)	mem 4879MB
[2022-05-31 01:09:45 MetaFG_0] (main.py 265): INFO Train: [20/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.3002 (0.3060)	loss 1.6453 (1.5679)	grad_norm 17.2552 (28.0327)	mem 4879MB
[2022-05-31 01:09:48 MetaFG_0] (main.py 265): INFO Train: [20/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2940 (0.3060)	loss 1.5058 (1.5682)	grad_norm 27.0559 (28.0201)	mem 4879MB
[2022-05-31 01:09:51 MetaFG_0] (main.py 265): INFO Train: [20/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.3007 (0.3060)	loss 1.8845 (1.5694)	grad_norm 28.9042 (27.9994)	mem 4879MB
[2022-05-31 01:09:54 MetaFG_0] (main.py 265): INFO Train: [20/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2993 (0.3060)	loss 1.6406 (1.5697)	grad_norm 16.2319 (27.9957)	mem 4879MB
[2022-05-31 01:09:58 MetaFG_0] (main.py 265): INFO Train: [20/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.3012 (0.3060)	loss 1.4670 (1.5674)	grad_norm 30.5729 (28.0506)	mem 4879MB
[2022-05-31 01:10:01 MetaFG_0] (main.py 265): INFO Train: [20/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2939 (0.3060)	loss 1.6091 (1.5671)	grad_norm 32.2175 (28.0233)	mem 4879MB
[2022-05-31 01:10:04 MetaFG_0] (main.py 265): INFO Train: [20/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2933 (0.3059)	loss 1.7620 (1.5670)	grad_norm 18.6552 (27.9957)	mem 4879MB
[2022-05-31 01:10:07 MetaFG_0] (main.py 265): INFO Train: [20/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2975 (0.3059)	loss 1.7550 (1.5688)	grad_norm 17.9092 (27.9913)	mem 4879MB
[2022-05-31 01:10:10 MetaFG_0] (main.py 265): INFO Train: [20/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2933 (0.3058)	loss 1.6328 (1.5683)	grad_norm 136.1227 (28.1835)	mem 4879MB
[2022-05-31 01:10:13 MetaFG_0] (main.py 265): INFO Train: [20/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2918 (0.3058)	loss 1.7701 (1.5688)	grad_norm 31.0631 (28.2150)	mem 4879MB
[2022-05-31 01:10:16 MetaFG_0] (main.py 265): INFO Train: [20/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2939 (0.3057)	loss 1.7089 (1.5695)	grad_norm 41.1982 (28.2423)	mem 4879MB
[2022-05-31 01:10:19 MetaFG_0] (main.py 265): INFO Train: [20/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2921 (0.3057)	loss 1.6152 (1.5679)	grad_norm 23.4217 (28.2330)	mem 4879MB
[2022-05-31 01:10:22 MetaFG_0] (main.py 265): INFO Train: [20/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2977 (0.3057)	loss 1.6922 (1.5680)	grad_norm 26.9233 (28.1918)	mem 4879MB
[2022-05-31 01:10:25 MetaFG_0] (main.py 265): INFO Train: [20/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2932 (0.3057)	loss 1.6576 (1.5697)	grad_norm 37.9450 (28.2192)	mem 4879MB
[2022-05-31 01:10:28 MetaFG_0] (main.py 265): INFO Train: [20/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2932 (0.3056)	loss 1.2473 (1.5679)	grad_norm 19.9633 (28.2816)	mem 4879MB
[2022-05-31 01:10:31 MetaFG_0] (main.py 265): INFO Train: [20/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2932 (0.3056)	loss 1.2259 (1.5679)	grad_norm 19.6913 (28.1875)	mem 4879MB
[2022-05-31 01:10:34 MetaFG_0] (main.py 265): INFO Train: [20/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2983 (0.3056)	loss 1.7093 (1.5675)	grad_norm 44.0642 (28.2484)	mem 4879MB
[2022-05-31 01:10:37 MetaFG_0] (main.py 265): INFO Train: [20/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2919 (0.3055)	loss 1.8275 (1.5668)	grad_norm 33.1692 (28.2159)	mem 4879MB
[2022-05-31 01:10:40 MetaFG_0] (main.py 265): INFO Train: [20/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2980 (0.3055)	loss 1.6389 (1.5673)	grad_norm 15.2143 (28.2173)	mem 4879MB
[2022-05-31 01:10:43 MetaFG_0] (main.py 265): INFO Train: [20/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2918 (0.3055)	loss 1.7946 (1.5667)	grad_norm 26.6149 (28.1516)	mem 4879MB
[2022-05-31 01:10:46 MetaFG_0] (main.py 265): INFO Train: [20/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2923 (0.3055)	loss 1.8750 (1.5654)	grad_norm 23.4887 (28.1077)	mem 4879MB
[2022-05-31 01:10:49 MetaFG_0] (main.py 265): INFO Train: [20/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2918 (0.3055)	loss 1.8574 (1.5661)	grad_norm 24.7829 (28.1163)	mem 4879MB
[2022-05-31 01:10:52 MetaFG_0] (main.py 265): INFO Train: [20/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2984 (0.3054)	loss 1.6548 (1.5667)	grad_norm 19.9706 (inf)	mem 4879MB
[2022-05-31 01:10:55 MetaFG_0] (main.py 265): INFO Train: [20/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2986 (0.3054)	loss 1.7947 (1.5664)	grad_norm 23.1258 (inf)	mem 4879MB
[2022-05-31 01:10:58 MetaFG_0] (main.py 265): INFO Train: [20/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2924 (0.3054)	loss 1.7384 (1.5668)	grad_norm 31.4990 (inf)	mem 4879MB
[2022-05-31 01:11:01 MetaFG_0] (main.py 265): INFO Train: [20/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2978 (0.3053)	loss 1.4100 (1.5662)	grad_norm 46.5891 (inf)	mem 4879MB
[2022-05-31 01:11:04 MetaFG_0] (main.py 265): INFO Train: [20/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2933 (0.3053)	loss 1.6617 (1.5664)	grad_norm 32.9691 (inf)	mem 4879MB
[2022-05-31 01:11:07 MetaFG_0] (main.py 265): INFO Train: [20/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3023 (0.3053)	loss 1.6266 (1.5651)	grad_norm 39.1260 (inf)	mem 4879MB
[2022-05-31 01:11:10 MetaFG_0] (main.py 265): INFO Train: [20/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.3054 (0.3053)	loss 1.6177 (1.5643)	grad_norm 24.1210 (inf)	mem 4879MB
[2022-05-31 01:11:14 MetaFG_0] (main.py 265): INFO Train: [20/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2941 (0.3053)	loss 1.8478 (1.5629)	grad_norm 21.0180 (inf)	mem 4879MB
[2022-05-31 01:11:17 MetaFG_0] (main.py 265): INFO Train: [20/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.3030 (0.3053)	loss 1.4696 (1.5625)	grad_norm 31.9029 (inf)	mem 4879MB
[2022-05-31 01:11:20 MetaFG_0] (main.py 265): INFO Train: [20/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.3011 (0.3053)	loss 1.7544 (1.5622)	grad_norm 16.0052 (inf)	mem 4879MB
[2022-05-31 01:11:23 MetaFG_0] (main.py 265): INFO Train: [20/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2938 (0.3053)	loss 1.1940 (1.5619)	grad_norm 42.7923 (inf)	mem 4879MB
[2022-05-31 01:11:26 MetaFG_0] (main.py 265): INFO Train: [20/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2998 (0.3053)	loss 1.7622 (1.5615)	grad_norm 28.5542 (inf)	mem 4879MB
[2022-05-31 01:11:29 MetaFG_0] (main.py 265): INFO Train: [20/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2986 (0.3053)	loss 1.7081 (1.5606)	grad_norm 17.2585 (inf)	mem 4879MB
[2022-05-31 01:11:32 MetaFG_0] (main.py 265): INFO Train: [20/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2935 (0.3053)	loss 1.7527 (1.5591)	grad_norm 29.6142 (inf)	mem 4879MB
[2022-05-31 01:11:35 MetaFG_0] (main.py 265): INFO Train: [20/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2921 (0.3052)	loss 1.5479 (1.5590)	grad_norm 15.8023 (inf)	mem 4879MB
[2022-05-31 01:11:38 MetaFG_0] (main.py 265): INFO Train: [20/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2929 (0.3052)	loss 1.7347 (1.5596)	grad_norm 25.6229 (inf)	mem 4879MB
[2022-05-31 01:11:41 MetaFG_0] (main.py 265): INFO Train: [20/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2936 (0.3052)	loss 1.4443 (1.5597)	grad_norm 26.9014 (inf)	mem 4879MB
[2022-05-31 01:11:44 MetaFG_0] (main.py 265): INFO Train: [20/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2926 (0.3052)	loss 0.9895 (1.5589)	grad_norm 19.1668 (inf)	mem 4879MB
[2022-05-31 01:11:47 MetaFG_0] (main.py 265): INFO Train: [20/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2972 (0.3052)	loss 1.6441 (1.5593)	grad_norm 23.7282 (inf)	mem 4879MB
[2022-05-31 01:11:50 MetaFG_0] (main.py 265): INFO Train: [20/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2920 (0.3052)	loss 1.5141 (1.5604)	grad_norm 24.1039 (inf)	mem 4879MB
[2022-05-31 01:11:53 MetaFG_0] (main.py 265): INFO Train: [20/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2987 (0.3051)	loss 1.6483 (1.5606)	grad_norm 18.2135 (inf)	mem 4879MB
[2022-05-31 01:11:56 MetaFG_0] (main.py 265): INFO Train: [20/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2984 (0.3051)	loss 1.5634 (1.5609)	grad_norm 36.1660 (inf)	mem 4879MB
[2022-05-31 01:11:59 MetaFG_0] (main.py 265): INFO Train: [20/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2922 (0.3051)	loss 1.2695 (1.5601)	grad_norm 16.4765 (inf)	mem 4879MB
[2022-05-31 01:12:02 MetaFG_0] (main.py 265): INFO Train: [20/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2969 (0.3051)	loss 1.4986 (1.5612)	grad_norm 13.9222 (inf)	mem 4879MB
[2022-05-31 01:12:05 MetaFG_0] (main.py 265): INFO Train: [20/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2980 (0.3051)	loss 1.4752 (1.5613)	grad_norm 19.3980 (inf)	mem 4879MB
[2022-05-31 01:12:08 MetaFG_0] (main.py 265): INFO Train: [20/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2935 (0.3051)	loss 1.3155 (1.5622)	grad_norm 49.3788 (inf)	mem 4879MB
[2022-05-31 01:12:11 MetaFG_0] (main.py 265): INFO Train: [20/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2917 (0.3051)	loss 1.8067 (1.5627)	grad_norm 36.0289 (inf)	mem 4879MB
[2022-05-31 01:12:14 MetaFG_0] (main.py 265): INFO Train: [20/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2979 (0.3051)	loss 1.5665 (1.5628)	grad_norm 17.8333 (inf)	mem 4879MB
[2022-05-31 01:12:17 MetaFG_0] (main.py 265): INFO Train: [20/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2934 (0.3050)	loss 1.6821 (1.5621)	grad_norm 18.6038 (inf)	mem 4879MB
[2022-05-31 01:12:20 MetaFG_0] (main.py 265): INFO Train: [20/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.3000 (0.3050)	loss 1.5828 (1.5625)	grad_norm 17.6825 (inf)	mem 4879MB
[2022-05-31 01:12:23 MetaFG_0] (main.py 265): INFO Train: [20/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2991 (0.3050)	loss 1.7025 (1.5624)	grad_norm 19.5552 (inf)	mem 4879MB
[2022-05-31 01:12:27 MetaFG_0] (main.py 265): INFO Train: [20/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2924 (0.3050)	loss 1.2130 (1.5623)	grad_norm 22.1065 (inf)	mem 4879MB
[2022-05-31 01:12:30 MetaFG_0] (main.py 265): INFO Train: [20/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2943 (0.3050)	loss 1.5847 (1.5611)	grad_norm 16.1189 (inf)	mem 4879MB
[2022-05-31 01:12:33 MetaFG_0] (main.py 265): INFO Train: [20/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2953 (0.3050)	loss 1.1836 (1.5601)	grad_norm 37.5153 (inf)	mem 4879MB
[2022-05-31 01:12:36 MetaFG_0] (main.py 265): INFO Train: [20/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2975 (0.3050)	loss 1.5891 (1.5608)	grad_norm 15.6631 (inf)	mem 4879MB
[2022-05-31 01:12:39 MetaFG_0] (main.py 265): INFO Train: [20/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2917 (0.3050)	loss 1.8711 (1.5611)	grad_norm 24.1774 (inf)	mem 4879MB
[2022-05-31 01:12:42 MetaFG_0] (main.py 265): INFO Train: [20/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2930 (0.3050)	loss 1.6438 (1.5616)	grad_norm 22.3802 (inf)	mem 4879MB
[2022-05-31 01:12:45 MetaFG_0] (main.py 265): INFO Train: [20/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2924 (0.3050)	loss 1.3524 (1.5606)	grad_norm 15.7658 (inf)	mem 4879MB
[2022-05-31 01:12:48 MetaFG_0] (main.py 265): INFO Train: [20/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2946 (0.3050)	loss 1.3444 (1.5602)	grad_norm 22.1746 (inf)	mem 4879MB
[2022-05-31 01:12:51 MetaFG_0] (main.py 265): INFO Train: [20/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2920 (0.3050)	loss 1.2843 (1.5604)	grad_norm 21.3108 (inf)	mem 4879MB
[2022-05-31 01:12:54 MetaFG_0] (main.py 265): INFO Train: [20/300][1080/1562]	eta 0:02:26 lr 0.000006	time 0.2929 (0.3050)	loss 1.7053 (1.5609)	grad_norm 38.9125 (inf)	mem 4879MB
[2022-05-31 01:12:57 MetaFG_0] (main.py 265): INFO Train: [20/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2976 (0.3050)	loss 1.7521 (1.5616)	grad_norm 29.1025 (inf)	mem 4879MB
[2022-05-31 01:13:00 MetaFG_0] (main.py 265): INFO Train: [20/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2918 (0.3050)	loss 1.4862 (1.5619)	grad_norm 16.6073 (inf)	mem 4879MB
[2022-05-31 01:13:03 MetaFG_0] (main.py 265): INFO Train: [20/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2938 (0.3049)	loss 1.3284 (1.5621)	grad_norm 26.1646 (inf)	mem 4879MB
[2022-05-31 01:13:06 MetaFG_0] (main.py 265): INFO Train: [20/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2934 (0.3049)	loss 1.3535 (1.5623)	grad_norm 18.3612 (inf)	mem 4879MB
[2022-05-31 01:13:09 MetaFG_0] (main.py 265): INFO Train: [20/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.3011 (0.3049)	loss 1.8168 (1.5616)	grad_norm 29.5852 (inf)	mem 4879MB
[2022-05-31 01:13:12 MetaFG_0] (main.py 265): INFO Train: [20/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2949 (0.3049)	loss 1.8387 (1.5618)	grad_norm 30.1235 (inf)	mem 4879MB
[2022-05-31 01:13:15 MetaFG_0] (main.py 265): INFO Train: [20/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2924 (0.3049)	loss 1.7301 (1.5632)	grad_norm 20.4022 (inf)	mem 4879MB
[2022-05-31 01:13:18 MetaFG_0] (main.py 265): INFO Train: [20/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2947 (0.3049)	loss 1.9204 (1.5630)	grad_norm 29.3182 (inf)	mem 4879MB
[2022-05-31 01:13:21 MetaFG_0] (main.py 265): INFO Train: [20/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2936 (0.3049)	loss 1.4455 (1.5635)	grad_norm 27.7931 (inf)	mem 4879MB
[2022-05-31 01:13:24 MetaFG_0] (main.py 265): INFO Train: [20/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2938 (0.3049)	loss 1.5641 (1.5639)	grad_norm 27.8774 (inf)	mem 4879MB
[2022-05-31 01:13:27 MetaFG_0] (main.py 265): INFO Train: [20/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2988 (0.3049)	loss 1.4407 (1.5634)	grad_norm 34.6807 (inf)	mem 4879MB
[2022-05-31 01:13:30 MetaFG_0] (main.py 265): INFO Train: [20/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2925 (0.3049)	loss 1.7787 (1.5640)	grad_norm 21.7729 (inf)	mem 4879MB
[2022-05-31 01:13:33 MetaFG_0] (main.py 265): INFO Train: [20/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2932 (0.3049)	loss 1.3430 (1.5635)	grad_norm 30.9578 (inf)	mem 4879MB
[2022-05-31 01:13:36 MetaFG_0] (main.py 265): INFO Train: [20/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3083 (0.3049)	loss 1.4213 (1.5633)	grad_norm 34.4747 (inf)	mem 4879MB
[2022-05-31 01:13:40 MetaFG_0] (main.py 265): INFO Train: [20/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2985 (0.3049)	loss 1.5348 (1.5631)	grad_norm 18.0453 (inf)	mem 4879MB
[2022-05-31 01:13:43 MetaFG_0] (main.py 265): INFO Train: [20/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2935 (0.3049)	loss 1.4896 (1.5625)	grad_norm 29.4126 (inf)	mem 4879MB
[2022-05-31 01:13:46 MetaFG_0] (main.py 265): INFO Train: [20/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2932 (0.3049)	loss 1.8475 (1.5622)	grad_norm 24.3062 (inf)	mem 4879MB
[2022-05-31 01:13:49 MetaFG_0] (main.py 265): INFO Train: [20/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2974 (0.3048)	loss 1.7539 (1.5620)	grad_norm 25.2334 (inf)	mem 4879MB
[2022-05-31 01:13:52 MetaFG_0] (main.py 265): INFO Train: [20/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2995 (0.3048)	loss 1.3308 (1.5617)	grad_norm 45.3951 (inf)	mem 4879MB
[2022-05-31 01:13:55 MetaFG_0] (main.py 265): INFO Train: [20/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2993 (0.3048)	loss 1.1967 (1.5610)	grad_norm 28.8294 (inf)	mem 4879MB
[2022-05-31 01:13:58 MetaFG_0] (main.py 265): INFO Train: [20/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2983 (0.3048)	loss 1.5675 (1.5608)	grad_norm 24.7553 (inf)	mem 4879MB
[2022-05-31 01:14:01 MetaFG_0] (main.py 265): INFO Train: [20/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2937 (0.3048)	loss 1.5611 (1.5606)	grad_norm 30.5163 (inf)	mem 4879MB
[2022-05-31 01:14:04 MetaFG_0] (main.py 265): INFO Train: [20/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2919 (0.3048)	loss 1.4274 (1.5601)	grad_norm 29.6334 (inf)	mem 4879MB
[2022-05-31 01:14:07 MetaFG_0] (main.py 265): INFO Train: [20/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2990 (0.3048)	loss 1.8402 (1.5598)	grad_norm 17.8097 (inf)	mem 4879MB
[2022-05-31 01:14:10 MetaFG_0] (main.py 265): INFO Train: [20/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2934 (0.3048)	loss 1.6335 (1.5600)	grad_norm 21.1262 (inf)	mem 4879MB
[2022-05-31 01:14:13 MetaFG_0] (main.py 265): INFO Train: [20/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2948 (0.3048)	loss 1.8272 (1.5606)	grad_norm 37.7929 (inf)	mem 4879MB
[2022-05-31 01:14:16 MetaFG_0] (main.py 265): INFO Train: [20/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3027 (0.3048)	loss 1.4961 (1.5606)	grad_norm 27.4064 (inf)	mem 4879MB
[2022-05-31 01:14:19 MetaFG_0] (main.py 265): INFO Train: [20/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3031 (0.3048)	loss 1.4217 (1.5605)	grad_norm 22.2460 (inf)	mem 4879MB
[2022-05-31 01:14:22 MetaFG_0] (main.py 265): INFO Train: [20/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2931 (0.3048)	loss 1.7513 (1.5609)	grad_norm 25.3658 (inf)	mem 4879MB
[2022-05-31 01:14:25 MetaFG_0] (main.py 265): INFO Train: [20/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2920 (0.3047)	loss 1.7863 (1.5603)	grad_norm 17.3538 (inf)	mem 4879MB
[2022-05-31 01:14:28 MetaFG_0] (main.py 265): INFO Train: [20/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2917 (0.3047)	loss 1.7104 (1.5598)	grad_norm 12.3442 (inf)	mem 4879MB
[2022-05-31 01:14:31 MetaFG_0] (main.py 265): INFO Train: [20/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2926 (0.3047)	loss 1.6836 (1.5608)	grad_norm 23.8810 (inf)	mem 4879MB
[2022-05-31 01:14:34 MetaFG_0] (main.py 265): INFO Train: [20/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2936 (0.3047)	loss 1.7358 (1.5609)	grad_norm 18.9194 (inf)	mem 4879MB
[2022-05-31 01:14:37 MetaFG_0] (main.py 265): INFO Train: [20/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2918 (0.3047)	loss 1.8359 (1.5604)	grad_norm 23.3622 (inf)	mem 4879MB
[2022-05-31 01:14:40 MetaFG_0] (main.py 265): INFO Train: [20/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2934 (0.3047)	loss 1.6494 (1.5605)	grad_norm 23.6488 (inf)	mem 4879MB
[2022-05-31 01:14:43 MetaFG_0] (main.py 265): INFO Train: [20/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2958 (0.3047)	loss 1.7773 (1.5607)	grad_norm 28.1623 (inf)	mem 4879MB
[2022-05-31 01:14:46 MetaFG_0] (main.py 265): INFO Train: [20/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2979 (0.3047)	loss 1.7468 (1.5598)	grad_norm 26.9826 (inf)	mem 4879MB
[2022-05-31 01:14:49 MetaFG_0] (main.py 265): INFO Train: [20/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2960 (0.3047)	loss 1.6753 (1.5589)	grad_norm 28.8835 (inf)	mem 4879MB
[2022-05-31 01:14:52 MetaFG_0] (main.py 265): INFO Train: [20/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2922 (0.3047)	loss 1.6393 (1.5587)	grad_norm 28.2926 (inf)	mem 4879MB
[2022-05-31 01:14:55 MetaFG_0] (main.py 265): INFO Train: [20/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2937 (0.3047)	loss 1.7078 (1.5591)	grad_norm 29.0244 (inf)	mem 4879MB
[2022-05-31 01:14:59 MetaFG_0] (main.py 265): INFO Train: [20/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2939 (0.3047)	loss 1.5567 (1.5595)	grad_norm 25.8709 (inf)	mem 4879MB
[2022-05-31 01:15:02 MetaFG_0] (main.py 265): INFO Train: [20/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2939 (0.3047)	loss 1.5336 (1.5597)	grad_norm 25.1573 (inf)	mem 4879MB
[2022-05-31 01:15:05 MetaFG_0] (main.py 265): INFO Train: [20/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2921 (0.3047)	loss 1.3199 (1.5595)	grad_norm 39.4728 (inf)	mem 4879MB
[2022-05-31 01:15:08 MetaFG_0] (main.py 265): INFO Train: [20/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2986 (0.3047)	loss 1.1416 (1.5597)	grad_norm 21.9078 (inf)	mem 4879MB
[2022-05-31 01:15:11 MetaFG_0] (main.py 265): INFO Train: [20/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2995 (0.3047)	loss 1.6278 (1.5599)	grad_norm 19.9132 (inf)	mem 4879MB
[2022-05-31 01:15:14 MetaFG_0] (main.py 265): INFO Train: [20/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2977 (0.3047)	loss 1.5568 (1.5602)	grad_norm 20.7600 (inf)	mem 4879MB
[2022-05-31 01:15:17 MetaFG_0] (main.py 265): INFO Train: [20/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3004 (0.3047)	loss 1.4608 (1.5604)	grad_norm 24.6335 (inf)	mem 4879MB
[2022-05-31 01:15:20 MetaFG_0] (main.py 265): INFO Train: [20/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2913 (0.3047)	loss 1.5281 (1.5606)	grad_norm 26.7646 (inf)	mem 4879MB
[2022-05-31 01:15:20 MetaFG_0] (main.py 272): INFO EPOCH 20 training takes 0:07:56
[2022-05-31 01:15:20 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_20.pth saving......
[2022-05-31 01:15:21 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_20.pth saved !!!
[2022-05-31 01:15:21 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:15:23 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:15:23 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:15:23 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.683 (0.683)	Loss 0.9641 (0.9641)	Acc@1 90.625 (90.625)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 01:15:24 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.149)	Loss 0.9036 (1.0547)	Acc@1 81.250 (76.136)	Acc@5 100.000 (96.591)	Mem 4879MB
[2022-05-31 01:15:25 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.123)	Loss 1.0007 (1.0453)	Acc@1 75.000 (77.083)	Acc@5 100.000 (96.726)	Mem 4879MB
[2022-05-31 01:15:26 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.102 (0.114)	Loss 0.7473 (1.0216)	Acc@1 87.500 (77.621)	Acc@5 100.000 (96.673)	Mem 4879MB
[2022-05-31 01:15:27 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.099 (0.110)	Loss 1.0158 (0.9989)	Acc@1 87.500 (79.040)	Acc@5 96.875 (97.180)	Mem 4879MB
[2022-05-31 01:15:28 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.106)	Loss 0.7256 (0.9879)	Acc@1 93.750 (78.799)	Acc@5 100.000 (97.549)	Mem 4879MB
[2022-05-31 01:15:29 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.099 (0.105)	Loss 0.9194 (0.9906)	Acc@1 84.375 (78.586)	Acc@5 96.875 (97.285)	Mem 4879MB
[2022-05-31 01:15:30 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.8789 (0.9961)	Acc@1 87.500 (78.917)	Acc@5 100.000 (97.139)	Mem 4879MB
[2022-05-31 01:15:31 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.103)	Loss 0.9385 (0.9957)	Acc@1 81.250 (78.704)	Acc@5 96.875 (97.261)	Mem 4879MB
[2022-05-31 01:15:32 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.102)	Loss 0.9234 (0.9988)	Acc@1 78.125 (78.571)	Acc@5 96.875 (97.287)	Mem 4879MB
[2022-05-31 01:15:33 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.095 (0.101)	Loss 1.3868 (1.0040)	Acc@1 59.375 (78.249)	Acc@5 90.625 (97.308)	Mem 4879MB
[2022-05-31 01:15:34 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.095 (0.100)	Loss 1.4348 (1.0120)	Acc@1 62.500 (78.069)	Acc@5 90.625 (97.157)	Mem 4879MB
[2022-05-31 01:15:35 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 1.2275 (1.0102)	Acc@1 65.625 (77.815)	Acc@5 90.625 (97.133)	Mem 4879MB
[2022-05-31 01:15:36 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.108 (0.100)	Loss 1.1456 (1.0063)	Acc@1 68.750 (78.030)	Acc@5 93.750 (97.209)	Mem 4879MB
[2022-05-31 01:15:37 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 1.0439 (1.0012)	Acc@1 78.125 (78.191)	Acc@5 93.750 (97.185)	Mem 4879MB
[2022-05-31 01:15:38 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.099)	Loss 0.9566 (0.9991)	Acc@1 75.000 (78.125)	Acc@5 96.875 (97.185)	Mem 4879MB
[2022-05-31 01:15:39 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.095 (0.099)	Loss 1.0205 (1.0026)	Acc@1 71.875 (77.892)	Acc@5 96.875 (97.205)	Mem 4879MB
[2022-05-31 01:15:39 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.090 (0.098)	Loss 0.9666 (1.0022)	Acc@1 78.125 (77.906)	Acc@5 100.000 (97.259)	Mem 4879MB
[2022-05-31 01:15:40 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 0.8852 (1.0014)	Acc@1 84.375 (77.831)	Acc@5 100.000 (97.238)	Mem 4879MB
[2022-05-31 01:15:41 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.7933 (0.9966)	Acc@1 90.625 (78.076)	Acc@5 100.000 (97.268)	Mem 4879MB
[2022-05-31 01:15:42 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.098)	Loss 0.9567 (0.9977)	Acc@1 78.125 (77.985)	Acc@5 100.000 (97.279)	Mem 4879MB
[2022-05-31 01:15:43 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.092 (0.097)	Loss 1.1046 (0.9987)	Acc@1 65.625 (78.021)	Acc@5 100.000 (97.260)	Mem 4879MB
[2022-05-31 01:15:44 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 1.3608 (0.9968)	Acc@1 75.000 (78.182)	Acc@5 93.750 (97.257)	Mem 4879MB
[2022-05-31 01:15:45 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.099 (0.097)	Loss 1.1010 (0.9958)	Acc@1 78.125 (78.220)	Acc@5 93.750 (97.213)	Mem 4879MB
[2022-05-31 01:15:46 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.098 (0.097)	Loss 1.0352 (0.9996)	Acc@1 65.625 (77.995)	Acc@5 96.875 (97.173)	Mem 4879MB
[2022-05-31 01:15:47 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.097)	Loss 0.9432 (1.0055)	Acc@1 78.125 (77.826)	Acc@5 96.875 (97.099)	Mem 4879MB
[2022-05-31 01:15:48 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 1.0295 (1.0053)	Acc@1 71.875 (77.790)	Acc@5 100.000 (97.162)	Mem 4879MB
[2022-05-31 01:15:49 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.088 (0.097)	Loss 1.4567 (1.0037)	Acc@1 65.625 (77.825)	Acc@5 90.625 (97.175)	Mem 4879MB
[2022-05-31 01:15:50 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 1.0223 (1.0063)	Acc@1 78.125 (77.680)	Acc@5 100.000 (97.142)	Mem 4879MB
[2022-05-31 01:15:51 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 0.8747 (1.0033)	Acc@1 78.125 (77.835)	Acc@5 100.000 (97.165)	Mem 4879MB
[2022-05-31 01:15:52 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 1.2891 (1.0026)	Acc@1 68.750 (77.793)	Acc@5 93.750 (97.166)	Mem 4879MB
[2022-05-31 01:15:53 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8831 (1.0015)	Acc@1 87.500 (77.763)	Acc@5 100.000 (97.166)	Mem 4879MB
[2022-05-31 01:15:53 MetaFG_0] (main.py 330): INFO  * Acc@1 77.750 Acc@5 97.170
[2022-05-31 01:15:53 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 77.8%
[2022-05-31 01:15:53 MetaFG_0] (main.py 171): INFO Max accuracy: 77.75%
[2022-05-31 01:15:54 MetaFG_0] (main.py 265): INFO Train: [21/300][0/1562]	eta 0:26:30 lr 0.000006	time 1.0180 (1.0180)	loss 1.1189 (1.1189)	grad_norm 15.3843 (15.3843)	mem 4879MB
[2022-05-31 01:15:57 MetaFG_0] (main.py 265): INFO Train: [21/300][10/1562]	eta 0:09:42 lr 0.000006	time 0.2923 (0.3752)	loss 1.7409 (1.4844)	grad_norm 19.6591 (23.2125)	mem 4879MB
[2022-05-31 01:16:00 MetaFG_0] (main.py 265): INFO Train: [21/300][20/1562]	eta 0:08:46 lr 0.000006	time 0.2929 (0.3414)	loss 1.4617 (1.5561)	grad_norm 21.6236 (23.5929)	mem 4879MB
[2022-05-31 01:16:03 MetaFG_0] (main.py 265): INFO Train: [21/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2932 (0.3293)	loss 1.4158 (1.5638)	grad_norm 28.7902 (26.4030)	mem 4879MB
[2022-05-31 01:16:06 MetaFG_0] (main.py 265): INFO Train: [21/300][40/1562]	eta 0:08:11 lr 0.000006	time 0.2990 (0.3232)	loss 1.6080 (1.5708)	grad_norm 21.7341 (26.9462)	mem 4879MB
[2022-05-31 01:16:09 MetaFG_0] (main.py 265): INFO Train: [21/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2991 (0.3198)	loss 1.8819 (1.5999)	grad_norm 23.6533 (26.6762)	mem 4879MB
[2022-05-31 01:16:12 MetaFG_0] (main.py 265): INFO Train: [21/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2924 (0.3172)	loss 1.4387 (1.6008)	grad_norm 31.6383 (26.7806)	mem 4879MB
[2022-05-31 01:16:15 MetaFG_0] (main.py 265): INFO Train: [21/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2915 (0.3153)	loss 1.4901 (1.5862)	grad_norm 17.4142 (26.4817)	mem 4879MB
[2022-05-31 01:16:18 MetaFG_0] (main.py 265): INFO Train: [21/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2919 (0.3139)	loss 1.4430 (1.5719)	grad_norm 31.6073 (27.0988)	mem 4879MB
[2022-05-31 01:16:21 MetaFG_0] (main.py 265): INFO Train: [21/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2933 (0.3127)	loss 1.5009 (1.5614)	grad_norm 33.2007 (27.0229)	mem 4879MB
[2022-05-31 01:16:24 MetaFG_0] (main.py 265): INFO Train: [21/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2956 (0.3117)	loss 1.5018 (1.5600)	grad_norm 15.9146 (26.8955)	mem 4879MB
[2022-05-31 01:16:27 MetaFG_0] (main.py 265): INFO Train: [21/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.3013 (0.3110)	loss 1.6208 (1.5574)	grad_norm 37.9664 (27.2482)	mem 4879MB
[2022-05-31 01:16:30 MetaFG_0] (main.py 265): INFO Train: [21/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2932 (0.3104)	loss 1.7196 (1.5560)	grad_norm 21.8711 (27.2123)	mem 4879MB
[2022-05-31 01:16:33 MetaFG_0] (main.py 265): INFO Train: [21/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2927 (0.3098)	loss 1.6270 (1.5519)	grad_norm 27.0481 (27.1524)	mem 4879MB
[2022-05-31 01:16:36 MetaFG_0] (main.py 265): INFO Train: [21/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.3030 (0.3096)	loss 1.7689 (1.5591)	grad_norm 24.8736 (27.0996)	mem 4879MB
[2022-05-31 01:16:40 MetaFG_0] (main.py 265): INFO Train: [21/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2917 (0.3092)	loss 1.3844 (1.5541)	grad_norm 18.4116 (27.2690)	mem 4879MB
[2022-05-31 01:16:43 MetaFG_0] (main.py 265): INFO Train: [21/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2987 (0.3090)	loss 1.1800 (1.5435)	grad_norm 28.3460 (27.2182)	mem 4879MB
[2022-05-31 01:16:46 MetaFG_0] (main.py 265): INFO Train: [21/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2917 (0.3087)	loss 1.8452 (1.5464)	grad_norm 26.3257 (27.0813)	mem 4879MB
[2022-05-31 01:16:49 MetaFG_0] (main.py 265): INFO Train: [21/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2921 (0.3084)	loss 1.1846 (1.5392)	grad_norm 33.4903 (27.2570)	mem 4879MB
[2022-05-31 01:16:52 MetaFG_0] (main.py 265): INFO Train: [21/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2986 (0.3082)	loss 1.3406 (1.5412)	grad_norm 80.8899 (27.4979)	mem 4879MB
[2022-05-31 01:16:55 MetaFG_0] (main.py 265): INFO Train: [21/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2935 (0.3080)	loss 1.4909 (1.5411)	grad_norm 37.7572 (27.7221)	mem 4879MB
[2022-05-31 01:16:58 MetaFG_0] (main.py 265): INFO Train: [21/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2933 (0.3079)	loss 1.6458 (1.5434)	grad_norm 26.2806 (27.6482)	mem 4879MB
[2022-05-31 01:17:01 MetaFG_0] (main.py 265): INFO Train: [21/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2940 (0.3077)	loss 1.8517 (1.5426)	grad_norm 20.9551 (27.5386)	mem 4879MB
[2022-05-31 01:17:04 MetaFG_0] (main.py 265): INFO Train: [21/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2983 (0.3075)	loss 1.3631 (1.5453)	grad_norm 23.3721 (27.5525)	mem 4879MB
[2022-05-31 01:17:07 MetaFG_0] (main.py 265): INFO Train: [21/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2931 (0.3074)	loss 1.5567 (1.5469)	grad_norm 20.5018 (27.5204)	mem 4879MB
[2022-05-31 01:17:10 MetaFG_0] (main.py 265): INFO Train: [21/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2971 (0.3074)	loss 1.4963 (1.5503)	grad_norm 15.6542 (27.3483)	mem 4879MB
[2022-05-31 01:17:13 MetaFG_0] (main.py 265): INFO Train: [21/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2990 (0.3072)	loss 1.4340 (1.5528)	grad_norm 27.8643 (27.3018)	mem 4879MB
[2022-05-31 01:17:16 MetaFG_0] (main.py 265): INFO Train: [21/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2921 (0.3071)	loss 1.4060 (1.5502)	grad_norm 42.5936 (27.3895)	mem 4879MB
[2022-05-31 01:17:19 MetaFG_0] (main.py 265): INFO Train: [21/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2980 (0.3070)	loss 1.5845 (1.5526)	grad_norm 27.3916 (27.2344)	mem 4879MB
[2022-05-31 01:17:22 MetaFG_0] (main.py 265): INFO Train: [21/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2978 (0.3070)	loss 1.6000 (1.5553)	grad_norm 22.3967 (27.1968)	mem 4879MB
[2022-05-31 01:17:25 MetaFG_0] (main.py 265): INFO Train: [21/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2982 (0.3069)	loss 1.2997 (1.5537)	grad_norm 15.7826 (27.1659)	mem 4879MB
[2022-05-31 01:17:28 MetaFG_0] (main.py 265): INFO Train: [21/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2920 (0.3068)	loss 1.9379 (1.5579)	grad_norm 27.3962 (27.2477)	mem 4879MB
[2022-05-31 01:17:31 MetaFG_0] (main.py 265): INFO Train: [21/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2982 (0.3068)	loss 1.6369 (1.5564)	grad_norm 27.8925 (27.2058)	mem 4879MB
[2022-05-31 01:17:34 MetaFG_0] (main.py 265): INFO Train: [21/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2926 (0.3067)	loss 1.5569 (1.5557)	grad_norm 34.3380 (27.2468)	mem 4879MB
[2022-05-31 01:17:37 MetaFG_0] (main.py 265): INFO Train: [21/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2968 (0.3067)	loss 1.4018 (1.5515)	grad_norm 31.3314 (27.3163)	mem 4879MB
[2022-05-31 01:17:40 MetaFG_0] (main.py 265): INFO Train: [21/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2939 (0.3067)	loss 1.6010 (1.5481)	grad_norm 31.9432 (27.3268)	mem 4879MB
[2022-05-31 01:17:44 MetaFG_0] (main.py 265): INFO Train: [21/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2990 (0.3066)	loss 1.5630 (1.5492)	grad_norm 25.8276 (27.3744)	mem 4879MB
[2022-05-31 01:17:47 MetaFG_0] (main.py 265): INFO Train: [21/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2973 (0.3066)	loss 1.2289 (1.5501)	grad_norm 34.8895 (27.3669)	mem 4879MB
[2022-05-31 01:17:50 MetaFG_0] (main.py 265): INFO Train: [21/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.3009 (0.3065)	loss 1.7669 (1.5502)	grad_norm 28.5203 (27.2822)	mem 4879MB
[2022-05-31 01:17:53 MetaFG_0] (main.py 265): INFO Train: [21/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2999 (0.3065)	loss 1.8681 (1.5522)	grad_norm 19.5951 (27.2555)	mem 4879MB
[2022-05-31 01:17:56 MetaFG_0] (main.py 265): INFO Train: [21/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2982 (0.3064)	loss 1.6596 (1.5534)	grad_norm 59.8942 (27.3469)	mem 4879MB
[2022-05-31 01:17:59 MetaFG_0] (main.py 265): INFO Train: [21/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2975 (0.3064)	loss 1.5114 (1.5516)	grad_norm 26.8513 (27.3357)	mem 4879MB
[2022-05-31 01:18:02 MetaFG_0] (main.py 265): INFO Train: [21/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2995 (0.3063)	loss 1.3703 (1.5489)	grad_norm 22.1191 (27.4582)	mem 4879MB
[2022-05-31 01:18:05 MetaFG_0] (main.py 265): INFO Train: [21/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2978 (0.3063)	loss 1.4886 (1.5489)	grad_norm 27.5707 (27.5550)	mem 4879MB
[2022-05-31 01:18:08 MetaFG_0] (main.py 265): INFO Train: [21/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2925 (0.3062)	loss 1.8510 (1.5520)	grad_norm 15.2780 (27.4811)	mem 4879MB
[2022-05-31 01:18:11 MetaFG_0] (main.py 265): INFO Train: [21/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.3002 (0.3062)	loss 1.7706 (1.5513)	grad_norm 23.7682 (27.5141)	mem 4879MB
[2022-05-31 01:18:14 MetaFG_0] (main.py 265): INFO Train: [21/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2965 (0.3062)	loss 1.7197 (1.5516)	grad_norm 21.5169 (27.4915)	mem 4879MB
[2022-05-31 01:18:17 MetaFG_0] (main.py 265): INFO Train: [21/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2990 (0.3061)	loss 1.2773 (1.5510)	grad_norm 29.2018 (27.3910)	mem 4879MB
[2022-05-31 01:18:20 MetaFG_0] (main.py 265): INFO Train: [21/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2982 (0.3061)	loss 1.8804 (1.5515)	grad_norm 35.5582 (27.3092)	mem 4879MB
[2022-05-31 01:18:23 MetaFG_0] (main.py 265): INFO Train: [21/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.3009 (0.3060)	loss 1.5189 (1.5522)	grad_norm 43.7352 (27.3199)	mem 4879MB
[2022-05-31 01:18:26 MetaFG_0] (main.py 265): INFO Train: [21/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2918 (0.3060)	loss 1.4595 (1.5529)	grad_norm 15.4634 (27.1918)	mem 4879MB
[2022-05-31 01:18:29 MetaFG_0] (main.py 265): INFO Train: [21/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2999 (0.3060)	loss 1.4574 (1.5517)	grad_norm 18.2285 (27.2037)	mem 4879MB
[2022-05-31 01:18:32 MetaFG_0] (main.py 265): INFO Train: [21/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2922 (0.3060)	loss 1.7697 (1.5543)	grad_norm 19.3528 (27.1765)	mem 4879MB
[2022-05-31 01:18:35 MetaFG_0] (main.py 265): INFO Train: [21/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2920 (0.3059)	loss 1.2133 (1.5551)	grad_norm 17.6530 (27.0900)	mem 4879MB
[2022-05-31 01:18:38 MetaFG_0] (main.py 265): INFO Train: [21/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2965 (0.3059)	loss 1.3315 (1.5544)	grad_norm 19.0694 (27.0549)	mem 4879MB
[2022-05-31 01:18:41 MetaFG_0] (main.py 265): INFO Train: [21/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.3005 (0.3058)	loss 1.7239 (1.5556)	grad_norm 30.1993 (27.0752)	mem 4879MB
[2022-05-31 01:18:44 MetaFG_0] (main.py 265): INFO Train: [21/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2924 (0.3058)	loss 1.7975 (1.5553)	grad_norm 18.9256 (27.1411)	mem 4879MB
[2022-05-31 01:18:47 MetaFG_0] (main.py 265): INFO Train: [21/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2984 (0.3058)	loss 1.3349 (1.5514)	grad_norm 50.7449 (27.1639)	mem 4879MB
[2022-05-31 01:18:50 MetaFG_0] (main.py 265): INFO Train: [21/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2954 (0.3057)	loss 1.4304 (1.5524)	grad_norm 20.3125 (27.0751)	mem 4879MB
[2022-05-31 01:18:54 MetaFG_0] (main.py 265): INFO Train: [21/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2989 (0.3057)	loss 1.8344 (1.5529)	grad_norm 47.6044 (27.1644)	mem 4879MB
[2022-05-31 01:18:57 MetaFG_0] (main.py 265): INFO Train: [21/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2986 (0.3057)	loss 1.5210 (1.5524)	grad_norm 17.6623 (27.1559)	mem 4879MB
[2022-05-31 01:19:00 MetaFG_0] (main.py 265): INFO Train: [21/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2917 (0.3057)	loss 1.2454 (1.5500)	grad_norm 20.0500 (27.1413)	mem 4879MB
[2022-05-31 01:19:03 MetaFG_0] (main.py 265): INFO Train: [21/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2932 (0.3056)	loss 1.5405 (1.5493)	grad_norm 14.4715 (27.0357)	mem 4879MB
[2022-05-31 01:19:06 MetaFG_0] (main.py 265): INFO Train: [21/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2973 (0.3057)	loss 1.4216 (1.5481)	grad_norm 24.0374 (27.0179)	mem 4879MB
[2022-05-31 01:19:09 MetaFG_0] (main.py 265): INFO Train: [21/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2986 (0.3056)	loss 1.5708 (1.5473)	grad_norm 32.0997 (27.0533)	mem 4879MB
[2022-05-31 01:19:12 MetaFG_0] (main.py 265): INFO Train: [21/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2924 (0.3056)	loss 1.6041 (1.5469)	grad_norm 40.4090 (27.1329)	mem 4879MB
[2022-05-31 01:19:15 MetaFG_0] (main.py 265): INFO Train: [21/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2921 (0.3056)	loss 1.6933 (1.5468)	grad_norm 21.7211 (27.1504)	mem 4879MB
[2022-05-31 01:19:18 MetaFG_0] (main.py 265): INFO Train: [21/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2985 (0.3056)	loss 1.3134 (1.5484)	grad_norm 14.3570 (27.1606)	mem 4879MB
[2022-05-31 01:19:21 MetaFG_0] (main.py 265): INFO Train: [21/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2915 (0.3055)	loss 1.7636 (1.5505)	grad_norm 32.4023 (27.1558)	mem 4879MB
[2022-05-31 01:19:24 MetaFG_0] (main.py 265): INFO Train: [21/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2980 (0.3055)	loss 1.4925 (1.5498)	grad_norm 21.5813 (27.1041)	mem 4879MB
[2022-05-31 01:19:27 MetaFG_0] (main.py 265): INFO Train: [21/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2930 (0.3055)	loss 1.6003 (1.5516)	grad_norm 44.8399 (27.1297)	mem 4879MB
[2022-05-31 01:19:30 MetaFG_0] (main.py 265): INFO Train: [21/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2983 (0.3055)	loss 1.5279 (1.5517)	grad_norm 26.7816 (27.1028)	mem 4879MB
[2022-05-31 01:19:33 MetaFG_0] (main.py 265): INFO Train: [21/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.3005 (0.3055)	loss 1.6477 (1.5519)	grad_norm 25.9031 (27.1199)	mem 4879MB
[2022-05-31 01:19:36 MetaFG_0] (main.py 265): INFO Train: [21/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2991 (0.3055)	loss 1.5006 (1.5522)	grad_norm 27.9475 (27.0714)	mem 4879MB
[2022-05-31 01:19:39 MetaFG_0] (main.py 265): INFO Train: [21/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2921 (0.3055)	loss 1.2340 (1.5521)	grad_norm 27.2103 (27.0695)	mem 4879MB
[2022-05-31 01:19:42 MetaFG_0] (main.py 265): INFO Train: [21/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2926 (0.3055)	loss 1.5655 (1.5524)	grad_norm 27.4040 (27.0718)	mem 4879MB
[2022-05-31 01:19:45 MetaFG_0] (main.py 265): INFO Train: [21/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2925 (0.3055)	loss 1.1778 (1.5521)	grad_norm 31.8940 (27.0210)	mem 4879MB
[2022-05-31 01:19:48 MetaFG_0] (main.py 265): INFO Train: [21/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2914 (0.3055)	loss 1.4964 (1.5526)	grad_norm 18.9906 (27.0278)	mem 4879MB
[2022-05-31 01:19:51 MetaFG_0] (main.py 265): INFO Train: [21/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2923 (0.3054)	loss 1.2709 (1.5533)	grad_norm 22.2486 (27.0729)	mem 4879MB
[2022-05-31 01:19:54 MetaFG_0] (main.py 265): INFO Train: [21/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2921 (0.3054)	loss 1.7014 (1.5534)	grad_norm 36.2771 (27.0980)	mem 4879MB
[2022-05-31 01:19:57 MetaFG_0] (main.py 265): INFO Train: [21/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2935 (0.3054)	loss 1.6610 (1.5544)	grad_norm 35.3124 (27.1047)	mem 4879MB
[2022-05-31 01:20:01 MetaFG_0] (main.py 265): INFO Train: [21/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2946 (0.3054)	loss 1.8012 (1.5550)	grad_norm 26.2178 (27.0867)	mem 4879MB
[2022-05-31 01:20:04 MetaFG_0] (main.py 265): INFO Train: [21/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2938 (0.3054)	loss 1.8427 (1.5553)	grad_norm 38.6327 (27.1380)	mem 4879MB
[2022-05-31 01:20:07 MetaFG_0] (main.py 265): INFO Train: [21/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2922 (0.3054)	loss 1.1714 (1.5563)	grad_norm 28.8794 (27.1370)	mem 4879MB
[2022-05-31 01:20:10 MetaFG_0] (main.py 265): INFO Train: [21/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2999 (0.3054)	loss 1.1217 (1.5547)	grad_norm 26.5459 (27.1348)	mem 4879MB
[2022-05-31 01:20:13 MetaFG_0] (main.py 265): INFO Train: [21/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2922 (0.3054)	loss 1.5233 (1.5538)	grad_norm 22.2090 (27.1272)	mem 4879MB
[2022-05-31 01:20:16 MetaFG_0] (main.py 265): INFO Train: [21/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2982 (0.3053)	loss 1.3753 (1.5538)	grad_norm 31.4436 (27.1236)	mem 4879MB
[2022-05-31 01:20:19 MetaFG_0] (main.py 265): INFO Train: [21/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2984 (0.3053)	loss 1.6021 (1.5532)	grad_norm 24.9775 (27.1497)	mem 4879MB
[2022-05-31 01:20:22 MetaFG_0] (main.py 265): INFO Train: [21/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2935 (0.3053)	loss 1.6626 (1.5534)	grad_norm 24.6875 (27.1461)	mem 4879MB
[2022-05-31 01:20:25 MetaFG_0] (main.py 265): INFO Train: [21/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2916 (0.3053)	loss 1.4580 (1.5518)	grad_norm 20.0705 (27.1669)	mem 4879MB
[2022-05-31 01:20:28 MetaFG_0] (main.py 265): INFO Train: [21/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2973 (0.3053)	loss 1.2461 (1.5513)	grad_norm 22.9012 (27.1848)	mem 4879MB
[2022-05-31 01:20:31 MetaFG_0] (main.py 265): INFO Train: [21/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2932 (0.3053)	loss 1.8093 (1.5521)	grad_norm 17.9961 (27.1787)	mem 4879MB
[2022-05-31 01:20:34 MetaFG_0] (main.py 265): INFO Train: [21/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2991 (0.3053)	loss 1.2514 (1.5501)	grad_norm 31.3426 (27.2164)	mem 4879MB
[2022-05-31 01:20:37 MetaFG_0] (main.py 265): INFO Train: [21/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2963 (0.3053)	loss 1.6410 (1.5505)	grad_norm 29.7338 (27.2107)	mem 4879MB
[2022-05-31 01:20:40 MetaFG_0] (main.py 265): INFO Train: [21/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2991 (0.3052)	loss 1.6756 (1.5517)	grad_norm 13.1491 (27.2324)	mem 4879MB
[2022-05-31 01:20:43 MetaFG_0] (main.py 265): INFO Train: [21/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2915 (0.3052)	loss 1.5287 (1.5516)	grad_norm 17.9312 (27.2266)	mem 4879MB
[2022-05-31 01:20:46 MetaFG_0] (main.py 265): INFO Train: [21/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2920 (0.3052)	loss 1.0947 (1.5502)	grad_norm 28.6100 (27.2620)	mem 4879MB
[2022-05-31 01:20:49 MetaFG_0] (main.py 265): INFO Train: [21/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2917 (0.3052)	loss 1.7734 (1.5498)	grad_norm 32.4833 (27.2755)	mem 4879MB
[2022-05-31 01:20:52 MetaFG_0] (main.py 265): INFO Train: [21/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2929 (0.3052)	loss 1.1548 (1.5485)	grad_norm 36.5757 (27.2957)	mem 4879MB
[2022-05-31 01:20:55 MetaFG_0] (main.py 265): INFO Train: [21/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2984 (0.3052)	loss 1.7124 (1.5479)	grad_norm 19.2346 (27.2644)	mem 4879MB
[2022-05-31 01:20:58 MetaFG_0] (main.py 265): INFO Train: [21/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2954 (0.3052)	loss 1.8472 (1.5488)	grad_norm 37.7517 (27.3174)	mem 4879MB
[2022-05-31 01:21:01 MetaFG_0] (main.py 265): INFO Train: [21/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2979 (0.3051)	loss 1.3964 (1.5486)	grad_norm 26.1545 (27.3071)	mem 4879MB
[2022-05-31 01:21:04 MetaFG_0] (main.py 265): INFO Train: [21/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2932 (0.3051)	loss 1.0792 (1.5489)	grad_norm 39.1318 (27.3089)	mem 4879MB
[2022-05-31 01:21:07 MetaFG_0] (main.py 265): INFO Train: [21/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2972 (0.3051)	loss 1.8112 (1.5493)	grad_norm 26.9926 (27.3134)	mem 4879MB
[2022-05-31 01:21:10 MetaFG_0] (main.py 265): INFO Train: [21/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2919 (0.3051)	loss 1.6433 (1.5494)	grad_norm 15.6012 (27.3246)	mem 4879MB
[2022-05-31 01:21:14 MetaFG_0] (main.py 265): INFO Train: [21/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2988 (0.3051)	loss 1.3856 (1.5497)	grad_norm 27.2517 (27.2638)	mem 4879MB
[2022-05-31 01:21:17 MetaFG_0] (main.py 265): INFO Train: [21/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2988 (0.3051)	loss 1.3511 (1.5507)	grad_norm 26.5985 (27.2709)	mem 4879MB
[2022-05-31 01:21:20 MetaFG_0] (main.py 265): INFO Train: [21/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2920 (0.3051)	loss 1.6375 (1.5514)	grad_norm 16.4638 (27.3207)	mem 4879MB
[2022-05-31 01:21:23 MetaFG_0] (main.py 265): INFO Train: [21/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3006 (0.3051)	loss 1.2158 (1.5512)	grad_norm 16.7287 (27.3100)	mem 4879MB
[2022-05-31 01:21:26 MetaFG_0] (main.py 265): INFO Train: [21/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2931 (0.3051)	loss 1.4435 (1.5510)	grad_norm 36.1980 (27.3446)	mem 4879MB
[2022-05-31 01:21:29 MetaFG_0] (main.py 265): INFO Train: [21/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2965 (0.3051)	loss 1.7189 (1.5509)	grad_norm 47.9239 (27.3155)	mem 4879MB
[2022-05-31 01:21:32 MetaFG_0] (main.py 265): INFO Train: [21/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.3006 (0.3051)	loss 1.6858 (1.5495)	grad_norm 27.8837 (27.2931)	mem 4879MB
[2022-05-31 01:21:35 MetaFG_0] (main.py 265): INFO Train: [21/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2999 (0.3051)	loss 1.5299 (1.5484)	grad_norm 29.0779 (27.3198)	mem 4879MB
[2022-05-31 01:21:38 MetaFG_0] (main.py 265): INFO Train: [21/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2934 (0.3051)	loss 1.3523 (1.5486)	grad_norm 23.5080 (27.3152)	mem 4879MB
[2022-05-31 01:21:41 MetaFG_0] (main.py 265): INFO Train: [21/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.3009 (0.3051)	loss 1.4362 (1.5485)	grad_norm 20.2831 (27.3078)	mem 4879MB
[2022-05-31 01:21:44 MetaFG_0] (main.py 265): INFO Train: [21/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2950 (0.3051)	loss 1.2598 (1.5472)	grad_norm 26.8238 (27.3513)	mem 4879MB
[2022-05-31 01:21:47 MetaFG_0] (main.py 265): INFO Train: [21/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2932 (0.3051)	loss 1.8936 (1.5474)	grad_norm 31.8653 (27.4205)	mem 4879MB
[2022-05-31 01:21:50 MetaFG_0] (main.py 265): INFO Train: [21/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2925 (0.3050)	loss 1.4643 (1.5471)	grad_norm 24.9292 (27.4600)	mem 4879MB
[2022-05-31 01:21:53 MetaFG_0] (main.py 265): INFO Train: [21/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2938 (0.3050)	loss 1.3133 (1.5471)	grad_norm 25.0177 (27.4587)	mem 4879MB
[2022-05-31 01:21:56 MetaFG_0] (main.py 265): INFO Train: [21/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2988 (0.3050)	loss 1.1356 (1.5464)	grad_norm 25.8100 (27.4635)	mem 4879MB
[2022-05-31 01:21:59 MetaFG_0] (main.py 265): INFO Train: [21/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2940 (0.3050)	loss 1.5535 (1.5461)	grad_norm 14.3828 (27.4631)	mem 4879MB
[2022-05-31 01:22:02 MetaFG_0] (main.py 265): INFO Train: [21/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2976 (0.3050)	loss 1.1917 (1.5456)	grad_norm 25.5155 (27.4672)	mem 4879MB
[2022-05-31 01:22:05 MetaFG_0] (main.py 265): INFO Train: [21/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2927 (0.3050)	loss 1.5452 (1.5457)	grad_norm 33.5498 (27.4699)	mem 4879MB
[2022-05-31 01:22:08 MetaFG_0] (main.py 265): INFO Train: [21/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3011 (0.3050)	loss 1.6509 (1.5454)	grad_norm 35.4331 (27.5016)	mem 4879MB
[2022-05-31 01:22:11 MetaFG_0] (main.py 265): INFO Train: [21/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2948 (0.3050)	loss 1.4305 (1.5452)	grad_norm 28.6991 (27.4954)	mem 4879MB
[2022-05-31 01:22:14 MetaFG_0] (main.py 265): INFO Train: [21/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2989 (0.3050)	loss 1.3254 (1.5443)	grad_norm 29.7977 (27.4966)	mem 4879MB
[2022-05-31 01:22:17 MetaFG_0] (main.py 265): INFO Train: [21/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3011 (0.3050)	loss 1.7448 (1.5450)	grad_norm 17.3229 (27.4653)	mem 4879MB
[2022-05-31 01:22:20 MetaFG_0] (main.py 265): INFO Train: [21/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2999 (0.3050)	loss 1.6626 (1.5449)	grad_norm 30.0815 (27.4758)	mem 4879MB
[2022-05-31 01:22:24 MetaFG_0] (main.py 265): INFO Train: [21/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2984 (0.3050)	loss 1.2155 (1.5444)	grad_norm 30.8884 (27.4878)	mem 4879MB
[2022-05-31 01:22:27 MetaFG_0] (main.py 265): INFO Train: [21/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2991 (0.3050)	loss 1.6347 (1.5450)	grad_norm 28.8928 (27.5024)	mem 4879MB
[2022-05-31 01:22:30 MetaFG_0] (main.py 265): INFO Train: [21/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.3012 (0.3050)	loss 1.6598 (1.5453)	grad_norm 26.9060 (27.5073)	mem 4879MB
[2022-05-31 01:22:33 MetaFG_0] (main.py 265): INFO Train: [21/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2989 (0.3050)	loss 1.6264 (1.5439)	grad_norm 12.2501 (27.4827)	mem 4879MB
[2022-05-31 01:22:36 MetaFG_0] (main.py 265): INFO Train: [21/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2976 (0.3050)	loss 1.0527 (1.5438)	grad_norm 39.4878 (27.5076)	mem 4879MB
[2022-05-31 01:22:39 MetaFG_0] (main.py 265): INFO Train: [21/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.3003 (0.3050)	loss 1.7648 (1.5443)	grad_norm 22.0576 (27.5135)	mem 4879MB
[2022-05-31 01:22:42 MetaFG_0] (main.py 265): INFO Train: [21/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2990 (0.3050)	loss 1.6724 (1.5448)	grad_norm 16.2633 (27.4873)	mem 4879MB
[2022-05-31 01:22:45 MetaFG_0] (main.py 265): INFO Train: [21/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2930 (0.3050)	loss 1.8412 (1.5443)	grad_norm 34.4541 (27.5059)	mem 4879MB
[2022-05-31 01:22:48 MetaFG_0] (main.py 265): INFO Train: [21/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2931 (0.3050)	loss 1.5436 (1.5445)	grad_norm 25.7374 (27.5171)	mem 4879MB
[2022-05-31 01:22:51 MetaFG_0] (main.py 265): INFO Train: [21/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2917 (0.3049)	loss 1.8294 (1.5443)	grad_norm 34.2591 (27.5412)	mem 4879MB
[2022-05-31 01:22:54 MetaFG_0] (main.py 265): INFO Train: [21/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2920 (0.3049)	loss 1.5724 (1.5438)	grad_norm 33.8693 (27.5395)	mem 4879MB
[2022-05-31 01:22:57 MetaFG_0] (main.py 265): INFO Train: [21/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2934 (0.3049)	loss 1.2464 (1.5441)	grad_norm 37.5867 (27.5415)	mem 4879MB
[2022-05-31 01:23:00 MetaFG_0] (main.py 265): INFO Train: [21/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2938 (0.3049)	loss 1.6395 (1.5423)	grad_norm 31.3532 (27.5309)	mem 4879MB
[2022-05-31 01:23:03 MetaFG_0] (main.py 265): INFO Train: [21/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2922 (0.3049)	loss 1.4564 (1.5425)	grad_norm 20.4589 (27.5291)	mem 4879MB
[2022-05-31 01:23:06 MetaFG_0] (main.py 265): INFO Train: [21/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2916 (0.3049)	loss 1.3948 (1.5417)	grad_norm 16.4750 (27.5347)	mem 4879MB
[2022-05-31 01:23:09 MetaFG_0] (main.py 265): INFO Train: [21/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2975 (0.3049)	loss 1.1781 (1.5409)	grad_norm 34.3231 (27.5457)	mem 4879MB
[2022-05-31 01:23:12 MetaFG_0] (main.py 265): INFO Train: [21/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2931 (0.3049)	loss 1.5461 (1.5413)	grad_norm 21.6318 (27.5445)	mem 4879MB
[2022-05-31 01:23:15 MetaFG_0] (main.py 265): INFO Train: [21/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2981 (0.3049)	loss 1.3986 (1.5410)	grad_norm 24.6269 (27.5369)	mem 4879MB
[2022-05-31 01:23:18 MetaFG_0] (main.py 265): INFO Train: [21/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2922 (0.3049)	loss 1.6512 (1.5412)	grad_norm 22.8104 (27.5117)	mem 4879MB
[2022-05-31 01:23:21 MetaFG_0] (main.py 265): INFO Train: [21/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2929 (0.3049)	loss 1.6217 (1.5406)	grad_norm 23.9461 (27.5260)	mem 4879MB
[2022-05-31 01:23:24 MetaFG_0] (main.py 265): INFO Train: [21/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2989 (0.3049)	loss 1.4758 (1.5405)	grad_norm 39.7262 (27.5018)	mem 4879MB
[2022-05-31 01:23:27 MetaFG_0] (main.py 265): INFO Train: [21/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2944 (0.3049)	loss 1.6333 (1.5411)	grad_norm 17.3148 (27.5000)	mem 4879MB
[2022-05-31 01:23:30 MetaFG_0] (main.py 265): INFO Train: [21/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2957 (0.3049)	loss 1.7722 (1.5405)	grad_norm 25.3543 (27.5101)	mem 4879MB
[2022-05-31 01:23:34 MetaFG_0] (main.py 265): INFO Train: [21/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2916 (0.3049)	loss 1.7242 (1.5406)	grad_norm 28.2074 (27.5173)	mem 4879MB
[2022-05-31 01:23:37 MetaFG_0] (main.py 265): INFO Train: [21/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2950 (0.3049)	loss 1.6953 (1.5414)	grad_norm 21.7014 (27.5008)	mem 4879MB
[2022-05-31 01:23:40 MetaFG_0] (main.py 265): INFO Train: [21/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2942 (0.3049)	loss 1.4002 (1.5415)	grad_norm 25.8791 (27.4708)	mem 4879MB
[2022-05-31 01:23:43 MetaFG_0] (main.py 265): INFO Train: [21/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3003 (0.3049)	loss 1.8333 (1.5408)	grad_norm 27.9860 (27.4627)	mem 4879MB
[2022-05-31 01:23:46 MetaFG_0] (main.py 265): INFO Train: [21/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2929 (0.3049)	loss 1.5981 (1.5406)	grad_norm 19.3331 (27.4828)	mem 4879MB
[2022-05-31 01:23:49 MetaFG_0] (main.py 265): INFO Train: [21/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2913 (0.3049)	loss 1.4822 (1.5415)	grad_norm 16.9338 (27.4536)	mem 4879MB
[2022-05-31 01:23:49 MetaFG_0] (main.py 272): INFO EPOCH 21 training takes 0:07:56
[2022-05-31 01:23:49 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_21.pth saving......
[2022-05-31 01:23:50 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_21.pth saved !!!
[2022-05-31 01:23:50 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:23:51 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:23:51 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:23:52 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.658 (0.658)	Loss 0.8566 (0.8566)	Acc@1 87.500 (87.500)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 01:23:53 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.095 (0.149)	Loss 1.1074 (0.9656)	Acc@1 78.125 (79.261)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 01:23:54 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.123)	Loss 0.7575 (0.9822)	Acc@1 93.750 (77.976)	Acc@5 93.750 (97.024)	Mem 4879MB
[2022-05-31 01:23:55 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.088 (0.114)	Loss 1.1019 (0.9576)	Acc@1 62.500 (78.226)	Acc@5 100.000 (97.278)	Mem 4879MB
[2022-05-31 01:23:56 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.109)	Loss 1.0243 (0.9570)	Acc@1 75.000 (78.049)	Acc@5 96.875 (97.332)	Mem 4879MB
[2022-05-31 01:23:57 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.106)	Loss 1.0330 (0.9750)	Acc@1 81.250 (77.635)	Acc@5 100.000 (97.243)	Mem 4879MB
[2022-05-31 01:23:58 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.099 (0.104)	Loss 0.8749 (0.9781)	Acc@1 78.125 (77.715)	Acc@5 100.000 (97.285)	Mem 4879MB
[2022-05-31 01:23:59 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.095 (0.103)	Loss 0.8819 (0.9642)	Acc@1 87.500 (78.257)	Acc@5 96.875 (97.403)	Mem 4879MB
[2022-05-31 01:24:00 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.095 (0.102)	Loss 0.7051 (0.9567)	Acc@1 93.750 (78.819)	Acc@5 96.875 (97.299)	Mem 4879MB
[2022-05-31 01:24:01 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.9090 (0.9650)	Acc@1 78.125 (78.606)	Acc@5 93.750 (97.150)	Mem 4879MB
[2022-05-31 01:24:02 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 1.1353 (0.9635)	Acc@1 78.125 (78.713)	Acc@5 93.750 (97.153)	Mem 4879MB
[2022-05-31 01:24:02 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.095 (0.099)	Loss 1.0036 (0.9620)	Acc@1 68.750 (78.575)	Acc@5 96.875 (97.185)	Mem 4879MB
[2022-05-31 01:24:03 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 1.1643 (0.9607)	Acc@1 75.000 (78.796)	Acc@5 100.000 (97.262)	Mem 4879MB
[2022-05-31 01:24:04 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.8494 (0.9613)	Acc@1 75.000 (78.698)	Acc@5 100.000 (97.185)	Mem 4879MB
[2022-05-31 01:24:05 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.098)	Loss 0.7321 (0.9619)	Acc@1 90.625 (78.657)	Acc@5 100.000 (97.185)	Mem 4879MB
[2022-05-31 01:24:06 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.095 (0.098)	Loss 0.9212 (0.9608)	Acc@1 81.250 (78.663)	Acc@5 96.875 (97.227)	Mem 4879MB
[2022-05-31 01:24:07 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 1.1126 (0.9685)	Acc@1 75.000 (78.436)	Acc@5 96.875 (97.108)	Mem 4879MB
[2022-05-31 01:24:08 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.095 (0.098)	Loss 1.1343 (0.9666)	Acc@1 71.875 (78.545)	Acc@5 93.750 (97.167)	Mem 4879MB
[2022-05-31 01:24:09 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.091 (0.097)	Loss 1.0983 (0.9632)	Acc@1 65.625 (78.505)	Acc@5 100.000 (97.238)	Mem 4879MB
[2022-05-31 01:24:10 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.095 (0.097)	Loss 0.8584 (0.9625)	Acc@1 87.500 (78.698)	Acc@5 96.875 (97.219)	Mem 4879MB
[2022-05-31 01:24:11 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.097)	Loss 0.6870 (0.9600)	Acc@1 90.625 (78.747)	Acc@5 100.000 (97.248)	Mem 4879MB
[2022-05-31 01:24:12 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.097)	Loss 0.8971 (0.9596)	Acc@1 81.250 (78.717)	Acc@5 100.000 (97.290)	Mem 4879MB
[2022-05-31 01:24:13 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.9193 (0.9581)	Acc@1 75.000 (78.775)	Acc@5 100.000 (97.342)	Mem 4879MB
[2022-05-31 01:24:14 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.096)	Loss 1.2869 (0.9568)	Acc@1 68.750 (78.761)	Acc@5 93.750 (97.362)	Mem 4879MB
[2022-05-31 01:24:15 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.091 (0.096)	Loss 1.0366 (0.9548)	Acc@1 71.875 (78.799)	Acc@5 96.875 (97.342)	Mem 4879MB
[2022-05-31 01:24:16 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.096)	Loss 0.6730 (0.9546)	Acc@1 90.625 (78.797)	Acc@5 100.000 (97.311)	Mem 4879MB
[2022-05-31 01:24:17 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 1.0527 (0.9542)	Acc@1 78.125 (78.760)	Acc@5 93.750 (97.306)	Mem 4879MB
[2022-05-31 01:24:18 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.100 (0.096)	Loss 1.0471 (0.9547)	Acc@1 78.125 (78.782)	Acc@5 93.750 (97.313)	Mem 4879MB
[2022-05-31 01:24:19 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.100 (0.096)	Loss 0.9875 (0.9553)	Acc@1 81.250 (78.803)	Acc@5 100.000 (97.331)	Mem 4879MB
[2022-05-31 01:24:19 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.098 (0.096)	Loss 0.9144 (0.9570)	Acc@1 84.375 (78.705)	Acc@5 93.750 (97.315)	Mem 4879MB
[2022-05-31 01:24:20 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 1.1205 (0.9580)	Acc@1 75.000 (78.644)	Acc@5 90.625 (97.280)	Mem 4879MB
[2022-05-31 01:24:21 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7672 (0.9567)	Acc@1 78.125 (78.648)	Acc@5 100.000 (97.287)	Mem 4879MB
[2022-05-31 01:24:22 MetaFG_0] (main.py 330): INFO  * Acc@1 78.670 Acc@5 97.300
[2022-05-31 01:24:22 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 78.7%
[2022-05-31 01:24:22 MetaFG_0] (main.py 171): INFO Max accuracy: 78.67%
[2022-05-31 01:24:23 MetaFG_0] (main.py 265): INFO Train: [22/300][0/1562]	eta 0:26:33 lr 0.000006	time 1.0202 (1.0202)	loss 1.7246 (1.7246)	grad_norm 30.4307 (30.4307)	mem 4879MB
[2022-05-31 01:24:26 MetaFG_0] (main.py 265): INFO Train: [22/300][10/1562]	eta 0:09:39 lr 0.000006	time 0.2923 (0.3731)	loss 1.7035 (1.6715)	grad_norm 24.8851 (26.2454)	mem 4879MB
[2022-05-31 01:24:29 MetaFG_0] (main.py 265): INFO Train: [22/300][20/1562]	eta 0:08:45 lr 0.000006	time 0.2951 (0.3406)	loss 1.6037 (1.6081)	grad_norm 26.9624 (26.9108)	mem 4879MB
[2022-05-31 01:24:32 MetaFG_0] (main.py 265): INFO Train: [22/300][30/1562]	eta 0:08:23 lr 0.000006	time 0.2988 (0.3285)	loss 1.7665 (1.5960)	grad_norm 25.9916 (27.1810)	mem 4879MB
[2022-05-31 01:24:35 MetaFG_0] (main.py 265): INFO Train: [22/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2977 (0.3224)	loss 1.4242 (1.5909)	grad_norm 29.7103 (27.1961)	mem 4879MB
[2022-05-31 01:24:38 MetaFG_0] (main.py 265): INFO Train: [22/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2979 (0.3187)	loss 1.3029 (1.5737)	grad_norm 21.1604 (27.2288)	mem 4879MB
[2022-05-31 01:24:41 MetaFG_0] (main.py 265): INFO Train: [22/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2931 (0.3163)	loss 1.7930 (1.5538)	grad_norm 33.8395 (27.5596)	mem 4879MB
[2022-05-31 01:24:44 MetaFG_0] (main.py 265): INFO Train: [22/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.2966 (0.3144)	loss 1.0317 (1.5326)	grad_norm 32.9370 (28.1391)	mem 4879MB
[2022-05-31 01:24:47 MetaFG_0] (main.py 265): INFO Train: [22/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2978 (0.3131)	loss 1.3907 (1.5402)	grad_norm 30.2087 (28.0405)	mem 4879MB
[2022-05-31 01:24:50 MetaFG_0] (main.py 265): INFO Train: [22/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.3002 (0.3120)	loss 1.3172 (1.5477)	grad_norm 32.0342 (27.9561)	mem 4879MB
[2022-05-31 01:24:53 MetaFG_0] (main.py 265): INFO Train: [22/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2997 (0.3113)	loss 1.4106 (1.5359)	grad_norm 27.2710 (28.0186)	mem 4879MB
[2022-05-31 01:24:56 MetaFG_0] (main.py 265): INFO Train: [22/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2923 (0.3105)	loss 1.6421 (1.5278)	grad_norm 20.4660 (28.1432)	mem 4879MB
[2022-05-31 01:24:59 MetaFG_0] (main.py 265): INFO Train: [22/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2930 (0.3099)	loss 1.4258 (1.5277)	grad_norm 38.3603 (28.1797)	mem 4879MB
[2022-05-31 01:25:02 MetaFG_0] (main.py 265): INFO Train: [22/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2924 (0.3094)	loss 1.3581 (1.5337)	grad_norm 27.4835 (28.1623)	mem 4879MB
[2022-05-31 01:25:05 MetaFG_0] (main.py 265): INFO Train: [22/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2919 (0.3090)	loss 1.6281 (1.5352)	grad_norm 17.8640 (28.0352)	mem 4879MB
[2022-05-31 01:25:08 MetaFG_0] (main.py 265): INFO Train: [22/300][150/1562]	eta 0:07:15 lr 0.000006	time 0.2999 (0.3087)	loss 1.4064 (1.5268)	grad_norm 31.6984 (27.7736)	mem 4879MB
[2022-05-31 01:25:11 MetaFG_0] (main.py 265): INFO Train: [22/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2951 (0.3082)	loss 1.6386 (1.5242)	grad_norm 16.7073 (27.5467)	mem 4879MB
[2022-05-31 01:25:14 MetaFG_0] (main.py 265): INFO Train: [22/300][170/1562]	eta 0:07:08 lr 0.000006	time 0.2918 (0.3079)	loss 1.8711 (1.5274)	grad_norm 23.7680 (27.7346)	mem 4879MB
[2022-05-31 01:25:17 MetaFG_0] (main.py 265): INFO Train: [22/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2990 (0.3076)	loss 1.3409 (1.5236)	grad_norm 55.6779 (27.8571)	mem 4879MB
[2022-05-31 01:25:20 MetaFG_0] (main.py 265): INFO Train: [22/300][190/1562]	eta 0:07:01 lr 0.000006	time 0.2919 (0.3074)	loss 1.5568 (1.5258)	grad_norm 28.3469 (27.9127)	mem 4879MB
[2022-05-31 01:25:23 MetaFG_0] (main.py 265): INFO Train: [22/300][200/1562]	eta 0:06:58 lr 0.000006	time 0.2941 (0.3072)	loss 1.4040 (1.5220)	grad_norm 22.1424 (27.7678)	mem 4879MB
[2022-05-31 01:25:26 MetaFG_0] (main.py 265): INFO Train: [22/300][210/1562]	eta 0:06:55 lr 0.000006	time 0.2931 (0.3070)	loss 1.5860 (1.5244)	grad_norm 31.6164 (27.7590)	mem 4879MB
[2022-05-31 01:25:29 MetaFG_0] (main.py 265): INFO Train: [22/300][220/1562]	eta 0:06:51 lr 0.000006	time 0.2915 (0.3068)	loss 1.6596 (1.5262)	grad_norm 21.3839 (27.6021)	mem 4879MB
[2022-05-31 01:25:32 MetaFG_0] (main.py 265): INFO Train: [22/300][230/1562]	eta 0:06:48 lr 0.000006	time 0.2929 (0.3067)	loss 1.6979 (1.5232)	grad_norm 19.3168 (27.6347)	mem 4879MB
[2022-05-31 01:25:35 MetaFG_0] (main.py 265): INFO Train: [22/300][240/1562]	eta 0:06:45 lr 0.000006	time 0.2941 (0.3066)	loss 1.1925 (1.5224)	grad_norm 20.4811 (27.5156)	mem 4879MB
[2022-05-31 01:25:38 MetaFG_0] (main.py 265): INFO Train: [22/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2919 (0.3065)	loss 1.1270 (1.5183)	grad_norm 57.0546 (27.6736)	mem 4879MB
[2022-05-31 01:25:42 MetaFG_0] (main.py 265): INFO Train: [22/300][260/1562]	eta 0:06:38 lr 0.000006	time 0.2934 (0.3063)	loss 1.5931 (1.5159)	grad_norm 27.4710 (28.3590)	mem 4879MB
[2022-05-31 01:25:45 MetaFG_0] (main.py 265): INFO Train: [22/300][270/1562]	eta 0:06:35 lr 0.000006	time 0.2984 (0.3062)	loss 1.3548 (1.5168)	grad_norm 29.1195 (28.2866)	mem 4879MB
[2022-05-31 01:25:48 MetaFG_0] (main.py 265): INFO Train: [22/300][280/1562]	eta 0:06:32 lr 0.000006	time 0.2986 (0.3061)	loss 1.6552 (1.5124)	grad_norm 22.8078 (28.1761)	mem 4879MB
[2022-05-31 01:25:51 MetaFG_0] (main.py 265): INFO Train: [22/300][290/1562]	eta 0:06:29 lr 0.000006	time 0.2988 (0.3060)	loss 1.3912 (1.5109)	grad_norm 39.1589 (28.2031)	mem 4879MB
[2022-05-31 01:25:54 MetaFG_0] (main.py 265): INFO Train: [22/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2923 (0.3060)	loss 1.6376 (1.5100)	grad_norm 21.9552 (28.1494)	mem 4879MB
[2022-05-31 01:25:57 MetaFG_0] (main.py 265): INFO Train: [22/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2989 (0.3059)	loss 1.0431 (1.5091)	grad_norm 53.9440 (28.1350)	mem 4879MB
[2022-05-31 01:26:00 MetaFG_0] (main.py 265): INFO Train: [22/300][320/1562]	eta 0:06:19 lr 0.000006	time 0.2984 (0.3058)	loss 1.5627 (1.5104)	grad_norm 23.8255 (28.1181)	mem 4879MB
[2022-05-31 01:26:03 MetaFG_0] (main.py 265): INFO Train: [22/300][330/1562]	eta 0:06:16 lr 0.000006	time 0.2919 (0.3058)	loss 1.7125 (1.5151)	grad_norm 25.2887 (28.1577)	mem 4879MB
[2022-05-31 01:26:06 MetaFG_0] (main.py 265): INFO Train: [22/300][340/1562]	eta 0:06:13 lr 0.000006	time 0.2935 (0.3057)	loss 1.6176 (1.5198)	grad_norm 22.1128 (28.1557)	mem 4879MB
[2022-05-31 01:26:09 MetaFG_0] (main.py 265): INFO Train: [22/300][350/1562]	eta 0:06:10 lr 0.000006	time 0.2971 (0.3057)	loss 1.5830 (1.5237)	grad_norm 41.9497 (28.0407)	mem 4879MB
[2022-05-31 01:26:12 MetaFG_0] (main.py 265): INFO Train: [22/300][360/1562]	eta 0:06:07 lr 0.000006	time 0.2939 (0.3056)	loss 1.4380 (1.5234)	grad_norm 26.9364 (28.0828)	mem 4879MB
[2022-05-31 01:26:15 MetaFG_0] (main.py 265): INFO Train: [22/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2918 (0.3056)	loss 1.1671 (1.5238)	grad_norm 20.2905 (28.0864)	mem 4879MB
[2022-05-31 01:26:18 MetaFG_0] (main.py 265): INFO Train: [22/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2980 (0.3055)	loss 1.1847 (1.5215)	grad_norm 39.4952 (28.0152)	mem 4879MB
[2022-05-31 01:26:21 MetaFG_0] (main.py 265): INFO Train: [22/300][390/1562]	eta 0:05:57 lr 0.000006	time 0.2984 (0.3054)	loss 1.6434 (1.5230)	grad_norm 22.4198 (27.9865)	mem 4879MB
[2022-05-31 01:26:24 MetaFG_0] (main.py 265): INFO Train: [22/300][400/1562]	eta 0:05:54 lr 0.000006	time 0.2913 (0.3054)	loss 1.7923 (1.5214)	grad_norm 21.7190 (27.9718)	mem 4879MB
[2022-05-31 01:26:27 MetaFG_0] (main.py 265): INFO Train: [22/300][410/1562]	eta 0:05:51 lr 0.000006	time 0.2987 (0.3053)	loss 1.5100 (1.5231)	grad_norm 17.8042 (27.9047)	mem 4879MB
[2022-05-31 01:26:30 MetaFG_0] (main.py 265): INFO Train: [22/300][420/1562]	eta 0:05:48 lr 0.000006	time 0.2956 (0.3053)	loss 1.2255 (1.5230)	grad_norm 34.3210 (27.8821)	mem 4879MB
[2022-05-31 01:26:33 MetaFG_0] (main.py 265): INFO Train: [22/300][430/1562]	eta 0:05:45 lr 0.000006	time 0.2917 (0.3053)	loss 1.6320 (1.5233)	grad_norm 26.6463 (27.9117)	mem 4879MB
[2022-05-31 01:26:36 MetaFG_0] (main.py 265): INFO Train: [22/300][440/1562]	eta 0:05:42 lr 0.000006	time 0.2937 (0.3052)	loss 1.5020 (1.5232)	grad_norm 23.2536 (27.8494)	mem 4879MB
[2022-05-31 01:26:39 MetaFG_0] (main.py 265): INFO Train: [22/300][450/1562]	eta 0:05:39 lr 0.000006	time 0.2916 (0.3052)	loss 1.4031 (1.5240)	grad_norm 60.7924 (27.9083)	mem 4879MB
[2022-05-31 01:26:42 MetaFG_0] (main.py 265): INFO Train: [22/300][460/1562]	eta 0:05:36 lr 0.000006	time 0.2978 (0.3052)	loss 1.4198 (1.5234)	grad_norm 25.2036 (28.0034)	mem 4879MB
[2022-05-31 01:26:45 MetaFG_0] (main.py 265): INFO Train: [22/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.2931 (0.3051)	loss 1.6794 (1.5242)	grad_norm 25.0498 (27.9818)	mem 4879MB
[2022-05-31 01:26:48 MetaFG_0] (main.py 265): INFO Train: [22/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2994 (0.3051)	loss 1.7602 (1.5239)	grad_norm 25.7177 (28.1710)	mem 4879MB
[2022-05-31 01:26:51 MetaFG_0] (main.py 265): INFO Train: [22/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2937 (0.3051)	loss 1.5942 (1.5235)	grad_norm 20.4245 (28.2263)	mem 4879MB
[2022-05-31 01:26:54 MetaFG_0] (main.py 265): INFO Train: [22/300][500/1562]	eta 0:05:23 lr 0.000006	time 0.2935 (0.3051)	loss 1.3191 (1.5233)	grad_norm 23.2280 (28.1981)	mem 4879MB
[2022-05-31 01:26:57 MetaFG_0] (main.py 265): INFO Train: [22/300][510/1562]	eta 0:05:20 lr 0.000006	time 0.2923 (0.3050)	loss 1.4582 (1.5248)	grad_norm 22.1525 (28.1413)	mem 4879MB
[2022-05-31 01:27:00 MetaFG_0] (main.py 265): INFO Train: [22/300][520/1562]	eta 0:05:17 lr 0.000006	time 0.2983 (0.3050)	loss 1.6557 (1.5240)	grad_norm 18.4761 (28.1237)	mem 4879MB
[2022-05-31 01:27:04 MetaFG_0] (main.py 265): INFO Train: [22/300][530/1562]	eta 0:05:14 lr 0.000006	time 0.2925 (0.3050)	loss 1.5667 (1.5232)	grad_norm 27.8603 (28.0591)	mem 4879MB
[2022-05-31 01:27:07 MetaFG_0] (main.py 265): INFO Train: [22/300][540/1562]	eta 0:05:11 lr 0.000006	time 0.2914 (0.3050)	loss 1.4295 (1.5240)	grad_norm 38.3008 (28.0451)	mem 4879MB
[2022-05-31 01:27:10 MetaFG_0] (main.py 265): INFO Train: [22/300][550/1562]	eta 0:05:08 lr 0.000006	time 0.2927 (0.3049)	loss 1.7575 (1.5236)	grad_norm 29.7966 (28.0216)	mem 4879MB
[2022-05-31 01:27:13 MetaFG_0] (main.py 265): INFO Train: [22/300][560/1562]	eta 0:05:05 lr 0.000006	time 0.2997 (0.3049)	loss 1.3696 (1.5236)	grad_norm 21.0242 (27.9618)	mem 4879MB
[2022-05-31 01:27:16 MetaFG_0] (main.py 265): INFO Train: [22/300][570/1562]	eta 0:05:02 lr 0.000006	time 0.2934 (0.3049)	loss 1.2852 (1.5236)	grad_norm 33.2033 (27.9656)	mem 4879MB
[2022-05-31 01:27:19 MetaFG_0] (main.py 265): INFO Train: [22/300][580/1562]	eta 0:04:59 lr 0.000006	time 0.2982 (0.3049)	loss 1.5320 (1.5251)	grad_norm 16.1301 (27.9634)	mem 4879MB
[2022-05-31 01:27:22 MetaFG_0] (main.py 265): INFO Train: [22/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2982 (0.3049)	loss 1.8300 (1.5268)	grad_norm 43.1084 (27.9531)	mem 4879MB
[2022-05-31 01:27:25 MetaFG_0] (main.py 265): INFO Train: [22/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2981 (0.3048)	loss 1.7991 (1.5276)	grad_norm 25.0780 (28.0428)	mem 4879MB
[2022-05-31 01:27:28 MetaFG_0] (main.py 265): INFO Train: [22/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2977 (0.3048)	loss 1.5884 (1.5281)	grad_norm 25.5307 (27.9533)	mem 4879MB
[2022-05-31 01:27:31 MetaFG_0] (main.py 265): INFO Train: [22/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2913 (0.3048)	loss 1.6282 (1.5255)	grad_norm 29.1516 (27.9830)	mem 4879MB
[2022-05-31 01:27:34 MetaFG_0] (main.py 265): INFO Train: [22/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2929 (0.3048)	loss 1.6824 (1.5244)	grad_norm 30.7554 (28.0016)	mem 4879MB
[2022-05-31 01:27:37 MetaFG_0] (main.py 265): INFO Train: [22/300][640/1562]	eta 0:04:40 lr 0.000006	time 0.2916 (0.3047)	loss 1.3268 (1.5247)	grad_norm 21.7295 (27.9235)	mem 4879MB
[2022-05-31 01:27:40 MetaFG_0] (main.py 265): INFO Train: [22/300][650/1562]	eta 0:04:37 lr 0.000006	time 0.2922 (0.3047)	loss 1.6820 (1.5233)	grad_norm 20.7636 (27.9149)	mem 4879MB
[2022-05-31 01:27:43 MetaFG_0] (main.py 265): INFO Train: [22/300][660/1562]	eta 0:04:34 lr 0.000006	time 0.2924 (0.3047)	loss 1.6395 (1.5230)	grad_norm 36.9857 (27.8875)	mem 4879MB
[2022-05-31 01:27:46 MetaFG_0] (main.py 265): INFO Train: [22/300][670/1562]	eta 0:04:31 lr 0.000006	time 0.2930 (0.3047)	loss 1.7089 (1.5218)	grad_norm 36.7720 (27.7918)	mem 4879MB
[2022-05-31 01:27:49 MetaFG_0] (main.py 265): INFO Train: [22/300][680/1562]	eta 0:04:28 lr 0.000006	time 0.2930 (0.3047)	loss 1.0334 (1.5206)	grad_norm 23.7593 (27.7561)	mem 4879MB
[2022-05-31 01:27:52 MetaFG_0] (main.py 265): INFO Train: [22/300][690/1562]	eta 0:04:25 lr 0.000006	time 0.2980 (0.3047)	loss 1.6180 (1.5206)	grad_norm 38.5859 (27.7885)	mem 4879MB
[2022-05-31 01:27:55 MetaFG_0] (main.py 265): INFO Train: [22/300][700/1562]	eta 0:04:22 lr 0.000006	time 0.2917 (0.3046)	loss 1.3380 (1.5194)	grad_norm 62.2393 (27.8222)	mem 4879MB
[2022-05-31 01:27:58 MetaFG_0] (main.py 265): INFO Train: [22/300][710/1562]	eta 0:04:19 lr 0.000006	time 0.2928 (0.3046)	loss 1.1388 (1.5202)	grad_norm 26.4984 (27.7680)	mem 4879MB
[2022-05-31 01:28:01 MetaFG_0] (main.py 265): INFO Train: [22/300][720/1562]	eta 0:04:16 lr 0.000006	time 0.2928 (0.3046)	loss 1.3719 (1.5194)	grad_norm 28.3762 (27.7673)	mem 4879MB
[2022-05-31 01:28:04 MetaFG_0] (main.py 265): INFO Train: [22/300][730/1562]	eta 0:04:13 lr 0.000006	time 0.2928 (0.3046)	loss 1.8947 (1.5205)	grad_norm 53.0439 (27.8216)	mem 4879MB
[2022-05-31 01:28:07 MetaFG_0] (main.py 265): INFO Train: [22/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2946 (0.3045)	loss 1.3588 (1.5193)	grad_norm 38.0644 (27.8589)	mem 4879MB
[2022-05-31 01:28:10 MetaFG_0] (main.py 265): INFO Train: [22/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2920 (0.3045)	loss 1.6002 (1.5193)	grad_norm 21.1705 (27.8512)	mem 4879MB
[2022-05-31 01:28:13 MetaFG_0] (main.py 265): INFO Train: [22/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2935 (0.3045)	loss 1.7889 (1.5205)	grad_norm 28.5225 (27.8167)	mem 4879MB
[2022-05-31 01:28:16 MetaFG_0] (main.py 265): INFO Train: [22/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2919 (0.3045)	loss 1.5751 (1.5222)	grad_norm 18.6479 (27.8378)	mem 4879MB
[2022-05-31 01:28:19 MetaFG_0] (main.py 265): INFO Train: [22/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2987 (0.3045)	loss 1.6859 (1.5223)	grad_norm 26.6449 (27.8133)	mem 4879MB
[2022-05-31 01:28:22 MetaFG_0] (main.py 265): INFO Train: [22/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2933 (0.3044)	loss 1.5797 (1.5227)	grad_norm 18.3447 (27.7986)	mem 4879MB
[2022-05-31 01:28:25 MetaFG_0] (main.py 265): INFO Train: [22/300][800/1562]	eta 0:03:51 lr 0.000006	time 0.2979 (0.3044)	loss 1.6894 (1.5234)	grad_norm 27.6473 (27.8584)	mem 4879MB
[2022-05-31 01:28:28 MetaFG_0] (main.py 265): INFO Train: [22/300][810/1562]	eta 0:03:48 lr 0.000006	time 0.2981 (0.3044)	loss 1.6477 (1.5238)	grad_norm 23.6342 (27.8716)	mem 4879MB
[2022-05-31 01:28:31 MetaFG_0] (main.py 265): INFO Train: [22/300][820/1562]	eta 0:03:45 lr 0.000006	time 0.2933 (0.3044)	loss 1.6777 (1.5247)	grad_norm 34.8108 (27.8923)	mem 4879MB
[2022-05-31 01:28:35 MetaFG_0] (main.py 265): INFO Train: [22/300][830/1562]	eta 0:03:42 lr 0.000006	time 0.2933 (0.3044)	loss 1.5809 (1.5250)	grad_norm 39.5992 (27.9061)	mem 4879MB
[2022-05-31 01:28:38 MetaFG_0] (main.py 265): INFO Train: [22/300][840/1562]	eta 0:03:39 lr 0.000006	time 0.2999 (0.3044)	loss 1.6817 (1.5260)	grad_norm 18.3164 (27.8556)	mem 4879MB
[2022-05-31 01:28:41 MetaFG_0] (main.py 265): INFO Train: [22/300][850/1562]	eta 0:03:36 lr 0.000006	time 0.2932 (0.3044)	loss 1.5979 (1.5263)	grad_norm 14.5053 (27.8415)	mem 4879MB
[2022-05-31 01:28:44 MetaFG_0] (main.py 265): INFO Train: [22/300][860/1562]	eta 0:03:33 lr 0.000006	time 0.2996 (0.3044)	loss 1.5792 (1.5259)	grad_norm 15.1135 (27.8508)	mem 4879MB
[2022-05-31 01:28:47 MetaFG_0] (main.py 265): INFO Train: [22/300][870/1562]	eta 0:03:30 lr 0.000006	time 0.2983 (0.3044)	loss 1.6951 (1.5264)	grad_norm 21.8028 (27.8814)	mem 4879MB
[2022-05-31 01:28:50 MetaFG_0] (main.py 265): INFO Train: [22/300][880/1562]	eta 0:03:27 lr 0.000006	time 0.2919 (0.3044)	loss 1.6893 (1.5269)	grad_norm 21.5055 (27.8876)	mem 4879MB
[2022-05-31 01:28:53 MetaFG_0] (main.py 265): INFO Train: [22/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2922 (0.3044)	loss 1.8867 (1.5290)	grad_norm 34.9746 (27.8768)	mem 4879MB
[2022-05-31 01:28:56 MetaFG_0] (main.py 265): INFO Train: [22/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2952 (0.3044)	loss 1.6255 (1.5285)	grad_norm 18.3469 (27.8588)	mem 4879MB
[2022-05-31 01:28:59 MetaFG_0] (main.py 265): INFO Train: [22/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.3006 (0.3044)	loss 1.4136 (1.5281)	grad_norm 29.7691 (27.8459)	mem 4879MB
[2022-05-31 01:29:02 MetaFG_0] (main.py 265): INFO Train: [22/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2924 (0.3044)	loss 1.7397 (1.5272)	grad_norm 41.4396 (27.8706)	mem 4879MB
[2022-05-31 01:29:05 MetaFG_0] (main.py 265): INFO Train: [22/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2974 (0.3044)	loss 1.3000 (1.5263)	grad_norm 21.5965 (27.8874)	mem 4879MB
[2022-05-31 01:29:08 MetaFG_0] (main.py 265): INFO Train: [22/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2929 (0.3043)	loss 1.6142 (1.5267)	grad_norm 23.4489 (27.9086)	mem 4879MB
[2022-05-31 01:29:11 MetaFG_0] (main.py 265): INFO Train: [22/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2989 (0.3043)	loss 1.5059 (1.5266)	grad_norm 31.1153 (27.9149)	mem 4879MB
[2022-05-31 01:29:14 MetaFG_0] (main.py 265): INFO Train: [22/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2924 (0.3043)	loss 1.7047 (1.5261)	grad_norm 27.1435 (27.9097)	mem 4879MB
[2022-05-31 01:29:17 MetaFG_0] (main.py 265): INFO Train: [22/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2918 (0.3043)	loss 1.7604 (1.5267)	grad_norm 25.7638 (27.9029)	mem 4879MB
[2022-05-31 01:29:20 MetaFG_0] (main.py 265): INFO Train: [22/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2933 (0.3043)	loss 1.3761 (1.5262)	grad_norm 21.8736 (27.9154)	mem 4879MB
[2022-05-31 01:29:23 MetaFG_0] (main.py 265): INFO Train: [22/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2940 (0.3043)	loss 1.4691 (1.5259)	grad_norm 14.5269 (27.9041)	mem 4879MB
[2022-05-31 01:29:26 MetaFG_0] (main.py 265): INFO Train: [22/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2916 (0.3043)	loss 1.7618 (1.5248)	grad_norm 35.0283 (27.9458)	mem 4879MB
[2022-05-31 01:29:29 MetaFG_0] (main.py 265): INFO Train: [22/300][1010/1562]	eta 0:02:47 lr 0.000006	time 0.2930 (0.3043)	loss 1.5623 (1.5248)	grad_norm 19.2613 (27.9411)	mem 4879MB
[2022-05-31 01:29:32 MetaFG_0] (main.py 265): INFO Train: [22/300][1020/1562]	eta 0:02:44 lr 0.000006	time 0.2932 (0.3043)	loss 1.7932 (1.5243)	grad_norm 27.3678 (27.9091)	mem 4879MB
[2022-05-31 01:29:35 MetaFG_0] (main.py 265): INFO Train: [22/300][1030/1562]	eta 0:02:41 lr 0.000006	time 0.2916 (0.3042)	loss 1.2912 (1.5223)	grad_norm 41.5356 (27.9206)	mem 4879MB
[2022-05-31 01:29:38 MetaFG_0] (main.py 265): INFO Train: [22/300][1040/1562]	eta 0:02:38 lr 0.000006	time 0.2923 (0.3042)	loss 1.6926 (1.5223)	grad_norm 30.2200 (27.9344)	mem 4879MB
[2022-05-31 01:29:41 MetaFG_0] (main.py 265): INFO Train: [22/300][1050/1562]	eta 0:02:35 lr 0.000006	time 0.2925 (0.3042)	loss 1.7264 (1.5220)	grad_norm 39.4429 (27.9301)	mem 4879MB
[2022-05-31 01:29:44 MetaFG_0] (main.py 265): INFO Train: [22/300][1060/1562]	eta 0:02:32 lr 0.000006	time 0.2921 (0.3042)	loss 1.5536 (1.5223)	grad_norm 46.4950 (27.9702)	mem 4879MB
[2022-05-31 01:29:47 MetaFG_0] (main.py 265): INFO Train: [22/300][1070/1562]	eta 0:02:29 lr 0.000006	time 0.2920 (0.3042)	loss 1.4201 (1.5229)	grad_norm 39.9051 (28.0327)	mem 4879MB
[2022-05-31 01:29:50 MetaFG_0] (main.py 265): INFO Train: [22/300][1080/1562]	eta 0:02:26 lr 0.000006	time 0.2931 (0.3042)	loss 1.0835 (1.5227)	grad_norm 22.9252 (28.0164)	mem 4879MB
[2022-05-31 01:29:53 MetaFG_0] (main.py 265): INFO Train: [22/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2981 (0.3042)	loss 1.4063 (1.5221)	grad_norm 19.7476 (28.0150)	mem 4879MB
[2022-05-31 01:29:56 MetaFG_0] (main.py 265): INFO Train: [22/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2934 (0.3042)	loss 1.5364 (1.5222)	grad_norm 49.2733 (28.0185)	mem 4879MB
[2022-05-31 01:30:00 MetaFG_0] (main.py 265): INFO Train: [22/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2987 (0.3042)	loss 1.3656 (1.5227)	grad_norm 20.0336 (28.0376)	mem 4879MB
[2022-05-31 01:30:03 MetaFG_0] (main.py 265): INFO Train: [22/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2921 (0.3042)	loss 1.1658 (1.5234)	grad_norm 23.9244 (28.0256)	mem 4879MB
[2022-05-31 01:30:06 MetaFG_0] (main.py 265): INFO Train: [22/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2991 (0.3042)	loss 1.5685 (1.5232)	grad_norm 18.6679 (28.0328)	mem 4879MB
[2022-05-31 01:30:09 MetaFG_0] (main.py 265): INFO Train: [22/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2992 (0.3042)	loss 1.4626 (1.5235)	grad_norm 31.3072 (27.9987)	mem 4879MB
[2022-05-31 01:30:12 MetaFG_0] (main.py 265): INFO Train: [22/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2982 (0.3042)	loss 1.5294 (1.5239)	grad_norm 19.3258 (28.0133)	mem 4879MB
[2022-05-31 01:30:15 MetaFG_0] (main.py 265): INFO Train: [22/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2976 (0.3042)	loss 1.5381 (1.5235)	grad_norm 18.8290 (28.1081)	mem 4879MB
[2022-05-31 01:30:18 MetaFG_0] (main.py 265): INFO Train: [22/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2932 (0.3042)	loss 1.6694 (1.5231)	grad_norm 30.1906 (28.0996)	mem 4879MB
[2022-05-31 01:30:21 MetaFG_0] (main.py 265): INFO Train: [22/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2921 (0.3042)	loss 1.7424 (1.5232)	grad_norm 21.8809 (28.0708)	mem 4879MB
[2022-05-31 01:30:24 MetaFG_0] (main.py 265): INFO Train: [22/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2932 (0.3042)	loss 1.6033 (1.5230)	grad_norm 21.7770 (28.0361)	mem 4879MB
[2022-05-31 01:30:27 MetaFG_0] (main.py 265): INFO Train: [22/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2926 (0.3042)	loss 1.8659 (1.5233)	grad_norm 23.4539 (28.0096)	mem 4879MB
[2022-05-31 01:30:30 MetaFG_0] (main.py 265): INFO Train: [22/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2919 (0.3041)	loss 1.6790 (1.5234)	grad_norm 15.9433 (28.0169)	mem 4879MB
[2022-05-31 01:30:33 MetaFG_0] (main.py 265): INFO Train: [22/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2983 (0.3041)	loss 1.7022 (1.5240)	grad_norm 19.5340 (28.0672)	mem 4879MB
[2022-05-31 01:30:36 MetaFG_0] (main.py 265): INFO Train: [22/300][1230/1562]	eta 0:01:40 lr 0.000006	time 0.2977 (0.3041)	loss 1.5528 (1.5241)	grad_norm 32.1027 (28.0485)	mem 4879MB
[2022-05-31 01:30:39 MetaFG_0] (main.py 265): INFO Train: [22/300][1240/1562]	eta 0:01:37 lr 0.000006	time 0.2920 (0.3041)	loss 1.5133 (1.5255)	grad_norm 24.6719 (28.0378)	mem 4879MB
[2022-05-31 01:30:42 MetaFG_0] (main.py 265): INFO Train: [22/300][1250/1562]	eta 0:01:34 lr 0.000006	time 0.2917 (0.3041)	loss 1.2054 (1.5249)	grad_norm 43.4825 (28.0363)	mem 4879MB
[2022-05-31 01:30:45 MetaFG_0] (main.py 265): INFO Train: [22/300][1260/1562]	eta 0:01:31 lr 0.000006	time 0.2920 (0.3041)	loss 1.8878 (1.5258)	grad_norm 32.5841 (28.0297)	mem 4879MB
[2022-05-31 01:30:48 MetaFG_0] (main.py 265): INFO Train: [22/300][1270/1562]	eta 0:01:28 lr 0.000006	time 0.2982 (0.3041)	loss 1.4761 (1.5257)	grad_norm 36.8285 (28.0173)	mem 4879MB
[2022-05-31 01:30:51 MetaFG_0] (main.py 265): INFO Train: [22/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2984 (0.3041)	loss 1.1102 (1.5245)	grad_norm 33.0520 (28.0080)	mem 4879MB
[2022-05-31 01:30:54 MetaFG_0] (main.py 265): INFO Train: [22/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2921 (0.3041)	loss 1.4853 (1.5243)	grad_norm 26.4649 (27.9970)	mem 4879MB
[2022-05-31 01:30:57 MetaFG_0] (main.py 265): INFO Train: [22/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2932 (0.3041)	loss 1.2762 (1.5247)	grad_norm 19.1599 (27.9807)	mem 4879MB
[2022-05-31 01:31:00 MetaFG_0] (main.py 265): INFO Train: [22/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.3000 (0.3041)	loss 1.7342 (1.5244)	grad_norm 20.8729 (27.9886)	mem 4879MB
[2022-05-31 01:31:03 MetaFG_0] (main.py 265): INFO Train: [22/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3018 (0.3041)	loss 1.6571 (1.5244)	grad_norm 16.2455 (27.9621)	mem 4879MB
[2022-05-31 01:31:06 MetaFG_0] (main.py 265): INFO Train: [22/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.3000 (0.3041)	loss 1.7344 (1.5244)	grad_norm 21.5822 (27.9533)	mem 4879MB
[2022-05-31 01:31:09 MetaFG_0] (main.py 265): INFO Train: [22/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2948 (0.3041)	loss 1.6881 (1.5243)	grad_norm 18.0152 (27.9497)	mem 4879MB
[2022-05-31 01:31:12 MetaFG_0] (main.py 265): INFO Train: [22/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2930 (0.3041)	loss 1.7807 (1.5234)	grad_norm 23.0343 (27.9680)	mem 4879MB
[2022-05-31 01:31:15 MetaFG_0] (main.py 265): INFO Train: [22/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2977 (0.3041)	loss 1.2653 (1.5229)	grad_norm 20.6381 (27.9546)	mem 4879MB
[2022-05-31 01:31:18 MetaFG_0] (main.py 265): INFO Train: [22/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2931 (0.3041)	loss 1.0831 (1.5228)	grad_norm 31.8973 (27.9278)	mem 4879MB
[2022-05-31 01:31:21 MetaFG_0] (main.py 265): INFO Train: [22/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2942 (0.3041)	loss 1.7750 (1.5225)	grad_norm 23.0560 (27.8802)	mem 4879MB
[2022-05-31 01:31:24 MetaFG_0] (main.py 265): INFO Train: [22/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3023 (0.3040)	loss 1.5146 (1.5235)	grad_norm 17.3262 (27.8805)	mem 4879MB
[2022-05-31 01:31:28 MetaFG_0] (main.py 265): INFO Train: [22/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2923 (0.3040)	loss 0.8793 (1.5225)	grad_norm 40.2277 (27.8884)	mem 4879MB
[2022-05-31 01:31:31 MetaFG_0] (main.py 265): INFO Train: [22/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2919 (0.3040)	loss 1.1762 (1.5224)	grad_norm 44.2563 (27.9047)	mem 4879MB
[2022-05-31 01:31:34 MetaFG_0] (main.py 265): INFO Train: [22/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2930 (0.3040)	loss 1.4874 (1.5220)	grad_norm 16.9399 (27.9075)	mem 4879MB
[2022-05-31 01:31:37 MetaFG_0] (main.py 265): INFO Train: [22/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2966 (0.3040)	loss 1.6681 (1.5222)	grad_norm 28.0694 (27.9016)	mem 4879MB
[2022-05-31 01:31:40 MetaFG_0] (main.py 265): INFO Train: [22/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2926 (0.3040)	loss 1.7094 (1.5231)	grad_norm 22.4747 (27.9131)	mem 4879MB
[2022-05-31 01:31:43 MetaFG_0] (main.py 265): INFO Train: [22/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2982 (0.3040)	loss 1.5700 (1.5223)	grad_norm 20.5209 (27.9022)	mem 4879MB
[2022-05-31 01:31:46 MetaFG_0] (main.py 265): INFO Train: [22/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2995 (0.3040)	loss 1.5882 (1.5223)	grad_norm 17.1303 (27.8718)	mem 4879MB
[2022-05-31 01:31:49 MetaFG_0] (main.py 265): INFO Train: [22/300][1470/1562]	eta 0:00:27 lr 0.000006	time 0.2928 (0.3040)	loss 1.5520 (1.5222)	grad_norm 32.6770 (27.8771)	mem 4879MB
[2022-05-31 01:31:52 MetaFG_0] (main.py 265): INFO Train: [22/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2921 (0.3040)	loss 1.6727 (1.5228)	grad_norm 17.4339 (27.8687)	mem 4879MB
[2022-05-31 01:31:55 MetaFG_0] (main.py 265): INFO Train: [22/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2919 (0.3040)	loss 1.7300 (1.5219)	grad_norm 23.4343 (27.8567)	mem 4879MB
[2022-05-31 01:31:58 MetaFG_0] (main.py 265): INFO Train: [22/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2976 (0.3040)	loss 1.7281 (1.5218)	grad_norm 23.3671 (27.8550)	mem 4879MB
[2022-05-31 01:32:01 MetaFG_0] (main.py 265): INFO Train: [22/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2975 (0.3040)	loss 1.5799 (1.5214)	grad_norm 19.8595 (27.8665)	mem 4879MB
[2022-05-31 01:32:04 MetaFG_0] (main.py 265): INFO Train: [22/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2925 (0.3040)	loss 1.2547 (1.5203)	grad_norm 29.2805 (27.9026)	mem 4879MB
[2022-05-31 01:32:07 MetaFG_0] (main.py 265): INFO Train: [22/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2912 (0.3040)	loss 1.6699 (1.5204)	grad_norm 16.4320 (27.8963)	mem 4879MB
[2022-05-31 01:32:10 MetaFG_0] (main.py 265): INFO Train: [22/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2919 (0.3040)	loss 1.5852 (1.5199)	grad_norm 22.9833 (27.8692)	mem 4879MB
[2022-05-31 01:32:13 MetaFG_0] (main.py 265): INFO Train: [22/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2928 (0.3040)	loss 1.3008 (1.5197)	grad_norm 25.0149 (27.8390)	mem 4879MB
[2022-05-31 01:32:16 MetaFG_0] (main.py 265): INFO Train: [22/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2917 (0.3039)	loss 1.7572 (1.5200)	grad_norm 28.6708 (27.8682)	mem 4879MB
[2022-05-31 01:32:16 MetaFG_0] (main.py 272): INFO EPOCH 22 training takes 0:07:54
[2022-05-31 01:32:16 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_22.pth saving......
[2022-05-31 01:32:17 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_22.pth saved !!!
[2022-05-31 01:32:17 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:32:19 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:32:19 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:32:20 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.696 (0.696)	Loss 0.9059 (0.9059)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 01:32:21 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.095 (0.152)	Loss 1.2342 (0.9100)	Acc@1 71.875 (78.125)	Acc@5 96.875 (98.011)	Mem 4879MB
[2022-05-31 01:32:21 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.124)	Loss 0.8811 (0.9381)	Acc@1 87.500 (77.530)	Acc@5 100.000 (98.065)	Mem 4879MB
[2022-05-31 01:32:22 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.100 (0.114)	Loss 1.0387 (0.9349)	Acc@1 78.125 (78.528)	Acc@5 96.875 (97.681)	Mem 4879MB
[2022-05-31 01:32:23 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.109)	Loss 0.6208 (0.9229)	Acc@1 87.500 (79.268)	Acc@5 100.000 (97.713)	Mem 4879MB
[2022-05-31 01:32:24 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.099 (0.106)	Loss 0.5687 (0.9040)	Acc@1 93.750 (80.515)	Acc@5 100.000 (97.917)	Mem 4879MB
[2022-05-31 01:32:25 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.105)	Loss 0.9783 (0.8932)	Acc@1 71.875 (80.430)	Acc@5 96.875 (98.002)	Mem 4879MB
[2022-05-31 01:32:26 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.104)	Loss 0.9188 (0.8969)	Acc@1 81.250 (80.238)	Acc@5 96.875 (97.843)	Mem 4879MB
[2022-05-31 01:32:27 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.8978 (0.8955)	Acc@1 75.000 (80.247)	Acc@5 93.750 (97.840)	Mem 4879MB
[2022-05-31 01:32:28 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.7260 (0.8986)	Acc@1 87.500 (80.082)	Acc@5 100.000 (97.905)	Mem 4879MB
[2022-05-31 01:32:29 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.102 (0.101)	Loss 1.1627 (0.8959)	Acc@1 71.875 (80.198)	Acc@5 100.000 (97.927)	Mem 4879MB
[2022-05-31 01:32:30 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.090 (0.100)	Loss 1.1738 (0.9042)	Acc@1 71.875 (79.927)	Acc@5 93.750 (97.889)	Mem 4879MB
[2022-05-31 01:32:31 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.100)	Loss 0.8617 (0.9072)	Acc@1 84.375 (79.778)	Acc@5 100.000 (97.908)	Mem 4879MB
[2022-05-31 01:32:32 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 1.1197 (0.9034)	Acc@1 75.000 (79.866)	Acc@5 90.625 (97.925)	Mem 4879MB
[2022-05-31 01:32:33 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 0.6896 (0.9075)	Acc@1 81.250 (79.721)	Acc@5 100.000 (97.872)	Mem 4879MB
[2022-05-31 01:32:34 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.092 (0.098)	Loss 0.9050 (0.9065)	Acc@1 81.250 (79.843)	Acc@5 96.875 (97.827)	Mem 4879MB
[2022-05-31 01:32:35 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 0.8403 (0.9012)	Acc@1 78.125 (79.969)	Acc@5 100.000 (97.845)	Mem 4879MB
[2022-05-31 01:32:36 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 0.5714 (0.9059)	Acc@1 93.750 (79.770)	Acc@5 100.000 (97.844)	Mem 4879MB
[2022-05-31 01:32:36 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.097)	Loss 0.8482 (0.9053)	Acc@1 84.375 (79.748)	Acc@5 93.750 (97.807)	Mem 4879MB
[2022-05-31 01:32:37 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.097)	Loss 1.0671 (0.9075)	Acc@1 78.125 (79.745)	Acc@5 93.750 (97.742)	Mem 4879MB
[2022-05-31 01:32:38 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 0.7872 (0.9083)	Acc@1 90.625 (79.726)	Acc@5 100.000 (97.746)	Mem 4879MB
[2022-05-31 01:32:39 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.090 (0.097)	Loss 0.6965 (0.9108)	Acc@1 87.500 (79.710)	Acc@5 100.000 (97.675)	Mem 4879MB
[2022-05-31 01:32:40 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 0.9312 (0.9099)	Acc@1 81.250 (79.751)	Acc@5 96.875 (97.681)	Mem 4879MB
[2022-05-31 01:32:41 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 0.8568 (0.9150)	Acc@1 78.125 (79.613)	Acc@5 96.875 (97.619)	Mem 4879MB
[2022-05-31 01:32:42 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.093 (0.096)	Loss 1.3529 (0.9143)	Acc@1 56.250 (79.564)	Acc@5 93.750 (97.627)	Mem 4879MB
[2022-05-31 01:32:43 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.095 (0.096)	Loss 1.1375 (0.9115)	Acc@1 65.625 (79.607)	Acc@5 96.875 (97.647)	Mem 4879MB
[2022-05-31 01:32:44 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 1.0150 (0.9121)	Acc@1 71.875 (79.574)	Acc@5 93.750 (97.617)	Mem 4879MB
[2022-05-31 01:32:45 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.9141 (0.9128)	Acc@1 87.500 (79.555)	Acc@5 93.750 (97.613)	Mem 4879MB
[2022-05-31 01:32:46 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.9181 (0.9119)	Acc@1 75.000 (79.604)	Acc@5 100.000 (97.609)	Mem 4879MB
[2022-05-31 01:32:47 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 0.7147 (0.9110)	Acc@1 87.500 (79.607)	Acc@5 100.000 (97.584)	Mem 4879MB
[2022-05-31 01:32:48 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.8924 (0.9120)	Acc@1 81.250 (79.630)	Acc@5 96.875 (97.560)	Mem 4879MB
[2022-05-31 01:32:49 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.095)	Loss 0.7790 (0.9088)	Acc@1 90.625 (79.793)	Acc@5 93.750 (97.538)	Mem 4879MB
[2022-05-31 01:32:49 MetaFG_0] (main.py 330): INFO  * Acc@1 79.840 Acc@5 97.540
[2022-05-31 01:32:49 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 79.8%
[2022-05-31 01:32:49 MetaFG_0] (main.py 171): INFO Max accuracy: 79.84%
[2022-05-31 01:32:50 MetaFG_0] (main.py 265): INFO Train: [23/300][0/1562]	eta 0:26:13 lr 0.000006	time 1.0073 (1.0073)	loss 1.3996 (1.3996)	grad_norm 28.9026 (28.9026)	mem 4879MB
[2022-05-31 01:32:53 MetaFG_0] (main.py 265): INFO Train: [23/300][10/1562]	eta 0:09:38 lr 0.000006	time 0.2984 (0.3725)	loss 1.6359 (1.4795)	grad_norm 21.8041 (25.9534)	mem 4879MB
[2022-05-31 01:32:56 MetaFG_0] (main.py 265): INFO Train: [23/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2978 (0.3402)	loss 1.7358 (1.5120)	grad_norm 19.9203 (26.0586)	mem 4879MB
[2022-05-31 01:32:59 MetaFG_0] (main.py 265): INFO Train: [23/300][30/1562]	eta 0:08:23 lr 0.000006	time 0.2934 (0.3286)	loss 1.7044 (1.5447)	grad_norm 38.4338 (26.7278)	mem 4879MB
[2022-05-31 01:33:02 MetaFG_0] (main.py 265): INFO Train: [23/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2925 (0.3225)	loss 1.2445 (1.5373)	grad_norm 37.9506 (27.1116)	mem 4879MB
[2022-05-31 01:33:05 MetaFG_0] (main.py 265): INFO Train: [23/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2936 (0.3187)	loss 1.6770 (1.5079)	grad_norm 21.0469 (26.8099)	mem 4879MB
[2022-05-31 01:33:08 MetaFG_0] (main.py 265): INFO Train: [23/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.2919 (0.3162)	loss 1.4852 (1.4878)	grad_norm 26.0760 (26.6307)	mem 4879MB
[2022-05-31 01:33:11 MetaFG_0] (main.py 265): INFO Train: [23/300][70/1562]	eta 0:07:48 lr 0.000006	time 0.2931 (0.3143)	loss 1.6313 (1.4885)	grad_norm 24.9113 (26.7979)	mem 4879MB
[2022-05-31 01:33:14 MetaFG_0] (main.py 265): INFO Train: [23/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2915 (0.3129)	loss 1.6959 (1.4833)	grad_norm 21.2294 (26.6247)	mem 4879MB
[2022-05-31 01:33:17 MetaFG_0] (main.py 265): INFO Train: [23/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2934 (0.3119)	loss 1.8586 (1.4968)	grad_norm 27.4672 (26.8938)	mem 4879MB
[2022-05-31 01:33:20 MetaFG_0] (main.py 265): INFO Train: [23/300][100/1562]	eta 0:07:34 lr 0.000006	time 0.2920 (0.3111)	loss 1.5787 (1.4944)	grad_norm 14.5224 (27.0550)	mem 4879MB
[2022-05-31 01:33:23 MetaFG_0] (main.py 265): INFO Train: [23/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2989 (0.3105)	loss 1.1462 (1.4999)	grad_norm 42.0568 (27.2052)	mem 4879MB
[2022-05-31 01:33:26 MetaFG_0] (main.py 265): INFO Train: [23/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2996 (0.3099)	loss 1.8129 (1.5106)	grad_norm 20.4045 (27.2670)	mem 4879MB
[2022-05-31 01:33:29 MetaFG_0] (main.py 265): INFO Train: [23/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2995 (0.3094)	loss 1.5483 (1.5077)	grad_norm 18.2435 (27.1450)	mem 4879MB
[2022-05-31 01:33:32 MetaFG_0] (main.py 265): INFO Train: [23/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2998 (0.3090)	loss 1.3002 (1.5130)	grad_norm 33.2381 (27.2722)	mem 4879MB
[2022-05-31 01:33:35 MetaFG_0] (main.py 265): INFO Train: [23/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2987 (0.3088)	loss 1.4818 (1.5085)	grad_norm 21.7514 (27.2415)	mem 4879MB
[2022-05-31 01:33:38 MetaFG_0] (main.py 265): INFO Train: [23/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2997 (0.3087)	loss 1.4642 (1.5080)	grad_norm 11.8521 (27.0201)	mem 4879MB
[2022-05-31 01:33:42 MetaFG_0] (main.py 265): INFO Train: [23/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2981 (0.3085)	loss 1.3906 (1.5110)	grad_norm 18.7707 (27.3022)	mem 4879MB
[2022-05-31 01:33:45 MetaFG_0] (main.py 265): INFO Train: [23/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2934 (0.3082)	loss 1.4112 (1.4988)	grad_norm 18.5993 (27.2400)	mem 4879MB
[2022-05-31 01:33:48 MetaFG_0] (main.py 265): INFO Train: [23/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2981 (0.3081)	loss 1.4761 (1.4956)	grad_norm 29.9423 (27.2641)	mem 4879MB
[2022-05-31 01:33:51 MetaFG_0] (main.py 265): INFO Train: [23/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.3027 (0.3080)	loss 1.1093 (1.4971)	grad_norm 49.8913 (27.5455)	mem 4879MB
[2022-05-31 01:33:54 MetaFG_0] (main.py 265): INFO Train: [23/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2980 (0.3078)	loss 1.2614 (1.4945)	grad_norm 28.3801 (27.6770)	mem 4879MB
[2022-05-31 01:33:57 MetaFG_0] (main.py 265): INFO Train: [23/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2983 (0.3076)	loss 1.5587 (1.4950)	grad_norm 27.8658 (27.7213)	mem 4879MB
[2022-05-31 01:34:00 MetaFG_0] (main.py 265): INFO Train: [23/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2938 (0.3074)	loss 1.9125 (1.4980)	grad_norm 24.5412 (27.7663)	mem 4879MB
[2022-05-31 01:34:03 MetaFG_0] (main.py 265): INFO Train: [23/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2939 (0.3073)	loss 1.4025 (1.4984)	grad_norm 26.9660 (27.6334)	mem 4879MB
[2022-05-31 01:34:06 MetaFG_0] (main.py 265): INFO Train: [23/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2937 (0.3072)	loss 1.3945 (1.5001)	grad_norm 25.1222 (27.6777)	mem 4879MB
[2022-05-31 01:34:09 MetaFG_0] (main.py 265): INFO Train: [23/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2934 (0.3071)	loss 1.7589 (1.5005)	grad_norm 19.6268 (27.8640)	mem 4879MB
[2022-05-31 01:34:12 MetaFG_0] (main.py 265): INFO Train: [23/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2935 (0.3070)	loss 1.4376 (1.5012)	grad_norm 48.2207 (28.0210)	mem 4879MB
[2022-05-31 01:34:15 MetaFG_0] (main.py 265): INFO Train: [23/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.3048 (0.3068)	loss 1.6423 (1.5001)	grad_norm 19.6086 (27.9188)	mem 4879MB
[2022-05-31 01:34:18 MetaFG_0] (main.py 265): INFO Train: [23/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2926 (0.3067)	loss 1.3838 (1.5000)	grad_norm 21.5749 (28.0031)	mem 4879MB
[2022-05-31 01:34:21 MetaFG_0] (main.py 265): INFO Train: [23/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2983 (0.3067)	loss 1.4301 (1.5002)	grad_norm 34.0619 (28.1304)	mem 4879MB
[2022-05-31 01:34:24 MetaFG_0] (main.py 265): INFO Train: [23/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2932 (0.3065)	loss 1.0872 (1.4984)	grad_norm 16.0089 (28.0936)	mem 4879MB
[2022-05-31 01:34:27 MetaFG_0] (main.py 265): INFO Train: [23/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2974 (0.3065)	loss 1.7387 (1.5009)	grad_norm 28.9156 (28.1208)	mem 4879MB
[2022-05-31 01:34:30 MetaFG_0] (main.py 265): INFO Train: [23/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2916 (0.3064)	loss 1.5616 (1.5022)	grad_norm 27.8615 (28.2120)	mem 4879MB
[2022-05-31 01:34:33 MetaFG_0] (main.py 265): INFO Train: [23/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2932 (0.3063)	loss 1.9514 (1.5031)	grad_norm 44.3482 (28.2333)	mem 4879MB
[2022-05-31 01:34:36 MetaFG_0] (main.py 265): INFO Train: [23/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2916 (0.3062)	loss 1.6888 (1.5070)	grad_norm 54.8891 (28.2452)	mem 4879MB
[2022-05-31 01:34:39 MetaFG_0] (main.py 265): INFO Train: [23/300][360/1562]	eta 0:06:07 lr 0.000006	time 0.2919 (0.3061)	loss 1.6662 (1.5089)	grad_norm 25.0307 (28.1386)	mem 4879MB
[2022-05-31 01:34:42 MetaFG_0] (main.py 265): INFO Train: [23/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2932 (0.3061)	loss 1.8423 (1.5083)	grad_norm 27.0960 (28.0638)	mem 4879MB
[2022-05-31 01:34:45 MetaFG_0] (main.py 265): INFO Train: [23/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2923 (0.3060)	loss 1.2780 (1.5089)	grad_norm 17.2529 (28.0662)	mem 4879MB
[2022-05-31 01:34:48 MetaFG_0] (main.py 265): INFO Train: [23/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2990 (0.3059)	loss 1.2039 (1.5056)	grad_norm 21.3870 (28.0660)	mem 4879MB
[2022-05-31 01:34:51 MetaFG_0] (main.py 265): INFO Train: [23/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2940 (0.3059)	loss 1.8095 (1.5046)	grad_norm 33.0541 (28.0568)	mem 4879MB
[2022-05-31 01:34:54 MetaFG_0] (main.py 265): INFO Train: [23/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2925 (0.3058)	loss 1.2555 (1.5021)	grad_norm 52.8115 (28.2652)	mem 4879MB
[2022-05-31 01:34:57 MetaFG_0] (main.py 265): INFO Train: [23/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2934 (0.3058)	loss 1.4499 (1.5005)	grad_norm 33.5620 (28.3096)	mem 4879MB
[2022-05-31 01:35:01 MetaFG_0] (main.py 265): INFO Train: [23/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2989 (0.3057)	loss 1.6485 (1.5019)	grad_norm 18.9480 (28.2957)	mem 4879MB
[2022-05-31 01:35:04 MetaFG_0] (main.py 265): INFO Train: [23/300][440/1562]	eta 0:05:42 lr 0.000006	time 0.2922 (0.3056)	loss 1.4279 (1.5014)	grad_norm 38.9733 (28.2798)	mem 4879MB
[2022-05-31 01:35:07 MetaFG_0] (main.py 265): INFO Train: [23/300][450/1562]	eta 0:05:39 lr 0.000006	time 0.3001 (0.3056)	loss 1.3201 (1.5014)	grad_norm 31.5234 (28.4293)	mem 4879MB
[2022-05-31 01:35:10 MetaFG_0] (main.py 265): INFO Train: [23/300][460/1562]	eta 0:05:36 lr 0.000006	time 0.2931 (0.3055)	loss 1.5189 (1.5024)	grad_norm 34.7907 (28.5352)	mem 4879MB
[2022-05-31 01:35:13 MetaFG_0] (main.py 265): INFO Train: [23/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.2936 (0.3055)	loss 1.6891 (1.5008)	grad_norm 21.8584 (28.4748)	mem 4879MB
[2022-05-31 01:35:16 MetaFG_0] (main.py 265): INFO Train: [23/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2930 (0.3055)	loss 1.6779 (1.5015)	grad_norm 23.6386 (28.5242)	mem 4879MB
[2022-05-31 01:35:19 MetaFG_0] (main.py 265): INFO Train: [23/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2922 (0.3054)	loss 0.9758 (1.5001)	grad_norm 15.8429 (28.4325)	mem 4879MB
[2022-05-31 01:35:22 MetaFG_0] (main.py 265): INFO Train: [23/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2979 (0.3054)	loss 1.0943 (1.5014)	grad_norm 28.1151 (28.4688)	mem 4879MB
[2022-05-31 01:35:25 MetaFG_0] (main.py 265): INFO Train: [23/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2918 (0.3054)	loss 1.6673 (1.5043)	grad_norm 18.9534 (28.4518)	mem 4879MB
[2022-05-31 01:35:28 MetaFG_0] (main.py 265): INFO Train: [23/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2923 (0.3053)	loss 1.5500 (1.5051)	grad_norm 19.4237 (28.4179)	mem 4879MB
[2022-05-31 01:35:31 MetaFG_0] (main.py 265): INFO Train: [23/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2989 (0.3053)	loss 1.6788 (1.5050)	grad_norm 24.6336 (28.4007)	mem 4879MB
[2022-05-31 01:35:34 MetaFG_0] (main.py 265): INFO Train: [23/300][540/1562]	eta 0:05:11 lr 0.000006	time 0.2934 (0.3052)	loss 0.9200 (1.5032)	grad_norm 32.4314 (28.3665)	mem 4879MB
[2022-05-31 01:35:37 MetaFG_0] (main.py 265): INFO Train: [23/300][550/1562]	eta 0:05:08 lr 0.000006	time 0.2981 (0.3052)	loss 1.6427 (1.5020)	grad_norm 27.3938 (28.4620)	mem 4879MB
[2022-05-31 01:35:40 MetaFG_0] (main.py 265): INFO Train: [23/300][560/1562]	eta 0:05:05 lr 0.000006	time 0.2934 (0.3052)	loss 1.6122 (1.5022)	grad_norm 15.9601 (28.3835)	mem 4879MB
[2022-05-31 01:35:43 MetaFG_0] (main.py 265): INFO Train: [23/300][570/1562]	eta 0:05:02 lr 0.000006	time 0.2932 (0.3052)	loss 1.4184 (1.5032)	grad_norm 25.0878 (28.3980)	mem 4879MB
[2022-05-31 01:35:46 MetaFG_0] (main.py 265): INFO Train: [23/300][580/1562]	eta 0:04:59 lr 0.000006	time 0.2920 (0.3051)	loss 1.6592 (1.5043)	grad_norm 30.6470 (28.4199)	mem 4879MB
[2022-05-31 01:35:49 MetaFG_0] (main.py 265): INFO Train: [23/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2989 (0.3051)	loss 1.5904 (1.5052)	grad_norm 18.8749 (28.4025)	mem 4879MB
[2022-05-31 01:35:52 MetaFG_0] (main.py 265): INFO Train: [23/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2957 (0.3051)	loss 1.2162 (1.5054)	grad_norm 32.9803 (28.3583)	mem 4879MB
[2022-05-31 01:35:55 MetaFG_0] (main.py 265): INFO Train: [23/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2931 (0.3050)	loss 1.4016 (1.5058)	grad_norm 34.8587 (28.3805)	mem 4879MB
[2022-05-31 01:35:58 MetaFG_0] (main.py 265): INFO Train: [23/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2993 (0.3050)	loss 1.7085 (1.5075)	grad_norm 34.2506 (28.3088)	mem 4879MB
[2022-05-31 01:36:01 MetaFG_0] (main.py 265): INFO Train: [23/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2928 (0.3050)	loss 1.2922 (1.5079)	grad_norm 19.8127 (28.3843)	mem 4879MB
[2022-05-31 01:36:04 MetaFG_0] (main.py 265): INFO Train: [23/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2975 (0.3050)	loss 1.5062 (1.5066)	grad_norm 22.1901 (28.4533)	mem 4879MB
[2022-05-31 01:36:07 MetaFG_0] (main.py 265): INFO Train: [23/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2997 (0.3050)	loss 1.7664 (1.5085)	grad_norm 41.1123 (28.4991)	mem 4879MB
[2022-05-31 01:36:10 MetaFG_0] (main.py 265): INFO Train: [23/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2923 (0.3049)	loss 1.6301 (1.5090)	grad_norm 18.3500 (28.5100)	mem 4879MB
[2022-05-31 01:36:13 MetaFG_0] (main.py 265): INFO Train: [23/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2943 (0.3049)	loss 1.5778 (1.5086)	grad_norm 33.2837 (28.3767)	mem 4879MB
[2022-05-31 01:36:16 MetaFG_0] (main.py 265): INFO Train: [23/300][680/1562]	eta 0:04:28 lr 0.000006	time 0.2916 (0.3049)	loss 1.1823 (1.5099)	grad_norm 14.7603 (28.3474)	mem 4879MB
[2022-05-31 01:36:19 MetaFG_0] (main.py 265): INFO Train: [23/300][690/1562]	eta 0:04:25 lr 0.000006	time 0.2931 (0.3049)	loss 1.6574 (1.5099)	grad_norm 13.0776 (28.2424)	mem 4879MB
[2022-05-31 01:36:22 MetaFG_0] (main.py 265): INFO Train: [23/300][700/1562]	eta 0:04:22 lr 0.000006	time 0.2927 (0.3049)	loss 1.3909 (1.5086)	grad_norm 26.8459 (28.2338)	mem 4879MB
[2022-05-31 01:36:26 MetaFG_0] (main.py 265): INFO Train: [23/300][710/1562]	eta 0:04:19 lr 0.000006	time 0.2920 (0.3049)	loss 1.7879 (1.5077)	grad_norm 26.4854 (28.2021)	mem 4879MB
[2022-05-31 01:36:29 MetaFG_0] (main.py 265): INFO Train: [23/300][720/1562]	eta 0:04:16 lr 0.000006	time 0.2920 (0.3049)	loss 1.4633 (1.5067)	grad_norm 19.0111 (28.2602)	mem 4879MB
[2022-05-31 01:36:32 MetaFG_0] (main.py 265): INFO Train: [23/300][730/1562]	eta 0:04:13 lr 0.000006	time 0.2922 (0.3049)	loss 1.7935 (1.5087)	grad_norm 38.0049 (28.2062)	mem 4879MB
[2022-05-31 01:36:35 MetaFG_0] (main.py 265): INFO Train: [23/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2964 (0.3049)	loss 1.4982 (1.5095)	grad_norm 21.0090 (28.1914)	mem 4879MB
[2022-05-31 01:36:38 MetaFG_0] (main.py 265): INFO Train: [23/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2921 (0.3048)	loss 1.2007 (1.5079)	grad_norm 36.3774 (28.1670)	mem 4879MB
[2022-05-31 01:36:41 MetaFG_0] (main.py 265): INFO Train: [23/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2923 (0.3048)	loss 1.3204 (1.5067)	grad_norm 24.4721 (28.2231)	mem 4879MB
[2022-05-31 01:36:44 MetaFG_0] (main.py 265): INFO Train: [23/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2936 (0.3048)	loss 1.4210 (1.5067)	grad_norm 38.9864 (28.1977)	mem 4879MB
[2022-05-31 01:36:47 MetaFG_0] (main.py 265): INFO Train: [23/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2951 (0.3048)	loss 1.4817 (1.5066)	grad_norm 43.9715 (28.2008)	mem 4879MB
[2022-05-31 01:36:50 MetaFG_0] (main.py 265): INFO Train: [23/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2995 (0.3048)	loss 1.4022 (1.5052)	grad_norm 28.5239 (28.1696)	mem 4879MB
[2022-05-31 01:36:53 MetaFG_0] (main.py 265): INFO Train: [23/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2982 (0.3048)	loss 1.7342 (1.5064)	grad_norm 17.4843 (28.1292)	mem 4879MB
[2022-05-31 01:36:56 MetaFG_0] (main.py 265): INFO Train: [23/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2920 (0.3048)	loss 1.7135 (1.5049)	grad_norm 19.1400 (28.0960)	mem 4879MB
[2022-05-31 01:36:59 MetaFG_0] (main.py 265): INFO Train: [23/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2942 (0.3048)	loss 1.3996 (1.5049)	grad_norm 18.0149 (28.0852)	mem 4879MB
[2022-05-31 01:37:02 MetaFG_0] (main.py 265): INFO Train: [23/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2977 (0.3048)	loss 1.5111 (1.5067)	grad_norm 36.0778 (28.0985)	mem 4879MB
[2022-05-31 01:37:05 MetaFG_0] (main.py 265): INFO Train: [23/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2933 (0.3048)	loss 1.3628 (1.5073)	grad_norm 20.1315 (28.0802)	mem 4879MB
[2022-05-31 01:37:08 MetaFG_0] (main.py 265): INFO Train: [23/300][850/1562]	eta 0:03:36 lr 0.000006	time 0.2993 (0.3048)	loss 1.2385 (1.5073)	grad_norm 38.4657 (28.0755)	mem 4879MB
[2022-05-31 01:37:11 MetaFG_0] (main.py 265): INFO Train: [23/300][860/1562]	eta 0:03:33 lr 0.000006	time 0.2925 (0.3047)	loss 1.2413 (1.5059)	grad_norm 21.1789 (28.0352)	mem 4879MB
[2022-05-31 01:37:14 MetaFG_0] (main.py 265): INFO Train: [23/300][870/1562]	eta 0:03:30 lr 0.000006	time 0.2919 (0.3047)	loss 1.5728 (1.5060)	grad_norm 17.7722 (28.0371)	mem 4879MB
[2022-05-31 01:37:17 MetaFG_0] (main.py 265): INFO Train: [23/300][880/1562]	eta 0:03:27 lr 0.000006	time 0.2924 (0.3047)	loss 1.0993 (1.5049)	grad_norm 32.0479 (28.0560)	mem 4879MB
[2022-05-31 01:37:20 MetaFG_0] (main.py 265): INFO Train: [23/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2940 (0.3047)	loss 1.6648 (1.5048)	grad_norm 25.1430 (28.0638)	mem 4879MB
[2022-05-31 01:37:23 MetaFG_0] (main.py 265): INFO Train: [23/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2985 (0.3047)	loss 1.5855 (1.5054)	grad_norm 22.9481 (28.1795)	mem 4879MB
[2022-05-31 01:37:26 MetaFG_0] (main.py 265): INFO Train: [23/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2935 (0.3047)	loss 1.6666 (1.5043)	grad_norm 17.5379 (28.1764)	mem 4879MB
[2022-05-31 01:37:29 MetaFG_0] (main.py 265): INFO Train: [23/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2924 (0.3047)	loss 1.2213 (1.5045)	grad_norm 32.7645 (28.1472)	mem 4879MB
[2022-05-31 01:37:32 MetaFG_0] (main.py 265): INFO Train: [23/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2988 (0.3047)	loss 1.5864 (1.5039)	grad_norm 38.4520 (28.1676)	mem 4879MB
[2022-05-31 01:37:35 MetaFG_0] (main.py 265): INFO Train: [23/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2943 (0.3047)	loss 1.5200 (1.5042)	grad_norm 26.2426 (28.1807)	mem 4879MB
[2022-05-31 01:37:39 MetaFG_0] (main.py 265): INFO Train: [23/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2995 (0.3047)	loss 1.5308 (1.5041)	grad_norm 28.0167 (28.1753)	mem 4879MB
[2022-05-31 01:37:42 MetaFG_0] (main.py 265): INFO Train: [23/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2998 (0.3047)	loss 1.5181 (1.5038)	grad_norm 27.1239 (28.1694)	mem 4879MB
[2022-05-31 01:37:45 MetaFG_0] (main.py 265): INFO Train: [23/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2924 (0.3047)	loss 1.3981 (1.5042)	grad_norm 23.7301 (28.1585)	mem 4879MB
[2022-05-31 01:37:48 MetaFG_0] (main.py 265): INFO Train: [23/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2928 (0.3047)	loss 1.1480 (1.5038)	grad_norm 21.8550 (28.1859)	mem 4879MB
[2022-05-31 01:37:51 MetaFG_0] (main.py 265): INFO Train: [23/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2929 (0.3047)	loss 1.4665 (1.5047)	grad_norm 41.9120 (28.2316)	mem 4879MB
[2022-05-31 01:37:54 MetaFG_0] (main.py 265): INFO Train: [23/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2991 (0.3047)	loss 1.2714 (1.5044)	grad_norm 26.8963 (28.2396)	mem 4879MB
[2022-05-31 01:37:57 MetaFG_0] (main.py 265): INFO Train: [23/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2982 (0.3047)	loss 1.8197 (1.5059)	grad_norm 31.0322 (28.2796)	mem 4879MB
[2022-05-31 01:38:00 MetaFG_0] (main.py 265): INFO Train: [23/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2923 (0.3047)	loss 1.6024 (1.5058)	grad_norm 26.3531 (28.2778)	mem 4879MB
[2022-05-31 01:38:03 MetaFG_0] (main.py 265): INFO Train: [23/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2922 (0.3046)	loss 1.4206 (1.5056)	grad_norm 13.4139 (28.3709)	mem 4879MB
[2022-05-31 01:38:06 MetaFG_0] (main.py 265): INFO Train: [23/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2921 (0.3046)	loss 1.5597 (1.5067)	grad_norm 25.7115 (28.3799)	mem 4879MB
[2022-05-31 01:38:09 MetaFG_0] (main.py 265): INFO Train: [23/300][1050/1562]	eta 0:02:35 lr 0.000006	time 0.2976 (0.3046)	loss 1.6630 (1.5069)	grad_norm 15.1320 (28.4069)	mem 4879MB
[2022-05-31 01:38:12 MetaFG_0] (main.py 265): INFO Train: [23/300][1060/1562]	eta 0:02:32 lr 0.000006	time 0.2976 (0.3046)	loss 1.5545 (1.5078)	grad_norm 19.7167 (28.3758)	mem 4879MB
[2022-05-31 01:38:15 MetaFG_0] (main.py 265): INFO Train: [23/300][1070/1562]	eta 0:02:29 lr 0.000006	time 0.2926 (0.3046)	loss 1.5858 (1.5080)	grad_norm 26.3955 (28.3395)	mem 4879MB
[2022-05-31 01:38:18 MetaFG_0] (main.py 265): INFO Train: [23/300][1080/1562]	eta 0:02:26 lr 0.000006	time 0.2988 (0.3046)	loss 1.1703 (1.5079)	grad_norm 30.3569 (28.3332)	mem 4879MB
[2022-05-31 01:38:21 MetaFG_0] (main.py 265): INFO Train: [23/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2992 (0.3046)	loss 1.5811 (1.5078)	grad_norm 15.3195 (28.3339)	mem 4879MB
[2022-05-31 01:38:24 MetaFG_0] (main.py 265): INFO Train: [23/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2973 (0.3046)	loss 1.4451 (1.5070)	grad_norm 21.8068 (28.3015)	mem 4879MB
[2022-05-31 01:38:27 MetaFG_0] (main.py 265): INFO Train: [23/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2958 (0.3046)	loss 1.6896 (1.5074)	grad_norm 38.2147 (28.3134)	mem 4879MB
[2022-05-31 01:38:30 MetaFG_0] (main.py 265): INFO Train: [23/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.3016 (0.3046)	loss 1.2972 (1.5062)	grad_norm 58.0171 (28.2933)	mem 4879MB
[2022-05-31 01:38:33 MetaFG_0] (main.py 265): INFO Train: [23/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.3012 (0.3046)	loss 1.4151 (1.5062)	grad_norm 17.9878 (28.2812)	mem 4879MB
[2022-05-31 01:38:36 MetaFG_0] (main.py 265): INFO Train: [23/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2923 (0.3046)	loss 1.6999 (1.5064)	grad_norm 20.1032 (28.2859)	mem 4879MB
[2022-05-31 01:38:39 MetaFG_0] (main.py 265): INFO Train: [23/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2999 (0.3046)	loss 1.5965 (1.5066)	grad_norm 21.8262 (28.3120)	mem 4879MB
[2022-05-31 01:38:42 MetaFG_0] (main.py 265): INFO Train: [23/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2995 (0.3046)	loss 1.5660 (1.5066)	grad_norm 26.5529 (28.2852)	mem 4879MB
[2022-05-31 01:38:45 MetaFG_0] (main.py 265): INFO Train: [23/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2930 (0.3046)	loss 1.7341 (1.5065)	grad_norm 19.4512 (28.2635)	mem 4879MB
[2022-05-31 01:38:48 MetaFG_0] (main.py 265): INFO Train: [23/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2981 (0.3046)	loss 1.4430 (1.5059)	grad_norm 29.2526 (28.2142)	mem 4879MB
[2022-05-31 01:38:51 MetaFG_0] (main.py 265): INFO Train: [23/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2963 (0.3045)	loss 1.6287 (1.5066)	grad_norm 20.6289 (28.2512)	mem 4879MB
[2022-05-31 01:38:54 MetaFG_0] (main.py 265): INFO Train: [23/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2925 (0.3045)	loss 0.9954 (1.5055)	grad_norm 28.5135 (28.2254)	mem 4879MB
[2022-05-31 01:38:58 MetaFG_0] (main.py 265): INFO Train: [23/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3004 (0.3045)	loss 1.7620 (1.5062)	grad_norm 24.2207 (28.1935)	mem 4879MB
[2022-05-31 01:39:01 MetaFG_0] (main.py 265): INFO Train: [23/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2938 (0.3045)	loss 1.3925 (1.5058)	grad_norm 18.8032 (28.1640)	mem 4879MB
[2022-05-31 01:39:04 MetaFG_0] (main.py 265): INFO Train: [23/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2983 (0.3045)	loss 1.8286 (1.5062)	grad_norm 18.7074 (28.1618)	mem 4879MB
[2022-05-31 01:39:07 MetaFG_0] (main.py 265): INFO Train: [23/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2919 (0.3045)	loss 1.5688 (1.5060)	grad_norm 24.7978 (28.1985)	mem 4879MB
[2022-05-31 01:39:10 MetaFG_0] (main.py 265): INFO Train: [23/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2928 (0.3045)	loss 1.6097 (1.5066)	grad_norm 32.2598 (28.1860)	mem 4879MB
[2022-05-31 01:39:13 MetaFG_0] (main.py 265): INFO Train: [23/300][1260/1562]	eta 0:01:31 lr 0.000006	time 0.2921 (0.3045)	loss 1.2412 (1.5064)	grad_norm 31.4907 (28.1623)	mem 4879MB
[2022-05-31 01:39:16 MetaFG_0] (main.py 265): INFO Train: [23/300][1270/1562]	eta 0:01:28 lr 0.000006	time 0.2979 (0.3045)	loss 1.5166 (1.5058)	grad_norm 25.0902 (28.2002)	mem 4879MB
[2022-05-31 01:39:19 MetaFG_0] (main.py 265): INFO Train: [23/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2945 (0.3045)	loss 1.3041 (1.5050)	grad_norm 16.5285 (28.2134)	mem 4879MB
[2022-05-31 01:39:22 MetaFG_0] (main.py 265): INFO Train: [23/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2933 (0.3045)	loss 1.7697 (1.5049)	grad_norm 26.2961 (28.2467)	mem 4879MB
[2022-05-31 01:39:25 MetaFG_0] (main.py 265): INFO Train: [23/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2975 (0.3045)	loss 1.6379 (1.5052)	grad_norm 22.4425 (28.2123)	mem 4879MB
[2022-05-31 01:39:28 MetaFG_0] (main.py 265): INFO Train: [23/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2917 (0.3045)	loss 1.4848 (1.5056)	grad_norm 28.0601 (28.2386)	mem 4879MB
[2022-05-31 01:39:31 MetaFG_0] (main.py 265): INFO Train: [23/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2992 (0.3045)	loss 1.2239 (1.5055)	grad_norm 24.3872 (28.2472)	mem 4879MB
[2022-05-31 01:39:34 MetaFG_0] (main.py 265): INFO Train: [23/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2919 (0.3045)	loss 1.7204 (1.5058)	grad_norm 39.0554 (28.2422)	mem 4879MB
[2022-05-31 01:39:37 MetaFG_0] (main.py 265): INFO Train: [23/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2991 (0.3045)	loss 1.4033 (1.5058)	grad_norm 28.6415 (28.2307)	mem 4879MB
[2022-05-31 01:39:40 MetaFG_0] (main.py 265): INFO Train: [23/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2918 (0.3045)	loss 1.8140 (1.5054)	grad_norm 40.9109 (28.2402)	mem 4879MB
[2022-05-31 01:39:43 MetaFG_0] (main.py 265): INFO Train: [23/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2927 (0.3045)	loss 1.6336 (1.5059)	grad_norm 18.8301 (28.2243)	mem 4879MB
[2022-05-31 01:39:46 MetaFG_0] (main.py 265): INFO Train: [23/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2924 (0.3045)	loss 1.5532 (1.5058)	grad_norm 34.3804 (28.2119)	mem 4879MB
[2022-05-31 01:39:49 MetaFG_0] (main.py 265): INFO Train: [23/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3021 (0.3045)	loss 1.6739 (1.5059)	grad_norm 20.4124 (28.2083)	mem 4879MB
[2022-05-31 01:39:52 MetaFG_0] (main.py 265): INFO Train: [23/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2976 (0.3045)	loss 1.6791 (1.5067)	grad_norm 20.8870 (28.2133)	mem 4879MB
[2022-05-31 01:39:55 MetaFG_0] (main.py 265): INFO Train: [23/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2930 (0.3045)	loss 1.8137 (1.5070)	grad_norm 45.3869 (28.2133)	mem 4879MB
[2022-05-31 01:39:58 MetaFG_0] (main.py 265): INFO Train: [23/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2973 (0.3045)	loss 1.2895 (1.5067)	grad_norm 40.0420 (28.2642)	mem 4879MB
[2022-05-31 01:40:01 MetaFG_0] (main.py 265): INFO Train: [23/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2926 (0.3045)	loss 1.8817 (1.5074)	grad_norm 17.6842 (28.2773)	mem 4879MB
[2022-05-31 01:40:04 MetaFG_0] (main.py 265): INFO Train: [23/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2923 (0.3044)	loss 1.5231 (1.5078)	grad_norm 16.1695 (28.2819)	mem 4879MB
[2022-05-31 01:40:07 MetaFG_0] (main.py 265): INFO Train: [23/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2992 (0.3044)	loss 1.2896 (1.5080)	grad_norm 21.3206 (28.2856)	mem 4879MB
[2022-05-31 01:40:11 MetaFG_0] (main.py 265): INFO Train: [23/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2925 (0.3044)	loss 1.2796 (1.5076)	grad_norm 29.5268 (28.3150)	mem 4879MB
[2022-05-31 01:40:14 MetaFG_0] (main.py 265): INFO Train: [23/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2945 (0.3044)	loss 1.2662 (1.5077)	grad_norm 26.9074 (28.3140)	mem 4879MB
[2022-05-31 01:40:17 MetaFG_0] (main.py 265): INFO Train: [23/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2932 (0.3044)	loss 1.2235 (1.5079)	grad_norm 26.2633 (28.2999)	mem 4879MB
[2022-05-31 01:40:20 MetaFG_0] (main.py 265): INFO Train: [23/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2919 (0.3044)	loss 1.8347 (1.5080)	grad_norm 16.4571 (28.3355)	mem 4879MB
[2022-05-31 01:40:23 MetaFG_0] (main.py 265): INFO Train: [23/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2917 (0.3044)	loss 1.2483 (1.5080)	grad_norm 23.6650 (28.3075)	mem 4879MB
[2022-05-31 01:40:26 MetaFG_0] (main.py 265): INFO Train: [23/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2974 (0.3044)	loss 1.6373 (1.5084)	grad_norm 30.2481 (28.3197)	mem 4879MB
[2022-05-31 01:40:29 MetaFG_0] (main.py 265): INFO Train: [23/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2995 (0.3044)	loss 1.5343 (1.5077)	grad_norm 32.5645 (28.2993)	mem 4879MB
[2022-05-31 01:40:32 MetaFG_0] (main.py 265): INFO Train: [23/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2996 (0.3045)	loss 1.7142 (1.5078)	grad_norm 18.4907 (28.3100)	mem 4879MB
[2022-05-31 01:40:35 MetaFG_0] (main.py 265): INFO Train: [23/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2978 (0.3044)	loss 1.4077 (1.5076)	grad_norm 34.8353 (28.2957)	mem 4879MB
[2022-05-31 01:40:38 MetaFG_0] (main.py 265): INFO Train: [23/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2979 (0.3044)	loss 1.8119 (1.5079)	grad_norm 22.3995 (28.2664)	mem 4879MB
[2022-05-31 01:40:41 MetaFG_0] (main.py 265): INFO Train: [23/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2978 (0.3044)	loss 1.7045 (1.5075)	grad_norm 25.9295 (28.2990)	mem 4879MB
[2022-05-31 01:40:44 MetaFG_0] (main.py 265): INFO Train: [23/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2915 (0.3044)	loss 1.2427 (1.5072)	grad_norm 27.4871 (28.2886)	mem 4879MB
[2022-05-31 01:40:44 MetaFG_0] (main.py 272): INFO EPOCH 23 training takes 0:07:55
[2022-05-31 01:40:44 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_23.pth saving......
[2022-05-31 01:40:45 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_23.pth saved !!!
[2022-05-31 01:40:45 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:40:47 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:40:47 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:40:47 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.627 (0.627)	Loss 0.7118 (0.7118)	Acc@1 87.500 (87.500)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 01:40:48 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.143)	Loss 1.1198 (0.9058)	Acc@1 75.000 (77.841)	Acc@5 93.750 (96.591)	Mem 4879MB
[2022-05-31 01:40:49 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.120)	Loss 0.8667 (0.8919)	Acc@1 81.250 (79.315)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 01:40:50 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.095 (0.112)	Loss 0.7482 (0.8946)	Acc@1 78.125 (77.722)	Acc@5 100.000 (97.379)	Mem 4879MB
[2022-05-31 01:40:51 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.107)	Loss 1.0096 (0.8875)	Acc@1 78.125 (78.277)	Acc@5 96.875 (97.409)	Mem 4879MB
[2022-05-31 01:40:52 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.105 (0.105)	Loss 0.6558 (0.8906)	Acc@1 90.625 (78.309)	Acc@5 100.000 (97.059)	Mem 4879MB
[2022-05-31 01:40:53 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.100 (0.104)	Loss 1.0267 (0.8883)	Acc@1 78.125 (78.637)	Acc@5 96.875 (97.336)	Mem 4879MB
[2022-05-31 01:40:54 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.102)	Loss 0.7482 (0.8863)	Acc@1 90.625 (78.741)	Acc@5 100.000 (97.447)	Mem 4879MB
[2022-05-31 01:40:55 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.101)	Loss 1.0216 (0.8953)	Acc@1 78.125 (78.279)	Acc@5 93.750 (97.492)	Mem 4879MB
[2022-05-31 01:40:56 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.100)	Loss 0.8305 (0.8952)	Acc@1 75.000 (78.297)	Acc@5 100.000 (97.390)	Mem 4879MB
[2022-05-31 01:40:57 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.099)	Loss 0.6153 (0.8790)	Acc@1 90.625 (78.837)	Acc@5 100.000 (97.432)	Mem 4879MB
[2022-05-31 01:40:58 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.099)	Loss 0.9861 (0.8761)	Acc@1 71.875 (78.913)	Acc@5 96.875 (97.382)	Mem 4879MB
[2022-05-31 01:40:59 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.098)	Loss 0.7967 (0.8763)	Acc@1 84.375 (78.796)	Acc@5 96.875 (97.340)	Mem 4879MB
[2022-05-31 01:41:00 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.100 (0.098)	Loss 0.7213 (0.8729)	Acc@1 87.500 (79.031)	Acc@5 100.000 (97.376)	Mem 4879MB
[2022-05-31 01:41:01 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.098)	Loss 0.8704 (0.8716)	Acc@1 78.125 (79.100)	Acc@5 96.875 (97.429)	Mem 4879MB
[2022-05-31 01:41:01 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.8077 (0.8683)	Acc@1 75.000 (79.325)	Acc@5 96.875 (97.537)	Mem 4879MB
[2022-05-31 01:41:02 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.097)	Loss 1.4087 (0.8722)	Acc@1 68.750 (79.484)	Acc@5 93.750 (97.457)	Mem 4879MB
[2022-05-31 01:41:03 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.098 (0.097)	Loss 0.9768 (0.8689)	Acc@1 84.375 (79.532)	Acc@5 96.875 (97.515)	Mem 4879MB
[2022-05-31 01:41:04 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.097)	Loss 0.9742 (0.8681)	Acc@1 81.250 (79.679)	Acc@5 93.750 (97.497)	Mem 4879MB
[2022-05-31 01:41:05 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.095 (0.097)	Loss 0.7008 (0.8721)	Acc@1 90.625 (79.614)	Acc@5 100.000 (97.448)	Mem 4879MB
[2022-05-31 01:41:06 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.096)	Loss 0.8158 (0.8735)	Acc@1 78.125 (79.586)	Acc@5 100.000 (97.512)	Mem 4879MB
[2022-05-31 01:41:07 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.096)	Loss 0.7071 (0.8737)	Acc@1 90.625 (79.621)	Acc@5 96.875 (97.527)	Mem 4879MB
[2022-05-31 01:41:08 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.096)	Loss 0.8024 (0.8721)	Acc@1 78.125 (79.539)	Acc@5 100.000 (97.554)	Mem 4879MB
[2022-05-31 01:41:09 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 0.8668 (0.8709)	Acc@1 75.000 (79.559)	Acc@5 100.000 (97.606)	Mem 4879MB
[2022-05-31 01:41:10 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.095 (0.096)	Loss 0.8623 (0.8684)	Acc@1 81.250 (79.681)	Acc@5 96.875 (97.575)	Mem 4879MB
[2022-05-31 01:41:11 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 0.7606 (0.8672)	Acc@1 84.375 (79.694)	Acc@5 96.875 (97.572)	Mem 4879MB
[2022-05-31 01:41:12 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.095 (0.096)	Loss 0.9813 (0.8674)	Acc@1 81.250 (79.682)	Acc@5 96.875 (97.593)	Mem 4879MB
[2022-05-31 01:41:13 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 1.0023 (0.8672)	Acc@1 71.875 (79.762)	Acc@5 100.000 (97.578)	Mem 4879MB
[2022-05-31 01:41:14 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.5507 (0.8661)	Acc@1 87.500 (79.860)	Acc@5 100.000 (97.609)	Mem 4879MB
[2022-05-31 01:41:15 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 0.6315 (0.8628)	Acc@1 87.500 (79.994)	Acc@5 96.875 (97.627)	Mem 4879MB
[2022-05-31 01:41:16 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 0.7907 (0.8652)	Acc@1 78.125 (79.838)	Acc@5 100.000 (97.654)	Mem 4879MB
[2022-05-31 01:41:16 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 1.0400 (0.8671)	Acc@1 84.375 (79.823)	Acc@5 93.750 (97.609)	Mem 4879MB
[2022-05-31 01:41:17 MetaFG_0] (main.py 330): INFO  * Acc@1 79.830 Acc@5 97.620
[2022-05-31 01:41:17 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 79.8%
[2022-05-31 01:41:17 MetaFG_0] (main.py 171): INFO Max accuracy: 79.84%
[2022-05-31 01:41:18 MetaFG_0] (main.py 265): INFO Train: [24/300][0/1562]	eta 0:25:24 lr 0.000006	time 0.9758 (0.9758)	loss 1.5752 (1.5752)	grad_norm 21.2786 (21.2786)	mem 4879MB
[2022-05-31 01:41:21 MetaFG_0] (main.py 265): INFO Train: [24/300][10/1562]	eta 0:09:33 lr 0.000006	time 0.2930 (0.3696)	loss 1.4956 (1.3403)	grad_norm 25.7328 (29.8664)	mem 4879MB
[2022-05-31 01:41:24 MetaFG_0] (main.py 265): INFO Train: [24/300][20/1562]	eta 0:08:41 lr 0.000006	time 0.2992 (0.3381)	loss 1.7759 (1.4243)	grad_norm 33.0570 (30.1259)	mem 4879MB
[2022-05-31 01:41:27 MetaFG_0] (main.py 265): INFO Train: [24/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2979 (0.3282)	loss 1.6268 (1.4405)	grad_norm 21.4411 (30.4636)	mem 4879MB
[2022-05-31 01:41:30 MetaFG_0] (main.py 265): INFO Train: [24/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2979 (0.3220)	loss 1.5275 (1.4518)	grad_norm 13.5748 (29.5386)	mem 4879MB
[2022-05-31 01:41:33 MetaFG_0] (main.py 265): INFO Train: [24/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.3016 (0.3186)	loss 1.4022 (1.4663)	grad_norm 44.3067 (29.0185)	mem 4879MB
[2022-05-31 01:41:36 MetaFG_0] (main.py 265): INFO Train: [24/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.3010 (0.3163)	loss 0.8265 (1.4730)	grad_norm 22.7897 (29.7325)	mem 4879MB
[2022-05-31 01:41:39 MetaFG_0] (main.py 265): INFO Train: [24/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.2928 (0.3145)	loss 1.5440 (1.4801)	grad_norm 31.3780 (28.9117)	mem 4879MB
[2022-05-31 01:41:42 MetaFG_0] (main.py 265): INFO Train: [24/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2930 (0.3131)	loss 1.0625 (1.4720)	grad_norm 42.6214 (28.2019)	mem 4879MB
[2022-05-31 01:41:45 MetaFG_0] (main.py 265): INFO Train: [24/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2924 (0.3121)	loss 1.8666 (1.4799)	grad_norm 17.6515 (28.9972)	mem 4879MB
[2022-05-31 01:41:48 MetaFG_0] (main.py 265): INFO Train: [24/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2981 (0.3115)	loss 1.4683 (1.4850)	grad_norm 32.5120 (29.1282)	mem 4879MB
[2022-05-31 01:41:51 MetaFG_0] (main.py 265): INFO Train: [24/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2921 (0.3107)	loss 1.2416 (1.4836)	grad_norm 43.1295 (29.0980)	mem 4879MB
[2022-05-31 01:41:54 MetaFG_0] (main.py 265): INFO Train: [24/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2938 (0.3101)	loss 1.3030 (1.4895)	grad_norm 59.4439 (29.5235)	mem 4879MB
[2022-05-31 01:41:57 MetaFG_0] (main.py 265): INFO Train: [24/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2925 (0.3095)	loss 1.6252 (1.4904)	grad_norm 21.2030 (29.1979)	mem 4879MB
[2022-05-31 01:42:00 MetaFG_0] (main.py 265): INFO Train: [24/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2988 (0.3090)	loss 1.5158 (1.4885)	grad_norm 17.1897 (28.9259)	mem 4879MB
[2022-05-31 01:42:03 MetaFG_0] (main.py 265): INFO Train: [24/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2980 (0.3088)	loss 1.2275 (1.4928)	grad_norm 59.5134 (29.1632)	mem 4879MB
[2022-05-31 01:42:06 MetaFG_0] (main.py 265): INFO Train: [24/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2924 (0.3085)	loss 1.5940 (1.4998)	grad_norm 37.4690 (29.3085)	mem 4879MB
[2022-05-31 01:42:09 MetaFG_0] (main.py 265): INFO Train: [24/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2945 (0.3082)	loss 1.1843 (1.5028)	grad_norm 33.8374 (29.3990)	mem 4879MB
[2022-05-31 01:42:12 MetaFG_0] (main.py 265): INFO Train: [24/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2918 (0.3081)	loss 1.2612 (1.5011)	grad_norm 26.8462 (29.2696)	mem 4879MB
[2022-05-31 01:42:15 MetaFG_0] (main.py 265): INFO Train: [24/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2922 (0.3079)	loss 1.4100 (1.5011)	grad_norm 28.2260 (29.0415)	mem 4879MB
[2022-05-31 01:42:19 MetaFG_0] (main.py 265): INFO Train: [24/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.3314 (0.3087)	loss 1.7966 (1.5026)	grad_norm 44.9850 (29.1179)	mem 4879MB
[2022-05-31 01:42:22 MetaFG_0] (main.py 265): INFO Train: [24/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2924 (0.3091)	loss 1.5042 (1.5033)	grad_norm 33.4780 (29.1470)	mem 4879MB
[2022-05-31 01:42:25 MetaFG_0] (main.py 265): INFO Train: [24/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2924 (0.3089)	loss 1.5273 (1.5055)	grad_norm 25.5266 (29.0678)	mem 4879MB
[2022-05-31 01:42:28 MetaFG_0] (main.py 265): INFO Train: [24/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2993 (0.3088)	loss 1.6708 (1.5072)	grad_norm 34.1234 (29.1471)	mem 4879MB
[2022-05-31 01:42:31 MetaFG_0] (main.py 265): INFO Train: [24/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.3001 (0.3087)	loss 1.4088 (1.5058)	grad_norm 24.7766 (29.0565)	mem 4879MB
[2022-05-31 01:42:34 MetaFG_0] (main.py 265): INFO Train: [24/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2932 (0.3085)	loss 1.4107 (1.5067)	grad_norm 22.5038 (29.1018)	mem 4879MB
[2022-05-31 01:42:37 MetaFG_0] (main.py 265): INFO Train: [24/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2932 (0.3083)	loss 1.6157 (1.5004)	grad_norm 18.4325 (29.2733)	mem 4879MB
[2022-05-31 01:42:40 MetaFG_0] (main.py 265): INFO Train: [24/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2919 (0.3082)	loss 1.5402 (1.4994)	grad_norm 13.4928 (29.2157)	mem 4879MB
[2022-05-31 01:42:43 MetaFG_0] (main.py 265): INFO Train: [24/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2924 (0.3080)	loss 1.6875 (1.5003)	grad_norm 18.4127 (29.1511)	mem 4879MB
[2022-05-31 01:42:46 MetaFG_0] (main.py 265): INFO Train: [24/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2978 (0.3078)	loss 1.3464 (1.4992)	grad_norm 22.3435 (28.9741)	mem 4879MB
[2022-05-31 01:42:49 MetaFG_0] (main.py 265): INFO Train: [24/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2939 (0.3077)	loss 1.2546 (1.4963)	grad_norm 23.7733 (28.8966)	mem 4879MB
[2022-05-31 01:42:52 MetaFG_0] (main.py 265): INFO Train: [24/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2921 (0.3076)	loss 0.9683 (1.4976)	grad_norm 22.2838 (28.7795)	mem 4879MB
[2022-05-31 01:42:55 MetaFG_0] (main.py 265): INFO Train: [24/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2961 (0.3075)	loss 1.4644 (1.4943)	grad_norm 53.8929 (28.8554)	mem 4879MB
[2022-05-31 01:42:58 MetaFG_0] (main.py 265): INFO Train: [24/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2975 (0.3074)	loss 1.0963 (1.4934)	grad_norm 24.9578 (28.9129)	mem 4879MB
[2022-05-31 01:43:01 MetaFG_0] (main.py 265): INFO Train: [24/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2948 (0.3073)	loss 1.2078 (1.4914)	grad_norm 35.8059 (29.0299)	mem 4879MB
[2022-05-31 01:43:05 MetaFG_0] (main.py 265): INFO Train: [24/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2916 (0.3072)	loss 1.3891 (1.4906)	grad_norm 29.3713 (29.1375)	mem 4879MB
[2022-05-31 01:43:08 MetaFG_0] (main.py 265): INFO Train: [24/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2984 (0.3072)	loss 1.6696 (1.4920)	grad_norm 22.7046 (29.1232)	mem 4879MB
[2022-05-31 01:43:11 MetaFG_0] (main.py 265): INFO Train: [24/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2986 (0.3072)	loss 1.8964 (1.4958)	grad_norm 18.3682 (29.0976)	mem 4879MB
[2022-05-31 01:43:14 MetaFG_0] (main.py 265): INFO Train: [24/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2977 (0.3071)	loss 1.3525 (1.4940)	grad_norm 31.2208 (29.0737)	mem 4879MB
[2022-05-31 01:43:17 MetaFG_0] (main.py 265): INFO Train: [24/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.3003 (0.3071)	loss 1.7365 (1.4958)	grad_norm 34.3558 (29.1112)	mem 4879MB
[2022-05-31 01:43:20 MetaFG_0] (main.py 265): INFO Train: [24/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2924 (0.3069)	loss 1.2429 (1.4973)	grad_norm 22.1555 (29.0479)	mem 4879MB
[2022-05-31 01:43:23 MetaFG_0] (main.py 265): INFO Train: [24/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2976 (0.3069)	loss 1.4208 (1.4967)	grad_norm 16.4673 (29.0128)	mem 4879MB
[2022-05-31 01:43:26 MetaFG_0] (main.py 265): INFO Train: [24/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2984 (0.3068)	loss 1.4879 (1.4961)	grad_norm 31.0986 (28.9383)	mem 4879MB
[2022-05-31 01:43:29 MetaFG_0] (main.py 265): INFO Train: [24/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2981 (0.3068)	loss 1.8260 (1.4956)	grad_norm 28.7991 (28.9831)	mem 4879MB
[2022-05-31 01:43:32 MetaFG_0] (main.py 265): INFO Train: [24/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2981 (0.3067)	loss 1.6473 (1.4956)	grad_norm 35.3162 (28.9043)	mem 4879MB
[2022-05-31 01:43:35 MetaFG_0] (main.py 265): INFO Train: [24/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2927 (0.3066)	loss 1.2545 (1.4944)	grad_norm 15.1190 (28.8111)	mem 4879MB
[2022-05-31 01:43:38 MetaFG_0] (main.py 265): INFO Train: [24/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2992 (0.3066)	loss 1.1747 (1.4927)	grad_norm 24.5279 (28.7382)	mem 4879MB
[2022-05-31 01:43:41 MetaFG_0] (main.py 265): INFO Train: [24/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.3008 (0.3066)	loss 1.6474 (1.4915)	grad_norm 24.9119 (28.6670)	mem 4879MB
[2022-05-31 01:43:44 MetaFG_0] (main.py 265): INFO Train: [24/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2933 (0.3065)	loss 1.5989 (1.4909)	grad_norm 27.4228 (28.6409)	mem 4879MB
[2022-05-31 01:43:47 MetaFG_0] (main.py 265): INFO Train: [24/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2986 (0.3065)	loss 1.3510 (1.4912)	grad_norm 33.1184 (28.7055)	mem 4879MB
[2022-05-31 01:43:50 MetaFG_0] (main.py 265): INFO Train: [24/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2935 (0.3065)	loss 1.3192 (1.4895)	grad_norm 16.8471 (28.6657)	mem 4879MB
[2022-05-31 01:43:53 MetaFG_0] (main.py 265): INFO Train: [24/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2987 (0.3064)	loss 1.7056 (1.4914)	grad_norm 23.3147 (28.6146)	mem 4879MB
[2022-05-31 01:43:56 MetaFG_0] (main.py 265): INFO Train: [24/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2925 (0.3064)	loss 1.1984 (1.4899)	grad_norm 37.2852 (28.6892)	mem 4879MB
[2022-05-31 01:43:59 MetaFG_0] (main.py 265): INFO Train: [24/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2999 (0.3063)	loss 1.5497 (1.4901)	grad_norm 32.4559 (28.6379)	mem 4879MB
[2022-05-31 01:44:02 MetaFG_0] (main.py 265): INFO Train: [24/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2928 (0.3063)	loss 1.5810 (1.4893)	grad_norm 27.1897 (28.6084)	mem 4879MB
[2022-05-31 01:44:05 MetaFG_0] (main.py 265): INFO Train: [24/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2931 (0.3062)	loss 1.1021 (1.4883)	grad_norm 22.8732 (28.6835)	mem 4879MB
[2022-05-31 01:44:08 MetaFG_0] (main.py 265): INFO Train: [24/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2929 (0.3062)	loss 1.8338 (1.4889)	grad_norm 36.2136 (28.6662)	mem 4879MB
[2022-05-31 01:44:12 MetaFG_0] (main.py 265): INFO Train: [24/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.3014 (0.3062)	loss 1.5141 (1.4894)	grad_norm 33.1814 (28.6127)	mem 4879MB
[2022-05-31 01:44:15 MetaFG_0] (main.py 265): INFO Train: [24/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2940 (0.3062)	loss 1.0575 (1.4889)	grad_norm 25.7968 (28.6767)	mem 4879MB
[2022-05-31 01:44:18 MetaFG_0] (main.py 265): INFO Train: [24/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2942 (0.3061)	loss 1.4342 (1.4892)	grad_norm 24.2031 (28.6236)	mem 4879MB
[2022-05-31 01:44:21 MetaFG_0] (main.py 265): INFO Train: [24/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2983 (0.3061)	loss 1.7266 (1.4894)	grad_norm 29.4981 (28.5474)	mem 4879MB
[2022-05-31 01:44:24 MetaFG_0] (main.py 265): INFO Train: [24/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2989 (0.3061)	loss 1.8676 (1.4903)	grad_norm 51.0939 (28.5665)	mem 4879MB
[2022-05-31 01:44:27 MetaFG_0] (main.py 265): INFO Train: [24/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2932 (0.3061)	loss 1.4908 (1.4906)	grad_norm 29.6256 (28.5905)	mem 4879MB
[2022-05-31 01:44:30 MetaFG_0] (main.py 265): INFO Train: [24/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2986 (0.3060)	loss 1.6497 (1.4921)	grad_norm 38.4118 (28.5441)	mem 4879MB
[2022-05-31 01:44:33 MetaFG_0] (main.py 265): INFO Train: [24/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3396 (0.3063)	loss 1.7643 (1.4931)	grad_norm 30.4897 (28.5442)	mem 4879MB
[2022-05-31 01:44:36 MetaFG_0] (main.py 265): INFO Train: [24/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2923 (0.3064)	loss 1.1382 (1.4930)	grad_norm 28.2978 (28.5807)	mem 4879MB
[2022-05-31 01:44:39 MetaFG_0] (main.py 265): INFO Train: [24/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2934 (0.3063)	loss 1.5637 (1.4937)	grad_norm 15.4884 (28.5680)	mem 4879MB
[2022-05-31 01:44:42 MetaFG_0] (main.py 265): INFO Train: [24/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2925 (0.3063)	loss 1.3565 (1.4954)	grad_norm 26.6797 (28.5589)	mem 4879MB
[2022-05-31 01:44:45 MetaFG_0] (main.py 265): INFO Train: [24/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2925 (0.3063)	loss 1.5917 (1.4962)	grad_norm 26.2146 (28.5133)	mem 4879MB
[2022-05-31 01:44:48 MetaFG_0] (main.py 265): INFO Train: [24/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2996 (0.3062)	loss 1.3170 (1.4950)	grad_norm 58.9657 (28.4717)	mem 4879MB
[2022-05-31 01:44:51 MetaFG_0] (main.py 265): INFO Train: [24/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2935 (0.3062)	loss 1.4129 (1.4953)	grad_norm 23.1085 (28.4693)	mem 4879MB
[2022-05-31 01:44:54 MetaFG_0] (main.py 265): INFO Train: [24/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2918 (0.3062)	loss 1.1002 (1.4951)	grad_norm 30.5253 (28.3957)	mem 4879MB
[2022-05-31 01:44:57 MetaFG_0] (main.py 265): INFO Train: [24/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2989 (0.3061)	loss 1.4610 (1.4962)	grad_norm 26.3798 (28.3423)	mem 4879MB
[2022-05-31 01:45:00 MetaFG_0] (main.py 265): INFO Train: [24/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2921 (0.3061)	loss 1.6448 (1.4961)	grad_norm 23.5202 (28.3136)	mem 4879MB
[2022-05-31 01:45:04 MetaFG_0] (main.py 265): INFO Train: [24/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2916 (0.3061)	loss 1.4279 (1.4957)	grad_norm 35.7796 (28.2897)	mem 4879MB
[2022-05-31 01:45:07 MetaFG_0] (main.py 265): INFO Train: [24/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.3000 (0.3061)	loss 1.7547 (1.4965)	grad_norm 34.6840 (28.2928)	mem 4879MB
[2022-05-31 01:45:10 MetaFG_0] (main.py 265): INFO Train: [24/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2997 (0.3061)	loss 1.5021 (1.4961)	grad_norm 18.4811 (28.2870)	mem 4879MB
[2022-05-31 01:45:13 MetaFG_0] (main.py 265): INFO Train: [24/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2987 (0.3061)	loss 1.7216 (1.4978)	grad_norm 25.4807 (28.2700)	mem 4879MB
[2022-05-31 01:45:16 MetaFG_0] (main.py 265): INFO Train: [24/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2984 (0.3060)	loss 1.0890 (1.4977)	grad_norm 33.9393 (28.2117)	mem 4879MB
[2022-05-31 01:45:19 MetaFG_0] (main.py 265): INFO Train: [24/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2992 (0.3060)	loss 1.7592 (1.4987)	grad_norm 30.2437 (28.1944)	mem 4879MB
[2022-05-31 01:45:22 MetaFG_0] (main.py 265): INFO Train: [24/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2990 (0.3060)	loss 1.2525 (1.4990)	grad_norm 22.6688 (28.2248)	mem 4879MB
[2022-05-31 01:45:25 MetaFG_0] (main.py 265): INFO Train: [24/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2978 (0.3059)	loss 1.5735 (1.4986)	grad_norm 39.7262 (28.2271)	mem 4879MB
[2022-05-31 01:45:28 MetaFG_0] (main.py 265): INFO Train: [24/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2941 (0.3059)	loss 1.1595 (1.4971)	grad_norm 17.7119 (28.2615)	mem 4879MB
[2022-05-31 01:45:31 MetaFG_0] (main.py 265): INFO Train: [24/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.3033 (0.3059)	loss 1.5268 (1.4972)	grad_norm 24.8802 (28.3420)	mem 4879MB
[2022-05-31 01:45:34 MetaFG_0] (main.py 265): INFO Train: [24/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2982 (0.3059)	loss 1.5875 (1.4966)	grad_norm 20.7322 (28.3830)	mem 4879MB
[2022-05-31 01:45:37 MetaFG_0] (main.py 265): INFO Train: [24/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.3023 (0.3059)	loss 1.4697 (1.4962)	grad_norm 23.8960 (28.3220)	mem 4879MB
[2022-05-31 01:45:40 MetaFG_0] (main.py 265): INFO Train: [24/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2981 (0.3059)	loss 1.2465 (1.4949)	grad_norm 26.5739 (28.3161)	mem 4879MB
[2022-05-31 01:45:43 MetaFG_0] (main.py 265): INFO Train: [24/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3032 (0.3059)	loss 1.4326 (1.4954)	grad_norm 16.8146 (28.3072)	mem 4879MB
[2022-05-31 01:45:46 MetaFG_0] (main.py 265): INFO Train: [24/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3003 (0.3059)	loss 1.6806 (1.4955)	grad_norm 28.5011 (28.3124)	mem 4879MB
[2022-05-31 01:45:49 MetaFG_0] (main.py 265): INFO Train: [24/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2981 (0.3058)	loss 1.7207 (1.4960)	grad_norm 40.8837 (28.3491)	mem 4879MB
[2022-05-31 01:45:52 MetaFG_0] (main.py 265): INFO Train: [24/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2988 (0.3058)	loss 1.6597 (1.4961)	grad_norm 25.7652 (28.3203)	mem 4879MB
[2022-05-31 01:45:55 MetaFG_0] (main.py 265): INFO Train: [24/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2994 (0.3058)	loss 1.8541 (1.4959)	grad_norm 50.7279 (28.3483)	mem 4879MB
[2022-05-31 01:45:58 MetaFG_0] (main.py 265): INFO Train: [24/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2933 (0.3058)	loss 1.7121 (1.4951)	grad_norm 32.6059 (28.4352)	mem 4879MB
[2022-05-31 01:46:01 MetaFG_0] (main.py 265): INFO Train: [24/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3026 (0.3058)	loss 1.4690 (1.4941)	grad_norm 28.1445 (28.4509)	mem 4879MB
[2022-05-31 01:46:04 MetaFG_0] (main.py 265): INFO Train: [24/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2948 (0.3058)	loss 1.8494 (1.4961)	grad_norm 29.0724 (28.4954)	mem 4879MB
[2022-05-31 01:46:07 MetaFG_0] (main.py 265): INFO Train: [24/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2917 (0.3057)	loss 1.8767 (1.4969)	grad_norm 36.6321 (28.4866)	mem 4879MB
[2022-05-31 01:46:11 MetaFG_0] (main.py 265): INFO Train: [24/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2919 (0.3058)	loss 1.6351 (1.4964)	grad_norm 22.9360 (28.4855)	mem 4879MB
[2022-05-31 01:46:14 MetaFG_0] (main.py 265): INFO Train: [24/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.3088 (0.3058)	loss 1.7236 (1.4952)	grad_norm 20.4833 (28.4148)	mem 4879MB
[2022-05-31 01:46:17 MetaFG_0] (main.py 265): INFO Train: [24/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2949 (0.3057)	loss 1.6017 (1.4954)	grad_norm 42.9248 (28.4331)	mem 4879MB
[2022-05-31 01:46:20 MetaFG_0] (main.py 265): INFO Train: [24/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2977 (0.3057)	loss 1.7969 (1.4960)	grad_norm 20.6258 (28.4316)	mem 4879MB
[2022-05-31 01:46:23 MetaFG_0] (main.py 265): INFO Train: [24/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2939 (0.3057)	loss 1.7054 (1.4962)	grad_norm 18.0156 (28.4458)	mem 4879MB
[2022-05-31 01:46:26 MetaFG_0] (main.py 265): INFO Train: [24/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2949 (0.3057)	loss 1.5393 (1.4952)	grad_norm 41.0743 (28.4692)	mem 4879MB
[2022-05-31 01:46:29 MetaFG_0] (main.py 265): INFO Train: [24/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2922 (0.3056)	loss 1.3590 (1.4961)	grad_norm 17.7080 (28.4680)	mem 4879MB
[2022-05-31 01:46:32 MetaFG_0] (main.py 265): INFO Train: [24/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3011 (0.3056)	loss 1.6975 (1.4975)	grad_norm 38.7373 (28.4459)	mem 4879MB
[2022-05-31 01:46:35 MetaFG_0] (main.py 265): INFO Train: [24/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2935 (0.3056)	loss 1.7083 (1.4972)	grad_norm 30.8011 (28.4498)	mem 4879MB
[2022-05-31 01:46:38 MetaFG_0] (main.py 265): INFO Train: [24/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3007 (0.3056)	loss 1.3491 (1.4966)	grad_norm 27.7898 (28.4498)	mem 4879MB
[2022-05-31 01:46:41 MetaFG_0] (main.py 265): INFO Train: [24/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2930 (0.3056)	loss 1.1897 (1.4953)	grad_norm 23.2312 (28.4196)	mem 4879MB
[2022-05-31 01:46:44 MetaFG_0] (main.py 265): INFO Train: [24/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2925 (0.3056)	loss 1.3363 (1.4957)	grad_norm 27.8334 (28.4040)	mem 4879MB
[2022-05-31 01:46:47 MetaFG_0] (main.py 265): INFO Train: [24/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2952 (0.3056)	loss 1.5525 (1.4959)	grad_norm 23.2371 (28.4020)	mem 4879MB
[2022-05-31 01:46:50 MetaFG_0] (main.py 265): INFO Train: [24/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2930 (0.3055)	loss 0.9208 (1.4960)	grad_norm 36.8080 (28.3626)	mem 4879MB
[2022-05-31 01:46:53 MetaFG_0] (main.py 265): INFO Train: [24/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2972 (0.3055)	loss 1.7058 (1.4957)	grad_norm 48.4013 (28.4000)	mem 4879MB
[2022-05-31 01:46:56 MetaFG_0] (main.py 265): INFO Train: [24/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2923 (0.3055)	loss 1.3791 (1.4952)	grad_norm 62.8438 (28.4582)	mem 4879MB
[2022-05-31 01:47:00 MetaFG_0] (main.py 265): INFO Train: [24/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2990 (0.3058)	loss 1.6472 (1.4950)	grad_norm 21.0745 (28.4153)	mem 4879MB
[2022-05-31 01:47:03 MetaFG_0] (main.py 265): INFO Train: [24/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2960 (0.3058)	loss 1.6312 (1.4950)	grad_norm 19.7727 (28.3910)	mem 4879MB
[2022-05-31 01:47:06 MetaFG_0] (main.py 265): INFO Train: [24/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2999 (0.3058)	loss 1.1703 (1.4949)	grad_norm 18.6399 (28.3958)	mem 4879MB
[2022-05-31 01:47:09 MetaFG_0] (main.py 265): INFO Train: [24/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3058)	loss 1.1335 (1.4947)	grad_norm 23.7375 (28.4038)	mem 4879MB
[2022-05-31 01:47:12 MetaFG_0] (main.py 265): INFO Train: [24/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2976 (0.3058)	loss 1.5637 (1.4942)	grad_norm 36.0078 (28.3916)	mem 4879MB
[2022-05-31 01:47:15 MetaFG_0] (main.py 265): INFO Train: [24/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2946 (0.3058)	loss 1.6226 (1.4942)	grad_norm 44.6991 (28.4066)	mem 4879MB
[2022-05-31 01:47:18 MetaFG_0] (main.py 265): INFO Train: [24/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2929 (0.3058)	loss 1.7064 (1.4936)	grad_norm 18.3361 (28.3723)	mem 4879MB
[2022-05-31 01:47:21 MetaFG_0] (main.py 265): INFO Train: [24/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2929 (0.3058)	loss 1.5320 (1.4940)	grad_norm 23.2610 (28.3364)	mem 4879MB
[2022-05-31 01:47:24 MetaFG_0] (main.py 265): INFO Train: [24/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2954 (0.3057)	loss 1.1082 (1.4936)	grad_norm 28.3713 (28.3658)	mem 4879MB
[2022-05-31 01:47:27 MetaFG_0] (main.py 265): INFO Train: [24/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2984 (0.3057)	loss 1.8365 (1.4935)	grad_norm 33.1807 (28.3781)	mem 4879MB
[2022-05-31 01:47:30 MetaFG_0] (main.py 265): INFO Train: [24/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2920 (0.3057)	loss 1.4714 (1.4938)	grad_norm 22.6374 (28.3693)	mem 4879MB
[2022-05-31 01:47:33 MetaFG_0] (main.py 265): INFO Train: [24/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2939 (0.3057)	loss 1.5956 (1.4944)	grad_norm 36.6101 (28.4028)	mem 4879MB
[2022-05-31 01:47:36 MetaFG_0] (main.py 265): INFO Train: [24/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2916 (0.3057)	loss 1.0449 (1.4940)	grad_norm 20.5156 (28.4171)	mem 4879MB
[2022-05-31 01:47:39 MetaFG_0] (main.py 265): INFO Train: [24/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2918 (0.3057)	loss 1.4613 (1.4949)	grad_norm 42.2751 (28.4575)	mem 4879MB
[2022-05-31 01:47:42 MetaFG_0] (main.py 265): INFO Train: [24/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2919 (0.3057)	loss 1.5488 (1.4948)	grad_norm 23.0439 (28.4677)	mem 4879MB
[2022-05-31 01:47:45 MetaFG_0] (main.py 265): INFO Train: [24/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2924 (0.3056)	loss 1.4912 (1.4946)	grad_norm 23.2441 (28.4398)	mem 4879MB
[2022-05-31 01:47:48 MetaFG_0] (main.py 265): INFO Train: [24/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2918 (0.3056)	loss 0.8437 (1.4932)	grad_norm 27.8449 (28.4373)	mem 4879MB
[2022-05-31 01:47:51 MetaFG_0] (main.py 265): INFO Train: [24/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2973 (0.3056)	loss 1.7006 (1.4931)	grad_norm 26.5963 (28.4533)	mem 4879MB
[2022-05-31 01:47:54 MetaFG_0] (main.py 265): INFO Train: [24/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2918 (0.3056)	loss 1.6619 (1.4933)	grad_norm 23.0044 (28.4311)	mem 4879MB
[2022-05-31 01:47:57 MetaFG_0] (main.py 265): INFO Train: [24/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2932 (0.3056)	loss 1.8677 (1.4941)	grad_norm 16.2431 (28.4277)	mem 4879MB
[2022-05-31 01:48:00 MetaFG_0] (main.py 265): INFO Train: [24/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3004 (0.3056)	loss 1.6849 (1.4935)	grad_norm 16.4629 (28.4048)	mem 4879MB
[2022-05-31 01:48:03 MetaFG_0] (main.py 265): INFO Train: [24/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2930 (0.3056)	loss 1.2510 (1.4929)	grad_norm 29.7084 (28.4070)	mem 4879MB
[2022-05-31 01:48:06 MetaFG_0] (main.py 265): INFO Train: [24/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2973 (0.3056)	loss 1.4815 (1.4916)	grad_norm 33.6434 (28.4070)	mem 4879MB
[2022-05-31 01:48:09 MetaFG_0] (main.py 265): INFO Train: [24/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3013 (0.3055)	loss 1.4316 (1.4909)	grad_norm 31.3647 (28.4039)	mem 4879MB
[2022-05-31 01:48:13 MetaFG_0] (main.py 265): INFO Train: [24/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2988 (0.3055)	loss 1.5672 (1.4903)	grad_norm 26.4747 (28.4391)	mem 4879MB
[2022-05-31 01:48:16 MetaFG_0] (main.py 265): INFO Train: [24/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2990 (0.3055)	loss 1.5195 (1.4900)	grad_norm 28.6163 (28.4456)	mem 4879MB
[2022-05-31 01:48:19 MetaFG_0] (main.py 265): INFO Train: [24/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3005 (0.3055)	loss 1.7274 (1.4909)	grad_norm 28.5563 (28.4660)	mem 4879MB
[2022-05-31 01:48:22 MetaFG_0] (main.py 265): INFO Train: [24/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2980 (0.3055)	loss 1.7361 (1.4915)	grad_norm 17.9820 (28.4788)	mem 4879MB
[2022-05-31 01:48:25 MetaFG_0] (main.py 265): INFO Train: [24/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2924 (0.3055)	loss 1.7276 (1.4921)	grad_norm 20.1046 (28.4606)	mem 4879MB
[2022-05-31 01:48:28 MetaFG_0] (main.py 265): INFO Train: [24/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3002 (0.3055)	loss 1.5462 (1.4926)	grad_norm 27.4687 (28.4540)	mem 4879MB
[2022-05-31 01:48:31 MetaFG_0] (main.py 265): INFO Train: [24/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2917 (0.3055)	loss 1.6813 (1.4931)	grad_norm 26.8930 (28.4834)	mem 4879MB
[2022-05-31 01:48:34 MetaFG_0] (main.py 265): INFO Train: [24/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2975 (0.3055)	loss 1.6374 (1.4930)	grad_norm 19.5885 (28.5118)	mem 4879MB
[2022-05-31 01:48:37 MetaFG_0] (main.py 265): INFO Train: [24/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2922 (0.3055)	loss 1.7688 (1.4926)	grad_norm 42.1384 (28.5494)	mem 4879MB
[2022-05-31 01:48:40 MetaFG_0] (main.py 265): INFO Train: [24/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2983 (0.3055)	loss 1.3262 (1.4920)	grad_norm 19.3664 (28.5218)	mem 4879MB
[2022-05-31 01:48:43 MetaFG_0] (main.py 265): INFO Train: [24/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2933 (0.3055)	loss 1.3165 (1.4918)	grad_norm 21.6711 (28.5156)	mem 4879MB
[2022-05-31 01:48:46 MetaFG_0] (main.py 265): INFO Train: [24/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2935 (0.3054)	loss 1.5561 (1.4927)	grad_norm 16.1327 (28.4967)	mem 4879MB
[2022-05-31 01:48:49 MetaFG_0] (main.py 265): INFO Train: [24/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.3050 (0.3054)	loss 1.2676 (1.4923)	grad_norm 27.7255 (28.4813)	mem 4879MB
[2022-05-31 01:48:52 MetaFG_0] (main.py 265): INFO Train: [24/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3026 (0.3054)	loss 1.7616 (1.4920)	grad_norm 36.8924 (28.4747)	mem 4879MB
[2022-05-31 01:48:55 MetaFG_0] (main.py 265): INFO Train: [24/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2918 (0.3054)	loss 1.5224 (1.4926)	grad_norm 43.0483 (28.4878)	mem 4879MB
[2022-05-31 01:48:58 MetaFG_0] (main.py 265): INFO Train: [24/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2974 (0.3054)	loss 1.3493 (1.4929)	grad_norm 26.9752 (28.4836)	mem 4879MB
[2022-05-31 01:49:01 MetaFG_0] (main.py 265): INFO Train: [24/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2951 (0.3054)	loss 1.2871 (1.4927)	grad_norm 49.1213 (28.4868)	mem 4879MB
[2022-05-31 01:49:04 MetaFG_0] (main.py 265): INFO Train: [24/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2934 (0.3054)	loss 1.3471 (1.4922)	grad_norm 17.1678 (28.4655)	mem 4879MB
[2022-05-31 01:49:07 MetaFG_0] (main.py 265): INFO Train: [24/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3306 (0.3054)	loss 1.3797 (1.4917)	grad_norm 64.1245 (28.4748)	mem 4879MB
[2022-05-31 01:49:11 MetaFG_0] (main.py 265): INFO Train: [24/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2917 (0.3056)	loss 1.5475 (1.4918)	grad_norm 40.7768 (28.5094)	mem 4879MB
[2022-05-31 01:49:14 MetaFG_0] (main.py 265): INFO Train: [24/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2912 (0.3056)	loss 1.6513 (1.4927)	grad_norm 28.9525 (28.4973)	mem 4879MB
[2022-05-31 01:49:14 MetaFG_0] (main.py 272): INFO EPOCH 24 training takes 0:07:57
[2022-05-31 01:49:14 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_24.pth saving......
[2022-05-31 01:49:15 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_24.pth saved !!!
[2022-05-31 01:49:15 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:49:17 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:49:17 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:49:17 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.714 (0.714)	Loss 0.8843 (0.8843)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 01:49:18 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.152)	Loss 0.8111 (0.8240)	Acc@1 84.375 (83.523)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 01:49:19 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.099 (0.124)	Loss 0.7663 (0.8559)	Acc@1 84.375 (81.250)	Acc@5 96.875 (97.917)	Mem 4879MB
[2022-05-31 01:49:20 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.089 (0.113)	Loss 1.3715 (0.8592)	Acc@1 59.375 (81.653)	Acc@5 96.875 (97.581)	Mem 4879MB
[2022-05-31 01:49:21 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.109)	Loss 0.9699 (0.8615)	Acc@1 78.125 (81.174)	Acc@5 93.750 (97.485)	Mem 4879MB
[2022-05-31 01:49:22 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.107)	Loss 1.0883 (0.8811)	Acc@1 75.000 (80.576)	Acc@5 100.000 (97.426)	Mem 4879MB
[2022-05-31 01:49:23 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.098 (0.105)	Loss 1.3527 (0.8840)	Acc@1 59.375 (80.277)	Acc@5 96.875 (97.285)	Mem 4879MB
[2022-05-31 01:49:24 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.108 (0.103)	Loss 0.8169 (0.8789)	Acc@1 87.500 (80.282)	Acc@5 100.000 (97.359)	Mem 4879MB
[2022-05-31 01:49:25 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 0.8335 (0.8745)	Acc@1 75.000 (80.556)	Acc@5 96.875 (97.299)	Mem 4879MB
[2022-05-31 01:49:26 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.102 (0.101)	Loss 0.9643 (0.8712)	Acc@1 78.125 (81.010)	Acc@5 96.875 (97.356)	Mem 4879MB
[2022-05-31 01:49:27 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 1.1324 (0.8773)	Acc@1 71.875 (80.755)	Acc@5 100.000 (97.463)	Mem 4879MB
[2022-05-31 01:49:28 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.103 (0.100)	Loss 0.7817 (0.8832)	Acc@1 87.500 (80.771)	Acc@5 96.875 (97.438)	Mem 4879MB
[2022-05-31 01:49:29 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.100)	Loss 0.9222 (0.8872)	Acc@1 81.250 (80.733)	Acc@5 96.875 (97.469)	Mem 4879MB
[2022-05-31 01:49:30 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.099 (0.099)	Loss 0.7040 (0.8920)	Acc@1 90.625 (80.725)	Acc@5 100.000 (97.352)	Mem 4879MB
[2022-05-31 01:49:30 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 0.8305 (0.8893)	Acc@1 81.250 (80.718)	Acc@5 96.875 (97.429)	Mem 4879MB
[2022-05-31 01:49:31 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.095 (0.099)	Loss 1.0352 (0.8892)	Acc@1 68.750 (80.733)	Acc@5 100.000 (97.496)	Mem 4879MB
[2022-05-31 01:49:32 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.7997 (0.8920)	Acc@1 81.250 (80.551)	Acc@5 100.000 (97.516)	Mem 4879MB
[2022-05-31 01:49:33 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.093 (0.098)	Loss 0.9576 (0.8837)	Acc@1 78.125 (80.811)	Acc@5 100.000 (97.588)	Mem 4879MB
[2022-05-31 01:49:34 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.098)	Loss 1.2741 (0.8887)	Acc@1 68.750 (80.594)	Acc@5 96.875 (97.531)	Mem 4879MB
[2022-05-31 01:49:35 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.8466 (0.8908)	Acc@1 84.375 (80.481)	Acc@5 100.000 (97.529)	Mem 4879MB
[2022-05-31 01:49:36 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.098)	Loss 0.8818 (0.8911)	Acc@1 81.250 (80.535)	Acc@5 96.875 (97.544)	Mem 4879MB
[2022-05-31 01:49:37 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.103 (0.098)	Loss 0.6963 (0.8892)	Acc@1 87.500 (80.643)	Acc@5 100.000 (97.527)	Mem 4879MB
[2022-05-31 01:49:38 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.097 (0.097)	Loss 1.0338 (0.8916)	Acc@1 75.000 (80.557)	Acc@5 90.625 (97.483)	Mem 4879MB
[2022-05-31 01:49:39 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.107 (0.097)	Loss 1.1682 (0.8895)	Acc@1 62.500 (80.574)	Acc@5 93.750 (97.470)	Mem 4879MB
[2022-05-31 01:49:40 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.088 (0.097)	Loss 0.6985 (0.8901)	Acc@1 90.625 (80.563)	Acc@5 96.875 (97.471)	Mem 4879MB
[2022-05-31 01:49:41 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.097)	Loss 1.0940 (0.8969)	Acc@1 68.750 (80.254)	Acc@5 100.000 (97.435)	Mem 4879MB
[2022-05-31 01:49:42 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.6316 (0.8965)	Acc@1 93.750 (80.208)	Acc@5 100.000 (97.450)	Mem 4879MB
[2022-05-31 01:49:43 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 1.0414 (0.8975)	Acc@1 78.125 (80.224)	Acc@5 93.750 (97.405)	Mem 4879MB
[2022-05-31 01:49:44 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.097)	Loss 0.9593 (0.8984)	Acc@1 75.000 (80.171)	Acc@5 96.875 (97.398)	Mem 4879MB
[2022-05-31 01:49:45 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.097)	Loss 0.9306 (0.9001)	Acc@1 78.125 (80.133)	Acc@5 100.000 (97.423)	Mem 4879MB
[2022-05-31 01:49:46 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.097)	Loss 0.8328 (0.8988)	Acc@1 75.000 (80.201)	Acc@5 100.000 (97.436)	Mem 4879MB
[2022-05-31 01:49:46 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.8303 (0.8998)	Acc@1 81.250 (80.215)	Acc@5 96.875 (97.448)	Mem 4879MB
[2022-05-31 01:49:47 MetaFG_0] (main.py 330): INFO  * Acc@1 80.220 Acc@5 97.440
[2022-05-31 01:49:47 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 80.2%
[2022-05-31 01:49:47 MetaFG_0] (main.py 171): INFO Max accuracy: 80.22%
[2022-05-31 01:49:48 MetaFG_0] (main.py 265): INFO Train: [25/300][0/1562]	eta 0:21:14 lr 0.000006	time 0.8161 (0.8161)	loss 1.2370 (1.2370)	grad_norm 36.2688 (36.2688)	mem 4879MB
[2022-05-31 01:49:51 MetaFG_0] (main.py 265): INFO Train: [25/300][10/1562]	eta 0:09:17 lr 0.000006	time 0.2938 (0.3593)	loss 1.6950 (1.3993)	grad_norm 19.5402 (26.9541)	mem 4879MB
[2022-05-31 01:49:54 MetaFG_0] (main.py 265): INFO Train: [25/300][20/1562]	eta 0:08:32 lr 0.000006	time 0.2916 (0.3327)	loss 1.4575 (1.4943)	grad_norm 44.9629 (29.2445)	mem 4879MB
[2022-05-31 01:49:57 MetaFG_0] (main.py 265): INFO Train: [25/300][30/1562]	eta 0:08:15 lr 0.000006	time 0.2932 (0.3235)	loss 1.4095 (1.5254)	grad_norm 62.0680 (31.2559)	mem 4879MB
[2022-05-31 01:50:00 MetaFG_0] (main.py 265): INFO Train: [25/300][40/1562]	eta 0:08:05 lr 0.000006	time 0.2917 (0.3187)	loss 1.6495 (1.5375)	grad_norm 20.0249 (29.4136)	mem 4879MB
[2022-05-31 01:50:03 MetaFG_0] (main.py 265): INFO Train: [25/300][50/1562]	eta 0:07:57 lr 0.000006	time 0.2923 (0.3156)	loss 1.2403 (1.5239)	grad_norm 26.1896 (28.7743)	mem 4879MB
[2022-05-31 01:50:06 MetaFG_0] (main.py 265): INFO Train: [25/300][60/1562]	eta 0:07:50 lr 0.000006	time 0.2914 (0.3135)	loss 1.7686 (1.5489)	grad_norm 22.1223 (28.7412)	mem 4879MB
[2022-05-31 01:50:09 MetaFG_0] (main.py 265): INFO Train: [25/300][70/1562]	eta 0:07:45 lr 0.000006	time 0.2928 (0.3120)	loss 1.1620 (1.5458)	grad_norm 74.9110 (29.2328)	mem 4879MB
[2022-05-31 01:50:12 MetaFG_0] (main.py 265): INFO Train: [25/300][80/1562]	eta 0:07:40 lr 0.000006	time 0.2919 (0.3108)	loss 1.3803 (1.5385)	grad_norm 36.6423 (29.0529)	mem 4879MB
[2022-05-31 01:50:15 MetaFG_0] (main.py 265): INFO Train: [25/300][90/1562]	eta 0:07:36 lr 0.000006	time 0.2982 (0.3100)	loss 1.6380 (1.5277)	grad_norm 38.0824 (29.5451)	mem 4879MB
[2022-05-31 01:50:18 MetaFG_0] (main.py 265): INFO Train: [25/300][100/1562]	eta 0:07:32 lr 0.000006	time 0.2979 (0.3094)	loss 1.5169 (1.5166)	grad_norm 14.8179 (29.2529)	mem 4879MB
[2022-05-31 01:50:21 MetaFG_0] (main.py 265): INFO Train: [25/300][110/1562]	eta 0:07:28 lr 0.000006	time 0.2977 (0.3090)	loss 1.5773 (1.5114)	grad_norm 20.1819 (29.1065)	mem 4879MB
[2022-05-31 01:50:24 MetaFG_0] (main.py 265): INFO Train: [25/300][120/1562]	eta 0:07:25 lr 0.000006	time 0.2933 (0.3087)	loss 1.4740 (1.5033)	grad_norm 29.0606 (28.7508)	mem 4879MB
[2022-05-31 01:50:27 MetaFG_0] (main.py 265): INFO Train: [25/300][130/1562]	eta 0:07:21 lr 0.000006	time 0.2986 (0.3083)	loss 1.6149 (1.5084)	grad_norm 43.8600 (29.2217)	mem 4879MB
[2022-05-31 01:50:30 MetaFG_0] (main.py 265): INFO Train: [25/300][140/1562]	eta 0:07:17 lr 0.000006	time 0.2946 (0.3080)	loss 1.2506 (1.5094)	grad_norm 32.4070 (29.2456)	mem 4879MB
[2022-05-31 01:50:33 MetaFG_0] (main.py 265): INFO Train: [25/300][150/1562]	eta 0:07:14 lr 0.000006	time 0.2982 (0.3076)	loss 1.1943 (1.5140)	grad_norm 22.4979 (29.0416)	mem 4879MB
[2022-05-31 01:50:36 MetaFG_0] (main.py 265): INFO Train: [25/300][160/1562]	eta 0:07:10 lr 0.000006	time 0.2980 (0.3074)	loss 1.4998 (1.5142)	grad_norm 18.0264 (28.8547)	mem 4879MB
[2022-05-31 01:50:39 MetaFG_0] (main.py 265): INFO Train: [25/300][170/1562]	eta 0:07:07 lr 0.000006	time 0.3004 (0.3072)	loss 1.6893 (1.5121)	grad_norm 32.3272 (29.1448)	mem 4879MB
[2022-05-31 01:50:42 MetaFG_0] (main.py 265): INFO Train: [25/300][180/1562]	eta 0:07:04 lr 0.000006	time 0.2965 (0.3071)	loss 1.4560 (1.5109)	grad_norm 16.5774 (28.9270)	mem 4879MB
[2022-05-31 01:50:45 MetaFG_0] (main.py 265): INFO Train: [25/300][190/1562]	eta 0:07:01 lr 0.000006	time 0.2979 (0.3069)	loss 1.5410 (1.5081)	grad_norm 16.7512 (28.8391)	mem 4879MB
[2022-05-31 01:50:48 MetaFG_0] (main.py 265): INFO Train: [25/300][200/1562]	eta 0:06:57 lr 0.000006	time 0.2995 (0.3068)	loss 1.6004 (1.5038)	grad_norm 24.9049 (28.8720)	mem 4879MB
[2022-05-31 01:50:51 MetaFG_0] (main.py 265): INFO Train: [25/300][210/1562]	eta 0:06:54 lr 0.000006	time 0.3035 (0.3067)	loss 1.6921 (1.5075)	grad_norm 25.0305 (28.9326)	mem 4879MB
[2022-05-31 01:50:55 MetaFG_0] (main.py 265): INFO Train: [25/300][220/1562]	eta 0:06:51 lr 0.000006	time 0.3034 (0.3067)	loss 1.6420 (1.5060)	grad_norm 50.1953 (28.8833)	mem 4879MB
[2022-05-31 01:50:58 MetaFG_0] (main.py 265): INFO Train: [25/300][230/1562]	eta 0:06:48 lr 0.000006	time 0.2961 (0.3065)	loss 1.4048 (1.5044)	grad_norm 20.5668 (28.9256)	mem 4879MB
[2022-05-31 01:51:01 MetaFG_0] (main.py 265): INFO Train: [25/300][240/1562]	eta 0:06:45 lr 0.000006	time 0.2984 (0.3064)	loss 1.1176 (1.4982)	grad_norm 43.1972 (29.3856)	mem 4879MB
[2022-05-31 01:51:04 MetaFG_0] (main.py 265): INFO Train: [25/300][250/1562]	eta 0:06:41 lr 0.000006	time 0.3003 (0.3064)	loss 1.6204 (1.4963)	grad_norm 35.4170 (29.6206)	mem 4879MB
[2022-05-31 01:51:07 MetaFG_0] (main.py 265): INFO Train: [25/300][260/1562]	eta 0:06:38 lr 0.000006	time 0.2986 (0.3062)	loss 1.4492 (1.4982)	grad_norm 49.7949 (29.8084)	mem 4879MB
[2022-05-31 01:51:10 MetaFG_0] (main.py 265): INFO Train: [25/300][270/1562]	eta 0:06:35 lr 0.000006	time 0.3129 (0.3062)	loss 1.7665 (1.5002)	grad_norm 24.9116 (29.8032)	mem 4879MB
[2022-05-31 01:51:13 MetaFG_0] (main.py 265): INFO Train: [25/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2990 (0.3072)	loss 1.0867 (1.4963)	grad_norm 37.1531 (30.0521)	mem 4879MB
[2022-05-31 01:51:16 MetaFG_0] (main.py 265): INFO Train: [25/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2978 (0.3071)	loss 1.2927 (1.4952)	grad_norm 44.7775 (30.1129)	mem 4879MB
[2022-05-31 01:51:19 MetaFG_0] (main.py 265): INFO Train: [25/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2921 (0.3069)	loss 1.1751 (1.4952)	grad_norm 69.7257 (30.2368)	mem 4879MB
[2022-05-31 01:51:22 MetaFG_0] (main.py 265): INFO Train: [25/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2977 (0.3069)	loss 1.5331 (1.4944)	grad_norm 24.4310 (30.1916)	mem 4879MB
[2022-05-31 01:51:25 MetaFG_0] (main.py 265): INFO Train: [25/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.3034 (0.3068)	loss 1.5441 (1.4948)	grad_norm 22.2468 (30.2432)	mem 4879MB
[2022-05-31 01:51:28 MetaFG_0] (main.py 265): INFO Train: [25/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2988 (0.3067)	loss 1.6511 (1.4919)	grad_norm 36.5984 (30.0872)	mem 4879MB
[2022-05-31 01:51:31 MetaFG_0] (main.py 265): INFO Train: [25/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2983 (0.3067)	loss 1.6398 (1.4921)	grad_norm 17.9969 (29.9588)	mem 4879MB
[2022-05-31 01:51:34 MetaFG_0] (main.py 265): INFO Train: [25/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2916 (0.3066)	loss 1.6597 (1.4962)	grad_norm 26.0978 (29.8808)	mem 4879MB
[2022-05-31 01:51:37 MetaFG_0] (main.py 265): INFO Train: [25/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.3033 (0.3066)	loss 1.3159 (1.4966)	grad_norm 15.2239 (29.8902)	mem 4879MB
[2022-05-31 01:51:40 MetaFG_0] (main.py 265): INFO Train: [25/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2935 (0.3065)	loss 1.5915 (1.4967)	grad_norm 17.8737 (29.7181)	mem 4879MB
[2022-05-31 01:51:44 MetaFG_0] (main.py 265): INFO Train: [25/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2939 (0.3065)	loss 1.5158 (1.4990)	grad_norm 14.6064 (29.6847)	mem 4879MB
[2022-05-31 01:51:47 MetaFG_0] (main.py 265): INFO Train: [25/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2922 (0.3064)	loss 1.3593 (1.4955)	grad_norm 28.5044 (29.6398)	mem 4879MB
[2022-05-31 01:51:50 MetaFG_0] (main.py 265): INFO Train: [25/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.3001 (0.3063)	loss 1.2103 (1.4979)	grad_norm 35.7029 (29.6459)	mem 4879MB
[2022-05-31 01:51:53 MetaFG_0] (main.py 265): INFO Train: [25/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2980 (0.3063)	loss 1.5096 (1.4959)	grad_norm 19.4197 (29.5828)	mem 4879MB
[2022-05-31 01:51:56 MetaFG_0] (main.py 265): INFO Train: [25/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2945 (0.3062)	loss 1.6722 (1.4973)	grad_norm 18.7592 (29.5586)	mem 4879MB
[2022-05-31 01:51:59 MetaFG_0] (main.py 265): INFO Train: [25/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.3059 (0.3061)	loss 1.6216 (1.4978)	grad_norm 16.2403 (29.5371)	mem 4879MB
[2022-05-31 01:52:02 MetaFG_0] (main.py 265): INFO Train: [25/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.3041 (0.3061)	loss 1.1486 (1.4976)	grad_norm 31.1823 (29.4870)	mem 4879MB
[2022-05-31 01:52:05 MetaFG_0] (main.py 265): INFO Train: [25/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2923 (0.3061)	loss 1.4846 (1.4985)	grad_norm 16.2671 (29.5035)	mem 4879MB
[2022-05-31 01:52:08 MetaFG_0] (main.py 265): INFO Train: [25/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2988 (0.3060)	loss 1.5080 (1.5000)	grad_norm 20.9128 (29.4503)	mem 4879MB
[2022-05-31 01:52:11 MetaFG_0] (main.py 265): INFO Train: [25/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2927 (0.3060)	loss 1.4633 (1.5017)	grad_norm 31.0986 (29.5216)	mem 4879MB
[2022-05-31 01:52:14 MetaFG_0] (main.py 265): INFO Train: [25/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2989 (0.3060)	loss 1.6391 (1.5017)	grad_norm 16.9790 (29.4792)	mem 4879MB
[2022-05-31 01:52:17 MetaFG_0] (main.py 265): INFO Train: [25/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2932 (0.3059)	loss 1.0200 (1.4998)	grad_norm 42.0943 (29.7042)	mem 4879MB
[2022-05-31 01:52:20 MetaFG_0] (main.py 265): INFO Train: [25/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2924 (0.3059)	loss 1.2707 (1.4979)	grad_norm 29.4866 (29.6957)	mem 4879MB
[2022-05-31 01:52:23 MetaFG_0] (main.py 265): INFO Train: [25/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2928 (0.3058)	loss 1.0695 (1.4964)	grad_norm 41.2130 (29.6460)	mem 4879MB
[2022-05-31 01:52:26 MetaFG_0] (main.py 265): INFO Train: [25/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2920 (0.3058)	loss 1.6793 (1.4964)	grad_norm 25.2300 (29.5669)	mem 4879MB
[2022-05-31 01:52:29 MetaFG_0] (main.py 265): INFO Train: [25/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2993 (0.3058)	loss 1.4192 (1.4978)	grad_norm 16.3133 (29.4538)	mem 4879MB
[2022-05-31 01:52:32 MetaFG_0] (main.py 265): INFO Train: [25/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2936 (0.3057)	loss 1.5575 (1.4996)	grad_norm 20.2073 (29.3937)	mem 4879MB
[2022-05-31 01:52:35 MetaFG_0] (main.py 265): INFO Train: [25/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2919 (0.3057)	loss 1.6672 (1.4996)	grad_norm 20.0829 (29.4346)	mem 4879MB
[2022-05-31 01:52:38 MetaFG_0] (main.py 265): INFO Train: [25/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2924 (0.3057)	loss 1.0327 (1.4983)	grad_norm 33.9215 (29.4117)	mem 4879MB
[2022-05-31 01:52:41 MetaFG_0] (main.py 265): INFO Train: [25/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2928 (0.3056)	loss 1.7130 (1.4984)	grad_norm 22.4924 (29.3408)	mem 4879MB
[2022-05-31 01:52:44 MetaFG_0] (main.py 265): INFO Train: [25/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2991 (0.3056)	loss 1.7963 (1.5002)	grad_norm 17.9972 (29.3757)	mem 4879MB
[2022-05-31 01:52:47 MetaFG_0] (main.py 265): INFO Train: [25/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.3005 (0.3056)	loss 1.2422 (1.5005)	grad_norm 45.6618 (29.3301)	mem 4879MB
[2022-05-31 01:52:50 MetaFG_0] (main.py 265): INFO Train: [25/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2919 (0.3056)	loss 1.8220 (1.5010)	grad_norm 18.3903 (29.2660)	mem 4879MB
[2022-05-31 01:52:53 MetaFG_0] (main.py 265): INFO Train: [25/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2981 (0.3056)	loss 1.6122 (1.5016)	grad_norm 25.4936 (29.2504)	mem 4879MB
[2022-05-31 01:52:57 MetaFG_0] (main.py 265): INFO Train: [25/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2943 (0.3056)	loss 1.4454 (1.5004)	grad_norm 9.6670 (29.1621)	mem 4879MB
[2022-05-31 01:53:00 MetaFG_0] (main.py 265): INFO Train: [25/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2939 (0.3056)	loss 1.8690 (1.5017)	grad_norm 32.5839 (29.1661)	mem 4879MB
[2022-05-31 01:53:03 MetaFG_0] (main.py 265): INFO Train: [25/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2930 (0.3055)	loss 1.5380 (1.5018)	grad_norm 34.5526 (29.1498)	mem 4879MB
[2022-05-31 01:53:06 MetaFG_0] (main.py 265): INFO Train: [25/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2993 (0.3055)	loss 1.6725 (1.5034)	grad_norm 26.5533 (29.1074)	mem 4879MB
[2022-05-31 01:53:09 MetaFG_0] (main.py 265): INFO Train: [25/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.3002 (0.3055)	loss 1.4100 (1.5020)	grad_norm 35.5981 (29.1216)	mem 4879MB
[2022-05-31 01:53:12 MetaFG_0] (main.py 265): INFO Train: [25/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2991 (0.3055)	loss 0.9111 (1.5011)	grad_norm 78.8052 (29.1559)	mem 4879MB
[2022-05-31 01:53:15 MetaFG_0] (main.py 265): INFO Train: [25/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2981 (0.3055)	loss 0.9622 (1.5016)	grad_norm 17.9199 (29.0784)	mem 4879MB
[2022-05-31 01:53:18 MetaFG_0] (main.py 265): INFO Train: [25/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2994 (0.3054)	loss 1.4902 (1.5010)	grad_norm 18.8931 (29.0113)	mem 4879MB
[2022-05-31 01:53:21 MetaFG_0] (main.py 265): INFO Train: [25/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2933 (0.3055)	loss 1.5962 (1.5020)	grad_norm 17.8796 (28.9927)	mem 4879MB
[2022-05-31 01:53:24 MetaFG_0] (main.py 265): INFO Train: [25/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2979 (0.3055)	loss 1.6445 (1.5019)	grad_norm 35.6210 (28.9873)	mem 4879MB
[2022-05-31 01:53:27 MetaFG_0] (main.py 265): INFO Train: [25/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2974 (0.3055)	loss 1.5064 (1.5020)	grad_norm 23.6978 (29.0034)	mem 4879MB
[2022-05-31 01:53:30 MetaFG_0] (main.py 265): INFO Train: [25/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3075 (0.3054)	loss 1.3632 (1.5012)	grad_norm 22.3612 (29.0209)	mem 4879MB
[2022-05-31 01:53:33 MetaFG_0] (main.py 265): INFO Train: [25/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2994 (0.3054)	loss 1.4523 (1.5013)	grad_norm 17.9735 (28.9706)	mem 4879MB
[2022-05-31 01:53:36 MetaFG_0] (main.py 265): INFO Train: [25/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2936 (0.3054)	loss 1.2027 (1.5001)	grad_norm 41.2669 (28.9917)	mem 4879MB
[2022-05-31 01:53:39 MetaFG_0] (main.py 265): INFO Train: [25/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2921 (0.3053)	loss 1.5454 (1.5002)	grad_norm 20.5403 (28.9843)	mem 4879MB
[2022-05-31 01:53:42 MetaFG_0] (main.py 265): INFO Train: [25/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2924 (0.3053)	loss 1.5597 (1.5011)	grad_norm 28.7008 (29.0279)	mem 4879MB
[2022-05-31 01:53:45 MetaFG_0] (main.py 265): INFO Train: [25/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2976 (0.3053)	loss 1.5541 (1.5006)	grad_norm 13.5495 (28.9786)	mem 4879MB
[2022-05-31 01:53:48 MetaFG_0] (main.py 265): INFO Train: [25/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.3116 (0.3054)	loss 1.3447 (1.5005)	grad_norm 17.4855 (29.0118)	mem 4879MB
[2022-05-31 01:53:52 MetaFG_0] (main.py 265): INFO Train: [25/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2928 (0.3056)	loss 1.3780 (1.4999)	grad_norm 25.6101 (29.0265)	mem 4879MB
[2022-05-31 01:53:55 MetaFG_0] (main.py 265): INFO Train: [25/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2917 (0.3056)	loss 1.7223 (1.5004)	grad_norm 29.6746 (29.0015)	mem 4879MB
[2022-05-31 01:53:58 MetaFG_0] (main.py 265): INFO Train: [25/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2992 (0.3056)	loss 1.4757 (1.5012)	grad_norm 20.5507 (28.9921)	mem 4879MB
[2022-05-31 01:54:01 MetaFG_0] (main.py 265): INFO Train: [25/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2918 (0.3056)	loss 1.3733 (1.4997)	grad_norm 72.3319 (28.9845)	mem 4879MB
[2022-05-31 01:54:04 MetaFG_0] (main.py 265): INFO Train: [25/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2992 (0.3055)	loss 1.2810 (1.4977)	grad_norm 40.7889 (29.0171)	mem 4879MB
[2022-05-31 01:54:07 MetaFG_0] (main.py 265): INFO Train: [25/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2936 (0.3055)	loss 1.4830 (1.4980)	grad_norm 27.8159 (29.0175)	mem 4879MB
[2022-05-31 01:54:10 MetaFG_0] (main.py 265): INFO Train: [25/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.3005 (0.3055)	loss 1.2766 (1.4973)	grad_norm 28.1229 (29.0361)	mem 4879MB
[2022-05-31 01:54:13 MetaFG_0] (main.py 265): INFO Train: [25/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3003 (0.3055)	loss 1.6106 (1.4983)	grad_norm 28.3136 (29.0522)	mem 4879MB
[2022-05-31 01:54:16 MetaFG_0] (main.py 265): INFO Train: [25/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2919 (0.3055)	loss 1.2940 (1.4974)	grad_norm 25.1109 (29.0269)	mem 4879MB
[2022-05-31 01:54:19 MetaFG_0] (main.py 265): INFO Train: [25/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2930 (0.[2022-05-31 08:30:41 MetaFG_0] (main.py 265): INFO Train: [72/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2931 (0.3081)	loss 1.7314 (1.3079)	grad_norm 38.3986 (30.4393)	mem 4879MB
[2022-05-31 08:30:44 MetaFG_0] (main.py 265): INFO Train: [72/300][300/1562]	eta 0:06:28 lr 0.000005	time 0.2962 (0.3079)	loss 1.5815 (1.3064)	grad_norm 12.5045 (30.2319)	mem 4879MB
3055)	loss 1.5401 (1.4986)	grad_norm 28.3442 (29.0343)	mem 4879MB
[2022-05-31 01:54:22 MetaFG_0] (main.py 265): INFO Train: [25/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2916 (0.3054)	loss 1.6032 (1.4993)	grad_norm 24.1509 (29.0164)	mem 4879MB
[2022-05-31 01:54:25 MetaFG_0] (main.py 265): INFO Train: [25/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2937 (0.3054)	loss 1.2932 (1.4983)	grad_norm 22.9732 (29.0117)	mem 4879MB
[2022-05-31 01:54:28 MetaFG_0] (main.py 265): INFO Train: [25/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2918 (0.3054)	loss 1.3047 (1.4982)	grad_norm 26.4658 (29.0280)	mem 4879MB
[2022-05-31 01:54:31 MetaFG_0] (main.py 265): INFO Train: [25/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2918 (0.3054)	loss 1.3617 (1.4979)	grad_norm 22.4957 (28.9916)	mem 4879MB
[2022-05-31 01:54:34 MetaFG_0] (main.py 265): INFO Train: [25/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2985 (0.3054)	loss 1.4171 (1.4994)	grad_norm 40.9253 (29.0226)	mem 4879MB
[2022-05-31 01:54:37 MetaFG_0] (main.py 265): INFO Train: [25/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2922 (0.3054)	loss 1.0577 (1.4985)	grad_norm 19.3792 (29.0056)	mem 4879MB
[2022-05-31 01:54:40 MetaFG_0] (main.py 265): INFO Train: [25/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2996 (0.3054)	loss 1.4469 (1.4986)	grad_norm 24.6629 (28.9743)	mem 4879MB
[2022-05-31 01:54:43 MetaFG_0] (main.py 265): INFO Train: [25/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2988 (0.3054)	loss 1.2886 (1.4974)	grad_norm 33.0180 (28.9713)	mem 4879MB
[2022-05-31 01:54:46 MetaFG_0] (main.py 265): INFO Train: [25/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2996 (0.3054)	loss 1.1588 (1.4961)	grad_norm 18.0030 (28.9303)	mem 4879MB
[2022-05-31 01:54:49 MetaFG_0] (main.py 265): INFO Train: [25/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2936 (0.3054)	loss 1.3736 (1.4950)	grad_norm 14.4271 (28.9186)	mem 4879MB
[2022-05-31 01:54:52 MetaFG_0] (main.py 265): INFO Train: [25/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3000 (0.3054)	loss 1.4464 (1.4946)	grad_norm 34.7858 (28.9651)	mem 4879MB
[2022-05-31 01:54:55 MetaFG_0] (main.py 265): INFO Train: [25/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2977 (0.3053)	loss 1.5043 (1.4944)	grad_norm 12.8324 (28.9444)	mem 4879MB
[2022-05-31 01:54:58 MetaFG_0] (main.py 265): INFO Train: [25/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2986 (0.3053)	loss 1.2204 (1.4950)	grad_norm 28.6192 (28.9243)	mem 4879MB
[2022-05-31 01:55:02 MetaFG_0] (main.py 265): INFO Train: [25/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2989 (0.3053)	loss 1.4419 (1.4951)	grad_norm 25.4873 (28.9447)	mem 4879MB
[2022-05-31 01:55:05 MetaFG_0] (main.py 265): INFO Train: [25/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2932 (0.3053)	loss 1.7422 (1.4955)	grad_norm 35.2059 (inf)	mem 4879MB
[2022-05-31 01:55:08 MetaFG_0] (main.py 265): INFO Train: [25/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2999 (0.3053)	loss 1.5078 (1.4959)	grad_norm 20.3899 (inf)	mem 4879MB
[2022-05-31 01:55:11 MetaFG_0] (main.py 265): INFO Train: [25/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2971 (0.3053)	loss 1.3748 (1.4961)	grad_norm 32.8715 (inf)	mem 4879MB
[2022-05-31 01:55:14 MetaFG_0] (main.py 265): INFO Train: [25/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2924 (0.3053)	loss 1.8169 (1.4954)	grad_norm 23.3078 (inf)	mem 4879MB
[2022-05-31 01:55:17 MetaFG_0] (main.py 265): INFO Train: [25/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2989 (0.3053)	loss 1.5431 (1.4957)	grad_norm 21.3914 (inf)	mem 4879MB
[2022-05-31 01:55:20 MetaFG_0] (main.py 265): INFO Train: [25/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2934 (0.3053)	loss 1.6751 (1.4959)	grad_norm 24.3386 (inf)	mem 4879MB
[2022-05-31 01:55:23 MetaFG_0] (main.py 265): INFO Train: [25/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2981 (0.3053)	loss 1.5553 (1.4962)	grad_norm 21.4039 (inf)	mem 4879MB
[2022-05-31 01:55:26 MetaFG_0] (main.py 265): INFO Train: [25/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2946 (0.3053)	loss 1.5919 (1.4963)	grad_norm 17.5978 (inf)	mem 4879MB
[2022-05-31 01:55:29 MetaFG_0] (main.py 265): INFO Train: [25/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2935 (0.3052)	loss 1.6495 (1.4960)	grad_norm 20.7539 (inf)	mem 4879MB
[2022-05-31 01:55:32 MetaFG_0] (main.py 265): INFO Train: [25/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2993 (0.3052)	loss 1.2772 (1.4957)	grad_norm 50.8257 (inf)	mem 4879MB
[2022-05-31 01:55:35 MetaFG_0] (main.py 265): INFO Train: [25/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2933 (0.3052)	loss 1.4581 (1.4953)	grad_norm 64.5711 (inf)	mem 4879MB
[2022-05-31 01:55:38 MetaFG_0] (main.py 265): INFO Train: [25/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2987 (0.3052)	loss 1.7044 (1.4955)	grad_norm 40.1153 (inf)	mem 4879MB
[2022-05-31 01:55:41 MetaFG_0] (main.py 265): INFO Train: [25/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2935 (0.3052)	loss 1.2827 (1.4951)	grad_norm 18.6995 (inf)	mem 4879MB
[2022-05-31 01:55:44 MetaFG_0] (main.py 265): INFO Train: [25/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2915 (0.3052)	loss 1.5796 (1.4950)	grad_norm 22.6689 (inf)	mem 4879MB
[2022-05-31 01:55:47 MetaFG_0] (main.py 265): INFO Train: [25/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2931 (0.3052)	loss 1.5468 (1.4949)	grad_norm 21.1124 (inf)	mem 4879MB
[2022-05-31 01:55:50 MetaFG_0] (main.py 265): INFO Train: [25/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2933 (0.3052)	loss 1.5076 (1.4940)	grad_norm 29.2641 (inf)	mem 4879MB
[2022-05-31 01:55:53 MetaFG_0] (main.py 265): INFO Train: [25/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2935 (0.3052)	loss 1.8524 (1.4937)	grad_norm 30.5510 (inf)	mem 4879MB
[2022-05-31 01:55:56 MetaFG_0] (main.py 265): INFO Train: [25/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2924 (0.3052)	loss 1.4528 (1.4935)	grad_norm 24.5486 (inf)	mem 4879MB
[2022-05-31 01:55:59 MetaFG_0] (main.py 265): INFO Train: [25/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2935 (0.3051)	loss 1.3275 (1.4943)	grad_norm 19.6707 (inf)	mem 4879MB
[2022-05-31 01:56:02 MetaFG_0] (main.py 265): INFO Train: [25/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3095 (0.3052)	loss 1.5807 (1.4948)	grad_norm 28.6720 (inf)	mem 4879MB
[2022-05-31 01:56:06 MetaFG_0] (main.py 265): INFO Train: [25/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.3031 (0.3053)	loss 1.7343 (1.4960)	grad_norm 27.5141 (inf)	mem 4879MB
[2022-05-31 01:56:09 MetaFG_0] (main.py 265): INFO Train: [25/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2930 (0.3053)	loss 1.6434 (1.4965)	grad_norm 35.4070 (inf)	mem 4879MB
[2022-05-31 01:56:12 MetaFG_0] (main.py 265): INFO Train: [25/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3002 (0.3053)	loss 1.6346 (1.4968)	grad_norm 21.1764 (inf)	mem 4879MB
[2022-05-31 01:56:15 MetaFG_0] (main.py 265): INFO Train: [25/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2997 (0.3053)	loss 1.2839 (1.4960)	grad_norm 34.2299 (inf)	mem 4879MB
[2022-05-31 01:56:18 MetaFG_0] (main.py 265): INFO Train: [25/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2924 (0.3053)	loss 1.5071 (1.4961)	grad_norm 20.8634 (inf)	mem 4879MB
[2022-05-31 01:56:21 MetaFG_0] (main.py 265): INFO Train: [25/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2916 (0.3053)	loss 1.6226 (1.4959)	grad_norm 25.8730 (inf)	mem 4879MB
[2022-05-31 01:56:24 MetaFG_0] (main.py 265): INFO Train: [25/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2986 (0.3053)	loss 1.5497 (1.4956)	grad_norm 24.0423 (inf)	mem 4879MB
[2022-05-31 01:56:27 MetaFG_0] (main.py 265): INFO Train: [25/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.3000 (0.3053)	loss 1.6558 (1.4954)	grad_norm 19.6482 (inf)	mem 4879MB
[2022-05-31 01:56:30 MetaFG_0] (main.py 265): INFO Train: [25/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2996 (0.3052)	loss 1.6929 (1.4954)	grad_norm 26.2151 (inf)	mem 4879MB
[2022-05-31 01:56:33 MetaFG_0] (main.py 265): INFO Train: [25/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2915 (0.3052)	loss 1.3897 (1.4954)	grad_norm 23.2388 (inf)	mem 4879MB
[2022-05-31 01:56:36 MetaFG_0] (main.py 265): INFO Train: [25/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2921 (0.3052)	loss 1.1129 (1.4957)	grad_norm 28.4134 (inf)	mem 4879MB
[2022-05-31 01:56:39 MetaFG_0] (main.py 265): INFO Train: [25/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2942 (0.3052)	loss 1.1662 (1.4954)	grad_norm 56.4197 (inf)	mem 4879MB
[2022-05-31 01:56:42 MetaFG_0] (main.py 265): INFO Train: [25/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2920 (0.3052)	loss 1.4931 (1.4949)	grad_norm 13.3487 (inf)	mem 4879MB
[2022-05-31 01:56:45 MetaFG_0] (main.py 265): INFO Train: [25/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2990 (0.3052)	loss 1.6942 (1.4948)	grad_norm 21.8429 (inf)	mem 4879MB
[2022-05-31 01:56:48 MetaFG_0] (main.py 265): INFO Train: [25/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2981 (0.3052)	loss 1.6196 (1.4949)	grad_norm 51.7387 (inf)	mem 4879MB
[2022-05-31 01:56:51 MetaFG_0] (main.py 265): INFO Train: [25/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3010 (0.3052)	loss 1.3465 (1.4944)	grad_norm 47.6318 (inf)	mem 4879MB
[2022-05-31 01:56:54 MetaFG_0] (main.py 265): INFO Train: [25/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2986 (0.3052)	loss 1.5703 (1.4953)	grad_norm 21.1979 (inf)	mem 4879MB
[2022-05-31 01:56:57 MetaFG_0] (main.py 265): INFO Train: [25/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2999 (0.3052)	loss 1.6282 (1.4959)	grad_norm 34.0272 (inf)	mem 4879MB
[2022-05-31 01:57:00 MetaFG_0] (main.py 265): INFO Train: [25/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2929 (0.3052)	loss 1.7304 (1.4968)	grad_norm 21.9669 (inf)	mem 4879MB
[2022-05-31 01:57:03 MetaFG_0] (main.py 265): INFO Train: [25/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2933 (0.3052)	loss 1.4005 (1.4970)	grad_norm 17.7691 (inf)	mem 4879MB
[2022-05-31 01:57:06 MetaFG_0] (main.py 265): INFO Train: [25/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2946 (0.3052)	loss 1.3578 (1.4973)	grad_norm 24.4325 (inf)	mem 4879MB
[2022-05-31 01:57:10 MetaFG_0] (main.py 265): INFO Train: [25/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2928 (0.3052)	loss 1.6816 (1.4976)	grad_norm 35.4959 (inf)	mem 4879MB
[2022-05-31 01:57:13 MetaFG_0] (main.py 265): INFO Train: [25/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2931 (0.3052)	loss 1.5257 (1.4973)	grad_norm 26.3503 (inf)	mem 4879MB
[2022-05-31 01:57:16 MetaFG_0] (main.py 265): INFO Train: [25/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2929 (0.3052)	loss 1.6465 (1.4978)	grad_norm 34.8195 (inf)	mem 4879MB
[2022-05-31 01:57:19 MetaFG_0] (main.py 265): INFO Train: [25/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.3000 (0.3052)	loss 1.2361 (1.4978)	grad_norm 37.5557 (inf)	mem 4879MB
[2022-05-31 01:57:22 MetaFG_0] (main.py 265): INFO Train: [25/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2929 (0.3052)	loss 1.7055 (1.4975)	grad_norm 25.7817 (inf)	mem 4879MB
[2022-05-31 01:57:25 MetaFG_0] (main.py 265): INFO Train: [25/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2993 (0.3051)	loss 1.5448 (1.4968)	grad_norm 23.1096 (inf)	mem 4879MB
[2022-05-31 01:57:28 MetaFG_0] (main.py 265): INFO Train: [25/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.3002 (0.3052)	loss 1.2402 (1.4959)	grad_norm 34.5485 (inf)	mem 4879MB
[2022-05-31 01:57:31 MetaFG_0] (main.py 265): INFO Train: [25/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2931 (0.3051)	loss 1.5734 (1.4965)	grad_norm 32.7879 (inf)	mem 4879MB
[2022-05-31 01:57:34 MetaFG_0] (main.py 265): INFO Train: [25/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2983 (0.3051)	loss 1.6798 (1.4968)	grad_norm 21.5583 (inf)	mem 4879MB
[2022-05-31 01:57:37 MetaFG_0] (main.py 265): INFO Train: [25/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2980 (0.3051)	loss 1.5107 (1.4959)	grad_norm 19.6027 (inf)	mem 4879MB
[2022-05-31 01:57:40 MetaFG_0] (main.py 265): INFO Train: [25/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2916 (0.3051)	loss 1.4508 (1.4958)	grad_norm 21.1745 (inf)	mem 4879MB
[2022-05-31 01:57:43 MetaFG_0] (main.py 265): INFO Train: [25/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2911 (0.3051)	loss 1.6434 (1.4952)	grad_norm 32.9553 (inf)	mem 4879MB
[2022-05-31 01:57:43 MetaFG_0] (main.py 272): INFO EPOCH 25 training takes 0:07:56
[2022-05-31 01:57:43 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_25.pth saving......
[2022-05-31 01:57:44 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_25.pth saved !!!
[2022-05-31 01:57:44 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 01:57:46 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 01:57:46 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 01:57:46 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.604 (0.604)	Loss 0.8302 (0.8302)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 01:57:47 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.144)	Loss 0.9439 (0.8139)	Acc@1 81.250 (82.955)	Acc@5 100.000 (97.443)	Mem 4879MB
[2022-05-31 01:57:48 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.120)	Loss 0.5618 (0.8331)	Acc@1 90.625 (80.208)	Acc@5 100.000 (97.619)	Mem 4879MB
[2022-05-31 01:57:49 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.112)	Loss 0.8854 (0.8349)	Acc@1 75.000 (80.242)	Acc@5 100.000 (97.581)	Mem 4879MB
[2022-05-31 01:57:50 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.099 (0.107)	Loss 0.8708 (0.8042)	Acc@1 78.125 (81.479)	Acc@5 100.000 (97.866)	Mem 4879MB
[2022-05-31 01:57:51 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.104)	Loss 0.6796 (0.8117)	Acc@1 84.375 (81.311)	Acc@5 96.875 (97.855)	Mem 4879MB
[2022-05-31 01:57:52 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.102)	Loss 0.7658 (0.8082)	Acc@1 87.500 (81.557)	Acc@5 96.875 (97.848)	Mem 4879MB
[2022-05-31 01:57:53 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.101)	Loss 0.8031 (0.8001)	Acc@1 84.375 (81.558)	Acc@5 96.875 (97.975)	Mem 4879MB
[2022-05-31 01:57:54 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.101)	Loss 0.7312 (0.7943)	Acc@1 84.375 (81.636)	Acc@5 100.000 (98.148)	Mem 4879MB
[2022-05-31 01:57:55 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.100)	Loss 0.8180 (0.8022)	Acc@1 87.500 (81.525)	Acc@5 100.000 (98.146)	Mem 4879MB
[2022-05-31 01:57:56 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.8594 (0.8042)	Acc@1 78.125 (81.436)	Acc@5 96.875 (98.082)	Mem 4879MB
[2022-05-31 01:57:57 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.099)	Loss 0.6614 (0.8133)	Acc@1 90.625 (81.166)	Acc@5 96.875 (97.860)	Mem 4879MB
[2022-05-31 01:57:58 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.092 (0.099)	Loss 0.8224 (0.8130)	Acc@1 81.250 (81.173)	Acc@5 100.000 (97.856)	Mem 4879MB
[2022-05-31 01:57:59 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.098)	Loss 1.0083 (0.8222)	Acc@1 71.875 (80.892)	Acc@5 96.875 (97.829)	Mem 4879MB
[2022-05-31 01:58:00 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.098 (0.098)	Loss 0.7382 (0.8196)	Acc@1 75.000 (80.984)	Acc@5 100.000 (97.872)	Mem 4879MB
[2022-05-31 01:58:01 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 0.4662 (0.8172)	Acc@1 100.000 (81.064)	Acc@5 100.000 (97.827)	Mem 4879MB
[2022-05-31 01:58:01 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.7003 (0.8186)	Acc@1 81.250 (80.823)	Acc@5 100.000 (97.807)	Mem 4879MB
[2022-05-31 01:58:02 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.9053 (0.8159)	Acc@1 71.875 (80.885)	Acc@5 96.875 (97.862)	Mem 4879MB
[2022-05-31 01:58:03 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 0.5940 (0.8176)	Acc@1 81.250 (80.801)	Acc@5 100.000 (97.842)	Mem 4879MB
[2022-05-31 01:58:04 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 0.6600 (0.8179)	Acc@1 87.500 (80.776)	Acc@5 100.000 (97.857)	Mem 4879MB
[2022-05-31 01:58:05 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 0.6860 (0.8196)	Acc@1 84.375 (80.613)	Acc@5 96.875 (97.870)	Mem 4879MB
[2022-05-31 01:58:06 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.7555 (0.8184)	Acc@1 87.500 (80.658)	Acc@5 100.000 (97.897)	Mem 4879MB
[2022-05-31 01:58:07 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.091 (0.097)	Loss 0.7184 (0.8152)	Acc@1 84.375 (80.755)	Acc@5 100.000 (97.936)	Mem 4879MB
[2022-05-31 01:58:08 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.088 (0.097)	Loss 0.5780 (0.8107)	Acc@1 96.875 (80.993)	Acc@5 100.000 (97.944)	Mem 4879MB
[2022-05-31 01:58:09 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 0.9206 (0.8121)	Acc@1 81.250 (80.952)	Acc@5 96.875 (97.886)	Mem 4879MB
[2022-05-31 01:58:10 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.108 (0.097)	Loss 0.9629 (0.8127)	Acc@1 78.125 (80.951)	Acc@5 93.750 (97.821)	Mem 4879MB
[2022-05-31 01:58:11 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.115 (0.098)	Loss 0.6006 (0.8119)	Acc@1 84.375 (81.011)	Acc@5 100.000 (97.809)	Mem 4879MB
[2022-05-31 01:58:12 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.098 (0.099)	Loss 0.8203 (0.8118)	Acc@1 81.250 (81.008)	Acc@5 100.000 (97.821)	Mem 4879MB
[2022-05-31 01:58:13 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.098 (0.099)	Loss 0.7650 (0.8125)	Acc@1 78.125 (80.950)	Acc@5 100.000 (97.820)	Mem 4879MB
[2022-05-31 01:58:14 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.093 (0.098)	Loss 0.7416 (0.8104)	Acc@1 87.500 (81.067)	Acc@5 96.875 (97.820)	Mem 4879MB
[2022-05-31 01:58:15 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.098)	Loss 0.5985 (0.8081)	Acc@1 93.750 (81.188)	Acc@5 96.875 (97.799)	Mem 4879MB
[2022-05-31 01:58:16 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.098)	Loss 0.8935 (0.8105)	Acc@1 78.125 (81.079)	Acc@5 96.875 (97.779)	Mem 4879MB
[2022-05-31 01:58:16 MetaFG_0] (main.py 330): INFO  * Acc@1 81.090 Acc@5 97.780
[2022-05-31 01:58:16 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 81.1%
[2022-05-31 01:58:16 MetaFG_0] (main.py 171): INFO Max accuracy: 81.09%
[2022-05-31 01:58:17 MetaFG_0] (main.py 265): INFO Train: [26/300][0/1562]	eta 0:24:39 lr 0.000006	time 0.9474 (0.9474)	loss 1.0037 (1.0037)	grad_norm 39.4441 (39.4441)	mem 4879MB
[2022-05-31 01:58:21 MetaFG_0] (main.py 265): INFO Train: [26/300][10/1562]	eta 0:09:34 lr 0.000006	time 0.3046 (0.3701)	loss 1.1865 (1.4836)	grad_norm 35.2956 (39.4625)	mem 4879MB
[2022-05-31 01:58:24 MetaFG_0] (main.py 265): INFO Train: [26/300][20/1562]	eta 0:08:41 lr 0.000006	time 0.2990 (0.3379)	loss 1.0237 (1.4278)	grad_norm 27.4626 (37.8054)	mem 4879MB
[2022-05-31 01:58:27 MetaFG_0] (main.py 265): INFO Train: [26/300][30/1562]	eta 0:08:21 lr 0.000006	time 0.2930 (0.3271)	loss 1.7802 (1.4768)	grad_norm 133.6730 (40.4463)	mem 4879MB
[2022-05-31 01:58:30 MetaFG_0] (main.py 265): INFO Train: [26/300][40/1562]	eta 0:08:08 lr 0.000006	time 0.2946 (0.3211)	loss 1.4600 (1.4681)	grad_norm 27.5307 (38.3663)	mem 4879MB
[2022-05-31 01:58:33 MetaFG_0] (main.py 265): INFO Train: [26/300][50/1562]	eta 0:08:00 lr 0.000006	time 0.2920 (0.3178)	loss 1.6758 (1.4707)	grad_norm 28.1123 (nan)	mem 4879MB
[2022-05-31 01:58:36 MetaFG_0] (main.py 265): INFO Train: [26/300][60/1562]	eta 0:07:53 lr 0.000006	time 0.2992 (0.3155)	loss 1.5548 (1.4899)	grad_norm 38.3610 (nan)	mem 4879MB
[2022-05-31 01:58:39 MetaFG_0] (main.py 265): INFO Train: [26/300][70/1562]	eta 0:07:48 lr 0.000006	time 0.2974 (0.3140)	loss 1.0583 (1.4823)	grad_norm 22.1370 (nan)	mem 4879MB
[2022-05-31 01:58:42 MetaFG_0] (main.py 265): INFO Train: [26/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2922 (0.3130)	loss 1.5335 (1.4754)	grad_norm 25.1922 (nan)	mem 4879MB
[2022-05-31 01:58:45 MetaFG_0] (main.py 265): INFO Train: [26/300][90/1562]	eta 0:07:38 lr 0.000006	time 0.2921 (0.3117)	loss 1.6919 (1.4763)	grad_norm 25.8253 (nan)	mem 4879MB
[2022-05-31 01:58:48 MetaFG_0] (main.py 265): INFO Train: [26/300][100/1562]	eta 0:07:34 lr 0.000006	time 0.2943 (0.3109)	loss 1.4386 (1.4663)	grad_norm 32.8479 (nan)	mem 4879MB
[2022-05-31 01:58:51 MetaFG_0] (main.py 265): INFO Train: [26/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2919 (0.3102)	loss 1.5231 (1.4763)	grad_norm 18.3555 (nan)	mem 4879MB
[2022-05-31 01:58:54 MetaFG_0] (main.py 265): INFO Train: [26/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2930 (0.3099)	loss 1.4582 (1.4764)	grad_norm 40.6700 (nan)	mem 4879MB
[2022-05-31 01:58:57 MetaFG_0] (main.py 265): INFO Train: [26/300][130/1562]	eta 0:07:22 lr 0.000006	time 0.2931 (0.3093)	loss 1.3070 (1.4711)	grad_norm 26.5032 (nan)	mem 4879MB
[2022-05-31 01:59:00 MetaFG_0] (main.py 265): INFO Train: [26/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2934 (0.3090)	loss 1.4050 (1.4749)	grad_norm 29.8637 (nan)	mem 4879MB
[2022-05-31 01:59:03 MetaFG_0] (main.py 265): INFO Train: [26/300][150/1562]	eta 0:07:15 lr 0.000006	time 0.2946 (0.3087)	loss 1.3684 (1.4700)	grad_norm 29.1686 (nan)	mem 4879MB
[2022-05-31 01:59:06 MetaFG_0] (main.py 265): INFO Train: [26/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2940 (0.3084)	loss 1.3129 (1.4647)	grad_norm 35.5481 (nan)	mem 4879MB
[2022-05-31 01:59:09 MetaFG_0] (main.py 265): INFO Train: [26/300][170/1562]	eta 0:07:08 lr 0.000006	time 0.2931 (0.3081)	loss 1.7685 (1.4652)	grad_norm 25.4419 (nan)	mem 4879MB
[2022-05-31 01:59:12 MetaFG_0] (main.py 265): INFO Train: [26/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2925 (0.3077)	loss 1.2258 (1.4622)	grad_norm 33.3820 (nan)	mem 4879MB
[2022-05-31 01:59:15 MetaFG_0] (main.py 265): INFO Train: [26/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2995 (0.3076)	loss 1.5839 (1.4599)	grad_norm 18.1182 (nan)	mem 4879MB
[2022-05-31 01:59:18 MetaFG_0] (main.py 265): INFO Train: [26/300][200/1562]	eta 0:06:58 lr 0.000006	time 0.2979 (0.3074)	loss 1.6543 (1.4620)	grad_norm 21.5769 (nan)	mem 4879MB
[2022-05-31 01:59:21 MetaFG_0] (main.py 265): INFO Train: [26/300][210/1562]	eta 0:06:55 lr 0.000006	time 0.2924 (0.3072)	loss 1.4908 (1.4623)	grad_norm 30.3430 (nan)	mem 4879MB
[2022-05-31 01:59:24 MetaFG_0] (main.py 265): INFO Train: [26/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2937 (0.3070)	loss 1.4416 (1.4603)	grad_norm 29.8259 (nan)	mem 4879MB
[2022-05-31 01:59:27 MetaFG_0] (main.py 265): INFO Train: [26/300][230/1562]	eta 0:06:48 lr 0.000006	time 0.2919 (0.3069)	loss 1.3247 (1.4573)	grad_norm 21.1307 (nan)	mem 4879MB
[2022-05-31 01:59:30 MetaFG_0] (main.py 265): INFO Train: [26/300][240/1562]	eta 0:06:45 lr 0.000006	time 0.2995 (0.3067)	loss 1.4107 (1.4533)	grad_norm 20.7615 (nan)	mem 4879MB
[2022-05-31 01:59:33 MetaFG_0] (main.py 265): INFO Train: [26/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2922 (0.3066)	loss 1.5416 (1.4571)	grad_norm 20.1755 (nan)	mem 4879MB
[2022-05-31 01:59:36 MetaFG_0] (main.py 265): INFO Train: [26/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2969 (0.3065)	loss 1.1458 (1.4583)	grad_norm 27.0870 (nan)	mem 4879MB
[2022-05-31 01:59:40 MetaFG_0] (main.py 265): INFO Train: [26/300][270/1562]	eta 0:06:35 lr 0.000006	time 0.2948 (0.3064)	loss 1.5628 (1.4600)	grad_norm 16.4572 (nan)	mem 4879MB
[2022-05-31 01:59:43 MetaFG_0] (main.py 265): INFO Train: [26/300][280/1562]	eta 0:06:32 lr 0.000006	time 0.2968 (0.3062)	loss 1.4659 (1.4610)	grad_norm 18.3157 (nan)	mem 4879MB
[2022-05-31 01:59:46 MetaFG_0] (main.py 265): INFO Train: [26/300][290/1562]	eta 0:06:29 lr 0.000006	time 0.2997 (0.3061)	loss 1.6477 (1.4638)	grad_norm 25.9273 (nan)	mem 4879MB
[2022-05-31 01:59:49 MetaFG_0] (main.py 265): INFO Train: [26/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2933 (0.3061)	loss 1.4841 (1.4595)	grad_norm 20.4499 (nan)	mem 4879MB
[2022-05-31 01:59:52 MetaFG_0] (main.py 265): INFO Train: [26/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.3006 (0.3061)	loss 1.4179 (1.4605)	grad_norm 28.4264 (nan)	mem 4879MB
[2022-05-31 01:59:55 MetaFG_0] (main.py 265): INFO Train: [26/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2921 (0.3061)	loss 1.5948 (1.4607)	grad_norm 24.2656 (nan)	mem 4879MB
[2022-05-31 01:59:58 MetaFG_0] (main.py 265): INFO Train: [26/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2993 (0.3061)	loss 1.5600 (1.4592)	grad_norm 33.2812 (nan)	mem 4879MB
[2022-05-31 02:00:01 MetaFG_0] (main.py 265): INFO Train: [26/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2992 (0.3061)	loss 1.6399 (1.4583)	grad_norm 40.5003 (nan)	mem 4879MB
[2022-05-31 02:00:04 MetaFG_0] (main.py 265): INFO Train: [26/300][350/1562]	eta 0:06:10 lr 0.000006	time 0.2924 (0.3060)	loss 1.7120 (1.4579)	grad_norm 26.2471 (nan)	mem 4879MB
[2022-05-31 02:00:07 MetaFG_0] (main.py 265): INFO Train: [26/300][360/1562]	eta 0:06:07 lr 0.000006	time 0.2980 (0.3060)	loss 1.4150 (1.4586)	grad_norm 20.9364 (nan)	mem 4879MB
[2022-05-31 02:00:10 MetaFG_0] (main.py 265): INFO Train: [26/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2922 (0.3059)	loss 1.7161 (1.4602)	grad_norm 41.7606 (nan)	mem 4879MB
[2022-05-31 02:00:13 MetaFG_0] (main.py 265): INFO Train: [26/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.3008 (0.3059)	loss 1.7416 (1.4624)	grad_norm 20.0152 (nan)	mem 4879MB
[2022-05-31 02:00:16 MetaFG_0] (main.py 265): INFO Train: [26/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2932 (0.3059)	loss 1.6927 (1.4633)	grad_norm 11.7433 (nan)	mem 4879MB
[2022-05-31 02:00:19 MetaFG_0] (main.py 265): INFO Train: [26/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2944 (0.3058)	loss 1.6534 (1.4641)	grad_norm 17.4620 (nan)	mem 4879MB
[2022-05-31 02:00:22 MetaFG_0] (main.py 265): INFO Train: [26/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2947 (0.3058)	loss 1.3655 (1.4644)	grad_norm 35.5659 (nan)	mem 4879MB
[2022-05-31 02:00:25 MetaFG_0] (main.py 265): INFO Train: [26/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.3360 (0.3059)	loss 1.4519 (1.4667)	grad_norm 24.1156 (nan)	mem 4879MB
[2022-05-31 02:00:29 MetaFG_0] (main.py 265): INFO Train: [26/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2940 (0.3065)	loss 1.5695 (1.4669)	grad_norm 25.3432 (nan)	mem 4879MB
[2022-05-31 02:00:32 MetaFG_0] (main.py 265): INFO Train: [26/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2998 (0.3064)	loss 1.3540 (1.4670)	grad_norm 28.3982 (nan)	mem 4879MB
[2022-05-31 02:00:35 MetaFG_0] (main.py 265): INFO Train: [26/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2936 (0.3063)	loss 1.4087 (1.4647)	grad_norm 40.9541 (nan)	mem 4879MB
[2022-05-31 02:00:38 MetaFG_0] (main.py 265): INFO Train: [26/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.3001 (0.3063)	loss 1.2694 (1.4665)	grad_norm 10.5298 (nan)	mem 4879MB
[2022-05-31 02:00:41 MetaFG_0] (main.py 265): INFO Train: [26/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2920 (0.3062)	loss 1.1976 (1.4673)	grad_norm 28.6456 (nan)	mem 4879MB
[2022-05-31 02:00:44 MetaFG_0] (main.py 265): INFO Train: [26/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2982 (0.3062)	loss 1.7135 (1.4699)	grad_norm 13.5146 (nan)	mem 4879MB
[2022-05-31 02:00:47 MetaFG_0] (main.py 265): INFO Train: [26/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2940 (0.3062)	loss 1.2292 (1.4717)	grad_norm 32.0540 (nan)	mem 4879MB
[2022-05-31 02:00:50 MetaFG_0] (main.py 265): INFO Train: [26/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2946 (0.3062)	loss 1.6203 (1.4717)	grad_norm 20.3453 (nan)	mem 4879MB
[2022-05-31 02:00:53 MetaFG_0] (main.py 265): INFO Train: [26/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2935 (0.3061)	loss 0.9712 (1.4724)	grad_norm 34.7368 (nan)	mem 4879MB
[2022-05-31 02:00:56 MetaFG_0] (main.py 265): INFO Train: [26/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2917 (0.3061)	loss 1.4837 (1.4732)	grad_norm 26.1067 (nan)	mem 4879MB
[2022-05-31 02:00:59 MetaFG_0] (main.py 265): INFO Train: [26/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2984 (0.3061)	loss 1.1464 (1.4758)	grad_norm 26.8360 (nan)	mem 4879MB
[2022-05-31 02:01:02 MetaFG_0] (main.py 265): INFO Train: [26/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2982 (0.3061)	loss 1.5535 (1.4774)	grad_norm 26.3075 (nan)	mem 4879MB
[2022-05-31 02:01:05 MetaFG_0] (main.py 265): INFO Train: [26/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2933 (0.3060)	loss 1.7590 (1.4776)	grad_norm 25.1447 (nan)	mem 4879MB
[2022-05-31 02:01:08 MetaFG_0] (main.py 265): INFO Train: [26/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2974 (0.3060)	loss 1.3636 (1.4774)	grad_norm 27.1855 (nan)	mem 4879MB
[2022-05-31 02:01:11 MetaFG_0] (main.py 265): INFO Train: [26/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2974 (0.3060)	loss 1.3895 (1.4774)	grad_norm 19.1588 (nan)	mem 4879MB
[2022-05-31 02:01:14 MetaFG_0] (main.py 265): INFO Train: [26/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2917 (0.3060)	loss 1.5626 (1.4764)	grad_norm 32.5804 (nan)	mem 4879MB
[2022-05-31 02:01:17 MetaFG_0] (main.py 265): INFO Train: [26/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2918 (0.3060)	loss 1.5705 (1.4750)	grad_norm 23.0786 (nan)	mem 4879MB
[2022-05-31 02:01:20 MetaFG_0] (main.py 265): INFO Train: [26/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2960 (0.3059)	loss 1.2617 (1.4752)	grad_norm 40.3661 (nan)	mem 4879MB
[2022-05-31 02:01:23 MetaFG_0] (main.py 265): INFO Train: [26/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2933 (0.3059)	loss 1.5385 (1.4750)	grad_norm 39.4053 (nan)	mem 4879MB
[2022-05-31 02:01:26 MetaFG_0] (main.py 265): INFO Train: [26/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3008 (0.3059)	loss 1.6887 (1.4726)	grad_norm 23.9322 (nan)	mem 4879MB
[2022-05-31 02:01:29 MetaFG_0] (main.py 265): INFO Train: [26/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2928 (0.3059)	loss 1.5501 (1.4708)	grad_norm 17.2756 (nan)	mem 4879MB
[2022-05-31 02:01:33 MetaFG_0] (main.py 265): INFO Train: [26/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2921 (0.3058)	loss 1.6466 (1.4714)	grad_norm 18.9413 (nan)	mem 4879MB
[2022-05-31 02:01:36 MetaFG_0] (main.py 265): INFO Train: [26/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2922 (0.3058)	loss 1.6947 (1.4709)	grad_norm 29.3528 (nan)	mem 4879MB
[2022-05-31 02:01:39 MetaFG_0] (main.py 265): INFO Train: [26/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2919 (0.3057)	loss 1.4208 (1.4714)	grad_norm 14.7428 (nan)	mem 4879MB
[2022-05-31 02:01:42 MetaFG_0] (main.py 265): INFO Train: [26/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2925 (0.3057)	loss 1.4407 (1.4728)	grad_norm 20.5926 (nan)	mem 4879MB
[2022-05-31 02:01:45 MetaFG_0] (main.py 265): INFO Train: [26/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2998 (0.3056)	loss 1.8010 (1.4742)	grad_norm 35.5472 (nan)	mem 4879MB
[2022-05-31 02:01:48 MetaFG_0] (main.py 265): INFO Train: [26/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2920 (0.3057)	loss 1.2045 (1.4748)	grad_norm 25.1446 (nan)	mem 4879MB
[2022-05-31 02:01:51 MetaFG_0] (main.py 265): INFO Train: [26/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2976 (0.3056)	loss 1.6023 (1.4739)	grad_norm 29.9885 (nan)	mem 4879MB
[2022-05-31 02:01:54 MetaFG_0] (main.py 265): INFO Train: [26/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2930 (0.3056)	loss 1.5899 (1.4735)	grad_norm 34.6941 (nan)	mem 4879MB
[2022-05-31 02:01:57 MetaFG_0] (main.py 265): INFO Train: [26/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2982 (0.3056)	loss 1.3111 (1.4739)	grad_norm 29.9800 (nan)	mem 4879MB
[2022-05-31 02:02:00 MetaFG_0] (main.py 265): INFO Train: [26/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2926 (0.3056)	loss 1.3267 (1.4725)	grad_norm 36.4393 (nan)	mem 4879MB
[2022-05-31 02:02:03 MetaFG_0] (main.py 265): INFO Train: [26/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2928 (0.3055)	loss 1.5752 (1.4736)	grad_norm 28.0308 (nan)	mem 4879MB
[2022-05-31 02:02:06 MetaFG_0] (main.py 265): INFO Train: [26/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2925 (0.3055)	loss 1.6983 (1.4742)	grad_norm 27.4483 (nan)	mem 4879MB
[2022-05-31 02:02:09 MetaFG_0] (main.py 265): INFO Train: [26/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.3000 (0.3055)	loss 1.6114 (1.4742)	grad_norm 26.7229 (nan)	mem 4879MB
[2022-05-31 02:02:12 MetaFG_0] (main.py 265): INFO Train: [26/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2982 (0.3055)	loss 1.7830 (1.4754)	grad_norm 29.0152 (nan)	mem 4879MB
[2022-05-31 02:02:15 MetaFG_0] (main.py 265): INFO Train: [26/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2931 (0.3055)	loss 1.5232 (1.4752)	grad_norm 17.0092 (nan)	mem 4879MB
[2022-05-31 02:02:18 MetaFG_0] (main.py 265): INFO Train: [26/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2973 (0.3055)	loss 1.3587 (1.4760)	grad_norm 15.1696 (nan)	mem 4879MB
[2022-05-31 02:02:21 MetaFG_0] (main.py 265): INFO Train: [26/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2919 (0.3054)	loss 1.1504 (1.4762)	grad_norm 29.1167 (nan)	mem 4879MB
[2022-05-31 02:02:24 MetaFG_0] (main.py 265): INFO Train: [26/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2923 (0.3054)	loss 1.7044 (1.4758)	grad_norm 19.5509 (nan)	mem 4879MB
[2022-05-31 02:02:27 MetaFG_0] (main.py 265): INFO Train: [26/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2925 (0.3054)	loss 1.3762 (1.4759)	grad_norm 27.6645 (nan)	mem 4879MB
[2022-05-31 02:02:30 MetaFG_0] (main.py 265): INFO Train: [26/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2935 (0.3054)	loss 1.7265 (1.4757)	grad_norm 36.3054 (nan)	mem 4879MB
[2022-05-31 02:02:33 MetaFG_0] (main.py 265): INFO Train: [26/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2922 (0.3054)	loss 1.8436 (1.4757)	grad_norm 24.3576 (nan)	mem 4879MB
[2022-05-31 02:02:36 MetaFG_0] (main.py 265): INFO Train: [26/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2922 (0.3054)	loss 1.4383 (1.4747)	grad_norm 40.0523 (nan)	mem 4879MB
[2022-05-31 02:02:39 MetaFG_0] (main.py 265): INFO Train: [26/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2931 (0.3054)	loss 1.4759 (1.4744)	grad_norm 25.1069 (nan)	mem 4879MB
[2022-05-31 02:02:42 MetaFG_0] (main.py 265): INFO Train: [26/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2916 (0.3054)	loss 1.4506 (1.4750)	grad_norm 18.0783 (nan)	mem 4879MB
[2022-05-31 02:02:46 MetaFG_0] (main.py 265): INFO Train: [26/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2935 (0.3054)	loss 1.0706 (1.4750)	grad_norm 52.8119 (nan)	mem 4879MB
[2022-05-31 02:02:49 MetaFG_0] (main.py 265): INFO Train: [26/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.3237 (0.3054)	loss 1.5150 (1.4732)	grad_norm 27.4252 (nan)	mem 4879MB
[2022-05-31 02:02:52 MetaFG_0] (main.py 265): INFO Train: [26/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2981 (0.3057)	loss 1.2402 (1.4736)	grad_norm 33.9754 (nan)	mem 4879MB
[2022-05-31 02:02:55 MetaFG_0] (main.py 265): INFO Train: [26/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2983 (0.3057)	loss 1.5797 (1.4741)	grad_norm 19.6043 (nan)	mem 4879MB
[2022-05-31 02:02:58 MetaFG_0] (main.py 265): INFO Train: [26/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2993 (0.3057)	loss 1.3854 (1.4738)	grad_norm 36.1612 (nan)	mem 4879MB
[2022-05-31 02:03:01 MetaFG_0] (main.py 265): INFO Train: [26/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3017 (0.3057)	loss 1.5325 (1.4741)	grad_norm 26.9881 (nan)	mem 4879MB
[2022-05-31 02:03:04 MetaFG_0] (main.py 265): INFO Train: [26/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2989 (0.3057)	loss 1.6869 (1.4735)	grad_norm 37.4960 (nan)	mem 4879MB
[2022-05-31 02:03:07 MetaFG_0] (main.py 265): INFO Train: [26/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2928 (0.3057)	loss 1.5786 (1.4750)	grad_norm 21.6115 (nan)	mem 4879MB
[2022-05-31 02:03:10 MetaFG_0] (main.py 265): INFO Train: [26/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.3002 (0.3057)	loss 1.5685 (1.4759)	grad_norm 18.7316 (nan)	mem 4879MB
[2022-05-31 02:03:13 MetaFG_0] (main.py 265): INFO Train: [26/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2921 (0.3057)	loss 1.3668 (1.4765)	grad_norm 33.7357 (nan)	mem 4879MB
[2022-05-31 02:03:16 MetaFG_0] (main.py 265): INFO Train: [26/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2932 (0.3056)	loss 1.5907 (1.4765)	grad_norm 98.7635 (nan)	mem 4879MB
[2022-05-31 02:03:19 MetaFG_0] (main.py 265): INFO Train: [26/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2921 (0.3056)	loss 1.3527 (1.4769)	grad_norm 31.7762 (nan)	mem 4879MB
[2022-05-31 02:03:22 MetaFG_0] (main.py 265): INFO Train: [26/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2917 (0.3056)	loss 1.6029 (1.4757)	grad_norm 24.9018 (nan)	mem 4879MB
[2022-05-31 02:03:25 MetaFG_0] (main.py 265): INFO Train: [26/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2995 (0.3056)	loss 1.7363 (1.4754)	grad_norm 36.5017 (nan)	mem 4879MB
[2022-05-31 02:03:28 MetaFG_0] (main.py 265): INFO Train: [26/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3002 (0.3056)	loss 1.6369 (1.4758)	grad_norm 24.8723 (nan)	mem 4879MB
[2022-05-31 02:03:32 MetaFG_0] (main.py 265): INFO Train: [26/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2922 (0.3056)	loss 1.4442 (1.4745)	grad_norm 29.8046 (nan)	mem 4879MB
[2022-05-31 02:03:35 MetaFG_0] (main.py 265): INFO Train: [26/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3008 (0.3056)	loss 1.6585 (1.4746)	grad_norm 22.5055 (nan)	mem 4879MB
[2022-05-31 02:03:38 MetaFG_0] (main.py 265): INFO Train: [26/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2933 (0.3056)	loss 1.6133 (1.4740)	grad_norm 19.3942 (nan)	mem 4879MB
[2022-05-31 02:03:41 MetaFG_0] (main.py 265): INFO Train: [26/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.3010 (0.3056)	loss 1.7625 (1.4743)	grad_norm 28.7249 (nan)	mem 4879MB
[2022-05-31 02:03:44 MetaFG_0] (main.py 265): INFO Train: [26/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2993 (0.3056)	loss 1.2546 (1.4741)	grad_norm 22.2214 (nan)	mem 4879MB
[2022-05-31 02:03:47 MetaFG_0] (main.py 265): INFO Train: [26/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2914 (0.3055)	loss 1.6629 (1.4743)	grad_norm 25.5709 (nan)	mem 4879MB
[2022-05-31 02:03:50 MetaFG_0] (main.py 265): INFO Train: [26/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2933 (0.3055)	loss 1.4460 (1.4738)	grad_norm 26.2295 (nan)	mem 4879MB
[2022-05-31 02:03:53 MetaFG_0] (main.py 265): INFO Train: [26/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2936 (0.3055)	loss 1.4749 (1.4739)	grad_norm 30.4651 (nan)	mem 4879MB
[2022-05-31 02:03:56 MetaFG_0] (main.py 265): INFO Train: [26/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2928 (0.3055)	loss 1.7485 (1.4731)	grad_norm 22.3647 (nan)	mem 4879MB
[2022-05-31 02:03:59 MetaFG_0] (main.py 265): INFO Train: [26/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2921 (0.3054)	loss 1.2499 (1.4727)	grad_norm 30.3717 (nan)	mem 4879MB
[2022-05-31 02:04:02 MetaFG_0] (main.py 265): INFO Train: [26/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2987 (0.3055)	loss 1.6227 (1.4726)	grad_norm 19.8586 (nan)	mem 4879MB
[2022-05-31 02:04:05 MetaFG_0] (main.py 265): INFO Train: [26/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2982 (0.3055)	loss 1.2790 (1.4717)	grad_norm 22.6047 (nan)	mem 4879MB
[2022-05-31 02:04:08 MetaFG_0] (main.py 265): INFO Train: [26/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2981 (0.3054)	loss 1.4974 (1.4719)	grad_norm 15.7152 (nan)	mem 4879MB
[2022-05-31 02:04:11 MetaFG_0] (main.py 265): INFO Train: [26/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2992 (0.3054)	loss 1.4427 (1.4711)	grad_norm 26.1513 (nan)	mem 4879MB
[2022-05-31 02:04:14 MetaFG_0] (main.py 265): INFO Train: [26/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2936 (0.3054)	loss 1.5064 (1.4708)	grad_norm 18.5049 (nan)	mem 4879MB
[2022-05-31 02:04:17 MetaFG_0] (main.py 265): INFO Train: [26/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3002 (0.3054)	loss 1.6910 (1.4711)	grad_norm 36.6307 (nan)	mem 4879MB
[2022-05-31 02:04:20 MetaFG_0] (main.py 265): INFO Train: [26/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2992 (0.3054)	loss 1.2628 (1.4711)	grad_norm 31.5642 (nan)	mem 4879MB
[2022-05-31 02:04:23 MetaFG_0] (main.py 265): INFO Train: [26/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2996 (0.3054)	loss 1.6096 (1.4714)	grad_norm 23.3312 (nan)	mem 4879MB
[2022-05-31 02:04:26 MetaFG_0] (main.py 265): INFO Train: [26/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2928 (0.3054)	loss 1.2643 (1.4702)	grad_norm 30.0968 (nan)	mem 4879MB
[2022-05-31 02:04:29 MetaFG_0] (main.py 265): INFO Train: [26/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2934 (0.3054)	loss 1.2842 (1.4699)	grad_norm 25.2889 (nan)	mem 4879MB
[2022-05-31 02:04:32 MetaFG_0] (main.py 265): INFO Train: [26/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2917 (0.3054)	loss 1.7103 (1.4691)	grad_norm 23.8014 (nan)	mem 4879MB
[2022-05-31 02:04:35 MetaFG_0] (main.py 265): INFO Train: [26/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.3058 (0.3054)	loss 1.2617 (1.4695)	grad_norm 36.1874 (nan)	mem 4879MB
[2022-05-31 02:04:39 MetaFG_0] (main.py 265): INFO Train: [26/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3011 (0.3054)	loss 1.0886 (1.4695)	grad_norm 21.5140 (nan)	mem 4879MB
[2022-05-31 02:04:42 MetaFG_0] (main.py 265): INFO Train: [26/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2982 (0.3054)	loss 1.4370 (1.4696)	grad_norm 25.1009 (nan)	mem 4879MB
[2022-05-31 02:04:45 MetaFG_0] (main.py 265): INFO Train: [26/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2978 (0.3054)	loss 1.0769 (1.4696)	grad_norm 29.1083 (nan)	mem 4879MB
[2022-05-31 02:04:48 MetaFG_0] (main.py 265): INFO Train: [26/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2920 (0.3054)	loss 1.5342 (1.4690)	grad_norm 35.0421 (nan)	mem 4879MB
[2022-05-31 02:04:51 MetaFG_0] (main.py 265): INFO Train: [26/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2924 (0.3054)	loss 1.7332 (1.4693)	grad_norm 21.4998 (nan)	mem 4879MB
[2022-05-31 02:04:54 MetaFG_0] (main.py 265): INFO Train: [26/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2945 (0.3053)	loss 1.5483 (1.4710)	grad_norm 36.9123 (nan)	mem 4879MB
[2022-05-31 02:04:57 MetaFG_0] (main.py 265): INFO Train: [26/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2927 (0.3053)	loss 1.4567 (1.4699)	grad_norm 30.4967 (nan)	mem 4879MB
[2022-05-31 02:05:00 MetaFG_0] (main.py 265): INFO Train: [26/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2990 (0.3053)	loss 1.5651 (1.4698)	grad_norm 35.1095 (nan)	mem 4879MB
[2022-05-31 02:05:03 MetaFG_0] (main.py 265): INFO Train: [26/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2957 (0.3053)	loss 1.3565 (1.4700)	grad_norm 27.5590 (nan)	mem 4879MB
[2022-05-31 02:05:06 MetaFG_0] (main.py 265): INFO Train: [26/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2988 (0.3053)	loss 1.8679 (1.4705)	grad_norm 27.0495 (nan)	mem 4879MB
[2022-05-31 02:05:09 MetaFG_0] (main.py 265): INFO Train: [26/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2982 (0.3053)	loss 1.6873 (1.4713)	grad_norm 27.9677 (nan)	mem 4879MB
[2022-05-31 02:05:12 MetaFG_0] (main.py 265): INFO Train: [26/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2929 (0.3053)	loss 1.5155 (1.4711)	grad_norm 23.8562 (nan)	mem 4879MB
[2022-05-31 02:05:15 MetaFG_0] (main.py 265): INFO Train: [26/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2937 (0.3053)	loss 1.8068 (1.4718)	grad_norm 33.8165 (nan)	mem 4879MB
[2022-05-31 02:05:18 MetaFG_0] (main.py 265): INFO Train: [26/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2921 (0.3053)	loss 1.5627 (1.4724)	grad_norm 17.4634 (nan)	mem 4879MB
[2022-05-31 02:05:21 MetaFG_0] (main.py 265): INFO Train: [26/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2981 (0.3053)	loss 1.4523 (1.4724)	grad_norm 41.5257 (nan)	mem 4879MB
[2022-05-31 02:05:24 MetaFG_0] (main.py 265): INFO Train: [26/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2932 (0.3053)	loss 1.7533 (1.4721)	grad_norm 30.3676 (nan)	mem 4879MB
[2022-05-31 02:05:27 MetaFG_0] (main.py 265): INFO Train: [26/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2930 (0.3053)	loss 1.7882 (1.4720)	grad_norm 58.3206 (nan)	mem 4879MB
[2022-05-31 02:05:30 MetaFG_0] (main.py 265): INFO Train: [26/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2931 (0.3053)	loss 1.6776 (1.4723)	grad_norm 36.5435 (nan)	mem 4879MB
[2022-05-31 02:05:33 MetaFG_0] (main.py 265): INFO Train: [26/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2932 (0.3053)	loss 1.6331 (1.4727)	grad_norm 34.0804 (nan)	mem 4879MB
[2022-05-31 02:05:36 MetaFG_0] (main.py 265): INFO Train: [26/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2987 (0.3053)	loss 1.1707 (1.4732)	grad_norm 27.9632 (nan)	mem 4879MB
[2022-05-31 02:05:39 MetaFG_0] (main.py 265): INFO Train: [26/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.3011 (0.3053)	loss 1.7732 (1.4737)	grad_norm 46.8531 (nan)	mem 4879MB
[2022-05-31 02:05:42 MetaFG_0] (main.py 265): INFO Train: [26/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2920 (0.3052)	loss 1.7076 (1.4744)	grad_norm 25.3295 (nan)	mem 4879MB
[2022-05-31 02:05:45 MetaFG_0] (main.py 265): INFO Train: [26/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2936 (0.3052)	loss 1.6939 (1.4739)	grad_norm 22.0652 (nan)	mem 4879MB
[2022-05-31 02:05:49 MetaFG_0] (main.py 265): INFO Train: [26/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2987 (0.3052)	loss 1.2470 (1.4732)	grad_norm 32.4555 (nan)	mem 4879MB
[2022-05-31 02:05:52 MetaFG_0] (main.py 265): INFO Train: [26/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2919 (0.3052)	loss 1.7276 (1.4732)	grad_norm 23.8754 (nan)	mem 4879MB
[2022-05-31 02:05:55 MetaFG_0] (main.py 265): INFO Train: [26/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2936 (0.3052)	loss 1.4355 (1.4732)	grad_norm 21.3821 (nan)	mem 4879MB
[2022-05-31 02:05:58 MetaFG_0] (main.py 265): INFO Train: [26/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2921 (0.3052)	loss 1.7144 (1.4739)	grad_norm 20.1076 (nan)	mem 4879MB
[2022-05-31 02:06:01 MetaFG_0] (main.py 265): INFO Train: [26/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2963 (0.3052)	loss 1.7956 (1.4735)	grad_norm 39.8954 (nan)	mem 4879MB
[2022-05-31 02:06:04 MetaFG_0] (main.py 265): INFO Train: [26/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2928 (0.3052)	loss 1.0892 (1.4732)	grad_norm 39.6701 (nan)	mem 4879MB
[2022-05-31 02:06:07 MetaFG_0] (main.py 265): INFO Train: [26/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2926 (0.3052)	loss 1.2642 (1.4731)	grad_norm 31.3394 (nan)	mem 4879MB
[2022-05-31 02:06:10 MetaFG_0] (main.py 265): INFO Train: [26/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2993 (0.3051)	loss 1.1599 (1.4730)	grad_norm 38.5057 (nan)	mem 4879MB
[2022-05-31 02:06:13 MetaFG_0] (main.py 265): INFO Train: [26/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2914 (0.3051)	loss 1.8098 (1.4730)	grad_norm 21.2485 (nan)	mem 4879MB
[2022-05-31 02:06:13 MetaFG_0] (main.py 272): INFO EPOCH 26 training takes 0:07:56
[2022-05-31 02:06:13 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_26.pth saving......
[2022-05-31 02:06:14 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_26.pth saved !!!
[2022-05-31 02:06:14 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:06:15 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:06:15 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:06:16 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.673 (0.673)	Loss 0.6711 (0.6711)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 02:06:17 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.149)	Loss 0.7646 (0.6835)	Acc@1 84.375 (82.955)	Acc@5 96.875 (98.864)	Mem 4879MB
[2022-05-31 02:06:18 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.091 (0.123)	Loss 0.9560 (0.7349)	Acc@1 75.000 (81.250)	Acc@5 96.875 (98.512)	Mem 4879MB
[2022-05-31 02:06:19 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.114)	Loss 0.6282 (0.7616)	Acc@1 90.625 (81.552)	Acc@5 96.875 (98.085)	Mem 4879MB
[2022-05-31 02:06:20 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.099 (0.109)	Loss 1.1014 (0.7798)	Acc@1 65.625 (80.716)	Acc@5 100.000 (98.247)	Mem 4879MB
[2022-05-31 02:06:21 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.106)	Loss 0.7710 (0.7717)	Acc@1 84.375 (81.127)	Acc@5 96.875 (98.162)	Mem 4879MB
[2022-05-31 02:06:22 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.093 (0.104)	Loss 1.3036 (0.7687)	Acc@1 71.875 (81.865)	Acc@5 90.625 (98.053)	Mem 4879MB
[2022-05-31 02:06:23 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.103)	Loss 0.8979 (0.7656)	Acc@1 71.875 (81.998)	Acc@5 93.750 (98.063)	Mem 4879MB
[2022-05-31 02:06:24 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.093 (0.102)	Loss 0.8883 (0.7626)	Acc@1 84.375 (82.677)	Acc@5 93.750 (97.840)	Mem 4879MB
[2022-05-31 02:06:25 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.9923 (0.7679)	Acc@1 81.250 (82.486)	Acc@5 90.625 (97.699)	Mem 4879MB
[2022-05-31 02:06:26 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.109 (0.100)	Loss 0.7562 (0.7680)	Acc@1 81.250 (82.426)	Acc@5 93.750 (97.710)	Mem 4879MB
[2022-05-31 02:06:26 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.100)	Loss 0.5730 (0.7657)	Acc@1 93.750 (82.461)	Acc@5 100.000 (97.776)	Mem 4879MB
[2022-05-31 02:06:27 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.100)	Loss 0.5111 (0.7694)	Acc@1 96.875 (82.309)	Acc@5 96.875 (97.753)	Mem 4879MB
[2022-05-31 02:06:28 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.5181 (0.7691)	Acc@1 90.625 (82.276)	Acc@5 100.000 (97.829)	Mem 4879MB
[2022-05-31 02:06:29 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 0.8610 (0.7723)	Acc@1 78.125 (82.203)	Acc@5 100.000 (97.784)	Mem 4879MB
[2022-05-31 02:06:30 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.8411 (0.7671)	Acc@1 81.250 (82.471)	Acc@5 100.000 (97.827)	Mem 4879MB
[2022-05-31 02:06:31 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.3823 (0.7773)	Acc@1 100.000 (82.259)	Acc@5 100.000 (97.710)	Mem 4879MB
[2022-05-31 02:06:32 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.8890 (0.7762)	Acc@1 75.000 (82.145)	Acc@5 100.000 (97.825)	Mem 4879MB
[2022-05-31 02:06:33 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.097)	Loss 0.8988 (0.7810)	Acc@1 75.000 (82.131)	Acc@5 96.875 (97.704)	Mem 4879MB
[2022-05-31 02:06:34 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.8642 (0.7833)	Acc@1 78.125 (82.003)	Acc@5 100.000 (97.726)	Mem 4879MB
[2022-05-31 02:06:35 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.095 (0.097)	Loss 0.9794 (0.7866)	Acc@1 71.875 (81.748)	Acc@5 100.000 (97.730)	Mem 4879MB
[2022-05-31 02:06:36 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 1.0027 (0.7869)	Acc@1 62.500 (81.694)	Acc@5 100.000 (97.734)	Mem 4879MB
[2022-05-31 02:06:37 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.7110 (0.7895)	Acc@1 81.250 (81.632)	Acc@5 100.000 (97.752)	Mem 4879MB
[2022-05-31 02:06:38 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 0.5638 (0.7899)	Acc@1 90.625 (81.615)	Acc@5 100.000 (97.781)	Mem 4879MB
[2022-05-31 02:06:39 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.088 (0.097)	Loss 0.7689 (0.7949)	Acc@1 84.375 (81.457)	Acc@5 100.000 (97.770)	Mem 4879MB
[2022-05-31 02:06:40 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 1.2067 (0.7969)	Acc@1 75.000 (81.449)	Acc@5 96.875 (97.796)	Mem 4879MB
[2022-05-31 02:06:41 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.102 (0.097)	Loss 0.4230 (0.7946)	Acc@1 96.875 (81.561)	Acc@5 100.000 (97.797)	Mem 4879MB
[2022-05-31 02:06:41 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.089 (0.096)	Loss 0.6823 (0.7961)	Acc@1 81.250 (81.469)	Acc@5 100.000 (97.774)	Mem 4879MB
[2022-05-31 02:06:42 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.098 (0.096)	Loss 0.5979 (0.7939)	Acc@1 84.375 (81.573)	Acc@5 100.000 (97.809)	Mem 4879MB
[2022-05-31 02:06:43 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.090 (0.096)	Loss 0.6515 (0.7952)	Acc@1 84.375 (81.551)	Acc@5 100.000 (97.788)	Mem 4879MB
[2022-05-31 02:06:44 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 1.0207 (0.7964)	Acc@1 75.000 (81.551)	Acc@5 93.750 (97.789)	Mem 4879MB
[2022-05-31 02:06:45 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8413 (0.7963)	Acc@1 84.375 (81.551)	Acc@5 96.875 (97.789)	Mem 4879MB
[2022-05-31 02:06:45 MetaFG_0] (main.py 330): INFO  * Acc@1 81.510 Acc@5 97.770
[2022-05-31 02:06:45 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 81.5%
[2022-05-31 02:06:45 MetaFG_0] (main.py 171): INFO Max accuracy: 81.51%
[2022-05-31 02:06:46 MetaFG_0] (main.py 265): INFO Train: [27/300][0/1562]	eta 0:23:55 lr 0.000006	time 0.9192 (0.9192)	loss 1.3787 (1.3787)	grad_norm 27.3446 (27.3446)	mem 4879MB
[2022-05-31 02:06:50 MetaFG_0] (main.py 265): INFO Train: [27/300][10/1562]	eta 0:09:31 lr 0.000006	time 0.2934 (0.3681)	loss 1.2078 (1.5042)	grad_norm 28.1728 (26.6873)	mem 4879MB
[2022-05-31 02:06:53 MetaFG_0] (main.py 265): INFO Train: [27/300][20/1562]	eta 0:08:39 lr 0.000006	time 0.2924 (0.3366)	loss 1.6651 (1.5251)	grad_norm 32.2148 (31.0184)	mem 4879MB
[2022-05-31 02:06:56 MetaFG_0] (main.py 265): INFO Train: [27/300][30/1562]	eta 0:08:19 lr 0.000006	time 0.2986 (0.3262)	loss 1.6375 (1.5345)	grad_norm 34.5448 (29.2512)	mem 4879MB
[2022-05-31 02:06:59 MetaFG_0] (main.py 265): INFO Train: [27/300][40/1562]	eta 0:08:08 lr 0.000006	time 0.2924 (0.3207)	loss 1.6642 (1.5136)	grad_norm 22.0407 (30.1585)	mem 4879MB
[2022-05-31 02:07:02 MetaFG_0] (main.py 265): INFO Train: [27/300][50/1562]	eta 0:07:59 lr 0.000006	time 0.2936 (0.3173)	loss 1.1798 (1.4949)	grad_norm 33.5419 (30.4320)	mem 4879MB
[2022-05-31 02:07:05 MetaFG_0] (main.py 265): INFO Train: [27/300][60/1562]	eta 0:07:53 lr 0.000006	time 0.2988 (0.3153)	loss 1.6439 (1.5122)	grad_norm 18.1049 (29.8072)	mem 4879MB
[2022-05-31 02:07:08 MetaFG_0] (main.py 265): INFO Train: [27/300][70/1562]	eta 0:07:48 lr 0.000006	time 0.2975 (0.3138)	loss 1.6478 (1.4966)	grad_norm 21.6243 (29.7219)	mem 4879MB
[2022-05-31 02:07:11 MetaFG_0] (main.py 265): INFO Train: [27/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2934 (0.3125)	loss 1.6866 (1.4979)	grad_norm 28.9601 (29.4187)	mem 4879MB
[2022-05-31 02:07:14 MetaFG_0] (main.py 265): INFO Train: [27/300][90/1562]	eta 0:07:38 lr 0.000006	time 0.3013 (0.3116)	loss 1.7108 (1.5056)	grad_norm 16.8587 (29.8326)	mem 4879MB
[2022-05-31 02:07:17 MetaFG_0] (main.py 265): INFO Train: [27/300][100/1562]	eta 0:07:34 lr 0.000006	time 0.3004 (0.3108)	loss 1.3875 (1.4865)	grad_norm 31.7715 (29.4809)	mem 4879MB
[2022-05-31 02:07:20 MetaFG_0] (main.py 265): INFO Train: [27/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2933 (0.3103)	loss 1.7868 (1.4902)	grad_norm 33.9991 (29.3240)	mem 4879MB
[2022-05-31 02:07:23 MetaFG_0] (main.py 265): INFO Train: [27/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2965 (0.3097)	loss 1.5523 (1.4815)	grad_norm 31.1913 (29.7277)	mem 4879MB
[2022-05-31 02:07:26 MetaFG_0] (main.py 265): INFO Train: [27/300][130/1562]	eta 0:07:22 lr 0.000006	time 0.2981 (0.3092)	loss 1.1169 (1.4859)	grad_norm 29.6519 (29.7754)	mem 4879MB
[2022-05-31 02:07:29 MetaFG_0] (main.py 265): INFO Train: [27/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2935 (0.3088)	loss 1.2195 (1.4912)	grad_norm 30.2276 (30.0870)	mem 4879MB
[2022-05-31 02:07:32 MetaFG_0] (main.py 265): INFO Train: [27/300][150/1562]	eta 0:07:15 lr 0.000006	time 0.2990 (0.3087)	loss 1.5825 (1.4990)	grad_norm 26.1463 (29.7067)	mem 4879MB
[2022-05-31 02:07:35 MetaFG_0] (main.py 265): INFO Train: [27/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2935 (0.3084)	loss 1.5172 (1.4986)	grad_norm 24.8674 (29.6760)	mem 4879MB
[2022-05-31 02:07:38 MetaFG_0] (main.py 265): INFO Train: [27/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2989 (0.3082)	loss 1.4107 (1.4987)	grad_norm 55.3203 (29.6272)	mem 4879MB
[2022-05-31 02:07:41 MetaFG_0] (main.py 265): INFO Train: [27/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2981 (0.3080)	loss 1.6536 (1.4914)	grad_norm 29.2197 (29.4102)	mem 4879MB
[2022-05-31 02:07:44 MetaFG_0] (main.py 265): INFO Train: [27/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2922 (0.3077)	loss 1.2123 (1.4889)	grad_norm 20.6414 (29.2467)	mem 4879MB
[2022-05-31 02:07:47 MetaFG_0] (main.py 265): INFO Train: [27/300][200/1562]	eta 0:06:58 lr 0.000006	time 0.2937 (0.3076)	loss 1.6733 (1.4886)	grad_norm 15.4555 (29.1120)	mem 4879MB
[2022-05-31 02:07:50 MetaFG_0] (main.py 265): INFO Train: [27/300][210/1562]	eta 0:06:55 lr 0.000006	time 0.3032 (0.3075)	loss 1.2053 (1.4812)	grad_norm 46.4066 (29.1309)	mem 4879MB
[2022-05-31 02:07:53 MetaFG_0] (main.py 265): INFO Train: [27/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2942 (0.3074)	loss 1.6969 (1.4851)	grad_norm 27.2493 (29.2251)	mem 4879MB
[2022-05-31 02:07:56 MetaFG_0] (main.py 265): INFO Train: [27/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2921 (0.3072)	loss 1.6254 (1.4880)	grad_norm 22.7581 (29.2399)	mem 4879MB
[2022-05-31 02:08:00 MetaFG_0] (main.py 265): INFO Train: [27/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2918 (0.3072)	loss 1.3985 (1.4856)	grad_norm 32.3917 (29.1936)	mem 4879MB
[2022-05-31 02:08:03 MetaFG_0] (main.py 265): INFO Train: [27/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2920 (0.3070)	loss 1.5750 (1.4861)	grad_norm 26.2100 (29.0137)	mem 4879MB
[2022-05-31 02:08:06 MetaFG_0] (main.py 265): INFO Train: [27/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2922 (0.3070)	loss 1.5397 (1.4873)	grad_norm 14.2717 (28.9334)	mem 4879MB
[2022-05-31 02:08:09 MetaFG_0] (main.py 265): INFO Train: [27/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2929 (0.3068)	loss 1.3797 (1.4825)	grad_norm 23.6649 (28.9664)	mem 4879MB
[2022-05-31 02:08:12 MetaFG_0] (main.py 265): INFO Train: [27/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2917 (0.3068)	loss 1.0536 (1.4836)	grad_norm 30.3068 (29.3752)	mem 4879MB
[2022-05-31 02:08:15 MetaFG_0] (main.py 265): INFO Train: [27/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2921 (0.3066)	loss 1.6935 (1.4852)	grad_norm 21.9242 (29.2761)	mem 4879MB
[2022-05-31 02:08:18 MetaFG_0] (main.py 265): INFO Train: [27/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2983 (0.3065)	loss 1.6037 (1.4837)	grad_norm 23.4781 (29.3195)	mem 4879MB
[2022-05-31 02:08:21 MetaFG_0] (main.py 265): INFO Train: [27/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2948 (0.3065)	loss 1.3633 (1.4835)	grad_norm 23.0938 (29.2246)	mem 4879MB
[2022-05-31 02:08:24 MetaFG_0] (main.py 265): INFO Train: [27/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2938 (0.3064)	loss 1.7448 (1.4848)	grad_norm 23.6452 (29.3811)	mem 4879MB
[2022-05-31 02:08:27 MetaFG_0] (main.py 265): INFO Train: [27/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2918 (0.3063)	loss 1.5504 (1.4900)	grad_norm 18.2272 (29.2195)	mem 4879MB
[2022-05-31 02:08:30 MetaFG_0] (main.py 265): INFO Train: [27/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2922 (0.3062)	loss 1.3842 (1.4870)	grad_norm 20.7990 (29.1891)	mem 4879MB
[2022-05-31 02:08:33 MetaFG_0] (main.py 265): INFO Train: [27/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2988 (0.3061)	loss 1.4515 (1.4869)	grad_norm 45.7161 (29.1752)	mem 4879MB
[2022-05-31 02:08:36 MetaFG_0] (main.py 265): INFO Train: [27/300][360/1562]	eta 0:06:07 lr 0.000006	time 0.2974 (0.3061)	loss 1.5860 (1.4868)	grad_norm 33.4673 (29.3566)	mem 4879MB
[2022-05-31 02:08:39 MetaFG_0] (main.py 265): INFO Train: [27/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2972 (0.3060)	loss 1.5920 (1.4867)	grad_norm 28.4792 (29.3637)	mem 4879MB
[2022-05-31 02:08:42 MetaFG_0] (main.py 265): INFO Train: [27/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2979 (0.3060)	loss 1.6142 (1.4859)	grad_norm 40.8001 (29.4531)	mem 4879MB
[2022-05-31 02:08:45 MetaFG_0] (main.py 265): INFO Train: [27/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2923 (0.3060)	loss 1.4111 (1.4811)	grad_norm 33.2952 (29.5458)	mem 4879MB
[2022-05-31 02:08:48 MetaFG_0] (main.py 265): INFO Train: [27/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2929 (0.3059)	loss 1.3125 (1.4827)	grad_norm 32.2843 (29.4199)	mem 4879MB
[2022-05-31 02:08:51 MetaFG_0] (main.py 265): INFO Train: [27/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2959 (0.3058)	loss 1.5066 (1.4812)	grad_norm 17.6156 (29.3538)	mem 4879MB
[2022-05-31 02:08:54 MetaFG_0] (main.py 265): INFO Train: [27/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2926 (0.3058)	loss 1.7017 (1.4823)	grad_norm 23.5925 (29.3679)	mem 4879MB
[2022-05-31 02:08:57 MetaFG_0] (main.py 265): INFO Train: [27/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2921 (0.3058)	loss 1.1969 (1.4794)	grad_norm 32.1154 (29.4643)	mem 4879MB
[2022-05-31 02:09:00 MetaFG_0] (main.py 265): INFO Train: [27/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2925 (0.3057)	loss 1.3432 (1.4777)	grad_norm 18.9967 (29.4973)	mem 4879MB
[2022-05-31 02:09:03 MetaFG_0] (main.py 265): INFO Train: [27/300][450/1562]	eta 0:05:39 lr 0.000006	time 0.2920 (0.3057)	loss 1.1266 (1.4767)	grad_norm 45.8241 (29.5965)	mem 4879MB
[2022-05-31 02:09:06 MetaFG_0] (main.py 265): INFO Train: [27/300][460/1562]	eta 0:05:36 lr 0.000006	time 0.2913 (0.3057)	loss 1.3843 (1.4773)	grad_norm 19.4014 (29.7484)	mem 4879MB
[2022-05-31 02:09:09 MetaFG_0] (main.py 265): INFO Train: [27/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.3002 (0.3057)	loss 1.6333 (1.4791)	grad_norm 40.9444 (29.6912)	mem 4879MB
[2022-05-31 02:09:13 MetaFG_0] (main.py 265): INFO Train: [27/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2927 (0.3057)	loss 1.1374 (1.4800)	grad_norm 49.3810 (29.6368)	mem 4879MB
[2022-05-31 02:09:16 MetaFG_0] (main.py 265): INFO Train: [27/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2921 (0.3057)	loss 1.2042 (1.4780)	grad_norm 26.9044 (29.5630)	mem 4879MB
[2022-05-31 02:09:19 MetaFG_0] (main.py 265): INFO Train: [27/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2929 (0.3056)	loss 1.6746 (1.4771)	grad_norm 19.4001 (29.5698)	mem 4879MB
[2022-05-31 02:09:22 MetaFG_0] (main.py 265): INFO Train: [27/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2928 (0.3056)	loss 1.6454 (1.4761)	grad_norm 29.8675 (29.5560)	mem 4879MB
[2022-05-31 02:09:25 MetaFG_0] (main.py 265): INFO Train: [27/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2923 (0.3056)	loss 1.2889 (1.4741)	grad_norm 32.3963 (29.4437)	mem 4879MB
[2022-05-31 02:09:28 MetaFG_0] (main.py 265): INFO Train: [27/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2935 (0.3055)	loss 1.3300 (1.4740)	grad_norm 16.6167 (29.4824)	mem 4879MB
[2022-05-31 02:09:31 MetaFG_0] (main.py 265): INFO Train: [27/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2928 (0.3055)	loss 1.9135 (1.4745)	grad_norm 32.2416 (29.4930)	mem 4879MB
[2022-05-31 02:09:34 MetaFG_0] (main.py 265): INFO Train: [27/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2912 (0.3054)	loss 1.3060 (1.4728)	grad_norm 33.2647 (29.5488)	mem 4879MB
[2022-05-31 02:09:37 MetaFG_0] (main.py 265): INFO Train: [27/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2996 (0.3054)	loss 1.5709 (1.4759)	grad_norm 29.4299 (29.6271)	mem 4879MB
[2022-05-31 02:09:40 MetaFG_0] (main.py 265): INFO Train: [27/300][570/1562]	eta 0:05:02 lr 0.000006	time 0.2937 (0.3054)	loss 1.5339 (1.4774)	grad_norm 15.1784 (29.5301)	mem 4879MB
[2022-05-31 02:09:43 MetaFG_0] (main.py 265): INFO Train: [27/300][580/1562]	eta 0:04:59 lr 0.000006	time 0.2958 (0.3054)	loss 0.9491 (1.4757)	grad_norm 26.3550 (29.5813)	mem 4879MB
[2022-05-31 02:09:46 MetaFG_0] (main.py 265): INFO Train: [27/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2974 (0.3054)	loss 1.6597 (1.4755)	grad_norm 36.6795 (29.6310)	mem 4879MB
[2022-05-31 02:09:49 MetaFG_0] (main.py 265): INFO Train: [27/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2981 (0.3054)	loss 1.5681 (1.4741)	grad_norm 28.1436 (29.5913)	mem 4879MB
[2022-05-31 02:09:52 MetaFG_0] (main.py 265): INFO Train: [27/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2927 (0.3053)	loss 1.7477 (1.4730)	grad_norm 17.4887 (29.5604)	mem 4879MB
[2022-05-31 02:09:55 MetaFG_0] (main.py 265): INFO Train: [27/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2998 (0.3053)	loss 1.3002 (1.4720)	grad_norm 24.9709 (29.5066)	mem 4879MB
[2022-05-31 02:09:58 MetaFG_0] (main.py 265): INFO Train: [27/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.3017 (0.3053)	loss 1.5866 (1.4730)	grad_norm 22.1607 (29.4884)	mem 4879MB
[2022-05-31 02:10:01 MetaFG_0] (main.py 265): INFO Train: [27/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2923 (0.3053)	loss 1.1548 (1.4712)	grad_norm 47.2617 (29.5287)	mem 4879MB
[2022-05-31 02:10:04 MetaFG_0] (main.py 265): INFO Train: [27/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2982 (0.3052)	loss 1.6035 (1.4723)	grad_norm 31.0217 (29.4917)	mem 4879MB
[2022-05-31 02:10:07 MetaFG_0] (main.py 265): INFO Train: [27/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.3016 (0.3052)	loss 1.3916 (1.4713)	grad_norm 29.5884 (29.3984)	mem 4879MB
[2022-05-31 02:10:10 MetaFG_0] (main.py 265): INFO Train: [27/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2934 (0.3052)	loss 1.5226 (1.4711)	grad_norm 15.7571 (29.3853)	mem 4879MB
[2022-05-31 02:10:13 MetaFG_0] (main.py 265): INFO Train: [27/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2981 (0.3052)	loss 1.6490 (1.4706)	grad_norm 16.7872 (29.3588)	mem 4879MB
[2022-05-31 02:10:16 MetaFG_0] (main.py 265): INFO Train: [27/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2927 (0.3052)	loss 1.4380 (1.4710)	grad_norm 29.5631 (29.4227)	mem 4879MB
[2022-05-31 02:10:19 MetaFG_0] (main.py 265): INFO Train: [27/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2986 (0.3052)	loss 1.6138 (1.4711)	grad_norm 16.0435 (29.3898)	mem 4879MB
[2022-05-31 02:10:22 MetaFG_0] (main.py 265): INFO Train: [27/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2935 (0.3052)	loss 1.6924 (1.4711)	grad_norm 24.3453 (29.3828)	mem 4879MB
[2022-05-31 02:10:26 MetaFG_0] (main.py 265): INFO Train: [27/300][720/1562]	eta 0:04:16 lr 0.000006	time 0.2920 (0.3052)	loss 1.4835 (1.4700)	grad_norm 22.5011 (29.4176)	mem 4879MB
[2022-05-31 02:10:29 MetaFG_0] (main.py 265): INFO Train: [27/300][730/1562]	eta 0:04:13 lr 0.000006	time 0.2932 (0.3052)	loss 1.6847 (1.4710)	grad_norm 39.0185 (29.6231)	mem 4879MB
[2022-05-31 02:10:32 MetaFG_0] (main.py 265): INFO Train: [27/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2992 (0.3051)	loss 1.7319 (1.4710)	grad_norm 40.3780 (29.6352)	mem 4879MB
[2022-05-31 02:10:35 MetaFG_0] (main.py 265): INFO Train: [27/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.3057 (0.3052)	loss 1.7062 (1.4702)	grad_norm 28.1083 (29.6507)	mem 4879MB
[2022-05-31 02:10:38 MetaFG_0] (main.py 265): INFO Train: [27/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2927 (0.3052)	loss 1.1490 (1.4698)	grad_norm 36.5585 (29.7121)	mem 4879MB
[2022-05-31 02:10:41 MetaFG_0] (main.py 265): INFO Train: [27/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2926 (0.3051)	loss 0.9932 (1.4675)	grad_norm 19.7085 (29.6804)	mem 4879MB
[2022-05-31 02:10:44 MetaFG_0] (main.py 265): INFO Train: [27/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2927 (0.3051)	loss 1.4415 (1.4680)	grad_norm 42.7899 (29.7462)	mem 4879MB
[2022-05-31 02:10:47 MetaFG_0] (main.py 265): INFO Train: [27/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2926 (0.3051)	loss 1.3928 (1.4668)	grad_norm 15.4384 (29.7801)	mem 4879MB
[2022-05-31 02:10:50 MetaFG_0] (main.py 265): INFO Train: [27/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2989 (0.3051)	loss 0.9690 (1.4670)	grad_norm 26.2369 (29.7821)	mem 4879MB
[2022-05-31 02:10:53 MetaFG_0] (main.py 265): INFO Train: [27/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2981 (0.3051)	loss 1.3626 (1.4674)	grad_norm 24.0935 (29.7719)	mem 4879MB
[2022-05-31 02:10:56 MetaFG_0] (main.py 265): INFO Train: [27/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2975 (0.3051)	loss 1.0561 (1.4665)	grad_norm 31.3835 (29.8358)	mem 4879MB
[2022-05-31 02:10:59 MetaFG_0] (main.py 265): INFO Train: [27/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2982 (0.3051)	loss 1.1362 (1.4666)	grad_norm 27.3055 (29.7738)	mem 4879MB
[2022-05-31 02:11:02 MetaFG_0] (main.py 265): INFO Train: [27/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2982 (0.3051)	loss 1.7523 (1.4668)	grad_norm 38.7489 (29.7303)	mem 4879MB
[2022-05-31 02:11:05 MetaFG_0] (main.py 265): INFO Train: [27/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2982 (0.3051)	loss 1.6393 (1.4663)	grad_norm 14.8328 (29.6979)	mem 4879MB
[2022-05-31 02:11:08 MetaFG_0] (main.py 265): INFO Train: [27/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2920 (0.3051)	loss 1.4206 (1.4670)	grad_norm 22.2791 (29.6894)	mem 4879MB
[2022-05-31 02:11:11 MetaFG_0] (main.py 265): INFO Train: [27/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2983 (0.3051)	loss 1.6440 (1.4668)	grad_norm 27.1715 (29.7526)	mem 4879MB
[2022-05-31 02:11:14 MetaFG_0] (main.py 265): INFO Train: [27/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2979 (0.3051)	loss 1.8095 (1.4670)	grad_norm 30.0400 (29.7063)	mem 4879MB
[2022-05-31 02:11:17 MetaFG_0] (main.py 265): INFO Train: [27/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2920 (0.3050)	loss 1.9486 (1.4678)	grad_norm 21.9126 (29.6911)	mem 4879MB
[2022-05-31 02:11:20 MetaFG_0] (main.py 265): INFO Train: [27/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2988 (0.3050)	loss 0.9402 (1.4675)	grad_norm 28.0627 (29.6263)	mem 4879MB
[2022-05-31 02:11:23 MetaFG_0] (main.py 265): INFO Train: [27/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2931 (0.3050)	loss 1.7542 (1.4691)	grad_norm 26.0136 (29.6049)	mem 4879MB
[2022-05-31 02:11:26 MetaFG_0] (main.py 265): INFO Train: [27/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2937 (0.3050)	loss 1.5078 (1.4701)	grad_norm 25.5430 (29.5595)	mem 4879MB
[2022-05-31 02:11:29 MetaFG_0] (main.py 265): INFO Train: [27/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2932 (0.3050)	loss 1.5256 (1.4695)	grad_norm 13.6867 (29.5378)	mem 4879MB
[2022-05-31 02:11:32 MetaFG_0] (main.py 265): INFO Train: [27/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2928 (0.3050)	loss 1.6148 (1.4705)	grad_norm 22.9014 (29.5191)	mem 4879MB
[2022-05-31 02:11:35 MetaFG_0] (main.py 265): INFO Train: [27/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2920 (0.3049)	loss 1.3555 (1.4699)	grad_norm 41.1609 (29.4736)	mem 4879MB
[2022-05-31 02:11:39 MetaFG_0] (main.py 265): INFO Train: [27/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2990 (0.3049)	loss 0.9385 (1.4683)	grad_norm 22.7678 (29.4381)	mem 4879MB
[2022-05-31 02:11:42 MetaFG_0] (main.py 265): INFO Train: [27/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2983 (0.3050)	loss 1.6256 (1.4678)	grad_norm 20.0140 (29.4024)	mem 4879MB
[2022-05-31 02:11:45 MetaFG_0] (main.py 265): INFO Train: [27/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2957 (0.3049)	loss 1.6216 (1.4672)	grad_norm 16.5215 (29.3724)	mem 4879MB
[2022-05-31 02:11:48 MetaFG_0] (main.py 265): INFO Train: [27/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2984 (0.3049)	loss 1.3210 (1.4678)	grad_norm 47.8146 (29.3330)	mem 4879MB
[2022-05-31 02:11:51 MetaFG_0] (main.py 265): INFO Train: [27/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3003 (0.3049)	loss 1.5696 (1.4682)	grad_norm 19.7182 (29.3322)	mem 4879MB
[2022-05-31 02:11:54 MetaFG_0] (main.py 265): INFO Train: [27/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2933 (0.3049)	loss 1.5583 (1.4692)	grad_norm 15.1006 (29.3216)	mem 4879MB
[2022-05-31 02:11:57 MetaFG_0] (main.py 265): INFO Train: [27/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3000 (0.3049)	loss 1.4193 (1.4689)	grad_norm 20.6467 (29.3111)	mem 4879MB
[2022-05-31 02:12:00 MetaFG_0] (main.py 265): INFO Train: [27/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2922 (0.3049)	loss 1.3862 (1.4686)	grad_norm 31.1865 (29.3314)	mem 4879MB
[2022-05-31 02:12:03 MetaFG_0] (main.py 265): INFO Train: [27/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2933 (0.3049)	loss 1.3165 (1.4691)	grad_norm 24.2972 (29.2791)	mem 4879MB
[2022-05-31 02:12:06 MetaFG_0] (main.py 265): INFO Train: [27/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2990 (0.3049)	loss 1.6537 (1.4685)	grad_norm 25.7051 (29.2197)	mem 4879MB
[2022-05-31 02:12:09 MetaFG_0] (main.py 265): INFO Train: [27/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2995 (0.3049)	loss 1.6283 (1.4687)	grad_norm 37.5739 (29.2120)	mem 4879MB
[2022-05-31 02:12:12 MetaFG_0] (main.py 265): INFO Train: [27/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2977 (0.3049)	loss 1.6406 (1.4693)	grad_norm 26.4225 (29.1680)	mem 4879MB
[2022-05-31 02:12:15 MetaFG_0] (main.py 265): INFO Train: [27/300][1080/1562]	eta 0:02:26 lr 0.000006	time 0.2944 (0.3049)	loss 1.4983 (1.4703)	grad_norm 24.5218 (29.1819)	mem 4879MB
[2022-05-31 02:12:18 MetaFG_0] (main.py 265): INFO Train: [27/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2916 (0.3049)	loss 1.5457 (1.4703)	grad_norm 23.4613 (29.1767)	mem 4879MB
[2022-05-31 02:12:21 MetaFG_0] (main.py 265): INFO Train: [27/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2937 (0.3049)	loss 1.6031 (1.4707)	grad_norm 44.3651 (29.2205)	mem 4879MB
[2022-05-31 02:12:24 MetaFG_0] (main.py 265): INFO Train: [27/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2936 (0.3049)	loss 1.4448 (1.4716)	grad_norm 16.8760 (29.1694)	mem 4879MB
[2022-05-31 02:12:27 MetaFG_0] (main.py 265): INFO Train: [27/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2952 (0.3049)	loss 1.1418 (1.4724)	grad_norm 32.3395 (29.1799)	mem 4879MB
[2022-05-31 02:12:30 MetaFG_0] (main.py 265): INFO Train: [27/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2933 (0.3049)	loss 1.3189 (1.4711)	grad_norm 46.4129 (29.1753)	mem 4879MB
[2022-05-31 02:12:33 MetaFG_0] (main.py 265): INFO Train: [27/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2942 (0.3049)	loss 1.4889 (1.4718)	grad_norm 19.9128 (29.1784)	mem 4879MB
[2022-05-31 02:12:36 MetaFG_0] (main.py 265): INFO Train: [27/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3049)	loss 1.6360 (1.4713)	grad_norm 28.3819 (29.1509)	mem 4879MB
[2022-05-31 02:12:39 MetaFG_0] (main.py 265): INFO Train: [27/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2998 (0.3049)	loss 1.5870 (1.4714)	grad_norm 19.4173 (29.1512)	mem 4879MB
[2022-05-31 02:12:43 MetaFG_0] (main.py 265): INFO Train: [27/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2952 (0.3049)	loss 1.5521 (1.4714)	grad_norm 22.4880 (29.1809)	mem 4879MB
[2022-05-31 02:12:46 MetaFG_0] (main.py 265): INFO Train: [27/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2993 (0.3049)	loss 1.1437 (1.4711)	grad_norm 32.2399 (29.1732)	mem 4879MB
[2022-05-31 02:12:49 MetaFG_0] (main.py 265): INFO Train: [27/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2981 (0.3049)	loss 1.6402 (1.4708)	grad_norm 34.9406 (29.1349)	mem 4879MB
[2022-05-31 02:12:52 MetaFG_0] (main.py 265): INFO Train: [27/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2959 (0.3049)	loss 1.2649 (1.4713)	grad_norm 17.1002 (29.1347)	mem 4879MB
[2022-05-31 02:12:55 MetaFG_0] (main.py 265): INFO Train: [27/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3045 (0.3048)	loss 1.8364 (1.4717)	grad_norm 45.7811 (29.1323)	mem 4879MB
[2022-05-31 02:12:58 MetaFG_0] (main.py 265): INFO Train: [27/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2957 (0.3048)	loss 1.3869 (1.4712)	grad_norm 18.1249 (29.1268)	mem 4879MB
[2022-05-31 02:13:01 MetaFG_0] (main.py 265): INFO Train: [27/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2976 (0.3048)	loss 1.6717 (1.4711)	grad_norm 28.5226 (29.0768)	mem 4879MB
[2022-05-31 02:13:04 MetaFG_0] (main.py 265): INFO Train: [27/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.3025 (0.3049)	loss 1.4659 (1.4711)	grad_norm 22.5381 (29.0708)	mem 4879MB
[2022-05-31 02:13:07 MetaFG_0] (main.py 265): INFO Train: [27/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2982 (0.3048)	loss 1.6915 (1.4716)	grad_norm 18.0179 (29.0263)	mem 4879MB
[2022-05-31 02:13:10 MetaFG_0] (main.py 265): INFO Train: [27/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2953 (0.3048)	loss 1.6327 (1.4722)	grad_norm 15.6266 (29.0322)	mem 4879MB
[2022-05-31 02:13:13 MetaFG_0] (main.py 265): INFO Train: [27/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2997 (0.3048)	loss 1.5693 (1.4724)	grad_norm 22.1046 (29.0544)	mem 4879MB
[2022-05-31 02:13:16 MetaFG_0] (main.py 265): INFO Train: [27/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2935 (0.3048)	loss 1.3454 (1.4728)	grad_norm 25.5990 (29.0371)	mem 4879MB
[2022-05-31 02:13:19 MetaFG_0] (main.py 265): INFO Train: [27/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.3011 (0.3048)	loss 0.9689 (1.4730)	grad_norm 41.2124 (29.0226)	mem 4879MB
[2022-05-31 02:13:22 MetaFG_0] (main.py 265): INFO Train: [27/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2981 (0.3048)	loss 1.7935 (1.4727)	grad_norm 39.4482 (29.0182)	mem 4879MB
[2022-05-31 02:13:25 MetaFG_0] (main.py 265): INFO Train: [27/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2990 (0.3048)	loss 1.5203 (1.4730)	grad_norm 31.2130 (29.0324)	mem 4879MB
[2022-05-31 02:13:28 MetaFG_0] (main.py 265): INFO Train: [27/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2917 (0.3048)	loss 1.4389 (1.4724)	grad_norm 69.7851 (29.0747)	mem 4879MB
[2022-05-31 02:13:31 MetaFG_0] (main.py 265): INFO Train: [27/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2923 (0.3048)	loss 1.1620 (1.4722)	grad_norm 21.1362 (29.0610)	mem 4879MB
[2022-05-31 02:13:34 MetaFG_0] (main.py 265): INFO Train: [27/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2982 (0.3048)	loss 1.0747 (1.4724)	grad_norm 35.0631 (29.0763)	mem 4879MB
[2022-05-31 02:13:37 MetaFG_0] (main.py 265): INFO Train: [27/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2974 (0.3048)	loss 1.4275 (1.4723)	grad_norm 39.6213 (29.2918)	mem 4879MB
[2022-05-31 02:13:40 MetaFG_0] (main.py 265): INFO Train: [27/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2915 (0.3047)	loss 1.6458 (1.4716)	grad_norm 20.7493 (29.3235)	mem 4879MB
[2022-05-31 02:13:43 MetaFG_0] (main.py 265): INFO Train: [27/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2963 (0.3047)	loss 1.3819 (1.4716)	grad_norm 26.1035 (29.3210)	mem 4879MB
[2022-05-31 02:13:46 MetaFG_0] (main.py 265): INFO Train: [27/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2922 (0.3047)	loss 1.6637 (1.4722)	grad_norm 32.3676 (29.3027)	mem 4879MB
[2022-05-31 02:13:49 MetaFG_0] (main.py 265): INFO Train: [27/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2942 (0.3047)	loss 1.6412 (1.4719)	grad_norm 20.8109 (29.2817)	mem 4879MB
[2022-05-31 02:13:52 MetaFG_0] (main.py 265): INFO Train: [27/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2939 (0.3047)	loss 0.8966 (1.4717)	grad_norm 17.2223 (29.2628)	mem 4879MB
[2022-05-31 02:13:55 MetaFG_0] (main.py 265): INFO Train: [27/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3000 (0.3047)	loss 0.9831 (1.4710)	grad_norm 19.2758 (29.2668)	mem 4879MB
[2022-05-31 02:13:59 MetaFG_0] (main.py 265): INFO Train: [27/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2933 (0.3047)	loss 1.8197 (1.4709)	grad_norm 28.5680 (29.2426)	mem 4879MB
[2022-05-31 02:14:02 MetaFG_0] (main.py 265): INFO Train: [27/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2927 (0.3047)	loss 1.1175 (1.4715)	grad_norm 22.3999 (29.2103)	mem 4879MB
[2022-05-31 02:14:05 MetaFG_0] (main.py 265): INFO Train: [27/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2936 (0.3047)	loss 1.4714 (1.4713)	grad_norm 20.9467 (29.2036)	mem 4879MB
[2022-05-31 02:14:08 MetaFG_0] (main.py 265): INFO Train: [27/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2955 (0.3047)	loss 1.2027 (1.4702)	grad_norm 30.8879 (29.1949)	mem 4879MB
[2022-05-31 02:14:11 MetaFG_0] (main.py 265): INFO Train: [27/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2935 (0.3047)	loss 1.2215 (1.4703)	grad_norm 36.3346 (29.1993)	mem 4879MB
[2022-05-31 02:14:14 MetaFG_0] (main.py 265): INFO Train: [27/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2981 (0.3047)	loss 1.5915 (1.4697)	grad_norm 41.2239 (29.2295)	mem 4879MB
[2022-05-31 02:14:17 MetaFG_0] (main.py 265): INFO Train: [27/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2992 (0.3047)	loss 1.8786 (1.4700)	grad_norm 37.9774 (29.2210)	mem 4879MB
[2022-05-31 02:14:20 MetaFG_0] (main.py 265): INFO Train: [27/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2947 (0.3047)	loss 1.4363 (1.4703)	grad_norm 28.0364 (29.2129)	mem 4879MB
[2022-05-31 02:14:23 MetaFG_0] (main.py 265): INFO Train: [27/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2993 (0.3047)	loss 1.3760 (1.4706)	grad_norm 20.3108 (29.1838)	mem 4879MB
[2022-05-31 02:14:26 MetaFG_0] (main.py 265): INFO Train: [27/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2987 (0.3047)	loss 1.5765 (1.4711)	grad_norm 29.6647 (29.1840)	mem 4879MB
[2022-05-31 02:14:29 MetaFG_0] (main.py 265): INFO Train: [27/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2921 (0.3047)	loss 1.6246 (1.4721)	grad_norm 38.4427 (29.1820)	mem 4879MB
[2022-05-31 02:14:32 MetaFG_0] (main.py 265): INFO Train: [27/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2997 (0.3047)	loss 1.4331 (1.4717)	grad_norm 30.8540 (29.1589)	mem 4879MB
[2022-05-31 02:14:35 MetaFG_0] (main.py 265): INFO Train: [27/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2999 (0.3047)	loss 1.7216 (1.4716)	grad_norm 17.1332 (29.1276)	mem 4879MB
[2022-05-31 02:14:38 MetaFG_0] (main.py 265): INFO Train: [27/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2940 (0.3048)	loss 1.2715 (1.4719)	grad_norm 33.9953 (29.1132)	mem 4879MB
[2022-05-31 02:14:41 MetaFG_0] (main.py 265): INFO Train: [27/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2916 (0.3047)	loss 1.6209 (1.4708)	grad_norm 22.8922 (29.1021)	mem 4879MB
[2022-05-31 02:14:42 MetaFG_0] (main.py 272): INFO EPOCH 27 training takes 0:07:56
[2022-05-31 02:14:42 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_27.pth saving......
[2022-05-31 02:14:43 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_27.pth saved !!!
[2022-05-31 02:14:43 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:14:44 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:14:44 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:14:44 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.610 (0.610)	Loss 0.6998 (0.6998)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 02:14:45 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.092 (0.144)	Loss 0.7637 (0.6936)	Acc@1 84.375 (82.955)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 02:14:46 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.099 (0.121)	Loss 1.2333 (0.7641)	Acc@1 65.625 (81.399)	Acc@5 87.500 (96.429)	Mem 4879MB
[2022-05-31 02:14:47 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.113)	Loss 0.5824 (0.7419)	Acc@1 84.375 (82.560)	Acc@5 100.000 (96.976)	Mem 4879MB
[2022-05-31 02:14:48 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.108)	Loss 0.7577 (0.7461)	Acc@1 78.125 (82.393)	Acc@5 100.000 (97.104)	Mem 4879MB
[2022-05-31 02:14:49 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.105)	Loss 0.7456 (0.7603)	Acc@1 78.125 (81.434)	Acc@5 100.000 (97.365)	Mem 4879MB
[2022-05-31 02:14:50 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.091 (0.105)	Loss 0.9172 (0.7647)	Acc@1 84.375 (81.352)	Acc@5 93.750 (97.285)	Mem 4879MB
[2022-05-31 02:14:51 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.7436 (0.7805)	Acc@1 84.375 (80.810)	Acc@5 100.000 (97.183)	Mem 4879MB
[2022-05-31 02:14:52 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.7831 (0.7731)	Acc@1 75.000 (80.864)	Acc@5 100.000 (97.454)	Mem 4879MB
[2022-05-31 02:14:53 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.7068 (0.7686)	Acc@1 87.500 (81.147)	Acc@5 100.000 (97.390)	Mem 4879MB
[2022-05-31 02:14:54 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.7183 (0.7701)	Acc@1 84.375 (81.126)	Acc@5 100.000 (97.463)	Mem 4879MB
[2022-05-31 02:14:55 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 0.4608 (0.7658)	Acc@1 93.750 (81.419)	Acc@5 100.000 (97.494)	Mem 4879MB
[2022-05-31 02:14:56 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.7510 (0.7655)	Acc@1 81.250 (81.612)	Acc@5 100.000 (97.598)	Mem 4879MB
[2022-05-31 02:14:57 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.095 (0.099)	Loss 0.5919 (0.7669)	Acc@1 87.500 (81.632)	Acc@5 100.000 (97.567)	Mem 4879MB
[2022-05-31 02:14:58 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.091 (0.099)	Loss 0.8246 (0.7683)	Acc@1 84.375 (81.605)	Acc@5 96.875 (97.584)	Mem 4879MB
[2022-05-31 02:14:59 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.8636 (0.7688)	Acc@1 75.000 (81.560)	Acc@5 93.750 (97.599)	Mem 4879MB
[2022-05-31 02:15:00 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.4708 (0.7608)	Acc@1 93.750 (81.832)	Acc@5 100.000 (97.671)	Mem 4879MB
[2022-05-31 02:15:01 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 0.5729 (0.7641)	Acc@1 90.625 (81.707)	Acc@5 100.000 (97.752)	Mem 4879MB
[2022-05-31 02:15:01 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.098)	Loss 0.7755 (0.7579)	Acc@1 78.125 (81.906)	Acc@5 96.875 (97.825)	Mem 4879MB
[2022-05-31 02:15:02 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.093 (0.097)	Loss 0.7778 (0.7576)	Acc@1 78.125 (81.921)	Acc@5 100.000 (97.824)	Mem 4879MB
[2022-05-31 02:15:03 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.097)	Loss 1.2292 (0.7540)	Acc@1 68.750 (82.121)	Acc@5 96.875 (97.839)	Mem 4879MB
[2022-05-31 02:15:04 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.088 (0.097)	Loss 0.5870 (0.7507)	Acc@1 84.375 (82.139)	Acc@5 100.000 (97.867)	Mem 4879MB
[2022-05-31 02:15:05 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.6344 (0.7566)	Acc@1 81.250 (81.915)	Acc@5 100.000 (97.837)	Mem 4879MB
[2022-05-31 02:15:06 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 0.7073 (0.7572)	Acc@1 78.125 (81.778)	Acc@5 100.000 (97.876)	Mem 4879MB
[2022-05-31 02:15:07 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.5960 (0.7548)	Acc@1 87.500 (81.872)	Acc@5 96.875 (97.886)	Mem 4879MB
[2022-05-31 02:15:08 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.4922 (0.7515)	Acc@1 90.625 (81.935)	Acc@5 100.000 (97.908)	Mem 4879MB
[2022-05-31 02:15:09 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.092 (0.097)	Loss 0.8885 (0.7494)	Acc@1 81.250 (81.992)	Acc@5 93.750 (97.857)	Mem 4879MB
[2022-05-31 02:15:10 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.5504 (0.7482)	Acc@1 87.500 (81.965)	Acc@5 100.000 (97.855)	Mem 4879MB
[2022-05-31 02:15:11 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.093 (0.096)	Loss 0.6427 (0.7495)	Acc@1 84.375 (81.928)	Acc@5 100.000 (97.854)	Mem 4879MB
[2022-05-31 02:15:12 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.099 (0.096)	Loss 0.6728 (0.7493)	Acc@1 84.375 (81.927)	Acc@5 100.000 (97.852)	Mem 4879MB
[2022-05-31 02:15:13 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.093 (0.096)	Loss 0.9055 (0.7471)	Acc@1 78.125 (82.029)	Acc@5 96.875 (97.882)	Mem 4879MB
[2022-05-31 02:15:14 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.6392 (0.7481)	Acc@1 81.250 (82.014)	Acc@5 100.000 (97.910)	Mem 4879MB
[2022-05-31 02:15:14 MetaFG_0] (main.py 330): INFO  * Acc@1 81.990 Acc@5 97.900
[2022-05-31 02:15:14 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 82.0%
[2022-05-31 02:15:14 MetaFG_0] (main.py 171): INFO Max accuracy: 81.99%
[2022-05-31 02:15:15 MetaFG_0] (main.py 265): INFO Train: [28/300][0/1562]	eta 0:27:07 lr 0.000006	time 1.0421 (1.0421)	loss 1.7165 (1.7165)	grad_norm 32.7885 (32.7885)	mem 4879MB
[2022-05-31 02:15:18 MetaFG_0] (main.py 265): INFO Train: [28/300][10/1562]	eta 0:09:42 lr 0.000006	time 0.2918 (0.3756)	loss 1.4082 (1.5465)	grad_norm 36.1539 (28.7838)	mem 4879MB
[2022-05-31 02:15:21 MetaFG_0] (main.py 265): INFO Train: [28/300][20/1562]	eta 0:08:47 lr 0.000006	time 0.2979 (0.3418)	loss 1.5988 (1.4882)	grad_norm 21.3354 (27.3900)	mem 4879MB
[2022-05-31 02:15:24 MetaFG_0] (main.py 265): INFO Train: [28/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2923 (0.3295)	loss 1.1920 (1.4856)	grad_norm 24.0440 (27.5988)	mem 4879MB
[2022-05-31 02:15:27 MetaFG_0] (main.py 265): INFO Train: [28/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2937 (0.3234)	loss 1.8533 (1.4831)	grad_norm 19.7827 (28.0108)	mem 4879MB
[2022-05-31 02:15:30 MetaFG_0] (main.py 265): INFO Train: [28/300][50/1562]	eta 0:08:02 lr 0.000006	time 0.2919 (0.3193)	loss 1.1000 (1.4730)	grad_norm 56.4779 (27.7884)	mem 4879MB
[2022-05-31 02:15:33 MetaFG_0] (main.py 265): INFO Train: [28/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2934 (0.3166)	loss 1.1951 (1.4654)	grad_norm 16.8846 (27.8081)	mem 4879MB
[2022-05-31 02:15:36 MetaFG_0] (main.py 265): INFO Train: [28/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.2986 (0.3150)	loss 1.6927 (1.4608)	grad_norm 33.0705 (27.5306)	mem 4879MB
[2022-05-31 02:15:39 MetaFG_0] (main.py 265): INFO Train: [28/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2922 (0.3137)	loss 1.6131 (1.4640)	grad_norm 21.3330 (27.7664)	mem 4879MB
[2022-05-31 02:15:42 MetaFG_0] (main.py 265): INFO Train: [28/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2920 (0.3126)	loss 1.5934 (1.4732)	grad_norm 32.0894 (27.1902)	mem 4879MB
[2022-05-31 02:15:45 MetaFG_0] (main.py 265): INFO Train: [28/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2946 (0.3118)	loss 1.1792 (1.4640)	grad_norm 38.0084 (27.5390)	mem 4879MB
[2022-05-31 02:15:48 MetaFG_0] (main.py 265): INFO Train: [28/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2944 (0.3110)	loss 1.5384 (1.4666)	grad_norm 29.2170 (27.4190)	mem 4879MB
[2022-05-31 02:15:51 MetaFG_0] (main.py 265): INFO Train: [28/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2927 (0.3104)	loss 1.5929 (1.4736)	grad_norm 35.7777 (27.4856)	mem 4879MB
[2022-05-31 02:15:54 MetaFG_0] (main.py 265): INFO Train: [28/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.3002 (0.3100)	loss 1.5415 (1.4712)	grad_norm 31.8161 (27.5608)	mem 4879MB
[2022-05-31 02:15:58 MetaFG_0] (main.py 265): INFO Train: [28/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.3038 (0.3096)	loss 1.6781 (1.4691)	grad_norm 35.8042 (27.3961)	mem 4879MB
[2022-05-31 02:16:01 MetaFG_0] (main.py 265): INFO Train: [28/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2982 (0.3093)	loss 1.6133 (1.4694)	grad_norm 21.5782 (27.6972)	mem 4879MB
[2022-05-31 02:16:04 MetaFG_0] (main.py 265): INFO Train: [28/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2976 (0.3090)	loss 1.4012 (1.4648)	grad_norm 30.3553 (27.9688)	mem 4879MB
[2022-05-31 02:16:07 MetaFG_0] (main.py 265): INFO Train: [28/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2973 (0.3085)	loss 1.5965 (1.4682)	grad_norm 23.2182 (27.7724)	mem 4879MB
[2022-05-31 02:16:10 MetaFG_0] (main.py 265): INFO Train: [28/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2978 (0.3084)	loss 1.7886 (1.4710)	grad_norm 29.6681 (27.7381)	mem 4879MB
[2022-05-31 02:16:13 MetaFG_0] (main.py 265): INFO Train: [28/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.3015 (0.3082)	loss 1.6164 (1.4745)	grad_norm 18.1087 (27.7580)	mem 4879MB
[2022-05-31 02:16:16 MetaFG_0] (main.py 265): INFO Train: [28/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.3016 (0.3080)	loss 1.4974 (1.4773)	grad_norm 18.0717 (27.6958)	mem 4879MB
[2022-05-31 02:16:19 MetaFG_0] (main.py 265): INFO Train: [28/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2935 (0.3077)	loss 1.5902 (1.4709)	grad_norm 14.6880 (27.5104)	mem 4879MB
[2022-05-31 02:16:22 MetaFG_0] (main.py 265): INFO Train: [28/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2929 (0.3076)	loss 1.1154 (1.4677)	grad_norm 36.5965 (27.5805)	mem 4879MB
[2022-05-31 02:16:25 MetaFG_0] (main.py 265): INFO Train: [28/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2996 (0.3074)	loss 1.3945 (1.4690)	grad_norm 26.9645 (27.7477)	mem 4879MB
[2022-05-31 02:16:28 MetaFG_0] (main.py 265): INFO Train: [28/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2993 (0.3073)	loss 1.5046 (1.4731)	grad_norm 35.4275 (27.7412)	mem 4879MB
[2022-05-31 02:16:31 MetaFG_0] (main.py 265): INFO Train: [28/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.3003 (0.3072)	loss 1.0276 (1.4705)	grad_norm 31.7721 (27.7316)	mem 4879MB
[2022-05-31 02:16:34 MetaFG_0] (main.py 265): INFO Train: [28/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2927 (0.3070)	loss 1.7359 (1.4745)	grad_norm 53.1685 (27.9362)	mem 4879MB
[2022-05-31 02:16:37 MetaFG_0] (main.py 265): INFO Train: [28/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.3007 (0.3069)	loss 1.2366 (1.4733)	grad_norm 40.1265 (28.0051)	mem 4879MB
[2022-05-31 02:16:40 MetaFG_0] (main.py 265): INFO Train: [28/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.3030 (0.3069)	loss 1.5134 (1.4709)	grad_norm 35.5965 (27.9928)	mem 4879MB
[2022-05-31 02:16:43 MetaFG_0] (main.py 265): INFO Train: [28/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2977 (0.3068)	loss 1.6695 (1.4681)	grad_norm 51.1532 (28.3960)	mem 4879MB
[2022-05-31 02:16:46 MetaFG_0] (main.py 265): INFO Train: [28/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2972 (0.3067)	loss 1.8976 (1.4725)	grad_norm 51.5856 (28.4721)	mem 4879MB
[2022-05-31 02:16:49 MetaFG_0] (main.py 265): INFO Train: [28/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2916 (0.3066)	loss 1.4195 (1.4724)	grad_norm 17.2623 (28.5329)	mem 4879MB
[2022-05-31 02:16:52 MetaFG_0] (main.py 265): INFO Train: [28/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.3088 (0.3066)	loss 1.2773 (1.4748)	grad_norm 19.5365 (28.3853)	mem 4879MB
[2022-05-31 02:16:55 MetaFG_0] (main.py 265): INFO Train: [28/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2921 (0.3065)	loss 1.6135 (1.4750)	grad_norm 18.1470 (28.3635)	mem 4879MB
[2022-05-31 02:16:58 MetaFG_0] (main.py 265): INFO Train: [28/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2918 (0.3064)	loss 1.6252 (1.4752)	grad_norm 29.5971 (28.4577)	mem 4879MB
[2022-05-31 02:17:01 MetaFG_0] (main.py 265): INFO Train: [28/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2995 (0.3064)	loss 1.2171 (1.4731)	grad_norm 20.1017 (28.5160)	mem 4879MB
[2022-05-31 02:17:04 MetaFG_0] (main.py 265): INFO Train: [28/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2933 (0.3064)	loss 1.5642 (1.4739)	grad_norm 41.4787 (28.4712)	mem 4879MB
[2022-05-31 02:17:08 MetaFG_0] (main.py 265): INFO Train: [28/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2935 (0.3063)	loss 1.7820 (1.4740)	grad_norm 30.2604 (28.5570)	mem 4879MB
[2022-05-31 02:17:11 MetaFG_0] (main.py 265): INFO Train: [28/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.3007 (0.3063)	loss 1.6076 (1.4761)	grad_norm 35.0257 (28.5185)	mem 4879MB
[2022-05-31 02:17:14 MetaFG_0] (main.py 265): INFO Train: [28/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2925 (0.3062)	loss 0.9895 (1.4767)	grad_norm 25.0307 (29.2020)	mem 4879MB
[2022-05-31 02:17:17 MetaFG_0] (main.py 265): INFO Train: [28/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2928 (0.3062)	loss 1.5143 (1.4757)	grad_norm 34.5961 (29.1852)	mem 4879MB
[2022-05-31 02:17:20 MetaFG_0] (main.py 265): INFO Train: [28/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2984 (0.3062)	loss 1.6458 (1.4764)	grad_norm 28.9995 (29.2855)	mem 4879MB
[2022-05-31 02:17:23 MetaFG_0] (main.py 265): INFO Train: [28/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2932 (0.3061)	loss 1.1864 (1.4756)	grad_norm 29.3890 (29.3317)	mem 4879MB
[2022-05-31 02:17:26 MetaFG_0] (main.py 265): INFO Train: [28/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2931 (0.3061)	loss 1.4425 (1.4773)	grad_norm 22.2282 (29.2421)	mem 4879MB
[2022-05-31 02:17:29 MetaFG_0] (main.py 265): INFO Train: [28/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2987 (0.3060)	loss 1.7156 (1.4777)	grad_norm 27.7839 (29.1651)	mem 4879MB
[2022-05-31 02:17:32 MetaFG_0] (main.py 265): INFO Train: [28/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2928 (0.3060)	loss 1.7838 (1.4787)	grad_norm 25.1002 (29.2559)	mem 4879MB
[2022-05-31 02:17:35 MetaFG_0] (main.py 265): INFO Train: [28/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2983 (0.3060)	loss 1.4114 (1.4801)	grad_norm 21.7724 (29.1810)	mem 4879MB
[2022-05-31 02:17:38 MetaFG_0] (main.py 265): INFO Train: [28/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2971 (0.3060)	loss 1.2306 (1.4794)	grad_norm 30.7453 (29.2616)	mem 4879MB
[2022-05-31 02:17:41 MetaFG_0] (main.py 265): INFO Train: [28/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2922 (0.3059)	loss 1.6612 (1.4785)	grad_norm 21.1838 (29.2302)	mem 4879MB
[2022-05-31 02:17:44 MetaFG_0] (main.py 265): INFO Train: [28/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2920 (0.3058)	loss 1.4481 (1.4782)	grad_norm 42.6858 (29.1705)	mem 4879MB
[2022-05-31 02:17:47 MetaFG_0] (main.py 265): INFO Train: [28/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2976 (0.3058)	loss 1.8241 (1.4790)	grad_norm 48.1760 (29.2154)	mem 4879MB
[2022-05-31 02:17:50 MetaFG_0] (main.py 265): INFO Train: [28/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2991 (0.3058)	loss 1.5427 (1.4804)	grad_norm 18.7797 (29.2260)	mem 4879MB
[2022-05-31 02:17:53 MetaFG_0] (main.py 265): INFO Train: [28/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2928 (0.3058)	loss 1.1468 (1.4809)	grad_norm 26.7597 (29.2024)	mem 4879MB
[2022-05-31 02:17:56 MetaFG_0] (main.py 265): INFO Train: [28/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2995 (0.3057)	loss 1.3550 (1.4801)	grad_norm 20.3770 (29.2005)	mem 4879MB
[2022-05-31 02:17:59 MetaFG_0] (main.py 265): INFO Train: [28/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2986 (0.3057)	loss 1.7081 (1.4804)	grad_norm 22.7878 (29.1801)	mem 4879MB
[2022-05-31 02:18:02 MetaFG_0] (main.py 265): INFO Train: [28/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2930 (0.3056)	loss 1.3390 (1.4776)	grad_norm 26.2125 (29.1564)	mem 4879MB
[2022-05-31 02:18:05 MetaFG_0] (main.py 265): INFO Train: [28/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2980 (0.3057)	loss 1.5595 (1.4769)	grad_norm 44.7549 (29.1474)	mem 4879MB
[2022-05-31 02:18:08 MetaFG_0] (main.py 265): INFO Train: [28/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2980 (0.3057)	loss 1.3964 (1.4776)	grad_norm 25.9079 (29.1591)	mem 4879MB
[2022-05-31 02:18:11 MetaFG_0] (main.py 265): INFO Train: [28/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2920 (0.3057)	loss 1.3490 (1.4770)	grad_norm 16.8904 (29.1066)	mem 4879MB
[2022-05-31 02:18:14 MetaFG_0] (main.py 265): INFO Train: [28/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.3002 (0.3056)	loss 1.1224 (1.4763)	grad_norm 17.4378 (29.1112)	mem 4879MB
[2022-05-31 02:18:18 MetaFG_0] (main.py 265): INFO Train: [28/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.3007 (0.3056)	loss 1.6400 (1.4776)	grad_norm 28.8194 (29.1158)	mem 4879MB
[2022-05-31 02:18:21 MetaFG_0] (main.py 265): INFO Train: [28/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2974 (0.3056)	loss 1.4344 (1.4768)	grad_norm 18.4428 (29.2336)	mem 4879MB
[2022-05-31 02:18:24 MetaFG_0] (main.py 265): INFO Train: [28/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2998 (0.3055)	loss 1.4538 (1.4779)	grad_norm 30.0981 (29.1737)	mem 4879MB
[2022-05-31 02:18:27 MetaFG_0] (main.py 265): INFO Train: [28/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2917 (0.3055)	loss 0.8747 (1.4772)	grad_norm 29.3683 (29.1509)	mem 4879MB
[2022-05-31 02:18:30 MetaFG_0] (main.py 265): INFO Train: [28/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2995 (0.3055)	loss 1.5748 (1.4783)	grad_norm 26.6918 (29.1139)	mem 4879MB
[2022-05-31 02:18:33 MetaFG_0] (main.py 265): INFO Train: [28/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2940 (0.3055)	loss 1.5503 (1.4787)	grad_norm 31.4607 (29.0923)	mem 4879MB
[2022-05-31 02:18:36 MetaFG_0] (main.py 265): INFO Train: [28/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2924 (0.3055)	loss 1.4789 (1.4792)	grad_norm 25.8400 (29.0232)	mem 4879MB
[2022-05-31 02:18:39 MetaFG_0] (main.py 265): INFO Train: [28/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2972 (0.3055)	loss 1.6147 (1.4785)	grad_norm 33.7439 (29.0468)	mem 4879MB
[2022-05-31 02:18:42 MetaFG_0] (main.py 265): INFO Train: [28/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2924 (0.3054)	loss 1.3905 (1.4767)	grad_norm 53.6620 (29.0754)	mem 4879MB
[2022-05-31 02:18:45 MetaFG_0] (main.py 265): INFO Train: [28/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2985 (0.3054)	loss 1.4952 (1.4763)	grad_norm 16.0430 (29.0860)	mem 4879MB
[2022-05-31 02:18:48 MetaFG_0] (main.py 265): INFO Train: [28/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2971 (0.3054)	loss 1.2764 (1.4770)	grad_norm 21.0582 (29.0536)	mem 4879MB
[2022-05-31 02:18:51 MetaFG_0] (main.py 265): INFO Train: [28/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2931 (0.3054)	loss 1.5292 (1.4772)	grad_norm 20.6162 (29.1012)	mem 4879MB
[2022-05-31 02:18:54 MetaFG_0] (main.py 265): INFO Train: [28/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2970 (0.3053)	loss 1.3494 (1.4749)	grad_norm 35.1616 (29.1859)	mem 4879MB
[2022-05-31 02:18:57 MetaFG_0] (main.py 265): INFO Train: [28/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2945 (0.3053)	loss 1.5237 (1.4762)	grad_norm 17.8887 (29.1355)	mem 4879MB
[2022-05-31 02:19:00 MetaFG_0] (main.py 265): INFO Train: [28/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2975 (0.3053)	loss 1.3822 (1.4751)	grad_norm 27.1656 (29.1347)	mem 4879MB
[2022-05-31 02:19:03 MetaFG_0] (main.py 265): INFO Train: [28/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.3013 (0.3053)	loss 1.1233 (1.4741)	grad_norm 29.6539 (29.1483)	mem 4879MB
[2022-05-31 02:19:06 MetaFG_0] (main.py 265): INFO Train: [28/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2927 (0.3052)	loss 1.1225 (1.4746)	grad_norm 28.4265 (29.0785)	mem 4879MB
[2022-05-31 02:19:09 MetaFG_0] (main.py 265): INFO Train: [28/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2937 (0.3052)	loss 1.3796 (1.4746)	grad_norm 27.7228 (29.0418)	mem 4879MB
[2022-05-31 02:19:12 MetaFG_0] (main.py 265): INFO Train: [28/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2933 (0.3052)	loss 1.0925 (1.4739)	grad_norm 16.5119 (29.0368)	mem 4879MB
[2022-05-31 02:19:15 MetaFG_0] (main.py 265): INFO Train: [28/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2953 (0.3051)	loss 1.3064 (1.4724)	grad_norm 29.7470 (29.0641)	mem 4879MB
[2022-05-31 02:19:18 MetaFG_0] (main.py 265): INFO Train: [28/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2923 (0.3051)	loss 1.7097 (1.4719)	grad_norm 33.1003 (29.0747)	mem 4879MB
[2022-05-31 02:19:21 MetaFG_0] (main.py 265): INFO Train: [28/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2940 (0.3051)	loss 1.7094 (1.4738)	grad_norm 31.6303 (29.0683)	mem 4879MB
[2022-05-31 02:19:24 MetaFG_0] (main.py 265): INFO Train: [28/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2916 (0.3051)	loss 1.4297 (1.4725)	grad_norm 44.6878 (29.1375)	mem 4879MB
[2022-05-31 02:19:27 MetaFG_0] (main.py 265): INFO Train: [28/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2931 (0.3051)	loss 1.1641 (1.4720)	grad_norm 43.1489 (29.0990)	mem 4879MB
[2022-05-31 02:19:30 MetaFG_0] (main.py 265): INFO Train: [28/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2922 (0.3051)	loss 1.2633 (1.4712)	grad_norm 29.8780 (29.1376)	mem 4879MB
[2022-05-31 02:19:33 MetaFG_0] (main.py 265): INFO Train: [28/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2926 (0.3051)	loss 1.4651 (1.4725)	grad_norm 24.9345 (29.1352)	mem 4879MB
[2022-05-31 02:19:37 MetaFG_0] (main.py 265): INFO Train: [28/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2988 (0.3051)	loss 1.0715 (1.4724)	grad_norm 32.3528 (29.2007)	mem 4879MB
[2022-05-31 02:19:40 MetaFG_0] (main.py 265): INFO Train: [28/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2933 (0.3050)	loss 1.5537 (1.4733)	grad_norm 21.0861 (29.2216)	mem 4879MB
[2022-05-31 02:19:43 MetaFG_0] (main.py 265): INFO Train: [28/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2974 (0.3050)	loss 1.3003 (1.4735)	grad_norm 29.7681 (29.1852)	mem 4879MB
[2022-05-31 02:19:46 MetaFG_0] (main.py 265): INFO Train: [28/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2992 (0.3050)	loss 1.8551 (1.4740)	grad_norm 30.2748 (29.1863)	mem 4879MB
[2022-05-31 02:19:49 MetaFG_0] (main.py 265): INFO Train: [28/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.3019 (0.3050)	loss 1.6674 (1.4736)	grad_norm 24.0577 (29.2023)	mem 4879MB
[2022-05-31 02:19:52 MetaFG_0] (main.py 265): INFO Train: [28/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2999 (0.3050)	loss 1.6183 (1.4752)	grad_norm 34.4756 (29.2499)	mem 4879MB
[2022-05-31 02:19:55 MetaFG_0] (main.py 265): INFO Train: [28/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2917 (0.3050)	loss 0.9691 (1.4748)	grad_norm 24.9256 (29.2544)	mem 4879MB
[2022-05-31 02:19:58 MetaFG_0] (main.py 265): INFO Train: [28/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.3009 (0.3050)	loss 1.4776 (1.4749)	grad_norm 15.9579 (29.2428)	mem 4879MB
[2022-05-31 02:20:01 MetaFG_0] (main.py 265): INFO Train: [28/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2927 (0.3050)	loss 1.2947 (1.4740)	grad_norm 23.6062 (29.2232)	mem 4879MB
[2022-05-31 02:20:04 MetaFG_0] (main.py 265): INFO Train: [28/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2979 (0.3050)	loss 1.7635 (1.4739)	grad_norm 36.3789 (29.2667)	mem 4879MB
[2022-05-31 02:20:07 MetaFG_0] (main.py 265): INFO Train: [28/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2971 (0.3050)	loss 1.1501 (1.4725)	grad_norm 38.1534 (29.2371)	mem 4879MB
[2022-05-31 02:20:10 MetaFG_0] (main.py 265): INFO Train: [28/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2920 (0.3053)	loss 1.0718 (1.4723)	grad_norm 20.5175 (29.1930)	mem 4879MB
[2022-05-31 02:20:13 MetaFG_0] (main.py 265): INFO Train: [28/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2929 (0.3053)	loss 1.5195 (1.4732)	grad_norm 45.1234 (29.1895)	mem 4879MB
[2022-05-31 02:20:16 MetaFG_0] (main.py 265): INFO Train: [28/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2922 (0.3053)	loss 1.6413 (1.4739)	grad_norm 18.3159 (29.1720)	mem 4879MB
[2022-05-31 02:20:19 MetaFG_0] (main.py 265): INFO Train: [28/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3006 (0.3053)	loss 1.2511 (1.4740)	grad_norm 21.4312 (29.1270)	mem 4879MB
[2022-05-31 02:20:23 MetaFG_0] (main.py 265): INFO Train: [28/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2931 (0.3053)	loss 1.6099 (1.4740)	grad_norm 19.8742 (29.0758)	mem 4879MB
[2022-05-31 02:20:26 MetaFG_0] (main.py 265): INFO Train: [28/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2989 (0.3053)	loss 1.5521 (1.4737)	grad_norm 31.6812 (29.0714)	mem 4879MB
[2022-05-31 02:20:29 MetaFG_0] (main.py 265): INFO Train: [28/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2936 (0.3053)	loss 1.7345 (1.4736)	grad_norm 25.3951 (29.0621)	mem 4879MB
[2022-05-31 02:20:32 MetaFG_0] (main.py 265): INFO Train: [28/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2917 (0.3052)	loss 1.6172 (1.4741)	grad_norm 18.6036 (29.0494)	mem 4879MB
[2022-05-31 02:20:35 MetaFG_0] (main.py 265): INFO Train: [28/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2988 (0.3052)	loss 1.4713 (1.4741)	grad_norm 25.2523 (29.0212)	mem 4879MB
[2022-05-31 02:20:38 MetaFG_0] (main.py 265): INFO Train: [28/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2931 (0.3052)	loss 1.5491 (1.4739)	grad_norm 17.9028 (28.9373)	mem 4879MB
[2022-05-31 02:20:41 MetaFG_0] (main.py 265): INFO Train: [28/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2919 (0.3052)	loss 1.2425 (1.4743)	grad_norm 54.9239 (28.9585)	mem 4879MB
[2022-05-31 02:20:44 MetaFG_0] (main.py 265): INFO Train: [28/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2976 (0.3052)	loss 1.7837 (1.4737)	grad_norm 26.5476 (28.9626)	mem 4879MB
[2022-05-31 02:20:47 MetaFG_0] (main.py 265): INFO Train: [28/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.3002 (0.3052)	loss 1.5259 (1.4734)	grad_norm 28.1602 (28.9506)	mem 4879MB
[2022-05-31 02:20:50 MetaFG_0] (main.py 265): INFO Train: [28/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2980 (0.3052)	loss 1.1052 (1.4729)	grad_norm 44.7294 (28.9479)	mem 4879MB
[2022-05-31 02:20:53 MetaFG_0] (main.py 265): INFO Train: [28/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2920 (0.3051)	loss 1.3843 (1.4724)	grad_norm 26.8021 (28.9667)	mem 4879MB
[2022-05-31 02:20:56 MetaFG_0] (main.py 265): INFO Train: [28/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2922 (0.3051)	loss 1.4669 (1.4721)	grad_norm 41.7362 (28.9484)	mem 4879MB
[2022-05-31 02:20:59 MetaFG_0] (main.py 265): INFO Train: [28/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2927 (0.3051)	loss 1.2575 (1.4714)	grad_norm 29.7096 (28.9939)	mem 4879MB
[2022-05-31 02:21:02 MetaFG_0] (main.py 265): INFO Train: [28/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2972 (0.3051)	loss 1.5598 (1.4715)	grad_norm 18.4156 (nan)	mem 4879MB
[2022-05-31 02:21:05 MetaFG_0] (main.py 265): INFO Train: [28/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2923 (0.3051)	loss 1.4470 (1.4716)	grad_norm 57.1218 (nan)	mem 4879MB
[2022-05-31 02:21:08 MetaFG_0] (main.py 265): INFO Train: [28/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2941 (0.3051)	loss 1.1924 (1.4718)	grad_norm 57.1030 (nan)	mem 4879MB
[2022-05-31 02:21:11 MetaFG_0] (main.py 265): INFO Train: [28/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2991 (0.3051)	loss 1.3898 (1.4713)	grad_norm 28.3020 (nan)	mem 4879MB
[2022-05-31 02:21:14 MetaFG_0] (main.py 265): INFO Train: [28/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2937 (0.3051)	loss 1.7988 (1.4715)	grad_norm 22.8000 (nan)	mem 4879MB
[2022-05-31 02:21:17 MetaFG_0] (main.py 265): INFO Train: [28/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2979 (0.3051)	loss 1.1999 (1.4707)	grad_norm 16.6578 (nan)	mem 4879MB
[2022-05-31 02:21:20 MetaFG_0] (main.py 265): INFO Train: [28/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2929 (0.3051)	loss 0.9698 (1.4698)	grad_norm 26.9925 (nan)	mem 4879MB
[2022-05-31 02:21:23 MetaFG_0] (main.py 265): INFO Train: [28/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2918 (0.3051)	loss 1.5480 (1.4707)	grad_norm 30.6119 (nan)	mem 4879MB
[2022-05-31 02:21:26 MetaFG_0] (main.py 265): INFO Train: [28/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3049 (0.3051)	loss 1.3931 (1.4708)	grad_norm 31.6769 (nan)	mem 4879MB
[2022-05-31 02:21:29 MetaFG_0] (main.py 265): INFO Train: [28/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2921 (0.3051)	loss 1.5735 (1.4703)	grad_norm 23.5436 (nan)	mem 4879MB
[2022-05-31 02:21:32 MetaFG_0] (main.py 265): INFO Train: [28/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2926 (0.3051)	loss 1.4445 (1.4709)	grad_norm 15.7944 (nan)	mem 4879MB
[2022-05-31 02:21:35 MetaFG_0] (main.py 265): INFO Train: [28/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2992 (0.3051)	loss 1.5071 (1.4698)	grad_norm 19.9066 (nan)	mem 4879MB
[2022-05-31 02:21:39 MetaFG_0] (main.py 265): INFO Train: [28/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2918 (0.3051)	loss 1.3184 (1.4696)	grad_norm 22.1843 (nan)	mem 4879MB
[2022-05-31 02:21:42 MetaFG_0] (main.py 265): INFO Train: [28/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2921 (0.3051)	loss 1.6638 (1.4702)	grad_norm 42.6911 (nan)	mem 4879MB
[2022-05-31 02:21:45 MetaFG_0] (main.py 265): INFO Train: [28/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2935 (0.3051)	loss 1.2519 (1.4697)	grad_norm 27.7270 (nan)	mem 4879MB
[2022-05-31 02:21:48 MetaFG_0] (main.py 265): INFO Train: [28/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2928 (0.3050)	loss 1.5241 (1.4694)	grad_norm 22.0811 (nan)	mem 4879MB
[2022-05-31 02:21:51 MetaFG_0] (main.py 265): INFO Train: [28/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2950 (0.3050)	loss 1.6786 (1.4702)	grad_norm 30.6029 (nan)	mem 4879MB
[2022-05-31 02:21:54 MetaFG_0] (main.py 265): INFO Train: [28/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.3000 (0.3050)	loss 1.5714 (1.4704)	grad_norm 23.5561 (nan)	mem 4879MB
[2022-05-31 02:21:57 MetaFG_0] (main.py 265): INFO Train: [28/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2977 (0.3050)	loss 1.6689 (1.4703)	grad_norm 21.9103 (nan)	mem 4879MB
[2022-05-31 02:22:00 MetaFG_0] (main.py 265): INFO Train: [28/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2922 (0.3050)	loss 1.3823 (1.4700)	grad_norm 20.8108 (nan)	mem 4879MB
[2022-05-31 02:22:03 MetaFG_0] (main.py 265): INFO Train: [28/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2946 (0.3050)	loss 1.3106 (1.4701)	grad_norm 67.4825 (nan)	mem 4879MB
[2022-05-31 02:22:06 MetaFG_0] (main.py 265): INFO Train: [28/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2917 (0.3051)	loss 1.4846 (1.4708)	grad_norm 23.2006 (nan)	mem 4879MB
[2022-05-31 02:22:09 MetaFG_0] (main.py 265): INFO Train: [28/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2995 (0.3050)	loss 1.4771 (1.4712)	grad_norm 26.1680 (nan)	mem 4879MB
[2022-05-31 02:22:12 MetaFG_0] (main.py 265): INFO Train: [28/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2976 (0.3050)	loss 1.5616 (1.4711)	grad_norm 28.5627 (nan)	mem 4879MB
[2022-05-31 02:22:15 MetaFG_0] (main.py 265): INFO Train: [28/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2997 (0.3050)	loss 1.1173 (1.4715)	grad_norm 23.7159 (nan)	mem 4879MB
[2022-05-31 02:22:18 MetaFG_0] (main.py 265): INFO Train: [28/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2993 (0.3050)	loss 1.6190 (1.4713)	grad_norm 18.2936 (nan)	mem 4879MB
[2022-05-31 02:22:21 MetaFG_0] (main.py 265): INFO Train: [28/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2965 (0.3050)	loss 1.6459 (1.4718)	grad_norm 31.7094 (nan)	mem 4879MB
[2022-05-31 02:22:24 MetaFG_0] (main.py 265): INFO Train: [28/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2918 (0.3050)	loss 1.7856 (1.4718)	grad_norm 54.7184 (nan)	mem 4879MB
[2022-05-31 02:22:27 MetaFG_0] (main.py 265): INFO Train: [28/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2959 (0.3050)	loss 1.2178 (1.4713)	grad_norm 21.7173 (nan)	mem 4879MB
[2022-05-31 02:22:30 MetaFG_0] (main.py 265): INFO Train: [28/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2927 (0.3050)	loss 1.5617 (1.4714)	grad_norm 26.9265 (nan)	mem 4879MB
[2022-05-31 02:22:33 MetaFG_0] (main.py 265): INFO Train: [28/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2938 (0.3050)	loss 1.4231 (1.4711)	grad_norm 27.6982 (nan)	mem 4879MB
[2022-05-31 02:22:36 MetaFG_0] (main.py 265): INFO Train: [28/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2931 (0.3050)	loss 1.2564 (1.4715)	grad_norm 19.0831 (nan)	mem 4879MB
[2022-05-31 02:22:39 MetaFG_0] (main.py 265): INFO Train: [28/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2918 (0.3050)	loss 1.2738 (1.4714)	grad_norm 31.3778 (nan)	mem 4879MB
[2022-05-31 02:22:42 MetaFG_0] (main.py 265): INFO Train: [28/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2934 (0.3050)	loss 1.4625 (1.4712)	grad_norm 17.6910 (nan)	mem 4879MB
[2022-05-31 02:22:46 MetaFG_0] (main.py 265): INFO Train: [28/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2919 (0.3050)	loss 1.5479 (1.4712)	grad_norm 19.9579 (nan)	mem 4879MB
[2022-05-31 02:22:49 MetaFG_0] (main.py 265): INFO Train: [28/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2985 (0.3050)	loss 1.6606 (1.4711)	grad_norm 24.3334 (nan)	mem 4879MB
[2022-05-31 02:22:52 MetaFG_0] (main.py 265): INFO Train: [28/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2923 (0.3050)	loss 1.6103 (1.4713)	grad_norm 28.8748 (nan)	mem 4879MB
[2022-05-31 02:22:55 MetaFG_0] (main.py 265): INFO Train: [28/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2979 (0.3050)	loss 1.4880 (1.4708)	grad_norm 24.6645 (nan)	mem 4879MB
[2022-05-31 02:22:58 MetaFG_0] (main.py 265): INFO Train: [28/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2998 (0.3050)	loss 1.4479 (1.4705)	grad_norm 23.3057 (nan)	mem 4879MB
[2022-05-31 02:23:01 MetaFG_0] (main.py 265): INFO Train: [28/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2912 (0.3050)	loss 1.4192 (1.4708)	grad_norm 16.4837 (nan)	mem 4879MB
[2022-05-31 02:23:04 MetaFG_0] (main.py 265): INFO Train: [28/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2991 (0.3050)	loss 1.2273 (1.4707)	grad_norm 22.3990 (nan)	mem 4879MB
[2022-05-31 02:23:07 MetaFG_0] (main.py 265): INFO Train: [28/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2943 (0.3050)	loss 1.4593 (1.4712)	grad_norm 32.3763 (nan)	mem 4879MB
[2022-05-31 02:23:10 MetaFG_0] (main.py 265): INFO Train: [28/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2927 (0.3049)	loss 1.7081 (1.4717)	grad_norm 63.4755 (nan)	mem 4879MB
[2022-05-31 02:23:10 MetaFG_0] (main.py 272): INFO EPOCH 28 training takes 0:07:56
[2022-05-31 02:23:10 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_28.pth saving......
[2022-05-31 02:23:11 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_28.pth saved !!!
[2022-05-31 02:23:11 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:23:13 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:23:13 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:23:13 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.550 (0.550)	Loss 0.9161 (0.9161)	Acc@1 78.125 (78.125)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 02:23:14 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.141)	Loss 0.8124 (0.7567)	Acc@1 84.375 (83.523)	Acc@5 100.000 (98.011)	Mem 4879MB
[2022-05-31 02:23:15 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.091 (0.118)	Loss 0.8248 (0.7729)	Acc@1 84.375 (83.631)	Acc@5 93.750 (97.173)	Mem 4879MB
[2022-05-31 02:23:16 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.093 (0.111)	Loss 0.8218 (0.7877)	Acc@1 84.375 (82.661)	Acc@5 100.000 (97.379)	Mem 4879MB
[2022-05-31 02:23:17 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.105 (0.108)	Loss 0.9051 (0.7839)	Acc@1 71.875 (82.470)	Acc@5 96.875 (97.561)	Mem 4879MB
[2022-05-31 02:23:18 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.105)	Loss 1.1303 (0.7940)	Acc@1 65.625 (82.047)	Acc@5 96.875 (97.365)	Mem 4879MB
[2022-05-31 02:23:19 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.103)	Loss 1.0972 (0.7849)	Acc@1 81.250 (82.275)	Acc@5 93.750 (97.541)	Mem 4879MB
[2022-05-31 02:23:20 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.6604 (0.7796)	Acc@1 90.625 (82.350)	Acc@5 100.000 (97.711)	Mem 4879MB
[2022-05-31 02:23:21 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.091 (0.101)	Loss 0.5914 (0.7663)	Acc@1 96.875 (82.909)	Acc@5 100.000 (97.840)	Mem 4879MB
[2022-05-31 02:23:22 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 0.7905 (0.7825)	Acc@1 84.375 (82.383)	Acc@5 100.000 (97.871)	Mem 4879MB
[2022-05-31 02:23:23 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.6713 (0.7900)	Acc@1 93.750 (82.147)	Acc@5 100.000 (97.896)	Mem 4879MB
[2022-05-31 02:23:24 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.099 (0.099)	Loss 0.5656 (0.7803)	Acc@1 93.750 (82.545)	Acc@5 100.000 (98.001)	Mem 4879MB
[2022-05-31 02:23:25 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.7219 (0.7849)	Acc@1 87.500 (82.231)	Acc@5 96.875 (97.960)	Mem 4879MB
[2022-05-31 02:23:26 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.098 (0.098)	Loss 0.9067 (0.7878)	Acc@1 81.250 (82.252)	Acc@5 100.000 (97.925)	Mem 4879MB
[2022-05-31 02:23:27 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.098)	Loss 0.5938 (0.7881)	Acc@1 81.250 (82.181)	Acc@5 100.000 (97.939)	Mem 4879MB
[2022-05-31 02:23:27 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 1.0105 (0.7893)	Acc@1 65.625 (81.974)	Acc@5 96.875 (97.910)	Mem 4879MB
[2022-05-31 02:23:28 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.111 (0.097)	Loss 0.7138 (0.7940)	Acc@1 90.625 (81.949)	Acc@5 100.000 (97.865)	Mem 4879MB
[2022-05-31 02:23:29 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.097)	Loss 1.0096 (0.7908)	Acc@1 78.125 (82.054)	Acc@5 100.000 (97.917)	Mem 4879MB
[2022-05-31 02:23:30 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.097)	Loss 1.1496 (0.7916)	Acc@1 78.125 (82.096)	Acc@5 87.500 (97.876)	Mem 4879MB
[2022-05-31 02:23:31 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.100 (0.097)	Loss 0.9305 (0.7934)	Acc@1 81.250 (82.117)	Acc@5 93.750 (97.857)	Mem 4879MB
[2022-05-31 02:23:32 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.8943 (0.7989)	Acc@1 84.375 (81.996)	Acc@5 96.875 (97.808)	Mem 4879MB
[2022-05-31 02:23:33 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.8693 (0.7966)	Acc@1 84.375 (82.020)	Acc@5 100.000 (97.823)	Mem 4879MB
[2022-05-31 02:23:34 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.096)	Loss 0.6836 (0.7952)	Acc@1 87.500 (82.014)	Acc@5 100.000 (97.851)	Mem 4879MB
[2022-05-31 02:23:35 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.096)	Loss 0.7599 (0.7945)	Acc@1 81.250 (82.075)	Acc@5 100.000 (97.835)	Mem 4879MB
[2022-05-31 02:23:36 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.096)	Loss 0.6505 (0.7950)	Acc@1 84.375 (82.015)	Acc@5 100.000 (97.822)	Mem 4879MB
[2022-05-31 02:23:37 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 0.9616 (0.7960)	Acc@1 65.625 (81.947)	Acc@5 96.875 (97.834)	Mem 4879MB
[2022-05-31 02:23:38 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.104 (0.096)	Loss 0.5692 (0.7954)	Acc@1 93.750 (81.992)	Acc@5 96.875 (97.845)	Mem 4879MB
[2022-05-31 02:23:39 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 1.0437 (0.7972)	Acc@1 75.000 (82.023)	Acc@5 96.875 (97.798)	Mem 4879MB
[2022-05-31 02:23:40 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.101 (0.096)	Loss 0.8871 (0.7989)	Acc@1 75.000 (81.906)	Acc@5 93.750 (97.754)	Mem 4879MB
[2022-05-31 02:23:41 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.6971 (0.7946)	Acc@1 84.375 (81.991)	Acc@5 96.875 (97.799)	Mem 4879MB
[2022-05-31 02:23:42 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.7379 (0.7953)	Acc@1 84.375 (82.008)	Acc@5 100.000 (97.799)	Mem 4879MB
[2022-05-31 02:23:42 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 0.8054 (0.7981)	Acc@1 78.125 (81.943)	Acc@5 100.000 (97.779)	Mem 4879MB
[2022-05-31 02:23:43 MetaFG_0] (main.py 330): INFO  * Acc@1 81.960 Acc@5 97.790
[2022-05-31 02:23:43 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 82.0%
[2022-05-31 02:23:43 MetaFG_0] (main.py 171): INFO Max accuracy: 81.99%
[2022-05-31 02:23:44 MetaFG_0] (main.py 265): INFO Train: [29/300][0/1562]	eta 0:25:18 lr 0.000006	time 0.9722 (0.9722)	loss 1.6432 (1.6432)	grad_norm 25.8093 (25.8093)	mem 4879MB
[2022-05-31 02:23:47 MetaFG_0] (main.py 265): INFO Train: [29/300][10/1562]	eta 0:09:38 lr 0.000006	time 0.2926 (0.3726)	loss 1.6132 (1.5512)	grad_norm 15.2572 (25.9681)	mem 4879MB
[2022-05-31 02:23:50 MetaFG_0] (main.py 265): INFO Train: [29/300][20/1562]	eta 0:08:45 lr 0.000006	time 0.2937 (0.3407)	loss 1.6364 (1.5361)	grad_norm 18.0459 (25.9315)	mem 4879MB
[2022-05-31 02:23:53 MetaFG_0] (main.py 265): INFO Train: [29/300][30/1562]	eta 0:08:23 lr 0.000006	time 0.2949 (0.3284)	loss 1.4949 (1.4692)	grad_norm 26.0683 (28.6258)	mem 4879MB
[2022-05-31 02:23:56 MetaFG_0] (main.py 265): INFO Train: [29/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2928 (0.3224)	loss 1.4143 (1.4500)	grad_norm 19.3369 (28.7030)	mem 4879MB
[2022-05-31 02:23:59 MetaFG_0] (main.py 265): INFO Train: [29/300][50/1562]	eta 0:08:02 lr 0.000006	time 0.2917 (0.3193)	loss 1.2426 (1.4549)	grad_norm 20.6715 (28.2713)	mem 4879MB
[2022-05-31 02:24:02 MetaFG_0] (main.py 265): INFO Train: [29/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2916 (0.3166)	loss 1.5943 (1.4393)	grad_norm 36.8107 (29.0451)	mem 4879MB
[2022-05-31 02:24:05 MetaFG_0] (main.py 265): INFO Train: [29/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.3002 (0.3146)	loss 1.5807 (1.4379)	grad_norm 21.5828 (29.5111)	mem 4879MB
[2022-05-31 02:24:08 MetaFG_0] (main.py 265): INFO Train: [29/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2921 (0.3131)	loss 0.8893 (1.4218)	grad_norm 20.3491 (29.2492)	mem 4879MB
[2022-05-31 02:24:11 MetaFG_0] (main.py 265): INFO Train: [29/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2925 (0.3122)	loss 1.6590 (1.4425)	grad_norm 32.1813 (29.1763)	mem 4879MB
[2022-05-31 02:24:14 MetaFG_0] (main.py 265): INFO Train: [29/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2978 (0.3113)	loss 1.0781 (1.4490)	grad_norm 22.0870 (29.2703)	mem 4879MB
[2022-05-31 02:24:17 MetaFG_0] (main.py 265): INFO Train: [29/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2978 (0.3105)	loss 1.5864 (1.4494)	grad_norm 25.9439 (28.9273)	mem 4879MB
[2022-05-31 02:24:20 MetaFG_0] (main.py 265): INFO Train: [29/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2985 (0.3098)	loss 1.5374 (1.4491)	grad_norm 21.4929 (28.9744)	mem 4879MB
[2022-05-31 02:24:23 MetaFG_0] (main.py 265): INFO Train: [29/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2931 (0.3094)	loss 1.5951 (1.4480)	grad_norm 34.7010 (29.1929)	mem 4879MB
[2022-05-31 02:24:26 MetaFG_0] (main.py 265): INFO Train: [29/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2972 (0.3092)	loss 1.6385 (1.4535)	grad_norm 19.9757 (28.8694)	mem 4879MB
[2022-05-31 02:24:29 MetaFG_0] (main.py 265): INFO Train: [29/300][150/1562]	eta 0:07:15 lr 0.000006	time 0.2975 (0.3088)	loss 1.1176 (1.4508)	grad_norm 73.8992 (28.9463)	mem 4879MB
[2022-05-31 02:24:32 MetaFG_0] (main.py 265): INFO Train: [29/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2933 (0.3084)	loss 1.2394 (1.4471)	grad_norm 18.5269 (28.9743)	mem 4879MB
[2022-05-31 02:24:35 MetaFG_0] (main.py 265): INFO Train: [29/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2982 (0.3082)	loss 1.3908 (1.4503)	grad_norm 28.9595 (28.7232)	mem 4879MB
[2022-05-31 02:24:38 MetaFG_0] (main.py 265): INFO Train: [29/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2934 (0.3080)	loss 1.1827 (1.4474)	grad_norm 37.0091 (28.7353)	mem 4879MB
[2022-05-31 02:24:41 MetaFG_0] (main.py 265): INFO Train: [29/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2936 (0.3078)	loss 1.6724 (1.4527)	grad_norm 16.9895 (28.6600)	mem 4879MB
[2022-05-31 02:24:45 MetaFG_0] (main.py 265): INFO Train: [29/300][200/1562]	eta 0:06:58 lr 0.000006	time 0.2936 (0.3076)	loss 1.7409 (1.4444)	grad_norm 47.1736 (28.8458)	mem 4879MB
[2022-05-31 02:24:48 MetaFG_0] (main.py 265): INFO Train: [29/300][210/1562]	eta 0:06:55 lr 0.000006	time 0.2926 (0.3075)	loss 0.8467 (1.4448)	grad_norm 21.1611 (29.0258)	mem 4879MB
[2022-05-31 02:24:51 MetaFG_0] (main.py 265): INFO Train: [29/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2994 (0.3074)	loss 1.2336 (1.4406)	grad_norm 28.7201 (29.0417)	mem 4879MB
[2022-05-31 02:24:54 MetaFG_0] (main.py 265): INFO Train: [29/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2923 (0.3072)	loss 1.2569 (1.4399)	grad_norm 44.5501 (29.2491)	mem 4879MB
[2022-05-31 02:24:57 MetaFG_0] (main.py 265): INFO Train: [29/300][240/1562]	eta 0:06:45 lr 0.000006	time 0.2921 (0.3070)	loss 1.5288 (1.4337)	grad_norm 25.4559 (29.3527)	mem 4879MB
[2022-05-31 02:25:00 MetaFG_0] (main.py 265): INFO Train: [29/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2950 (0.3069)	loss 1.7291 (1.4375)	grad_norm 55.0437 (29.4916)	mem 4879MB
[2022-05-31 02:25:03 MetaFG_0] (main.py 265): INFO Train: [29/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2981 (0.3068)	loss 1.5274 (1.4399)	grad_norm 22.6939 (29.5333)	mem 4879MB
[2022-05-31 02:25:06 MetaFG_0] (main.py 265): INFO Train: [29/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2976 (0.3067)	loss 1.3524 (1.4383)	grad_norm 17.7394 (29.3961)	mem 4879MB
[2022-05-31 02:25:09 MetaFG_0] (main.py 265): INFO Train: [29/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.3003 (0.3066)	loss 1.8760 (1.4372)	grad_norm 38.5382 (29.3834)	mem 4879MB
[2022-05-31 02:25:12 MetaFG_0] (main.py 265): INFO Train: [29/300][290/1562]	eta 0:06:29 lr 0.000006	time 0.2980 (0.3066)	loss 1.7817 (1.4366)	grad_norm 24.9824 (29.2925)	mem 4879MB
[2022-05-31 02:25:15 MetaFG_0] (main.py 265): INFO Train: [29/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2979 (0.3066)	loss 1.4334 (1.4396)	grad_norm 30.0343 (29.1313)	mem 4879MB
[2022-05-31 02:25:18 MetaFG_0] (main.py 265): INFO Train: [29/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2979 (0.3065)	loss 1.3783 (1.4367)	grad_norm 34.5115 (29.1705)	mem 4879MB
[2022-05-31 02:25:21 MetaFG_0] (main.py 265): INFO Train: [29/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2926 (0.3063)	loss 1.3980 (1.4390)	grad_norm 52.5479 (29.3486)	mem 4879MB
[2022-05-31 02:25:24 MetaFG_0] (main.py 265): INFO Train: [29/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.3006 (0.3063)	loss 1.0857 (1.4357)	grad_norm 33.3458 (29.5212)	mem 4879MB
[2022-05-31 02:25:27 MetaFG_0] (main.py 265): INFO Train: [29/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2997 (0.3063)	loss 1.2432 (1.4366)	grad_norm 91.5714 (29.6664)	mem 4879MB
[2022-05-31 02:25:30 MetaFG_0] (main.py 265): INFO Train: [29/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.3034 (0.3063)	loss 1.6646 (1.4337)	grad_norm 39.0857 (29.5881)	mem 4879MB
[2022-05-31 02:25:33 MetaFG_0] (main.py 265): INFO Train: [29/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2918 (0.3063)	loss 1.0227 (1.4303)	grad_norm 32.4429 (29.5312)	mem 4879MB
[2022-05-31 02:25:36 MetaFG_0] (main.py 265): INFO Train: [29/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2979 (0.3062)	loss 1.6245 (1.4344)	grad_norm 20.4426 (29.6305)	mem 4879MB
[2022-05-31 02:25:39 MetaFG_0] (main.py 265): INFO Train: [29/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2995 (0.3061)	loss 1.2133 (1.4360)	grad_norm 40.7140 (29.5925)	mem 4879MB
[2022-05-31 02:25:42 MetaFG_0] (main.py 265): INFO Train: [29/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2993 (0.3062)	loss 1.3904 (1.4381)	grad_norm 18.1816 (29.3891)	mem 4879MB
[2022-05-31 02:25:45 MetaFG_0] (main.py 265): INFO Train: [29/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.3018 (0.3061)	loss 1.7234 (1.4414)	grad_norm 16.6133 (29.3700)	mem 4879MB
[2022-05-31 02:25:48 MetaFG_0] (main.py 265): INFO Train: [29/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2998 (0.3061)	loss 1.6835 (1.4413)	grad_norm 29.6607 (29.3302)	mem 4879MB
[2022-05-31 02:25:52 MetaFG_0] (main.py 265): INFO Train: [29/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2931 (0.3061)	loss 1.6222 (1.4419)	grad_norm 35.6581 (29.4038)	mem 4879MB
[2022-05-31 02:25:55 MetaFG_0] (main.py 265): INFO Train: [29/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2921 (0.3060)	loss 1.2668 (1.4443)	grad_norm 28.1823 (29.2919)	mem 4879MB
[2022-05-31 02:25:58 MetaFG_0] (main.py 265): INFO Train: [29/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2976 (0.3060)	loss 1.2380 (1.4431)	grad_norm 40.3305 (29.3369)	mem 4879MB
[2022-05-31 02:26:01 MetaFG_0] (main.py 265): INFO Train: [29/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2932 (0.3059)	loss 1.5255 (1.4437)	grad_norm 34.2555 (29.3480)	mem 4879MB
[2022-05-31 02:26:04 MetaFG_0] (main.py 265): INFO Train: [29/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.3025 (0.3059)	loss 1.3866 (1.4417)	grad_norm 16.3150 (29.3552)	mem 4879MB
[2022-05-31 02:26:07 MetaFG_0] (main.py 265): INFO Train: [29/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.3054 (0.3059)	loss 1.1619 (1.4421)	grad_norm 23.5580 (29.3209)	mem 4879MB
[2022-05-31 02:26:10 MetaFG_0] (main.py 265): INFO Train: [29/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2976 (0.3058)	loss 1.3583 (1.4432)	grad_norm 19.2932 (29.3677)	mem 4879MB
[2022-05-31 02:26:13 MetaFG_0] (main.py 265): INFO Train: [29/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2923 (0.3058)	loss 1.6407 (1.4438)	grad_norm 20.3395 (29.3751)	mem 4879MB
[2022-05-31 02:26:16 MetaFG_0] (main.py 265): INFO Train: [29/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2926 (0.3057)	loss 1.1845 (1.4412)	grad_norm 58.4025 (29.5158)	mem 4879MB
[2022-05-31 02:26:19 MetaFG_0] (main.py 265): INFO Train: [29/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2924 (0.3057)	loss 1.4572 (1.4401)	grad_norm 22.7525 (29.7087)	mem 4879MB
[2022-05-31 02:26:22 MetaFG_0] (main.py 265): INFO Train: [29/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2994 (0.3057)	loss 1.4826 (1.4400)	grad_norm 37.0340 (29.6387)	mem 4879MB
[2022-05-31 02:26:25 MetaFG_0] (main.py 265): INFO Train: [29/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2994 (0.3057)	loss 1.6125 (1.4411)	grad_norm 21.5174 (29.6424)	mem 4879MB
[2022-05-31 02:26:28 MetaFG_0] (main.py 265): INFO Train: [29/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2978 (0.3057)	loss 1.0946 (1.4415)	grad_norm 25.9491 (29.6516)	mem 4879MB
[2022-05-31 02:26:31 MetaFG_0] (main.py 265): INFO Train: [29/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2918 (0.3057)	loss 1.2851 (1.4414)	grad_norm 35.8027 (29.9491)	mem 4879MB
[2022-05-31 02:26:34 MetaFG_0] (main.py 265): INFO Train: [29/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2937 (0.3056)	loss 1.4362 (1.4405)	grad_norm 34.4939 (29.9638)	mem 4879MB
[2022-05-31 02:26:37 MetaFG_0] (main.py 265): INFO Train: [29/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2935 (0.3056)	loss 1.1996 (1.4379)	grad_norm 27.4454 (29.9872)	mem 4879MB
[2022-05-31 02:26:40 MetaFG_0] (main.py 265): INFO Train: [29/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2930 (0.3056)	loss 1.6201 (1.4389)	grad_norm 32.5691 (29.9367)	mem 4879MB
[2022-05-31 02:26:43 MetaFG_0] (main.py 265): INFO Train: [29/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2921 (0.3055)	loss 1.5125 (1.4391)	grad_norm 32.0351 (29.9403)	mem 4879MB
[2022-05-31 02:26:46 MetaFG_0] (main.py 265): INFO Train: [29/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2922 (0.3055)	loss 1.6151 (1.4385)	grad_norm 33.1235 (29.8538)	mem 4879MB
[2022-05-31 02:26:49 MetaFG_0] (main.py 265): INFO Train: [29/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2928 (0.3055)	loss 1.5805 (1.4382)	grad_norm 38.4371 (29.9590)	mem 4879MB
[2022-05-31 02:26:52 MetaFG_0] (main.py 265): INFO Train: [29/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2929 (0.3054)	loss 1.3573 (1.4387)	grad_norm 31.8596 (29.9432)	mem 4879MB
[2022-05-31 02:26:55 MetaFG_0] (main.py 265): INFO Train: [29/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2986 (0.3054)	loss 1.2432 (1.4378)	grad_norm 23.2909 (29.9282)	mem 4879MB
[2022-05-31 02:26:58 MetaFG_0] (main.py 265): INFO Train: [29/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.3008 (0.3054)	loss 1.5527 (1.4394)	grad_norm 33.3503 (29.9039)	mem 4879MB
[2022-05-31 02:27:02 MetaFG_0] (main.py 265): INFO Train: [29/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2993 (0.3054)	loss 1.6432 (1.4419)	grad_norm 27.8437 (29.8386)	mem 4879MB
[2022-05-31 02:27:05 MetaFG_0] (main.py 265): INFO Train: [29/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2934 (0.3054)	loss 1.5547 (1.4423)	grad_norm 21.4515 (29.8362)	mem 4879MB
[2022-05-31 02:27:08 MetaFG_0] (main.py 265): INFO Train: [29/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2927 (0.3054)	loss 1.4221 (1.4434)	grad_norm 25.7579 (29.7669)	mem 4879MB
[2022-05-31 02:27:11 MetaFG_0] (main.py 265): INFO Train: [29/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2926 (0.3053)	loss 1.3602 (1.4433)	grad_norm 26.5113 (29.7479)	mem 4879MB
[2022-05-31 02:27:14 MetaFG_0] (main.py 265): INFO Train: [29/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2923 (0.3053)	loss 1.1369 (1.4425)	grad_norm 38.7542 (29.7654)	mem 4879MB
[2022-05-31 02:27:17 MetaFG_0] (main.py 265): INFO Train: [29/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2945 (0.3053)	loss 1.5666 (1.4437)	grad_norm 55.3522 (29.7731)	mem 4879MB
[2022-05-31 02:27:20 MetaFG_0] (main.py 265): INFO Train: [29/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2925 (0.3053)	loss 0.9388 (1.4438)	grad_norm 19.3039 (29.7988)	mem 4879MB
[2022-05-31 02:27:23 MetaFG_0] (main.py 265): INFO Train: [29/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2920 (0.3053)	loss 1.3484 (1.4439)	grad_norm 24.4320 (29.7867)	mem 4879MB
[2022-05-31 02:27:26 MetaFG_0] (main.py 265): INFO Train: [29/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2925 (0.3053)	loss 1.4734 (1.4451)	grad_norm 24.5455 (29.7235)	mem 4879MB
[2022-05-31 02:27:29 MetaFG_0] (main.py 265): INFO Train: [29/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.3008 (0.3053)	loss 1.6208 (1.4452)	grad_norm 34.0740 (29.8187)	mem 4879MB
[2022-05-31 02:27:32 MetaFG_0] (main.py 265): INFO Train: [29/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2987 (0.3053)	loss 1.4375 (1.4450)	grad_norm 38.2576 (29.8087)	mem 4879MB
[2022-05-31 02:27:35 MetaFG_0] (main.py 265): INFO Train: [29/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2923 (0.3053)	loss 1.5366 (1.4459)	grad_norm 34.6496 (29.8486)	mem 4879MB
[2022-05-31 02:27:38 MetaFG_0] (main.py 265): INFO Train: [29/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.3044 (0.3053)	loss 1.5294 (1.4462)	grad_norm 34.1259 (29.8458)	mem 4879MB
[2022-05-31 02:27:41 MetaFG_0] (main.py 265): INFO Train: [29/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2918 (0.3052)	loss 1.7855 (1.4464)	grad_norm 19.6798 (29.8071)	mem 4879MB
[2022-05-31 02:27:44 MetaFG_0] (main.py 265): INFO Train: [29/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2994 (0.3052)	loss 1.6298 (1.4461)	grad_norm 33.4878 (29.8293)	mem 4879MB
[2022-05-31 02:27:47 MetaFG_0] (main.py 265): INFO Train: [29/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2989 (0.3052)	loss 1.5891 (1.4456)	grad_norm 15.8461 (29.7876)	mem 4879MB
[2022-05-31 02:27:50 MetaFG_0] (main.py 265): INFO Train: [29/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2918 (0.3052)	loss 1.4119 (1.4458)	grad_norm 22.0559 (29.7379)	mem 4879MB
[2022-05-31 02:27:53 MetaFG_0] (main.py 265): INFO Train: [29/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2959 (0.3052)	loss 1.7449 (1.4472)	grad_norm 24.3495 (29.7014)	mem 4879MB
[2022-05-31 02:27:56 MetaFG_0] (main.py 265): INFO Train: [29/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2995 (0.3052)	loss 1.7065 (1.4480)	grad_norm 32.2892 (29.6970)	mem 4879MB
[2022-05-31 02:27:59 MetaFG_0] (main.py 265): INFO Train: [29/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.3004 (0.3052)	loss 1.4834 (1.4471)	grad_norm 35.2952 (29.7178)	mem 4879MB
[2022-05-31 02:28:02 MetaFG_0] (main.py 265): INFO Train: [29/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2936 (0.3052)	loss 1.6040 (1.4478)	grad_norm 22.4822 (29.7102)	mem 4879MB
[2022-05-31 02:28:05 MetaFG_0] (main.py 265): INFO Train: [29/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2989 (0.3052)	loss 1.7923 (1.4480)	grad_norm 33.8831 (29.6971)	mem 4879MB
[2022-05-31 02:28:08 MetaFG_0] (main.py 265): INFO Train: [29/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2950 (0.3051)	loss 1.5836 (1.4481)	grad_norm 28.2587 (29.7106)	mem 4879MB
[2022-05-31 02:28:12 MetaFG_0] (main.py 265): INFO Train: [29/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3008 (0.3052)	loss 1.5138 (1.4485)	grad_norm 33.8255 (29.6595)	mem 4879MB
[2022-05-31 02:28:15 MetaFG_0] (main.py 265): INFO Train: [29/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.3013 (0.3052)	loss 1.6166 (1.4495)	grad_norm 32.6836 (29.7084)	mem 4879MB
[2022-05-31 02:28:18 MetaFG_0] (main.py 265): INFO Train: [29/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2932 (0.3051)	loss 1.2382 (1.4494)	grad_norm 31.5665 (29.6988)	mem 4879MB
[2022-05-31 02:28:21 MetaFG_0] (main.py 265): INFO Train: [29/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2919 (0.3051)	loss 1.6044 (1.4492)	grad_norm 28.8709 (29.6729)	mem 4879MB
[2022-05-31 02:28:24 MetaFG_0] (main.py 265): INFO Train: [29/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.3000 (0.3051)	loss 1.2181 (1.4492)	grad_norm 36.3771 (29.6376)	mem 4879MB
[2022-05-31 02:28:27 MetaFG_0] (main.py 265): INFO Train: [29/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2976 (0.3051)	loss 1.5410 (1.4499)	grad_norm 25.1898 (29.6369)	mem 4879MB
[2022-05-31 02:28:30 MetaFG_0] (main.py 265): INFO Train: [29/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.3109 (0.3051)	loss 1.4614 (1.4490)	grad_norm 37.9663 (29.6058)	mem 4879MB
[2022-05-31 02:28:33 MetaFG_0] (main.py 265): INFO Train: [29/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2945 (0.3051)	loss 1.6512 (1.4488)	grad_norm 19.4086 (29.6140)	mem 4879MB
[2022-05-31 02:28:36 MetaFG_0] (main.py 265): INFO Train: [29/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2983 (0.3051)	loss 1.6516 (1.4496)	grad_norm 44.9636 (29.6703)	mem 4879MB
[2022-05-31 02:28:39 MetaFG_0] (main.py 265): INFO Train: [29/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2986 (0.3051)	loss 1.0965 (1.4485)	grad_norm 33.4595 (29.6536)	mem 4879MB
[2022-05-31 02:28:42 MetaFG_0] (main.py 265): INFO Train: [29/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2957 (0.3051)	loss 1.5665 (1.4485)	grad_norm 23.4175 (29.6066)	mem 4879MB
[2022-05-31 02:28:45 MetaFG_0] (main.py 265): INFO Train: [29/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2919 (0.3051)	loss 1.8400 (1.4501)	grad_norm 24.9644 (29.5750)	mem 4879MB
[2022-05-31 02:28:48 MetaFG_0] (main.py 265): INFO Train: [29/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2982 (0.3051)	loss 1.5387 (1.4508)	grad_norm 25.5541 (29.6246)	mem 4879MB
[2022-05-31 02:28:51 MetaFG_0] (main.py 265): INFO Train: [29/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2940 (0.3051)	loss 1.4956 (1.4491)	grad_norm 27.4822 (29.5988)	mem 4879MB
[2022-05-31 02:28:54 MetaFG_0] (main.py 265): INFO Train: [29/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2923 (0.3051)	loss 1.4503 (1.4506)	grad_norm 31.8560 (29.5542)	mem 4879MB
[2022-05-31 02:28:57 MetaFG_0] (main.py 265): INFO Train: [29/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2972 (0.3050)	loss 1.4116 (1.4504)	grad_norm 25.8084 (29.5132)	mem 4879MB
[2022-05-31 02:29:00 MetaFG_0] (main.py 265): INFO Train: [29/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2936 (0.3050)	loss 1.4696 (1.4510)	grad_norm 31.9208 (29.5343)	mem 4879MB
[2022-05-31 02:29:03 MetaFG_0] (main.py 265): INFO Train: [29/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2919 (0.3050)	loss 1.2604 (1.4510)	grad_norm 26.4581 (29.5193)	mem 4879MB
[2022-05-31 02:29:06 MetaFG_0] (main.py 265): INFO Train: [29/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2924 (0.3050)	loss 1.6398 (1.4508)	grad_norm 24.0322 (29.5206)	mem 4879MB
[2022-05-31 02:29:09 MetaFG_0] (main.py 265): INFO Train: [29/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2927 (0.3050)	loss 1.6036 (1.4524)	grad_norm 29.6691 (29.4645)	mem 4879MB
[2022-05-31 02:29:12 MetaFG_0] (main.py 265): INFO Train: [29/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2983 (0.3050)	loss 1.2424 (1.4519)	grad_norm 17.4034 (29.4234)	mem 4879MB
[2022-05-31 02:29:15 MetaFG_0] (main.py 265): INFO Train: [29/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.3022 (0.3050)	loss 1.2542 (1.4510)	grad_norm 21.5283 (29.3606)	mem 4879MB
[2022-05-31 02:29:19 MetaFG_0] (main.py 265): INFO Train: [29/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2921 (0.3050)	loss 1.5095 (1.4516)	grad_norm 39.6441 (29.4305)	mem 4879MB
[2022-05-31 02:29:22 MetaFG_0] (main.py 265): INFO Train: [29/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2980 (0.3050)	loss 1.5532 (1.4522)	grad_norm 24.7347 (29.3657)	mem 4879MB
[2022-05-31 02:29:25 MetaFG_0] (main.py 265): INFO Train: [29/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2928 (0.3050)	loss 1.2133 (1.4523)	grad_norm 40.6953 (29.3401)	mem 4879MB
[2022-05-31 02:29:28 MetaFG_0] (main.py 265): INFO Train: [29/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2920 (0.3050)	loss 1.3899 (1.4523)	grad_norm 49.2356 (29.3789)	mem 4879MB
[2022-05-31 02:29:31 MetaFG_0] (main.py 265): INFO Train: [29/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2933 (0.3050)	loss 1.3626 (1.4515)	grad_norm 65.4056 (29.4111)	mem 4879MB
[2022-05-31 02:29:34 MetaFG_0] (main.py 265): INFO Train: [29/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2945 (0.3050)	loss 1.3039 (1.4510)	grad_norm 36.2909 (29.3958)	mem 4879MB
[2022-05-31 02:29:37 MetaFG_0] (main.py 265): INFO Train: [29/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2923 (0.3050)	loss 1.5794 (1.4521)	grad_norm 26.1090 (29.4025)	mem 4879MB
[2022-05-31 02:29:40 MetaFG_0] (main.py 265): INFO Train: [29/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2991 (0.3050)	loss 1.5836 (1.4521)	grad_norm 36.7414 (29.4045)	mem 4879MB
[2022-05-31 02:29:43 MetaFG_0] (main.py 265): INFO Train: [29/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2985 (0.3050)	loss 1.6790 (1.4515)	grad_norm 26.3578 (29.3875)	mem 4879MB
[2022-05-31 02:29:46 MetaFG_0] (main.py 265): INFO Train: [29/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2920 (0.3050)	loss 1.5622 (1.4514)	grad_norm 19.4648 (29.3916)	mem 4879MB
[2022-05-31 02:29:49 MetaFG_0] (main.py 265): INFO Train: [29/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2938 (0.3050)	loss 1.1006 (1.4503)	grad_norm 42.7638 (29.4049)	mem 4879MB
[2022-05-31 02:29:52 MetaFG_0] (main.py 265): INFO Train: [29/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2925 (0.3050)	loss 1.3297 (1.4498)	grad_norm 28.9097 (29.4406)	mem 4879MB
[2022-05-31 02:29:55 MetaFG_0] (main.py 265): INFO Train: [29/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2924 (0.3050)	loss 1.5561 (1.4494)	grad_norm 36.3896 (29.4688)	mem 4879MB
[2022-05-31 02:29:58 MetaFG_0] (main.py 265): INFO Train: [29/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2999 (0.3050)	loss 1.4317 (1.4490)	grad_norm 19.2998 (29.4737)	mem 4879MB
[2022-05-31 02:30:01 MetaFG_0] (main.py 265): INFO Train: [29/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2936 (0.3050)	loss 1.5604 (1.4490)	grad_norm 24.2507 (29.4733)	mem 4879MB
[2022-05-31 02:30:04 MetaFG_0] (main.py 265): INFO Train: [29/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2937 (0.3050)	loss 1.6656 (1.4489)	grad_norm 32.5450 (29.4786)	mem 4879MB
[2022-05-31 02:30:07 MetaFG_0] (main.py 265): INFO Train: [29/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2926 (0.3050)	loss 0.9400 (1.4483)	grad_norm 87.6382 (29.5463)	mem 4879MB
[2022-05-31 02:30:10 MetaFG_0] (main.py 265): INFO Train: [29/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2918 (0.3050)	loss 1.2822 (1.4489)	grad_norm 31.6262 (29.5472)	mem 4879MB
[2022-05-31 02:30:13 MetaFG_0] (main.py 265): INFO Train: [29/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2983 (0.3049)	loss 1.5273 (1.4493)	grad_norm 39.0175 (29.5502)	mem 4879MB
[2022-05-31 02:30:16 MetaFG_0] (main.py 265): INFO Train: [29/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2980 (0.3049)	loss 1.4909 (1.4491)	grad_norm 22.4638 (29.5328)	mem 4879MB
[2022-05-31 02:30:19 MetaFG_0] (main.py 265): INFO Train: [29/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2983 (0.3049)	loss 1.1717 (1.4490)	grad_norm 61.0021 (29.5427)	mem 4879MB
[2022-05-31 02:30:22 MetaFG_0] (main.py 265): INFO Train: [29/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2922 (0.3049)	loss 1.6526 (1.4496)	grad_norm 14.9430 (29.5229)	mem 4879MB
[2022-05-31 02:30:26 MetaFG_0] (main.py 265): INFO Train: [29/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2993 (0.3049)	loss 1.1787 (1.4489)	grad_norm 16.3915 (29.5061)	mem 4879MB
[2022-05-31 02:30:29 MetaFG_0] (main.py 265): INFO Train: [29/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2923 (0.3049)	loss 1.2809 (1.4486)	grad_norm 22.8827 (29.5311)	mem 4879MB
[2022-05-31 02:30:32 MetaFG_0] (main.py 265): INFO Train: [29/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2986 (0.3049)	loss 1.7778 (1.4492)	grad_norm 26.1258 (29.5045)	mem 4879MB
[2022-05-31 02:30:35 MetaFG_0] (main.py 265): INFO Train: [29/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2922 (0.3049)	loss 1.5500 (1.4498)	grad_norm 41.7739 (29.4924)	mem 4879MB
[2022-05-31 02:30:38 MetaFG_0] (main.py 265): INFO Train: [29/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2935 (0.3049)	loss 1.0680 (1.4488)	grad_norm 36.6944 (29.4984)	mem 4879MB
[2022-05-31 02:30:41 MetaFG_0] (main.py 265): INFO Train: [29/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2924 (0.3049)	loss 1.6001 (1.4492)	grad_norm 31.9881 (29.5555)	mem 4879MB
[2022-05-31 02:30:44 MetaFG_0] (main.py 265): INFO Train: [29/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2940 (0.3049)	loss 1.4725 (1.4490)	grad_norm 24.2231 (29.5402)	mem 4879MB
[2022-05-31 02:30:47 MetaFG_0] (main.py 265): INFO Train: [29/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2936 (0.3049)	loss 1.6195 (1.4487)	grad_norm 33.7992 (29.5672)	mem 4879MB
[2022-05-31 02:30:50 MetaFG_0] (main.py 265): INFO Train: [29/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2928 (0.3049)	loss 1.1965 (1.4483)	grad_norm 23.4986 (29.5410)	mem 4879MB
[2022-05-31 02:30:53 MetaFG_0] (main.py 265): INFO Train: [29/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2921 (0.3049)	loss 1.5342 (1.4475)	grad_norm 23.4361 (29.5870)	mem 4879MB
[2022-05-31 02:30:56 MetaFG_0] (main.py 265): INFO Train: [29/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2935 (0.3049)	loss 1.6189 (1.4476)	grad_norm 29.1625 (29.5657)	mem 4879MB
[2022-05-31 02:30:59 MetaFG_0] (main.py 265): INFO Train: [29/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2982 (0.3049)	loss 1.6917 (1.4475)	grad_norm 24.5698 (29.5589)	mem 4879MB
[2022-05-31 02:31:02 MetaFG_0] (main.py 265): INFO Train: [29/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2981 (0.3049)	loss 1.5576 (1.4477)	grad_norm 16.9236 (29.5710)	mem 4879MB
[2022-05-31 02:31:05 MetaFG_0] (main.py 265): INFO Train: [29/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2986 (0.3049)	loss 1.7482 (1.4468)	grad_norm 18.8419 (29.5412)	mem 4879MB
[2022-05-31 02:31:08 MetaFG_0] (main.py 265): INFO Train: [29/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3029 (0.3049)	loss 1.6912 (1.4465)	grad_norm 19.5978 (29.5373)	mem 4879MB
[2022-05-31 02:31:11 MetaFG_0] (main.py 265): INFO Train: [29/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.3004 (0.3049)	loss 1.1235 (1.4468)	grad_norm 22.4037 (29.5143)	mem 4879MB
[2022-05-31 02:31:14 MetaFG_0] (main.py 265): INFO Train: [29/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2987 (0.3049)	loss 1.6517 (1.4476)	grad_norm 42.4472 (29.5498)	mem 4879MB
[2022-05-31 02:31:17 MetaFG_0] (main.py 265): INFO Train: [29/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3100 (0.3049)	loss 1.5729 (1.4477)	grad_norm 38.8479 (29.5264)	mem 4879MB
[2022-05-31 02:31:20 MetaFG_0] (main.py 265): INFO Train: [29/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2955 (0.3049)	loss 1.5018 (1.4486)	grad_norm 25.3925 (29.5254)	mem 4879MB
[2022-05-31 02:31:23 MetaFG_0] (main.py 265): INFO Train: [29/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2982 (0.3049)	loss 1.6005 (1.4484)	grad_norm 31.4398 (29.5478)	mem 4879MB
[2022-05-31 02:31:26 MetaFG_0] (main.py 265): INFO Train: [29/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3011 (0.3049)	loss 1.4929 (1.4487)	grad_norm 55.7796 (29.5800)	mem 4879MB
[2022-05-31 02:31:30 MetaFG_0] (main.py 265): INFO Train: [29/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2932 (0.3049)	loss 1.4698 (1.4492)	grad_norm 28.3799 (29.5492)	mem 4879MB
[2022-05-31 02:31:33 MetaFG_0] (main.py 265): INFO Train: [29/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2924 (0.3049)	loss 1.2950 (1.4491)	grad_norm 34.0779 (29.5407)	mem 4879MB
[2022-05-31 02:31:36 MetaFG_0] (main.py 265): INFO Train: [29/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2928 (0.3049)	loss 1.0116 (1.4492)	grad_norm 20.9946 (29.5379)	mem 4879MB
[2022-05-31 02:31:39 MetaFG_0] (main.py 265): INFO Train: [29/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2920 (0.3049)	loss 1.6531 (1.4493)	grad_norm 31.5257 (29.5332)	mem 4879MB
[2022-05-31 02:31:39 MetaFG_0] (main.py 272): INFO EPOCH 29 training takes 0:07:56
[2022-05-31 02:31:39 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_29.pth saving......
[2022-05-31 02:31:40 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_29.pth saved !!!
[2022-05-31 02:31:40 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:31:41 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:31:41 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:31:42 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.699 (0.699)	Loss 1.0429 (1.0429)	Acc@1 68.750 (68.750)	Acc@5 93.750 (93.750)	Mem 4879MB
[2022-05-31 02:31:43 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.090 (0.151)	Loss 0.7264 (0.7734)	Acc@1 87.500 (81.534)	Acc@5 93.750 (97.727)	Mem 4879MB
[2022-05-31 02:31:44 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.123)	Loss 0.7685 (0.7395)	Acc@1 84.375 (82.292)	Acc@5 100.000 (97.768)	Mem 4879MB
[2022-05-31 02:31:45 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.114)	Loss 0.6292 (0.7634)	Acc@1 78.125 (82.056)	Acc@5 100.000 (97.984)	Mem 4879MB
[2022-05-31 02:31:46 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.109)	Loss 0.6890 (0.7726)	Acc@1 84.375 (81.479)	Acc@5 96.875 (97.942)	Mem 4879MB
[2022-05-31 02:31:47 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.105)	Loss 0.7207 (0.7754)	Acc@1 90.625 (81.556)	Acc@5 96.875 (97.794)	Mem 4879MB
[2022-05-31 02:31:48 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.097 (0.103)	Loss 0.7553 (0.7751)	Acc@1 84.375 (81.455)	Acc@5 96.875 (97.695)	Mem 4879MB
[2022-05-31 02:31:49 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.6454 (0.7777)	Acc@1 84.375 (81.118)	Acc@5 100.000 (97.667)	Mem 4879MB
[2022-05-31 02:31:49 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 0.7047 (0.7697)	Acc@1 84.375 (81.404)	Acc@5 100.000 (97.840)	Mem 4879MB
[2022-05-31 02:31:50 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.100)	Loss 1.0755 (0.7706)	Acc@1 75.000 (81.353)	Acc@5 93.750 (97.802)	Mem 4879MB
[2022-05-31 02:31:51 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 0.6428 (0.7599)	Acc@1 87.500 (81.714)	Acc@5 96.875 (97.834)	Mem 4879MB
[2022-05-31 02:31:52 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.099)	Loss 0.4649 (0.7573)	Acc@1 93.750 (81.926)	Acc@5 100.000 (97.889)	Mem 4879MB
[2022-05-31 02:31:53 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.8168 (0.7547)	Acc@1 87.500 (81.999)	Acc@5 100.000 (97.856)	Mem 4879MB
[2022-05-31 02:31:54 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.098)	Loss 0.8309 (0.7581)	Acc@1 75.000 (81.918)	Acc@5 100.000 (97.734)	Mem 4879MB
[2022-05-31 02:31:55 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.098)	Loss 0.7495 (0.7588)	Acc@1 78.125 (81.782)	Acc@5 96.875 (97.806)	Mem 4879MB
[2022-05-31 02:31:56 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.089 (0.098)	Loss 0.7390 (0.7580)	Acc@1 84.375 (81.933)	Acc@5 100.000 (97.848)	Mem 4879MB
[2022-05-31 02:31:57 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.097)	Loss 0.6843 (0.7602)	Acc@1 81.250 (81.774)	Acc@5 100.000 (97.904)	Mem 4879MB
[2022-05-31 02:31:58 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.097)	Loss 1.1058 (0.7588)	Acc@1 71.875 (81.871)	Acc@5 90.625 (97.862)	Mem 4879MB
[2022-05-31 02:31:59 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.097)	Loss 0.6764 (0.7610)	Acc@1 84.375 (81.768)	Acc@5 96.875 (97.859)	Mem 4879MB
[2022-05-31 02:32:00 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.092 (0.097)	Loss 0.3763 (0.7605)	Acc@1 93.750 (81.823)	Acc@5 100.000 (97.889)	Mem 4879MB
[2022-05-31 02:32:01 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.103 (0.097)	Loss 0.7022 (0.7620)	Acc@1 81.250 (81.810)	Acc@5 100.000 (97.854)	Mem 4879MB
[2022-05-31 02:32:02 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.7640 (0.7599)	Acc@1 81.250 (81.813)	Acc@5 96.875 (97.927)	Mem 4879MB
[2022-05-31 02:32:03 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.097)	Loss 0.7753 (0.7617)	Acc@1 81.250 (81.816)	Acc@5 100.000 (97.950)	Mem 4879MB
[2022-05-31 02:32:04 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.6942 (0.7608)	Acc@1 81.250 (81.791)	Acc@5 100.000 (97.984)	Mem 4879MB
[2022-05-31 02:32:05 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.8889 (0.7582)	Acc@1 78.125 (81.872)	Acc@5 96.875 (98.029)	Mem 4879MB
[2022-05-31 02:32:05 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 1.0545 (0.7589)	Acc@1 71.875 (81.860)	Acc@5 96.875 (98.033)	Mem 4879MB
[2022-05-31 02:32:06 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.088 (0.096)	Loss 0.5965 (0.7551)	Acc@1 93.750 (82.088)	Acc@5 100.000 (98.060)	Mem 4879MB
[2022-05-31 02:32:07 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 0.6809 (0.7533)	Acc@1 81.250 (82.069)	Acc@5 100.000 (98.086)	Mem 4879MB
[2022-05-31 02:32:08 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.6064 (0.7521)	Acc@1 87.500 (82.195)	Acc@5 100.000 (98.065)	Mem 4879MB
[2022-05-31 02:32:09 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.090 (0.096)	Loss 0.6692 (0.7517)	Acc@1 87.500 (82.227)	Acc@5 100.000 (98.046)	Mem 4879MB
[2022-05-31 02:32:10 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.088 (0.096)	Loss 0.6165 (0.7513)	Acc@1 87.500 (82.309)	Acc@5 96.875 (98.038)	Mem 4879MB
[2022-05-31 02:32:11 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5457 (0.7512)	Acc@1 93.750 (82.365)	Acc@5 100.000 (98.041)	Mem 4879MB
[2022-05-31 02:32:11 MetaFG_0] (main.py 330): INFO  * Acc@1 82.360 Acc@5 98.030
[2022-05-31 02:32:11 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 82.4%
[2022-05-31 02:32:11 MetaFG_0] (main.py 171): INFO Max accuracy: 82.36%
[2022-05-31 02:32:12 MetaFG_0] (main.py 265): INFO Train: [30/300][0/1562]	eta 0:29:03 lr 0.000006	time 1.1165 (1.1165)	loss 1.2654 (1.2654)	grad_norm 16.2633 (16.2633)	mem 4879MB
[2022-05-31 02:32:15 MetaFG_0] (main.py 265): INFO Train: [30/300][10/1562]	eta 0:09:47 lr 0.000006	time 0.3008 (0.3788)	loss 1.6450 (1.4616)	grad_norm 27.7656 (32.0335)	mem 4879MB
[2022-05-31 02:32:19 MetaFG_0] (main.py 265): INFO Train: [30/300][20/1562]	eta 0:08:49 lr 0.000006	time 0.2987 (0.3434)	loss 1.5879 (1.4237)	grad_norm 21.8491 (32.3761)	mem 4879MB
[2022-05-31 02:32:22 MetaFG_0] (main.py 265): INFO Train: [30/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2935 (0.3301)	loss 1.4321 (1.4678)	grad_norm 33.5359 (inf)	mem 4879MB
[2022-05-31 02:32:25 MetaFG_0] (main.py 265): INFO Train: [30/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2934 (0.3238)	loss 1.7835 (1.4382)	grad_norm 37.1615 (inf)	mem 4879MB
[2022-05-31 02:32:28 MetaFG_0] (main.py 265): INFO Train: [30/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2981 (0.3201)	loss 1.2561 (1.4226)	grad_norm 64.9822 (inf)	mem 4879MB
[2022-05-31 02:32:31 MetaFG_0] (main.py 265): INFO Train: [30/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2920 (0.3177)	loss 1.7123 (1.4134)	grad_norm 30.6811 (inf)	mem 4879MB
[2022-05-31 02:32:34 MetaFG_0] (main.py 265): INFO Train: [30/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2934 (0.3160)	loss 1.3504 (1.4238)	grad_norm 25.7903 (inf)	mem 4879MB
[2022-05-31 02:32:37 MetaFG_0] (main.py 265): INFO Train: [30/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2936 (0.3147)	loss 1.5197 (1.4450)	grad_norm 23.8742 (inf)	mem 4879MB
[2022-05-31 02:32:40 MetaFG_0] (main.py 265): INFO Train: [30/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2995 (0.3135)	loss 1.0798 (1.4497)	grad_norm 56.7558 (inf)	mem 4879MB
[2022-05-31 02:32:43 MetaFG_0] (main.py 265): INFO Train: [30/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2991 (0.3126)	loss 1.7408 (1.4587)	grad_norm 26.3961 (inf)	mem 4879MB
[2022-05-31 02:32:46 MetaFG_0] (main.py 265): INFO Train: [30/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.3068 (0.3120)	loss 1.3959 (1.4567)	grad_norm 33.8695 (inf)	mem 4879MB
[2022-05-31 02:32:49 MetaFG_0] (main.py 265): INFO Train: [30/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.3009 (0.3113)	loss 1.0983 (1.4495)	grad_norm 23.3381 (inf)	mem 4879MB
[2022-05-31 02:32:52 MetaFG_0] (main.py 265): INFO Train: [30/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2982 (0.3110)	loss 1.7286 (1.4418)	grad_norm 26.9818 (inf)	mem 4879MB
[2022-05-31 02:32:55 MetaFG_0] (main.py 265): INFO Train: [30/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2925 (0.3105)	loss 1.6029 (1.4424)	grad_norm 21.9830 (inf)	mem 4879MB
[2022-05-31 02:32:58 MetaFG_0] (main.py 265): INFO Train: [30/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2981 (0.3100)	loss 1.6808 (1.4518)	grad_norm 31.5869 (inf)	mem 4879MB
[2022-05-31 02:33:01 MetaFG_0] (main.py 265): INFO Train: [30/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2940 (0.3096)	loss 1.5767 (1.4539)	grad_norm 39.2760 (inf)	mem 4879MB
[2022-05-31 02:33:04 MetaFG_0] (main.py 265): INFO Train: [30/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2943 (0.3094)	loss 0.9835 (1.4489)	grad_norm 24.8086 (inf)	mem 4879MB
[2022-05-31 02:33:07 MetaFG_0] (main.py 265): INFO Train: [30/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2991 (0.3090)	loss 1.8756 (1.4587)	grad_norm 84.5488 (inf)	mem 4879MB
[2022-05-31 02:33:10 MetaFG_0] (main.py 265): INFO Train: [30/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2936 (0.3087)	loss 1.5247 (1.4592)	grad_norm 26.5601 (inf)	mem 4879MB
[2022-05-31 02:33:13 MetaFG_0] (main.py 265): INFO Train: [30/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2940 (0.3085)	loss 1.4154 (1.4521)	grad_norm 23.8860 (inf)	mem 4879MB
[2022-05-31 02:33:16 MetaFG_0] (main.py 265): INFO Train: [30/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2920 (0.3082)	loss 1.7374 (1.4492)	grad_norm 21.4055 (inf)	mem 4879MB
[2022-05-31 02:33:19 MetaFG_0] (main.py 265): INFO Train: [30/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2921 (0.3080)	loss 1.3485 (1.4498)	grad_norm 47.4035 (inf)	mem 4879MB
[2022-05-31 02:33:22 MetaFG_0] (main.py 265): INFO Train: [30/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2940 (0.3078)	loss 1.4161 (1.4507)	grad_norm 21.0282 (inf)	mem 4879MB
[2022-05-31 02:33:25 MetaFG_0] (main.py 265): INFO Train: [30/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.3009 (0.3077)	loss 1.3787 (1.4522)	grad_norm 18.5390 (inf)	mem 4879MB
[2022-05-31 02:33:29 MetaFG_0] (main.py 265): INFO Train: [30/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2922 (0.3075)	loss 1.5732 (1.4518)	grad_norm 29.1261 (inf)	mem 4879MB
[2022-05-31 02:33:32 MetaFG_0] (main.py 265): INFO Train: [30/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2942 (0.3074)	loss 1.1276 (1.4515)	grad_norm 55.0583 (inf)	mem 4879MB
[2022-05-31 02:33:35 MetaFG_0] (main.py 265): INFO Train: [30/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2925 (0.3073)	loss 1.4268 (1.4501)	grad_norm 49.2027 (inf)	mem 4879MB
[2022-05-31 02:33:38 MetaFG_0] (main.py 265): INFO Train: [30/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2993 (0.3072)	loss 1.5532 (1.4521)	grad_norm 27.7421 (inf)	mem 4879MB
[2022-05-31 02:33:41 MetaFG_0] (main.py 265): INFO Train: [30/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2939 (0.3071)	loss 1.2527 (1.4486)	grad_norm 30.5426 (inf)	mem 4879MB
[2022-05-31 02:33:44 MetaFG_0] (main.py 265): INFO Train: [30/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2937 (0.3071)	loss 1.4532 (1.4511)	grad_norm 23.2631 (inf)	mem 4879MB
[2022-05-31 02:33:47 MetaFG_0] (main.py 265): INFO Train: [30/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2924 (0.3070)	loss 1.5942 (1.4508)	grad_norm 16.3530 (inf)	mem 4879MB
[2022-05-31 02:33:50 MetaFG_0] (main.py 265): INFO Train: [30/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2937 (0.3070)	loss 1.6568 (1.4530)	grad_norm 20.3791 (inf)	mem 4879MB
[2022-05-31 02:33:53 MetaFG_0] (main.py 265): INFO Train: [30/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2942 (0.3069)	loss 1.0123 (1.4490)	grad_norm 30.4342 (inf)	mem 4879MB
[2022-05-31 02:33:56 MetaFG_0] (main.py 265): INFO Train: [30/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2931 (0.3068)	loss 1.2143 (1.4444)	grad_norm 38.7574 (inf)	mem 4879MB
[2022-05-31 02:33:59 MetaFG_0] (main.py 265): INFO Train: [30/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2964 (0.3067)	loss 1.4799 (1.4461)	grad_norm 22.8433 (inf)	mem 4879MB
[2022-05-31 02:34:02 MetaFG_0] (main.py 265): INFO Train: [30/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2919 (0.3066)	loss 1.7147 (1.4484)	grad_norm 23.2778 (inf)	mem 4879MB
[2022-05-31 02:34:05 MetaFG_0] (main.py 265): INFO Train: [30/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2999 (0.3066)	loss 1.4935 (1.4508)	grad_norm 25.6817 (inf)	mem 4879MB
[2022-05-31 02:34:08 MetaFG_0] (main.py 265): INFO Train: [30/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.3003 (0.3066)	loss 1.5266 (1.4505)	grad_norm 21.4941 (inf)	mem 4879MB
[2022-05-31 02:34:11 MetaFG_0] (main.py 265): INFO Train: [30/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2954 (0.3065)	loss 1.4577 (1.4496)	grad_norm 36.4777 (inf)	mem 4879MB
[2022-05-31 02:34:14 MetaFG_0] (main.py 265): INFO Train: [30/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2930 (0.3065)	loss 1.2343 (1.4490)	grad_norm 16.8560 (inf)	mem 4879MB
[2022-05-31 02:34:17 MetaFG_0] (main.py 265): INFO Train: [30/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2946 (0.3064)	loss 1.4412 (1.4501)	grad_norm 18.3233 (inf)	mem 4879MB
[2022-05-31 02:34:20 MetaFG_0] (main.py 265): INFO Train: [30/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2919 (0.3064)	loss 1.3792 (1.4516)	grad_norm 21.9259 (inf)	mem 4879MB
[2022-05-31 02:34:23 MetaFG_0] (main.py 265): INFO Train: [30/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2967 (0.3063)	loss 1.6039 (1.4531)	grad_norm 26.5810 (inf)	mem 4879MB
[2022-05-31 02:34:26 MetaFG_0] (main.py 265): INFO Train: [30/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2943 (0.3062)	loss 1.0901 (1.4511)	grad_norm 29.0847 (inf)	mem 4879MB
[2022-05-31 02:34:29 MetaFG_0] (main.py 265): INFO Train: [30/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2993 (0.3062)	loss 1.7351 (1.4529)	grad_norm 28.4194 (inf)	mem 4879MB
[2022-05-31 02:34:32 MetaFG_0] (main.py 265): INFO Train: [30/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2992 (0.3062)	loss 1.1328 (1.4515)	grad_norm 31.8718 (inf)	mem 4879MB
[2022-05-31 02:34:35 MetaFG_0] (main.py 265): INFO Train: [30/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2927 (0.3061)	loss 1.1880 (1.4529)	grad_norm 40.9739 (inf)	mem 4879MB
[2022-05-31 02:34:39 MetaFG_0] (main.py 265): INFO Train: [30/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2971 (0.3060)	loss 1.6075 (1.4522)	grad_norm 41.9139 (inf)	mem 4879MB
[2022-05-31 02:34:42 MetaFG_0] (main.py 265): INFO Train: [30/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2995 (0.3060)	loss 1.6541 (1.4532)	grad_norm 58.1757 (inf)	mem 4879MB
[2022-05-31 02:34:45 MetaFG_0] (main.py 265): INFO Train: [30/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2924 (0.3060)	loss 1.5215 (1.4541)	grad_norm 25.1936 (inf)	mem 4879MB
[2022-05-31 02:34:48 MetaFG_0] (main.py 265): INFO Train: [30/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2932 (0.3059)	loss 1.4095 (1.4531)	grad_norm 39.7323 (inf)	mem 4879MB
[2022-05-31 02:34:51 MetaFG_0] (main.py 265): INFO Train: [30/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2957 (0.3059)	loss 1.2312 (1.4532)	grad_norm 28.2206 (inf)	mem 4879MB
[2022-05-31 02:34:54 MetaFG_0] (main.py 265): INFO Train: [30/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2929 (0.3059)	loss 1.3627 (1.4532)	grad_norm 44.2098 (inf)	mem 4879MB
[2022-05-31 02:34:57 MetaFG_0] (main.py 265): INFO Train: [30/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2995 (0.3059)	loss 1.6504 (1.4548)	grad_norm 50.0696 (inf)	mem 4879MB
[2022-05-31 02:35:00 MetaFG_0] (main.py 265): INFO Train: [30/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2933 (0.3059)	loss 1.1382 (1.4535)	grad_norm 19.7214 (inf)	mem 4879MB
[2022-05-31 02:35:03 MetaFG_0] (main.py 265): INFO Train: [30/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2923 (0.3059)	loss 1.6327 (1.4546)	grad_norm 24.9319 (inf)	mem 4879MB
[2022-05-31 02:35:06 MetaFG_0] (main.py 265): INFO Train: [30/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2931 (0.3058)	loss 1.2020 (1.4541)	grad_norm 64.7459 (inf)	mem 4879MB
[2022-05-31 02:35:09 MetaFG_0] (main.py 265): INFO Train: [30/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2943 (0.3058)	loss 1.1661 (1.4560)	grad_norm 63.0098 (inf)	mem 4879MB
[2022-05-31 02:35:12 MetaFG_0] (main.py 265): INFO Train: [30/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2986 (0.3058)	loss 1.2582 (1.4564)	grad_norm 57.0142 (inf)	mem 4879MB
[2022-05-31 02:35:15 MetaFG_0] (main.py 265): INFO Train: [30/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2935 (0.3058)	loss 1.5216 (1.4547)	grad_norm 19.2054 (inf)	mem 4879MB
[2022-05-31 02:35:18 MetaFG_0] (main.py 265): INFO Train: [30/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2984 (0.3058)	loss 1.3842 (1.4556)	grad_norm 17.9011 (inf)	mem 4879MB
[2022-05-31 02:35:21 MetaFG_0] (main.py 265): INFO Train: [30/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2961 (0.3058)	loss 1.4193 (1.4567)	grad_norm 24.7701 (inf)	mem 4879MB
[2022-05-31 02:35:24 MetaFG_0] (main.py 265): INFO Train: [30/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2930 (0.3058)	loss 1.1256 (1.4563)	grad_norm 27.0790 (inf)	mem 4879MB
[2022-05-31 02:35:27 MetaFG_0] (main.py 265): INFO Train: [30/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2995 (0.3058)	loss 1.8147 (1.4561)	grad_norm 29.9238 (inf)	mem 4879MB
[2022-05-31 02:35:30 MetaFG_0] (main.py 265): INFO Train: [30/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2995 (0.3058)	loss 1.1084 (1.4551)	grad_norm 39.4058 (inf)	mem 4879MB
[2022-05-31 02:35:33 MetaFG_0] (main.py 265): INFO Train: [30/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2944 (0.3058)	loss 1.7241 (1.4552)	grad_norm 60.8699 (inf)	mem 4879MB
[2022-05-31 02:35:37 MetaFG_0] (main.py 265): INFO Train: [30/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2992 (0.3058)	loss 1.5227 (1.4563)	grad_norm 29.8378 (inf)	mem 4879MB
[2022-05-31 02:35:40 MetaFG_0] (main.py 265): INFO Train: [30/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2987 (0.3058)	loss 1.4026 (1.4574)	grad_norm 31.1859 (inf)	mem 4879MB
[2022-05-31 02:35:43 MetaFG_0] (main.py 265): INFO Train: [30/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2923 (0.3057)	loss 1.6313 (1.4570)	grad_norm 27.5785 (inf)	mem 4879MB
[2022-05-31 02:35:46 MetaFG_0] (main.py 265): INFO Train: [30/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.3001 (0.3057)	loss 0.9920 (1.4569)	grad_norm 30.8895 (inf)	mem 4879MB
[2022-05-31 02:35:49 MetaFG_0] (main.py 265): INFO Train: [30/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2929 (0.3057)	loss 1.3179 (1.4560)	grad_norm 59.0949 (inf)	mem 4879MB
[2022-05-31 02:35:52 MetaFG_0] (main.py 265): INFO Train: [30/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2934 (0.3057)	loss 1.4551 (1.4556)	grad_norm 28.9788 (inf)	mem 4879MB
[2022-05-31 02:35:55 MetaFG_0] (main.py 265): INFO Train: [30/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2929 (0.3057)	loss 1.0912 (1.4550)	grad_norm 76.2085 (inf)	mem 4879MB
[2022-05-31 02:35:58 MetaFG_0] (main.py 265): INFO Train: [30/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2925 (0.3057)	loss 1.3119 (1.4560)	grad_norm 24.0566 (inf)	mem 4879MB
[2022-05-31 02:36:01 MetaFG_0] (main.py 265): INFO Train: [30/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2946 (0.3056)	loss 1.5661 (1.4565)	grad_norm 40.5863 (inf)	mem 4879MB
[2022-05-31 02:36:04 MetaFG_0] (main.py 265): INFO Train: [30/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2924 (0.3056)	loss 1.7837 (1.4569)	grad_norm 34.2279 (inf)	mem 4879MB
[2022-05-31 02:36:07 MetaFG_0] (main.py 265): INFO Train: [30/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2990 (0.3057)	loss 1.5348 (1.4565)	grad_norm 34.3211 (inf)	mem 4879MB
[2022-05-31 02:36:10 MetaFG_0] (main.py 265): INFO Train: [30/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2922 (0.3057)	loss 1.6520 (1.4573)	grad_norm 29.9649 (inf)	mem 4879MB
[2022-05-31 02:36:13 MetaFG_0] (main.py 265): INFO Train: [30/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2941 (0.3056)	loss 1.1015 (1.4572)	grad_norm 18.2314 (inf)	mem 4879MB
[2022-05-31 02:36:16 MetaFG_0] (main.py 265): INFO Train: [30/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2941 (0.3056)	loss 1.6144 (1.4580)	grad_norm 27.2111 (inf)	mem 4879MB
[2022-05-31 02:36:19 MetaFG_0] (main.py 265): INFO Train: [30/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2922 (0.3056)	loss 1.3888 (1.4574)	grad_norm 31.4942 (inf)	mem 4879MB
[2022-05-31 02:36:22 MetaFG_0] (main.py 265): INFO Train: [30/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2917 (0.3056)	loss 1.4169 (1.4570)	grad_norm 22.6847 (inf)	mem 4879MB
[2022-05-31 02:36:25 MetaFG_0] (main.py 265): INFO Train: [30/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2948 (0.3056)	loss 1.3114 (1.4575)	grad_norm 25.6596 (inf)	mem 4879MB
[2022-05-31 02:36:28 MetaFG_0] (main.py 265): INFO Train: [30/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2926 (0.3056)	loss 1.7292 (1.4581)	grad_norm 29.2310 (inf)	mem 4879MB
[2022-05-31 02:36:31 MetaFG_0] (main.py 265): INFO Train: [30/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2926 (0.3056)	loss 1.6550 (1.4584)	grad_norm 34.4867 (inf)	mem 4879MB
[2022-05-31 02:36:34 MetaFG_0] (main.py 265): INFO Train: [30/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2961 (0.3056)	loss 1.5858 (1.4589)	grad_norm 16.1530 (inf)	mem 4879MB
[2022-05-31 02:36:37 MetaFG_0] (main.py 265): INFO Train: [30/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2990 (0.3056)	loss 1.5795 (1.4586)	grad_norm 31.9149 (inf)	mem 4879MB
[2022-05-31 02:36:41 MetaFG_0] (main.py 265): INFO Train: [30/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2994 (0.3056)	loss 1.6483 (1.4584)	grad_norm 19.4666 (inf)	mem 4879MB
[2022-05-31 02:36:44 MetaFG_0] (main.py 265): INFO Train: [30/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2947 (0.3056)	loss 1.7348 (1.4597)	grad_norm 20.2635 (inf)	mem 4879MB
[2022-05-31 02:36:47 MetaFG_0] (main.py 265): INFO Train: [30/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2998 (0.3056)	loss 1.4261 (1.4603)	grad_norm 21.4464 (inf)	mem 4879MB
[2022-05-31 02:36:50 MetaFG_0] (main.py 265): INFO Train: [30/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2939 (0.3056)	loss 1.6605 (1.4606)	grad_norm 25.5165 (inf)	mem 4879MB
[2022-05-31 02:36:53 MetaFG_0] (main.py 265): INFO Train: [30/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2929 (0.3056)	loss 1.0610 (1.4605)	grad_norm 35.5106 (inf)	mem 4879MB
[2022-05-31 02:36:56 MetaFG_0] (main.py 265): INFO Train: [30/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3004 (0.3056)	loss 1.6453 (1.4620)	grad_norm 28.2102 (inf)	mem 4879MB
[2022-05-31 02:36:59 MetaFG_0] (main.py 265): INFO Train: [30/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2944 (0.3056)	loss 1.3052 (1.4621)	grad_norm 18.1139 (inf)	mem 4879MB
[2022-05-31 02:37:02 MetaFG_0] (main.py 265): INFO Train: [30/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2928 (0.3055)	loss 1.3626 (1.4616)	grad_norm 16.5543 (inf)	mem 4879MB
[2022-05-31 02:37:05 MetaFG_0] (main.py 265): INFO Train: [30/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2936 (0.3055)	loss 1.5432 (1.4613)	grad_norm 53.7903 (inf)	mem 4879MB
[2022-05-31 02:37:08 MetaFG_0] (main.py 265): INFO Train: [30/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2978 (0.3055)	loss 1.5940 (1.4615)	grad_norm 27.3200 (inf)	mem 4879MB
[2022-05-31 02:37:11 MetaFG_0] (main.py 265): INFO Train: [30/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3002 (0.3055)	loss 1.2038 (1.4607)	grad_norm 28.7744 (inf)	mem 4879MB
[2022-05-31 02:37:14 MetaFG_0] (main.py 265): INFO Train: [30/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2988 (0.3055)	loss 1.6187 (1.4605)	grad_norm 20.3609 (inf)	mem 4879MB
[2022-05-31 02:37:17 MetaFG_0] (main.py 265): INFO Train: [30/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2933 (0.3055)	loss 1.4219 (1.4602)	grad_norm 21.1161 (inf)	mem 4879MB
[2022-05-31 02:37:20 MetaFG_0] (main.py 265): INFO Train: [30/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2956 (0.3055)	loss 1.7153 (1.4607)	grad_norm 28.4976 (inf)	mem 4879MB
[2022-05-31 02:37:23 MetaFG_0] (main.py 265): INFO Train: [30/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3010 (0.3055)	loss 1.8151 (1.4613)	grad_norm 30.7949 (inf)	mem 4879MB
[2022-05-31 02:37:26 MetaFG_0] (main.py 265): INFO Train: [30/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2935 (0.3055)	loss 1.3834 (1.4599)	grad_norm 25.1768 (inf)	mem 4879MB
[2022-05-31 02:37:29 MetaFG_0] (main.py 265): INFO Train: [30/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2926 (0.3055)	loss 1.4316 (1.4606)	grad_norm 37.9881 (inf)	mem 4879MB
[2022-05-31 02:37:32 MetaFG_0] (main.py 265): INFO Train: [30/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2925 (0.3055)	loss 1.3479 (1.4597)	grad_norm 23.1553 (inf)	mem 4879MB
[2022-05-31 02:37:35 MetaFG_0] (main.py 265): INFO Train: [30/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2922 (0.3055)	loss 1.0260 (1.4600)	grad_norm 54.2037 (inf)	mem 4879MB
[2022-05-31 02:37:38 MetaFG_0] (main.py 265): INFO Train: [30/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2944 (0.3055)	loss 1.6558 (1.4594)	grad_norm 39.8306 (inf)	mem 4879MB
[2022-05-31 02:37:42 MetaFG_0] (main.py 265): INFO Train: [30/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2920 (0.3055)	loss 1.3700 (1.4598)	grad_norm 27.9282 (inf)	mem 4879MB
[2022-05-31 02:37:45 MetaFG_0] (main.py 265): INFO Train: [30/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2939 (0.3055)	loss 1.5143 (1.4593)	grad_norm 25.9978 (inf)	mem 4879MB
[2022-05-31 02:37:48 MetaFG_0] (main.py 265): INFO Train: [30/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.3016 (0.3055)	loss 1.2992 (1.4591)	grad_norm 14.6718 (inf)	mem 4879MB
[2022-05-31 02:37:51 MetaFG_0] (main.py 265): INFO Train: [30/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2944 (0.3055)	loss 0.9964 (1.4574)	grad_norm 32.6478 (inf)	mem 4879MB
[2022-05-31 02:37:54 MetaFG_0] (main.py 265): INFO Train: [30/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2945 (0.3055)	loss 1.3696 (1.4570)	grad_norm 23.6203 (inf)	mem 4879MB
[2022-05-31 02:37:57 MetaFG_0] (main.py 265): INFO Train: [30/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2933 (0.3054)	loss 1.3738 (1.4570)	grad_norm 19.7581 (inf)	mem 4879MB
[2022-05-31 02:38:00 MetaFG_0] (main.py 265): INFO Train: [30/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2999 (0.3054)	loss 1.7642 (1.4574)	grad_norm 41.8217 (inf)	mem 4879MB
[2022-05-31 02:38:03 MetaFG_0] (main.py 265): INFO Train: [30/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2923 (0.3054)	loss 1.6162 (1.4582)	grad_norm 16.4448 (inf)	mem 4879MB
[2022-05-31 02:38:06 MetaFG_0] (main.py 265): INFO Train: [30/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2937 (0.3054)	loss 1.0349 (1.4584)	grad_norm 26.1804 (inf)	mem 4879MB
[2022-05-31 02:38:09 MetaFG_0] (main.py 265): INFO Train: [30/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2940 (0.3054)	loss 1.3293 (1.4583)	grad_norm 34.0405 (inf)	mem 4879MB
[2022-05-31 02:38:12 MetaFG_0] (main.py 265): INFO Train: [30/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2985 (0.3054)	loss 1.1350 (1.4590)	grad_norm 37.3783 (inf)	mem 4879MB
[2022-05-31 02:38:15 MetaFG_0] (main.py 265): INFO Train: [30/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2945 (0.3054)	loss 1.3828 (1.4599)	grad_norm 36.5344 (inf)	mem 4879MB
[2022-05-31 02:38:18 MetaFG_0] (main.py 265): INFO Train: [30/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2926 (0.3054)	loss 1.4231 (1.4598)	grad_norm 22.7073 (inf)	mem 4879MB
[2022-05-31 02:38:21 MetaFG_0] (main.py 265): INFO Train: [30/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2979 (0.3054)	loss 1.7078 (1.4595)	grad_norm 37.0806 (inf)	mem 4879MB
[2022-05-31 02:38:24 MetaFG_0] (main.py 265): INFO Train: [30/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2940 (0.3054)	loss 1.3734 (1.4591)	grad_norm 30.9723 (inf)	mem 4879MB
[2022-05-31 02:38:27 MetaFG_0] (main.py 265): INFO Train: [30/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2990 (0.3053)	loss 0.9792 (1.4583)	grad_norm 16.7268 (inf)	mem 4879MB
[2022-05-31 02:38:30 MetaFG_0] (main.py 265): INFO Train: [30/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2986 (0.3053)	loss 1.4855 (1.4570)	grad_norm 20.4930 (inf)	mem 4879MB
[2022-05-31 02:38:33 MetaFG_0] (main.py 265): INFO Train: [30/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2931 (0.3053)	loss 1.0672 (1.4570)	grad_norm 49.9932 (inf)	mem 4879MB
[2022-05-31 02:38:36 MetaFG_0] (main.py 265): INFO Train: [30/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2975 (0.3053)	loss 1.4796 (1.4567)	grad_norm 27.3454 (inf)	mem 4879MB
[2022-05-31 02:38:39 MetaFG_0] (main.py 265): INFO Train: [30/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2987 (0.3053)	loss 1.1906 (1.4568)	grad_norm 27.0734 (inf)	mem 4879MB
[2022-05-31 02:38:42 MetaFG_0] (main.py 265): INFO Train: [30/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2993 (0.3053)	loss 1.2475 (1.4558)	grad_norm 19.3773 (inf)	mem 4879MB
[2022-05-31 02:38:45 MetaFG_0] (main.py 265): INFO Train: [30/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2999 (0.3053)	loss 1.6786 (1.4558)	grad_norm 40.7565 (inf)	mem 4879MB
[2022-05-31 02:38:49 MetaFG_0] (main.py 265): INFO Train: [30/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2939 (0.3053)	loss 1.3948 (1.4564)	grad_norm 17.0044 (inf)	mem 4879MB
[2022-05-31 02:38:52 MetaFG_0] (main.py 265): INFO Train: [30/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.3244 (0.3055)	loss 1.2456 (1.4561)	grad_norm 19.7382 (inf)	mem 4879MB
[2022-05-31 02:38:55 MetaFG_0] (main.py 265): INFO Train: [30/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2933 (0.3056)	loss 1.6666 (1.4558)	grad_norm 61.4110 (inf)	mem 4879MB
[2022-05-31 02:38:58 MetaFG_0] (main.py 265): INFO Train: [30/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2924 (0.3056)	loss 0.8884 (1.4556)	grad_norm 19.2716 (inf)	mem 4879MB
[2022-05-31 02:39:01 MetaFG_0] (main.py 265): INFO Train: [30/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2996 (0.3056)	loss 1.1047 (1.4549)	grad_norm 30.4075 (inf)	mem 4879MB
[2022-05-31 02:39:04 MetaFG_0] (main.py 265): INFO Train: [30/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2987 (0.3056)	loss 1.5210 (1.4554)	grad_norm 27.6907 (inf)	mem 4879MB
[2022-05-31 02:39:07 MetaFG_0] (main.py 265): INFO Train: [30/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2929 (0.3056)	loss 1.3940 (1.4554)	grad_norm 21.5558 (inf)	mem 4879MB
[2022-05-31 02:39:10 MetaFG_0] (main.py 265): INFO Train: [30/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2929 (0.3056)	loss 1.5823 (1.4557)	grad_norm 20.7827 (inf)	mem 4879MB
[2022-05-31 02:39:13 MetaFG_0] (main.py 265): INFO Train: [30/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2922 (0.3056)	loss 1.2892 (1.4565)	grad_norm 26.7876 (inf)	mem 4879MB
[2022-05-31 02:39:16 MetaFG_0] (main.py 265): INFO Train: [30/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2934 (0.3056)	loss 1.2356 (1.4567)	grad_norm 25.4638 (inf)	mem 4879MB
[2022-05-31 02:39:19 MetaFG_0] (main.py 265): INFO Train: [30/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2922 (0.3056)	loss 1.2565 (1.4557)	grad_norm 45.5179 (inf)	mem 4879MB
[2022-05-31 02:39:22 MetaFG_0] (main.py 265): INFO Train: [30/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3019 (0.3056)	loss 1.3241 (1.4553)	grad_norm 46.9331 (inf)	mem 4879MB
[2022-05-31 02:39:26 MetaFG_0] (main.py 265): INFO Train: [30/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2995 (0.3056)	loss 1.7185 (1.4560)	grad_norm 24.1920 (inf)	mem 4879MB
[2022-05-31 02:39:29 MetaFG_0] (main.py 265): INFO Train: [30/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2929 (0.3056)	loss 1.4077 (1.4568)	grad_norm 19.8580 (inf)	mem 4879MB
[2022-05-31 02:39:32 MetaFG_0] (main.py 265): INFO Train: [30/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2948 (0.3056)	loss 1.5843 (1.4567)	grad_norm 20.4425 (inf)	mem 4879MB
[2022-05-31 02:39:35 MetaFG_0] (main.py 265): INFO Train: [30/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2925 (0.3056)	loss 1.7173 (1.4561)	grad_norm 35.0518 (inf)	mem 4879MB
[2022-05-31 02:39:38 MetaFG_0] (main.py 265): INFO Train: [30/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2986 (0.3056)	loss 1.0606 (1.4547)	grad_norm 24.1168 (inf)	mem 4879MB
[2022-05-31 02:39:41 MetaFG_0] (main.py 265): INFO Train: [30/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2982 (0.3056)	loss 1.4738 (1.4549)	grad_norm 36.2114 (inf)	mem 4879MB
[2022-05-31 02:39:44 MetaFG_0] (main.py 265): INFO Train: [30/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2923 (0.3056)	loss 1.6180 (1.4557)	grad_norm 19.3253 (inf)	mem 4879MB
[2022-05-31 02:39:47 MetaFG_0] (main.py 265): INFO Train: [30/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.3032 (0.3056)	loss 1.1824 (1.4550)	grad_norm 16.6740 (inf)	mem 4879MB
[2022-05-31 02:39:50 MetaFG_0] (main.py 265): INFO Train: [30/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2990 (0.3056)	loss 1.4363 (1.4554)	grad_norm 16.8460 (inf)	mem 4879MB
[2022-05-31 02:39:53 MetaFG_0] (main.py 265): INFO Train: [30/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2993 (0.3056)	loss 1.5058 (1.4558)	grad_norm 19.2514 (inf)	mem 4879MB
[2022-05-31 02:39:56 MetaFG_0] (main.py 265): INFO Train: [30/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2988 (0.3056)	loss 1.2988 (1.4562)	grad_norm 37.6119 (inf)	mem 4879MB
[2022-05-31 02:39:59 MetaFG_0] (main.py 265): INFO Train: [30/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2991 (0.3056)	loss 1.3157 (1.4565)	grad_norm 20.8045 (inf)	mem 4879MB
[2022-05-31 02:40:02 MetaFG_0] (main.py 265): INFO Train: [30/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2981 (0.3056)	loss 1.6955 (1.4574)	grad_norm 19.3578 (inf)	mem 4879MB
[2022-05-31 02:40:05 MetaFG_0] (main.py 265): INFO Train: [30/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2952 (0.3055)	loss 1.8272 (1.4574)	grad_norm 36.4674 (inf)	mem 4879MB
[2022-05-31 02:40:08 MetaFG_0] (main.py 265): INFO Train: [30/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2925 (0.3055)	loss 1.8112 (1.4577)	grad_norm 39.0567 (inf)	mem 4879MB
[2022-05-31 02:40:09 MetaFG_0] (main.py 272): INFO EPOCH 30 training takes 0:07:57
[2022-05-31 02:40:09 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_30.pth saving......
[2022-05-31 02:40:09 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_30.pth saved !!!
[2022-05-31 02:40:09 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:40:11 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:40:11 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:40:12 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.693 (0.693)	Loss 0.5422 (0.5422)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 02:40:13 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.150)	Loss 0.9719 (0.7181)	Acc@1 75.000 (82.670)	Acc@5 96.875 (98.011)	Mem 4879MB
[2022-05-31 02:40:14 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.123)	Loss 1.1184 (0.7312)	Acc@1 62.500 (82.440)	Acc@5 93.750 (98.512)	Mem 4879MB
[2022-05-31 02:40:14 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.113)	Loss 0.8292 (0.7471)	Acc@1 78.125 (81.754)	Acc@5 96.875 (98.286)	Mem 4879MB
[2022-05-31 02:40:15 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.109)	Loss 0.7272 (0.7528)	Acc@1 81.250 (81.784)	Acc@5 93.750 (97.942)	Mem 4879MB
[2022-05-31 02:40:16 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.106)	Loss 0.8950 (0.7543)	Acc@1 81.250 (81.863)	Acc@5 96.875 (97.672)	Mem 4879MB
[2022-05-31 02:40:17 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.104)	Loss 0.7798 (0.7440)	Acc@1 75.000 (82.428)	Acc@5 100.000 (97.746)	Mem 4879MB
[2022-05-31 02:40:18 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.097 (0.102)	Loss 0.4501 (0.7441)	Acc@1 90.625 (82.218)	Acc@5 100.000 (97.799)	Mem 4879MB
[2022-05-31 02:40:19 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.4546 (0.7399)	Acc@1 93.750 (82.485)	Acc@5 100.000 (97.917)	Mem 4879MB
[2022-05-31 02:40:20 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.105 (0.101)	Loss 0.8517 (0.7417)	Acc@1 75.000 (82.418)	Acc@5 100.000 (97.871)	Mem 4879MB
[2022-05-31 02:40:21 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 1.1410 (0.7422)	Acc@1 68.750 (82.302)	Acc@5 90.625 (97.896)	Mem 4879MB
[2022-05-31 02:40:22 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 1.0137 (0.7506)	Acc@1 78.125 (82.292)	Acc@5 96.875 (97.945)	Mem 4879MB
[2022-05-31 02:40:23 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.5617 (0.7423)	Acc@1 84.375 (82.515)	Acc@5 96.875 (98.037)	Mem 4879MB
[2022-05-31 02:40:24 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.102 (0.099)	Loss 0.6637 (0.7413)	Acc@1 87.500 (82.657)	Acc@5 96.875 (98.068)	Mem 4879MB
[2022-05-31 02:40:25 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.099)	Loss 0.8101 (0.7408)	Acc@1 81.250 (82.691)	Acc@5 96.875 (98.116)	Mem 4879MB
[2022-05-31 02:40:26 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.092 (0.098)	Loss 1.0875 (0.7446)	Acc@1 59.375 (82.471)	Acc@5 100.000 (98.117)	Mem 4879MB
[2022-05-31 02:40:27 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.098)	Loss 0.6750 (0.7452)	Acc@1 90.625 (82.609)	Acc@5 100.000 (98.117)	Mem 4879MB
[2022-05-31 02:40:28 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.098)	Loss 0.5571 (0.7520)	Acc@1 87.500 (82.511)	Acc@5 100.000 (97.971)	Mem 4879MB
[2022-05-31 02:40:29 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.8388 (0.7502)	Acc@1 81.250 (82.510)	Acc@5 100.000 (97.945)	Mem 4879MB
[2022-05-31 02:40:30 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.6627 (0.7521)	Acc@1 87.500 (82.526)	Acc@5 100.000 (97.955)	Mem 4879MB
[2022-05-31 02:40:31 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.088 (0.098)	Loss 0.7245 (0.7475)	Acc@1 75.000 (82.587)	Acc@5 100.000 (97.994)	Mem 4879MB
[2022-05-31 02:40:31 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.097)	Loss 0.8846 (0.7460)	Acc@1 78.125 (82.613)	Acc@5 100.000 (98.001)	Mem 4879MB
[2022-05-31 02:40:32 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.6448 (0.7449)	Acc@1 84.375 (82.763)	Acc@5 100.000 (97.992)	Mem 4879MB
[2022-05-31 02:40:33 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 0.6517 (0.7440)	Acc@1 87.500 (82.792)	Acc@5 100.000 (97.971)	Mem 4879MB
[2022-05-31 02:40:34 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.7690 (0.7440)	Acc@1 78.125 (82.793)	Acc@5 96.875 (97.977)	Mem 4879MB
[2022-05-31 02:40:35 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.7323 (0.7437)	Acc@1 75.000 (82.744)	Acc@5 96.875 (97.971)	Mem 4879MB
[2022-05-31 02:40:36 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.9249 (0.7452)	Acc@1 81.250 (82.663)	Acc@5 93.750 (97.953)	Mem 4879MB
[2022-05-31 02:40:37 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 0.5879 (0.7427)	Acc@1 87.500 (82.738)	Acc@5 100.000 (97.970)	Mem 4879MB
[2022-05-31 02:40:38 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.097 (0.097)	Loss 0.6965 (0.7451)	Acc@1 87.500 (82.640)	Acc@5 100.000 (97.965)	Mem 4879MB
[2022-05-31 02:40:39 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.092 (0.097)	Loss 1.0442 (0.7449)	Acc@1 68.750 (82.571)	Acc@5 96.875 (97.949)	Mem 4879MB
[2022-05-31 02:40:40 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.097)	Loss 0.7322 (0.7444)	Acc@1 90.625 (82.558)	Acc@5 93.750 (97.944)	Mem 4879MB
[2022-05-31 02:40:41 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5859 (0.7434)	Acc@1 87.500 (82.607)	Acc@5 100.000 (97.950)	Mem 4879MB
[2022-05-31 02:40:41 MetaFG_0] (main.py 330): INFO  * Acc@1 82.610 Acc@5 97.940
[2022-05-31 02:40:41 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 82.6%
[2022-05-31 02:40:41 MetaFG_0] (main.py 171): INFO Max accuracy: 82.61%
[2022-05-31 02:40:42 MetaFG_0] (main.py 265): INFO Train: [31/300][0/1562]	eta 0:22:23 lr 0.000006	time 0.8601 (0.8601)	loss 1.6497 (1.6497)	grad_norm 25.1757 (25.1757)	mem 4879MB
[2022-05-31 02:40:45 MetaFG_0] (main.py 265): INFO Train: [31/300][10/1562]	eta 0:09:35 lr 0.000006	time 0.2922 (0.3710)	loss 1.4876 (1.4466)	grad_norm 19.0109 (25.9436)	mem 4879MB
[2022-05-31 02:40:48 MetaFG_0] (main.py 265): INFO Train: [31/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2926 (0.3400)	loss 1.3421 (1.4881)	grad_norm 23.8585 (27.4421)	mem 4879MB
[2022-05-31 02:40:51 MetaFG_0] (main.py 265): INFO Train: [31/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2941 (0.3282)	loss 1.5699 (1.4872)	grad_norm 24.8629 (26.6935)	mem 4879MB
[2022-05-31 02:40:54 MetaFG_0] (main.py 265): INFO Train: [31/300][40/1562]	eta 0:08:11 lr 0.000006	time 0.2920 (0.3228)	loss 1.4846 (1.4767)	grad_norm 25.7768 (26.9625)	mem 4879MB
[2022-05-31 02:40:57 MetaFG_0] (main.py 265): INFO Train: [31/300][50/1562]	eta 0:08:02 lr 0.000006	time 0.2999 (0.3191)	loss 1.6354 (1.4584)	grad_norm 28.7608 (27.3237)	mem 4879MB
[2022-05-31 02:41:00 MetaFG_0] (main.py 265): INFO Train: [31/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2988 (0.3173)	loss 1.1983 (1.4726)	grad_norm 73.0236 (29.3247)	mem 4879MB
[2022-05-31 02:41:04 MetaFG_0] (main.py 265): INFO Train: [31/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2923 (0.3155)	loss 1.6515 (1.4816)	grad_norm 20.9280 (28.9375)	mem 4879MB
[2022-05-31 02:41:07 MetaFG_0] (main.py 265): INFO Train: [31/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2929 (0.3140)	loss 1.7588 (1.4772)	grad_norm 22.3821 (29.3844)	mem 4879MB
[2022-05-31 02:41:10 MetaFG_0] (main.py 265): INFO Train: [31/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2983 (0.3131)	loss 1.3612 (1.4789)	grad_norm 27.2188 (29.1197)	mem 4879MB
[2022-05-31 02:41:13 MetaFG_0] (main.py 265): INFO Train: [31/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.3073 (0.3123)	loss 1.4596 (1.4731)	grad_norm 22.5095 (29.0797)	mem 4879MB
[2022-05-31 02:41:16 MetaFG_0] (main.py 265): INFO Train: [31/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2995 (0.3117)	loss 1.5701 (1.4585)	grad_norm 31.7739 (29.0356)	mem 4879MB
[2022-05-31 02:41:19 MetaFG_0] (main.py 265): INFO Train: [31/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.3025 (0.3110)	loss 1.4783 (1.4587)	grad_norm 55.3406 (29.4794)	mem 4879MB
[2022-05-31 02:41:22 MetaFG_0] (main.py 265): INFO Train: [31/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.3047 (0.3106)	loss 1.4761 (1.4524)	grad_norm 32.7279 (29.3470)	mem 4879MB
[2022-05-31 02:41:25 MetaFG_0] (main.py 265): INFO Train: [31/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2964 (0.3102)	loss 1.5736 (1.4598)	grad_norm 24.9266 (29.1616)	mem 4879MB
[2022-05-31 02:41:28 MetaFG_0] (main.py 265): INFO Train: [31/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.3023 (0.3097)	loss 1.4108 (1.4546)	grad_norm 28.2488 (29.2400)	mem 4879MB
[2022-05-31 02:41:31 MetaFG_0] (main.py 265): INFO Train: [31/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2925 (0.3094)	loss 1.5996 (1.4532)	grad_norm 22.9912 (29.0837)	mem 4879MB
[2022-05-31 02:41:34 MetaFG_0] (main.py 265): INFO Train: [31/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2933 (0.3091)	loss 1.0198 (1.4493)	grad_norm 53.5961 (29.0066)	mem 4879MB
[2022-05-31 02:41:37 MetaFG_0] (main.py 265): INFO Train: [31/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2929 (0.3089)	loss 1.2330 (1.4475)	grad_norm 44.2121 (28.9547)	mem 4879MB
[2022-05-31 02:41:40 MetaFG_0] (main.py 265): INFO Train: [31/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2993 (0.3088)	loss 1.5862 (1.4486)	grad_norm 19.8890 (29.1414)	mem 4879MB
[2022-05-31 02:41:43 MetaFG_0] (main.py 265): INFO Train: [31/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2992 (0.3086)	loss 1.6394 (1.4538)	grad_norm 35.5614 (29.0337)	mem 4879MB
[2022-05-31 02:41:46 MetaFG_0] (main.py 265): INFO Train: [31/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2983 (0.3085)	loss 1.3744 (1.4547)	grad_norm 25.8726 (29.0805)	mem 4879MB
[2022-05-31 02:41:49 MetaFG_0] (main.py 265): INFO Train: [31/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2939 (0.3082)	loss 1.4049 (1.4527)	grad_norm 22.5230 (29.3262)	mem 4879MB
[2022-05-31 02:41:52 MetaFG_0] (main.py 265): INFO Train: [31/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2923 (0.3080)	loss 0.9805 (1.4509)	grad_norm 40.1454 (29.4345)	mem 4879MB
[2022-05-31 02:41:55 MetaFG_0] (main.py 265): INFO Train: [31/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2927 (0.3079)	loss 1.0285 (1.4475)	grad_norm 32.9881 (29.5654)	mem 4879MB
[2022-05-31 02:41:58 MetaFG_0] (main.py 265): INFO Train: [31/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2955 (0.3077)	loss 1.4246 (1.4443)	grad_norm 24.6950 (29.4894)	mem 4879MB
[2022-05-31 02:42:01 MetaFG_0] (main.py 265): INFO Train: [31/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2928 (0.3075)	loss 1.7426 (1.4499)	grad_norm 32.6751 (29.4396)	mem 4879MB
[2022-05-31 02:42:04 MetaFG_0] (main.py 265): INFO Train: [31/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2948 (0.3074)	loss 1.3456 (1.4466)	grad_norm 44.7565 (29.5198)	mem 4879MB
[2022-05-31 02:42:08 MetaFG_0] (main.py 265): INFO Train: [31/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2984 (0.3074)	loss 1.3898 (1.4429)	grad_norm 22.9817 (29.6391)	mem 4879MB
[2022-05-31 02:42:11 MetaFG_0] (main.py 265): INFO Train: [31/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.3000 (0.3072)	loss 1.1955 (1.4419)	grad_norm 45.8687 (29.7692)	mem 4879MB
[2022-05-31 02:42:14 MetaFG_0] (main.py 265): INFO Train: [31/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.3007 (0.3072)	loss 1.5872 (1.4404)	grad_norm 19.8303 (29.8999)	mem 4879MB
[2022-05-31 02:42:17 MetaFG_0] (main.py 265): INFO Train: [31/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2931 (0.3071)	loss 1.4719 (1.4383)	grad_norm 25.1948 (29.7779)	mem 4879MB
[2022-05-31 02:42:20 MetaFG_0] (main.py 265): INFO Train: [31/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.3009 (0.3071)	loss 1.4235 (1.4376)	grad_norm 36.1325 (29.9756)	mem 4879MB
[2022-05-31 02:42:23 MetaFG_0] (main.py 265): INFO Train: [31/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2938 (0.3071)	loss 1.1250 (1.4392)	grad_norm 37.9720 (29.9986)	mem 4879MB
[2022-05-31 02:42:26 MetaFG_0] (main.py 265): INFO Train: [31/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2935 (0.3070)	loss 1.8342 (1.4433)	grad_norm 23.4414 (29.8811)	mem 4879MB
[2022-05-31 02:42:29 MetaFG_0] (main.py 265): INFO Train: [31/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2936 (0.3069)	loss 1.3748 (1.4417)	grad_norm 28.6402 (29.9780)	mem 4879MB
[2022-05-31 02:42:32 MetaFG_0] (main.py 265): INFO Train: [31/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2924 (0.3068)	loss 1.2197 (1.4422)	grad_norm 22.0713 (29.8554)	mem 4879MB
[2022-05-31 02:42:35 MetaFG_0] (main.py 265): INFO Train: [31/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2949 (0.3068)	loss 1.6400 (1.4394)	grad_norm 24.0546 (29.7821)	mem 4879MB
[2022-05-31 02:42:38 MetaFG_0] (main.py 265): INFO Train: [31/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2997 (0.3068)	loss 1.5874 (1.4409)	grad_norm 25.0514 (29.8127)	mem 4879MB
[2022-05-31 02:42:41 MetaFG_0] (main.py 265): INFO Train: [31/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2990 (0.3067)	loss 1.3767 (1.4429)	grad_norm 38.1734 (29.8102)	mem 4879MB
[2022-05-31 02:42:44 MetaFG_0] (main.py 265): INFO Train: [31/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2952 (0.3067)	loss 1.0979 (1.4424)	grad_norm 30.6421 (29.8928)	mem 4879MB
[2022-05-31 02:42:47 MetaFG_0] (main.py 265): INFO Train: [31/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2926 (0.3066)	loss 1.6234 (1.4390)	grad_norm 49.0321 (29.9686)	mem 4879MB
[2022-05-31 02:42:50 MetaFG_0] (main.py 265): INFO Train: [31/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2921 (0.3066)	loss 1.1501 (1.4378)	grad_norm 24.1549 (29.9414)	mem 4879MB
[2022-05-31 02:42:53 MetaFG_0] (main.py 265): INFO Train: [31/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2943 (0.3065)	loss 1.4127 (1.4373)	grad_norm 17.9267 (29.8920)	mem 4879MB
[2022-05-31 02:42:56 MetaFG_0] (main.py 265): INFO Train: [31/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2926 (0.3065)	loss 1.2940 (1.4370)	grad_norm 29.3624 (29.9226)	mem 4879MB
[2022-05-31 02:42:59 MetaFG_0] (main.py 265): INFO Train: [31/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2932 (0.3064)	loss 1.5555 (1.4363)	grad_norm 26.6785 (29.9205)	mem 4879MB
[2022-05-31 02:43:02 MetaFG_0] (main.py 265): INFO Train: [31/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2980 (0.3063)	loss 1.5861 (1.4389)	grad_norm 41.9789 (29.8977)	mem 4879MB
[2022-05-31 02:43:05 MetaFG_0] (main.py 265): INFO Train: [31/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2940 (0.3063)	loss 1.6336 (1.4410)	grad_norm 28.9096 (29.8427)	mem 4879MB
[2022-05-31 02:43:08 MetaFG_0] (main.py 265): INFO Train: [31/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2940 (0.3062)	loss 1.5296 (1.4418)	grad_norm 22.0195 (29.7414)	mem 4879MB
[2022-05-31 02:43:11 MetaFG_0] (main.py 265): INFO Train: [31/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2941 (0.3062)	loss 1.3109 (1.4447)	grad_norm 20.1812 (29.8921)	mem 4879MB
[2022-05-31 02:43:15 MetaFG_0] (main.py 265): INFO Train: [31/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2980 (0.3062)	loss 1.4765 (1.4465)	grad_norm 38.7111 (nan)	mem 4879MB
[2022-05-31 02:43:18 MetaFG_0] (main.py 265): INFO Train: [31/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2941 (0.3061)	loss 1.5281 (1.4458)	grad_norm 22.0595 (nan)	mem 4879MB
[2022-05-31 02:43:21 MetaFG_0] (main.py 265): INFO Train: [31/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2925 (0.3061)	loss 1.4894 (1.4469)	grad_norm 31.9286 (nan)	mem 4879MB
[2022-05-31 02:43:24 MetaFG_0] (main.py 265): INFO Train: [31/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2923 (0.3061)	loss 0.9899 (1.4447)	grad_norm 23.6336 (nan)	mem 4879MB
[2022-05-31 02:43:27 MetaFG_0] (main.py 265): INFO Train: [31/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2924 (0.3061)	loss 1.1758 (1.4453)	grad_norm 27.0092 (nan)	mem 4879MB
[2022-05-31 02:43:30 MetaFG_0] (main.py 265): INFO Train: [31/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2921 (0.3060)	loss 1.1791 (1.4467)	grad_norm 26.6993 (nan)	mem 4879MB
[2022-05-31 02:43:33 MetaFG_0] (main.py 265): INFO Train: [31/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2980 (0.3060)	loss 1.5503 (1.4486)	grad_norm 25.4444 (nan)	mem 4879MB
[2022-05-31 02:43:36 MetaFG_0] (main.py 265): INFO Train: [31/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2981 (0.3060)	loss 1.1320 (1.4483)	grad_norm 21.4671 (nan)	mem 4879MB
[2022-05-31 02:43:39 MetaFG_0] (main.py 265): INFO Train: [31/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.3001 (0.3060)	loss 1.4881 (1.4477)	grad_norm 41.3927 (nan)	mem 4879MB
[2022-05-31 02:43:42 MetaFG_0] (main.py 265): INFO Train: [31/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2932 (0.3060)	loss 1.3904 (1.4500)	grad_norm 25.0792 (nan)	mem 4879MB
[2022-05-31 02:43:45 MetaFG_0] (main.py 265): INFO Train: [31/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2925 (0.3060)	loss 1.4356 (1.4502)	grad_norm 39.4991 (nan)	mem 4879MB
[2022-05-31 02:43:48 MetaFG_0] (main.py 265): INFO Train: [31/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2919 (0.3059)	loss 1.5325 (1.4489)	grad_norm 27.3235 (nan)	mem 4879MB
[2022-05-31 02:43:51 MetaFG_0] (main.py 265): INFO Train: [31/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2932 (0.3059)	loss 1.3695 (1.4467)	grad_norm 16.2301 (nan)	mem 4879MB
[2022-05-31 02:43:54 MetaFG_0] (main.py 265): INFO Train: [31/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2993 (0.3058)	loss 1.7765 (1.4486)	grad_norm 55.6485 (nan)	mem 4879MB
[2022-05-31 02:43:57 MetaFG_0] (main.py 265): INFO Train: [31/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3314 (0.3062)	loss 1.3038 (1.4487)	grad_norm 34.7270 (nan)	mem 4879MB
[2022-05-31 02:44:01 MetaFG_0] (main.py 265): INFO Train: [31/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2931 (0.3063)	loss 1.1969 (1.4486)	grad_norm 21.6783 (nan)	mem 4879MB
[2022-05-31 02:44:04 MetaFG_0] (main.py 265): INFO Train: [31/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2998 (0.3063)	loss 1.3155 (1.4488)	grad_norm 20.0696 (nan)	mem 4879MB
[2022-05-31 02:44:07 MetaFG_0] (main.py 265): INFO Train: [31/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.3005 (0.3063)	loss 1.3610 (1.4494)	grad_norm 29.7351 (nan)	mem 4879MB
[2022-05-31 02:44:10 MetaFG_0] (main.py 265): INFO Train: [31/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2968 (0.3064)	loss 1.3435 (1.4492)	grad_norm 31.7787 (nan)	mem 4879MB
[2022-05-31 02:44:13 MetaFG_0] (main.py 265): INFO Train: [31/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2953 (0.3063)	loss 1.4699 (1.4512)	grad_norm 23.9051 (nan)	mem 4879MB
[2022-05-31 02:44:16 MetaFG_0] (main.py 265): INFO Train: [31/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2976 (0.3063)	loss 1.1956 (1.4496)	grad_norm 25.7061 (nan)	mem 4879MB
[2022-05-31 02:44:19 MetaFG_0] (main.py 265): INFO Train: [31/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2995 (0.3063)	loss 1.4364 (1.4494)	grad_norm 67.4474 (nan)	mem 4879MB
[2022-05-31 02:44:22 MetaFG_0] (main.py 265): INFO Train: [31/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2935 (0.3063)	loss 1.0978 (1.4498)	grad_norm 35.7018 (nan)	mem 4879MB
[2022-05-31 02:44:25 MetaFG_0] (main.py 265): INFO Train: [31/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2931 (0.3063)	loss 1.5381 (1.4495)	grad_norm 22.2037 (nan)	mem 4879MB
[2022-05-31 02:44:28 MetaFG_0] (main.py 265): INFO Train: [31/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2922 (0.3063)	loss 1.6349 (1.4497)	grad_norm 28.4501 (nan)	mem 4879MB
[2022-05-31 02:44:31 MetaFG_0] (main.py 265): INFO Train: [31/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2938 (0.3062)	loss 1.3148 (1.4498)	grad_norm 29.1775 (nan)	mem 4879MB
[2022-05-31 02:44:34 MetaFG_0] (main.py 265): INFO Train: [31/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2918 (0.3062)	loss 1.1349 (1.4504)	grad_norm 33.2957 (nan)	mem 4879MB
[2022-05-31 02:44:37 MetaFG_0] (main.py 265): INFO Train: [31/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2926 (0.3062)	loss 1.4771 (1.4505)	grad_norm 17.8166 (nan)	mem 4879MB
[2022-05-31 02:44:40 MetaFG_0] (main.py 265): INFO Train: [31/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2934 (0.3062)	loss 1.5940 (1.4501)	grad_norm 30.1827 (nan)	mem 4879MB
[2022-05-31 02:44:43 MetaFG_0] (main.py 265): INFO Train: [31/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2941 (0.3061)	loss 1.8015 (1.4516)	grad_norm 38.4365 (nan)	mem 4879MB
[2022-05-31 02:44:46 MetaFG_0] (main.py 265): INFO Train: [31/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2925 (0.3061)	loss 1.4323 (1.4530)	grad_norm 32.8951 (nan)	mem 4879MB
[2022-05-31 02:44:49 MetaFG_0] (main.py 265): INFO Train: [31/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2917 (0.3061)	loss 1.3733 (1.4533)	grad_norm 32.4264 (nan)	mem 4879MB
[2022-05-31 02:44:52 MetaFG_0] (main.py 265): INFO Train: [31/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2919 (0.3061)	loss 1.3855 (1.4520)	grad_norm 35.3658 (nan)	mem 4879MB
[2022-05-31 02:44:55 MetaFG_0] (main.py 265): INFO Train: [31/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2934 (0.3061)	loss 1.2967 (1.4508)	grad_norm 24.8907 (nan)	mem 4879MB
[2022-05-31 02:44:59 MetaFG_0] (main.py 265): INFO Train: [31/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2921 (0.3060)	loss 1.5818 (1.4519)	grad_norm 19.9412 (nan)	mem 4879MB
[2022-05-31 02:45:02 MetaFG_0] (main.py 265): INFO Train: [31/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2992 (0.3060)	loss 1.3697 (1.4527)	grad_norm 22.1185 (nan)	mem 4879MB
[2022-05-31 02:45:05 MetaFG_0] (main.py 265): INFO Train: [31/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2935 (0.3060)	loss 0.8890 (1.4530)	grad_norm 29.1194 (nan)	mem 4879MB
[2022-05-31 02:45:08 MetaFG_0] (main.py 265): INFO Train: [31/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2927 (0.3060)	loss 1.4752 (1.4520)	grad_norm 26.2967 (nan)	mem 4879MB
[2022-05-31 02:45:11 MetaFG_0] (main.py 265): INFO Train: [31/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2923 (0.3059)	loss 1.4587 (1.4522)	grad_norm 16.7460 (nan)	mem 4879MB
[2022-05-31 02:45:14 MetaFG_0] (main.py 265): INFO Train: [31/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2984 (0.3059)	loss 1.5957 (1.4517)	grad_norm 21.5224 (nan)	mem 4879MB
[2022-05-31 02:45:17 MetaFG_0] (main.py 265): INFO Train: [31/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2920 (0.3059)	loss 1.4713 (1.4512)	grad_norm 17.6927 (nan)	mem 4879MB
[2022-05-31 02:45:20 MetaFG_0] (main.py 265): INFO Train: [31/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2925 (0.3059)	loss 1.5176 (1.4500)	grad_norm 19.6178 (nan)	mem 4879MB
[2022-05-31 02:45:23 MetaFG_0] (main.py 265): INFO Train: [31/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2984 (0.3059)	loss 1.3089 (1.4496)	grad_norm 28.8420 (nan)	mem 4879MB
[2022-05-31 02:45:26 MetaFG_0] (main.py 265): INFO Train: [31/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3002 (0.3059)	loss 1.2955 (1.4500)	grad_norm 15.0358 (nan)	mem 4879MB
[2022-05-31 02:45:29 MetaFG_0] (main.py 265): INFO Train: [31/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2930 (0.3059)	loss 1.3496 (1.4510)	grad_norm 23.7624 (nan)	mem 4879MB
[2022-05-31 02:45:32 MetaFG_0] (main.py 265): INFO Train: [31/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3000 (0.3059)	loss 1.7121 (1.4522)	grad_norm 22.8497 (nan)	mem 4879MB
[2022-05-31 02:45:35 MetaFG_0] (main.py 265): INFO Train: [31/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2936 (0.3059)	loss 1.4609 (1.4520)	grad_norm 24.7591 (nan)	mem 4879MB
[2022-05-31 02:45:38 MetaFG_0] (main.py 265): INFO Train: [31/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2982 (0.3059)	loss 1.7169 (1.4534)	grad_norm 35.5826 (nan)	mem 4879MB
[2022-05-31 02:45:41 MetaFG_0] (main.py 265): INFO Train: [31/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3007 (0.3058)	loss 1.6166 (1.4547)	grad_norm 28.0446 (nan)	mem 4879MB
[2022-05-31 02:45:44 MetaFG_0] (main.py 265): INFO Train: [31/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.3004 (0.3059)	loss 1.3621 (1.4534)	grad_norm 18.7744 (nan)	mem 4879MB
[2022-05-31 02:45:47 MetaFG_0] (main.py 265): INFO Train: [31/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2977 (0.3058)	loss 1.0974 (1.4525)	grad_norm 17.2086 (nan)	mem 4879MB
[2022-05-31 02:45:50 MetaFG_0] (main.py 265): INFO Train: [31/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3001 (0.3058)	loss 1.4544 (1.4526)	grad_norm 28.0957 (nan)	mem 4879MB
[2022-05-31 02:45:53 MetaFG_0] (main.py 265): INFO Train: [31/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3037 (0.3058)	loss 1.6134 (1.4523)	grad_norm 56.2247 (nan)	mem 4879MB
[2022-05-31 02:45:56 MetaFG_0] (main.py 265): INFO Train: [31/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3028 (0.3058)	loss 1.6248 (1.4533)	grad_norm 18.3980 (nan)	mem 4879MB
[2022-05-31 02:45:59 MetaFG_0] (main.py 265): INFO Train: [31/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2999 (0.3058)	loss 1.5272 (1.4540)	grad_norm 40.0879 (nan)	mem 4879MB
[2022-05-31 02:46:02 MetaFG_0] (main.py 265): INFO Train: [31/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2934 (0.3058)	loss 1.6206 (1.4528)	grad_norm 27.3294 (nan)	mem 4879MB
[2022-05-31 02:46:06 MetaFG_0] (main.py 265): INFO Train: [31/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2920 (0.3057)	loss 1.4313 (1.4521)	grad_norm 37.8110 (nan)	mem 4879MB
[2022-05-31 02:46:09 MetaFG_0] (main.py 265): INFO Train: [31/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2930 (0.3057)	loss 1.4140 (1.4514)	grad_norm 35.2697 (nan)	mem 4879MB
[2022-05-31 02:46:12 MetaFG_0] (main.py 265): INFO Train: [31/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2923 (0.3057)	loss 1.4962 (1.4509)	grad_norm 43.8372 (nan)	mem 4879MB
[2022-05-31 02:46:15 MetaFG_0] (main.py 265): INFO Train: [31/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2953 (0.3057)	loss 1.6017 (1.4514)	grad_norm 21.9337 (nan)	mem 4879MB
[2022-05-31 02:46:18 MetaFG_0] (main.py 265): INFO Train: [31/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2984 (0.3057)	loss 1.7732 (1.4516)	grad_norm 39.9700 (nan)	mem 4879MB
[2022-05-31 02:46:21 MetaFG_0] (main.py 265): INFO Train: [31/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.3059 (0.3057)	loss 1.3179 (1.4505)	grad_norm 20.6087 (nan)	mem 4879MB
[2022-05-31 02:46:24 MetaFG_0] (main.py 265): INFO Train: [31/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.3078 (0.3057)	loss 1.2386 (1.4506)	grad_norm 34.9226 (nan)	mem 4879MB
[2022-05-31 02:46:27 MetaFG_0] (main.py 265): INFO Train: [31/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2922 (0.3057)	loss 1.7452 (1.4505)	grad_norm 37.5183 (nan)	mem 4879MB
[2022-05-31 02:46:30 MetaFG_0] (main.py 265): INFO Train: [31/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2922 (0.3056)	loss 1.2158 (1.4492)	grad_norm 36.2042 (nan)	mem 4879MB
[2022-05-31 02:46:33 MetaFG_0] (main.py 265): INFO Train: [31/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3056)	loss 1.0250 (1.4487)	grad_norm 25.1626 (nan)	mem 4879MB
[2022-05-31 02:46:36 MetaFG_0] (main.py 265): INFO Train: [31/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2928 (0.3056)	loss 1.7450 (1.4487)	grad_norm 16.3839 (nan)	mem 4879MB
[2022-05-31 02:46:39 MetaFG_0] (main.py 265): INFO Train: [31/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2984 (0.3056)	loss 1.3389 (1.4493)	grad_norm 11.7270 (nan)	mem 4879MB
[2022-05-31 02:46:42 MetaFG_0] (main.py 265): INFO Train: [31/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2919 (0.3056)	loss 1.2018 (1.4490)	grad_norm 22.8711 (nan)	mem 4879MB
[2022-05-31 02:46:45 MetaFG_0] (main.py 265): INFO Train: [31/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2921 (0.3056)	loss 1.8452 (1.4491)	grad_norm 31.0651 (nan)	mem 4879MB
[2022-05-31 02:46:48 MetaFG_0] (main.py 265): INFO Train: [31/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2933 (0.3056)	loss 1.7628 (1.4487)	grad_norm 19.4671 (nan)	mem 4879MB
[2022-05-31 02:46:51 MetaFG_0] (main.py 265): INFO Train: [31/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2920 (0.3056)	loss 1.3340 (1.4479)	grad_norm 13.3457 (nan)	mem 4879MB
[2022-05-31 02:46:54 MetaFG_0] (main.py 265): INFO Train: [31/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3001 (0.3056)	loss 1.3732 (1.4481)	grad_norm 29.1251 (nan)	mem 4879MB
[2022-05-31 02:46:57 MetaFG_0] (main.py 265): INFO Train: [31/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2988 (0.3056)	loss 0.9857 (1.4476)	grad_norm 23.4082 (nan)	mem 4879MB
[2022-05-31 02:47:00 MetaFG_0] (main.py 265): INFO Train: [31/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2926 (0.3056)	loss 1.2191 (1.4474)	grad_norm 36.6788 (nan)	mem 4879MB
[2022-05-31 02:47:03 MetaFG_0] (main.py 265): INFO Train: [31/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2947 (0.3055)	loss 1.7877 (1.4474)	grad_norm 45.6966 (nan)	mem 4879MB
[2022-05-31 02:47:06 MetaFG_0] (main.py 265): INFO Train: [31/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2919 (0.3055)	loss 1.3877 (1.4474)	grad_norm 21.7899 (nan)	mem 4879MB
[2022-05-31 02:47:09 MetaFG_0] (main.py 265): INFO Train: [31/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2921 (0.3055)	loss 1.6642 (1.4475)	grad_norm 38.2193 (nan)	mem 4879MB
[2022-05-31 02:47:12 MetaFG_0] (main.py 265): INFO Train: [31/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2939 (0.3055)	loss 1.7449 (1.4477)	grad_norm 33.4378 (nan)	mem 4879MB
[2022-05-31 02:47:16 MetaFG_0] (main.py 265): INFO Train: [31/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2950 (0.3055)	loss 1.6904 (1.4466)	grad_norm 47.2328 (nan)	mem 4879MB
[2022-05-31 02:47:19 MetaFG_0] (main.py 265): INFO Train: [31/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2945 (0.3055)	loss 1.2835 (1.4463)	grad_norm 18.2246 (nan)	mem 4879MB
[2022-05-31 02:47:22 MetaFG_0] (main.py 265): INFO Train: [31/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2990 (0.3055)	loss 1.4019 (1.4465)	grad_norm 35.5075 (nan)	mem 4879MB
[2022-05-31 02:47:25 MetaFG_0] (main.py 265): INFO Train: [31/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2924 (0.3055)	loss 1.6547 (1.4472)	grad_norm 33.2007 (nan)	mem 4879MB
[2022-05-31 02:47:28 MetaFG_0] (main.py 265): INFO Train: [31/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2948 (0.3055)	loss 1.1045 (1.4463)	grad_norm 20.6667 (nan)	mem 4879MB
[2022-05-31 02:47:31 MetaFG_0] (main.py 265): INFO Train: [31/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2930 (0.3055)	loss 1.5464 (1.4461)	grad_norm 38.3090 (nan)	mem 4879MB
[2022-05-31 02:47:34 MetaFG_0] (main.py 265): INFO Train: [31/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2925 (0.3055)	loss 0.8688 (1.4458)	grad_norm 21.2170 (nan)	mem 4879MB
[2022-05-31 02:47:37 MetaFG_0] (main.py 265): INFO Train: [31/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3017 (0.3055)	loss 1.7003 (1.4466)	grad_norm 20.5337 (nan)	mem 4879MB
[2022-05-31 02:47:40 MetaFG_0] (main.py 265): INFO Train: [31/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2931 (0.3055)	loss 1.0944 (1.4463)	grad_norm 23.7676 (nan)	mem 4879MB
[2022-05-31 02:47:43 MetaFG_0] (main.py 265): INFO Train: [31/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3003 (0.3055)	loss 0.9946 (1.4460)	grad_norm 24.8066 (nan)	mem 4879MB
[2022-05-31 02:47:46 MetaFG_0] (main.py 265): INFO Train: [31/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3001 (0.3054)	loss 1.4732 (1.4459)	grad_norm 18.2405 (nan)	mem 4879MB
[2022-05-31 02:47:49 MetaFG_0] (main.py 265): INFO Train: [31/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2931 (0.3054)	loss 1.5722 (1.4464)	grad_norm 31.3660 (nan)	mem 4879MB
[2022-05-31 02:47:52 MetaFG_0] (main.py 265): INFO Train: [31/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3004 (0.3054)	loss 1.4707 (1.4458)	grad_norm 53.4093 (nan)	mem 4879MB
[2022-05-31 02:47:55 MetaFG_0] (main.py 265): INFO Train: [31/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2928 (0.3055)	loss 1.7498 (1.4460)	grad_norm 29.7294 (nan)	mem 4879MB
[2022-05-31 02:47:58 MetaFG_0] (main.py 265): INFO Train: [31/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2936 (0.3054)	loss 1.3614 (1.4459)	grad_norm 19.5070 (nan)	mem 4879MB
[2022-05-31 02:48:01 MetaFG_0] (main.py 265): INFO Train: [31/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2922 (0.3054)	loss 1.2831 (1.4459)	grad_norm 28.9492 (nan)	mem 4879MB
[2022-05-31 02:48:04 MetaFG_0] (main.py 265): INFO Train: [31/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2922 (0.3054)	loss 1.4865 (1.4467)	grad_norm 33.9596 (nan)	mem 4879MB
[2022-05-31 02:48:07 MetaFG_0] (main.py 265): INFO Train: [31/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2933 (0.3054)	loss 1.3828 (1.4469)	grad_norm 17.6035 (nan)	mem 4879MB
[2022-05-31 02:48:10 MetaFG_0] (main.py 265): INFO Train: [31/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2921 (0.3054)	loss 1.5436 (1.4467)	grad_norm 27.3375 (nan)	mem 4879MB
[2022-05-31 02:48:13 MetaFG_0] (main.py 265): INFO Train: [31/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2945 (0.3054)	loss 1.7170 (1.4475)	grad_norm 34.4762 (nan)	mem 4879MB
[2022-05-31 02:48:17 MetaFG_0] (main.py 265): INFO Train: [31/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2979 (0.3054)	loss 1.4068 (1.4475)	grad_norm 21.2983 (nan)	mem 4879MB
[2022-05-31 02:48:20 MetaFG_0] (main.py 265): INFO Train: [31/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3003 (0.3054)	loss 1.8869 (1.4477)	grad_norm 32.5180 (nan)	mem 4879MB
[2022-05-31 02:48:23 MetaFG_0] (main.py 265): INFO Train: [31/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2980 (0.3054)	loss 1.5108 (1.4484)	grad_norm 26.4530 (nan)	mem 4879MB
[2022-05-31 02:48:26 MetaFG_0] (main.py 265): INFO Train: [31/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2996 (0.3054)	loss 1.5960 (1.4493)	grad_norm 24.3063 (nan)	mem 4879MB
[2022-05-31 02:48:29 MetaFG_0] (main.py 265): INFO Train: [31/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2925 (0.3054)	loss 1.2529 (1.4489)	grad_norm 21.9252 (nan)	mem 4879MB
[2022-05-31 02:48:32 MetaFG_0] (main.py 265): INFO Train: [31/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3536 (0.3055)	loss 1.6254 (1.4492)	grad_norm 26.6905 (nan)	mem 4879MB
[2022-05-31 02:48:35 MetaFG_0] (main.py 265): INFO Train: [31/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3010 (0.3056)	loss 1.7548 (1.4487)	grad_norm 40.9287 (nan)	mem 4879MB
[2022-05-31 02:48:38 MetaFG_0] (main.py 265): INFO Train: [31/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2922 (0.3055)	loss 1.4844 (1.4493)	grad_norm 18.0780 (nan)	mem 4879MB
[2022-05-31 02:48:39 MetaFG_0] (main.py 272): INFO EPOCH 31 training takes 0:07:57
[2022-05-31 02:48:39 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_31.pth saving......
[2022-05-31 02:48:39 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_31.pth saved !!!
[2022-05-31 02:48:39 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:48:41 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:48:41 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:48:41 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.690 (0.690)	Loss 0.6319 (0.6319)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 02:48:42 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.150)	Loss 0.7624 (0.7783)	Acc@1 84.375 (84.091)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 02:48:43 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.123)	Loss 0.7098 (0.7362)	Acc@1 87.500 (85.863)	Acc@5 96.875 (98.363)	Mem 4879MB
[2022-05-31 02:48:44 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.114)	Loss 0.6167 (0.7525)	Acc@1 90.625 (85.181)	Acc@5 100.000 (98.286)	Mem 4879MB
[2022-05-31 02:48:45 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.109)	Loss 0.6081 (0.7603)	Acc@1 87.500 (84.832)	Acc@5 100.000 (97.942)	Mem 4879MB
[2022-05-31 02:48:46 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.101 (0.106)	Loss 0.6812 (0.7494)	Acc@1 84.375 (84.498)	Acc@5 100.000 (98.100)	Mem 4879MB
[2022-05-31 02:48:47 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 0.9622 (0.7526)	Acc@1 75.000 (84.631)	Acc@5 93.750 (98.053)	Mem 4879MB
[2022-05-31 02:48:48 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.095 (0.103)	Loss 0.8104 (0.7689)	Acc@1 81.250 (83.671)	Acc@5 100.000 (98.151)	Mem 4879MB
[2022-05-31 02:48:49 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 0.8599 (0.7652)	Acc@1 84.375 (83.642)	Acc@5 96.875 (98.341)	Mem 4879MB
[2022-05-31 02:48:50 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.6345 (0.7690)	Acc@1 84.375 (83.516)	Acc@5 100.000 (98.146)	Mem 4879MB
[2022-05-31 02:48:51 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.090 (0.100)	Loss 0.6085 (0.7710)	Acc@1 84.375 (83.385)	Acc@5 100.000 (98.051)	Mem 4879MB
[2022-05-31 02:48:52 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.100)	Loss 0.8610 (0.7761)	Acc@1 75.000 (83.108)	Acc@5 100.000 (98.086)	Mem 4879MB
[2022-05-31 02:48:53 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.098 (0.099)	Loss 0.6371 (0.7767)	Acc@1 81.250 (83.110)	Acc@5 100.000 (98.037)	Mem 4879MB
[2022-05-31 02:48:54 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 0.9119 (0.7776)	Acc@1 71.875 (83.039)	Acc@5 100.000 (98.020)	Mem 4879MB
[2022-05-31 02:48:55 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.7164 (0.7739)	Acc@1 81.250 (83.156)	Acc@5 96.875 (98.005)	Mem 4879MB
[2022-05-31 02:48:56 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.6273 (0.7763)	Acc@1 93.750 (83.009)	Acc@5 96.875 (97.993)	Mem 4879MB
[2022-05-31 02:48:57 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.8198 (0.7760)	Acc@1 75.000 (82.919)	Acc@5 96.875 (98.020)	Mem 4879MB
[2022-05-31 02:48:57 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.093 (0.098)	Loss 0.6342 (0.7742)	Acc@1 90.625 (82.931)	Acc@5 96.875 (98.026)	Mem 4879MB
[2022-05-31 02:48:58 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.106 (0.098)	Loss 0.7727 (0.7723)	Acc@1 81.250 (82.959)	Acc@5 100.000 (98.032)	Mem 4879MB
[2022-05-31 02:48:59 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.098)	Loss 0.7101 (0.7742)	Acc@1 81.250 (82.853)	Acc@5 100.000 (97.988)	Mem 4879MB
[2022-05-31 02:49:00 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.093 (0.098)	Loss 0.8299 (0.7697)	Acc@1 71.875 (82.976)	Acc@5 100.000 (98.041)	Mem 4879MB
[2022-05-31 02:49:01 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.097)	Loss 0.7852 (0.7685)	Acc@1 84.375 (83.101)	Acc@5 100.000 (98.060)	Mem 4879MB
[2022-05-31 02:49:02 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 0.8992 (0.7720)	Acc@1 71.875 (82.919)	Acc@5 96.875 (98.049)	Mem 4879MB
[2022-05-31 02:49:03 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.104 (0.097)	Loss 0.6500 (0.7712)	Acc@1 96.875 (82.995)	Acc@5 100.000 (98.025)	Mem 4879MB
[2022-05-31 02:49:04 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.098 (0.097)	Loss 0.6269 (0.7689)	Acc@1 87.500 (83.013)	Acc@5 100.000 (98.042)	Mem 4879MB
[2022-05-31 02:49:05 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.8379 (0.7681)	Acc@1 78.125 (82.993)	Acc@5 96.875 (98.033)	Mem 4879MB
[2022-05-31 02:49:06 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 0.7882 (0.7708)	Acc@1 81.250 (82.926)	Acc@5 96.875 (98.036)	Mem 4879MB
[2022-05-31 02:49:07 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 0.6526 (0.7715)	Acc@1 87.500 (82.968)	Acc@5 96.875 (97.994)	Mem 4879MB
[2022-05-31 02:49:08 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.8886 (0.7747)	Acc@1 75.000 (82.863)	Acc@5 96.875 (98.009)	Mem 4879MB
[2022-05-31 02:49:09 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.095 (0.097)	Loss 0.5743 (0.7719)	Acc@1 90.625 (82.990)	Acc@5 100.000 (98.046)	Mem 4879MB
[2022-05-31 02:49:10 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 0.6663 (0.7723)	Acc@1 87.500 (82.953)	Acc@5 100.000 (98.069)	Mem 4879MB
[2022-05-31 02:49:11 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.9795 (0.7716)	Acc@1 75.000 (82.968)	Acc@5 93.750 (98.091)	Mem 4879MB
[2022-05-31 02:49:11 MetaFG_0] (main.py 330): INFO  * Acc@1 82.950 Acc@5 98.090
[2022-05-31 02:49:11 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 83.0%
[2022-05-31 02:49:11 MetaFG_0] (main.py 171): INFO Max accuracy: 82.95%
[2022-05-31 02:49:12 MetaFG_0] (main.py 265): INFO Train: [32/300][0/1562]	eta 0:25:54 lr 0.000006	time 0.9950 (0.9950)	loss 1.2934 (1.2934)	grad_norm 29.0182 (29.0182)	mem 4879MB
[2022-05-31 02:49:15 MetaFG_0] (main.py 265): INFO Train: [32/300][10/1562]	eta 0:09:39 lr 0.000006	time 0.2936 (0.3731)	loss 1.0304 (1.3900)	grad_norm 36.8058 (29.2001)	mem 4879MB
[2022-05-31 02:49:18 MetaFG_0] (main.py 265): INFO Train: [32/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2990 (0.3403)	loss 1.1592 (1.3600)	grad_norm 37.2207 (30.6882)	mem 4879MB
[2022-05-31 02:49:21 MetaFG_0] (main.py 265): INFO Train: [32/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.3010 (0.3296)	loss 1.4986 (1.3868)	grad_norm 27.8369 (30.1284)	mem 4879MB
[2022-05-31 02:49:24 MetaFG_0] (main.py 265): INFO Train: [32/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2934 (0.3237)	loss 1.4351 (1.3890)	grad_norm 17.7630 (29.6547)	mem 4879MB
[2022-05-31 02:49:27 MetaFG_0] (main.py 265): INFO Train: [32/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2934 (0.3201)	loss 1.6495 (1.3917)	grad_norm 49.9533 (31.6197)	mem 4879MB
[2022-05-31 02:49:30 MetaFG_0] (main.py 265): INFO Train: [32/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2985 (0.3175)	loss 1.4193 (1.3793)	grad_norm 36.0152 (30.7122)	mem 4879MB
[2022-05-31 02:49:33 MetaFG_0] (main.py 265): INFO Train: [32/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2921 (0.3157)	loss 1.2003 (1.3950)	grad_norm 19.5925 (29.9878)	mem 4879MB
[2022-05-31 02:49:36 MetaFG_0] (main.py 265): INFO Train: [32/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2981 (0.3143)	loss 1.5231 (1.4083)	grad_norm 26.7139 (29.8666)	mem 4879MB
[2022-05-31 02:49:39 MetaFG_0] (main.py 265): INFO Train: [32/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2998 (0.3134)	loss 1.3589 (1.4165)	grad_norm 17.6092 (30.6685)	mem 4879MB
[2022-05-31 02:49:43 MetaFG_0] (main.py 265): INFO Train: [32/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2930 (0.3126)	loss 1.4905 (1.4186)	grad_norm 43.5164 (31.1582)	mem 4879MB
[2022-05-31 02:49:46 MetaFG_0] (main.py 265): INFO Train: [32/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.3001 (0.3122)	loss 1.6039 (1.4196)	grad_norm 34.8154 (30.8290)	mem 4879MB
[2022-05-31 02:49:49 MetaFG_0] (main.py 265): INFO Train: [32/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2936 (0.3116)	loss 1.6159 (1.4153)	grad_norm 24.2779 (30.2759)	mem 4879MB
[2022-05-31 02:49:52 MetaFG_0] (main.py 265): INFO Train: [32/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2939 (0.3112)	loss 1.5587 (1.4153)	grad_norm 32.3567 (30.1062)	mem 4879MB
[2022-05-31 02:49:55 MetaFG_0] (main.py 265): INFO Train: [32/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2928 (0.3107)	loss 1.6962 (1.4166)	grad_norm 26.5100 (30.0707)	mem 4879MB
[2022-05-31 02:49:58 MetaFG_0] (main.py 265): INFO Train: [32/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2982 (0.3105)	loss 1.4161 (1.4133)	grad_norm 20.3819 (29.9884)	mem 4879MB
[2022-05-31 02:50:01 MetaFG_0] (main.py 265): INFO Train: [32/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2924 (0.3101)	loss 1.1799 (1.4121)	grad_norm 31.0062 (30.0430)	mem 4879MB
[2022-05-31 02:50:04 MetaFG_0] (main.py 265): INFO Train: [32/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2982 (0.3097)	loss 1.8522 (1.4178)	grad_norm 33.8557 (29.9236)	mem 4879MB
[2022-05-31 02:50:07 MetaFG_0] (main.py 265): INFO Train: [32/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2925 (0.3094)	loss 1.6017 (1.4155)	grad_norm 30.2063 (29.8040)	mem 4879MB
[2022-05-31 02:50:10 MetaFG_0] (main.py 265): INFO Train: [32/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2923 (0.3091)	loss 1.2376 (1.4179)	grad_norm 38.6783 (29.8038)	mem 4879MB
[2022-05-31 02:50:13 MetaFG_0] (main.py 265): INFO Train: [32/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.3003 (0.3091)	loss 1.7731 (1.4196)	grad_norm 33.9688 (29.8704)	mem 4879MB
[2022-05-31 02:50:16 MetaFG_0] (main.py 265): INFO Train: [32/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2937 (0.3089)	loss 1.2520 (1.4233)	grad_norm 39.1435 (29.8991)	mem 4879MB
[2022-05-31 02:50:19 MetaFG_0] (main.py 265): INFO Train: [32/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2980 (0.3088)	loss 1.4979 (1.4252)	grad_norm 16.8661 (29.6592)	mem 4879MB
[2022-05-31 02:50:22 MetaFG_0] (main.py 265): INFO Train: [32/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2991 (0.3086)	loss 1.2841 (1.4229)	grad_norm 29.8910 (29.5540)	mem 4879MB
[2022-05-31 02:50:25 MetaFG_0] (main.py 265): INFO Train: [32/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2921 (0.3084)	loss 1.5220 (1.4243)	grad_norm 23.7712 (29.4522)	mem 4879MB
[2022-05-31 02:50:28 MetaFG_0] (main.py 265): INFO Train: [32/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2922 (0.3083)	loss 0.9050 (1.4214)	grad_norm 27.4769 (29.4554)	mem 4879MB
[2022-05-31 02:50:31 MetaFG_0] (main.py 265): INFO Train: [32/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2983 (0.3081)	loss 1.5271 (1.4249)	grad_norm 20.5015 (29.5008)	mem 4879MB
[2022-05-31 02:50:34 MetaFG_0] (main.py 265): INFO Train: [32/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2984 (0.3079)	loss 1.3765 (1.4236)	grad_norm 25.6218 (29.4256)	mem 4879MB
[2022-05-31 02:50:37 MetaFG_0] (main.py 265): INFO Train: [32/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2980 (0.3077)	loss 1.4345 (1.4245)	grad_norm 56.6758 (29.3986)	mem 4879MB
[2022-05-31 02:50:40 MetaFG_0] (main.py 265): INFO Train: [32/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2984 (0.3076)	loss 1.5491 (1.4265)	grad_norm 24.5514 (29.4272)	mem 4879MB
[2022-05-31 02:50:43 MetaFG_0] (main.py 265): INFO Train: [32/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2998 (0.3075)	loss 1.6833 (1.4256)	grad_norm 21.0068 (29.3332)	mem 4879MB
[2022-05-31 02:50:47 MetaFG_0] (main.py 265): INFO Train: [32/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.3007 (0.3074)	loss 1.8620 (1.4302)	grad_norm 36.4177 (29.3468)	mem 4879MB
[2022-05-31 02:50:50 MetaFG_0] (main.py 265): INFO Train: [32/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2992 (0.3074)	loss 1.3343 (1.4253)	grad_norm 31.8810 (29.3009)	mem 4879MB
[2022-05-31 02:50:53 MetaFG_0] (main.py 265): INFO Train: [32/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2919 (0.3072)	loss 1.2330 (1.4245)	grad_norm 34.9727 (29.2934)	mem 4879MB
[2022-05-31 02:50:56 MetaFG_0] (main.py 265): INFO Train: [32/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2979 (0.3071)	loss 1.3138 (1.4202)	grad_norm 23.6411 (29.2957)	mem 4879MB
[2022-05-31 02:50:59 MetaFG_0] (main.py 265): INFO Train: [32/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2976 (0.3071)	loss 1.3333 (1.4208)	grad_norm 22.8638 (29.2696)	mem 4879MB
[2022-05-31 02:51:02 MetaFG_0] (main.py 265): INFO Train: [32/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2990 (0.3071)	loss 1.7072 (1.4247)	grad_norm 27.4618 (29.2302)	mem 4879MB
[2022-05-31 02:51:05 MetaFG_0] (main.py 265): INFO Train: [32/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2997 (0.3070)	loss 1.5968 (1.4254)	grad_norm 34.9143 (29.3993)	mem 4879MB
[2022-05-31 02:51:08 MetaFG_0] (main.py 265): INFO Train: [32/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2928 (0.3071)	loss 1.5924 (1.4238)	grad_norm 17.2570 (29.4162)	mem 4879MB
[2022-05-31 02:51:11 MetaFG_0] (main.py 265): INFO Train: [32/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2924 (0.3070)	loss 1.1074 (1.4266)	grad_norm 30.8715 (29.3679)	mem 4879MB
[2022-05-31 02:51:14 MetaFG_0] (main.py 265): INFO Train: [32/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3004 (0.3070)	loss 1.5914 (1.4282)	grad_norm 14.8350 (29.3447)	mem 4879MB
[2022-05-31 02:51:17 MetaFG_0] (main.py 265): INFO Train: [32/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.3008 (0.3070)	loss 1.2850 (1.4286)	grad_norm 21.8924 (29.3994)	mem 4879MB
[2022-05-31 02:51:20 MetaFG_0] (main.py 265): INFO Train: [32/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2933 (0.3070)	loss 1.6066 (1.4336)	grad_norm 25.4307 (29.3496)	mem 4879MB
[2022-05-31 02:51:23 MetaFG_0] (main.py 265): INFO Train: [32/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2925 (0.3069)	loss 1.3863 (1.4361)	grad_norm 21.4785 (29.4609)	mem 4879MB
[2022-05-31 02:51:26 MetaFG_0] (main.py 265): INFO Train: [32/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.3045 (0.3069)	loss 1.5161 (1.4377)	grad_norm 25.5979 (29.5386)	mem 4879MB
[2022-05-31 02:51:29 MetaFG_0] (main.py 265): INFO Train: [32/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2978 (0.3068)	loss 1.4749 (1.4385)	grad_norm 19.8400 (29.5027)	mem 4879MB
[2022-05-31 02:51:32 MetaFG_0] (main.py 265): INFO Train: [32/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2993 (0.3069)	loss 1.1293 (1.4391)	grad_norm 23.8034 (29.5015)	mem 4879MB
[2022-05-31 02:51:35 MetaFG_0] (main.py 265): INFO Train: [32/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.3003 (0.3068)	loss 1.4649 (1.4400)	grad_norm 19.9607 (29.4202)	mem 4879MB
[2022-05-31 02:51:39 MetaFG_0] (main.py 265): INFO Train: [32/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2924 (0.3068)	loss 1.6431 (1.4426)	grad_norm 24.8860 (29.4587)	mem 4879MB
[2022-05-31 02:51:42 MetaFG_0] (main.py 265): INFO Train: [32/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2995 (0.3067)	loss 1.3985 (1.4437)	grad_norm 25.8241 (29.3511)	mem 4879MB
[2022-05-31 02:51:45 MetaFG_0] (main.py 265): INFO Train: [32/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2980 (0.3067)	loss 1.6295 (1.4446)	grad_norm 27.8116 (29.3354)	mem 4879MB
[2022-05-31 02:51:48 MetaFG_0] (main.py 265): INFO Train: [32/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2932 (0.3066)	loss 1.4864 (1.4442)	grad_norm 17.5913 (29.3553)	mem 4879MB
[2022-05-31 02:51:51 MetaFG_0] (main.py 265): INFO Train: [32/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2925 (0.3065)	loss 1.6762 (1.4450)	grad_norm 41.0759 (29.4021)	mem 4879MB
[2022-05-31 02:51:54 MetaFG_0] (main.py 265): INFO Train: [32/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2988 (0.3065)	loss 1.6026 (1.4449)	grad_norm 31.2553 (29.3854)	mem 4879MB
[2022-05-31 02:51:57 MetaFG_0] (main.py 265): INFO Train: [32/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2937 (0.3064)	loss 1.5736 (1.4424)	grad_norm 26.7848 (29.3406)	mem 4879MB
[2022-05-31 02:52:00 MetaFG_0] (main.py 265): INFO Train: [32/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2979 (0.3064)	loss 1.7546 (1.4443)	grad_norm 28.5646 (29.4147)	mem 4879MB
[2022-05-31 02:52:03 MetaFG_0] (main.py 265): INFO Train: [32/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2936 (0.3063)	loss 1.5672 (1.4458)	grad_norm 16.1386 (29.4386)	mem 4879MB
[2022-05-31 02:52:06 MetaFG_0] (main.py 265): INFO Train: [32/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2931 (0.3063)	loss 1.4456 (1.4466)	grad_norm 36.1085 (29.4515)	mem 4879MB
[2022-05-31 02:52:09 MetaFG_0] (main.py 265): INFO Train: [32/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2942 (0.3063)	loss 1.4070 (1.4473)	grad_norm 35.6368 (29.5682)	mem 4879MB
[2022-05-31 02:52:12 MetaFG_0] (main.py 265): INFO Train: [32/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2988 (0.3063)	loss 1.5178 (1.4482)	grad_norm 33.9798 (29.5958)	mem 4879MB
[2022-05-31 02:52:15 MetaFG_0] (main.py 265): INFO Train: [32/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2932 (0.3063)	loss 1.7710 (1.4488)	grad_norm 26.5829 (29.5884)	mem 4879MB
[2022-05-31 02:52:18 MetaFG_0] (main.py 265): INFO Train: [32/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2923 (0.3062)	loss 1.1758 (1.4489)	grad_norm 25.4746 (29.7166)	mem 4879MB
[2022-05-31 02:52:21 MetaFG_0] (main.py 265): INFO Train: [32/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2947 (0.3062)	loss 1.7971 (1.4477)	grad_norm 24.7492 (29.7068)	mem 4879MB
[2022-05-31 02:52:24 MetaFG_0] (main.py 265): INFO Train: [32/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2934 (0.3062)	loss 1.4242 (1.4471)	grad_norm 16.9161 (29.7514)	mem 4879MB
[2022-05-31 02:52:27 MetaFG_0] (main.py 265): INFO Train: [32/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2977 (0.3062)	loss 1.3942 (1.4458)	grad_norm 26.5933 (29.7285)	mem 4879MB
[2022-05-31 02:52:30 MetaFG_0] (main.py 265): INFO Train: [32/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2924 (0.3061)	loss 1.2175 (1.4439)	grad_norm 31.3143 (29.7540)	mem 4879MB
[2022-05-31 02:52:33 MetaFG_0] (main.py 265): INFO Train: [32/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2992 (0.3061)	loss 1.5425 (1.4451)	grad_norm 28.5005 (29.7981)	mem 4879MB
[2022-05-31 02:52:36 MetaFG_0] (main.py 265): INFO Train: [32/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2929 (0.3061)	loss 1.2741 (1.4425)	grad_norm 25.8218 (29.8329)	mem 4879MB
[2022-05-31 02:52:39 MetaFG_0] (main.py 265): INFO Train: [32/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2994 (0.3061)	loss 1.7126 (1.4433)	grad_norm 24.0303 (29.7716)	mem 4879MB
[2022-05-31 02:52:42 MetaFG_0] (main.py 265): INFO Train: [32/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2923 (0.3061)	loss 1.6339 (1.4446)	grad_norm 32.7968 (29.8206)	mem 4879MB
[2022-05-31 02:52:46 MetaFG_0] (main.py 265): INFO Train: [32/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2992 (0.3061)	loss 1.6626 (1.4442)	grad_norm 40.1521 (29.7637)	mem 4879MB
[2022-05-31 02:52:49 MetaFG_0] (main.py 265): INFO Train: [32/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2920 (0.3061)	loss 1.5177 (1.4455)	grad_norm 28.9424 (29.7994)	mem 4879MB
[2022-05-31 02:52:52 MetaFG_0] (main.py 265): INFO Train: [32/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2982 (0.3060)	loss 1.2166 (1.4455)	grad_norm 22.4562 (29.7604)	mem 4879MB
[2022-05-31 02:52:55 MetaFG_0] (main.py 265): INFO Train: [32/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2925 (0.3060)	loss 1.4807 (1.4455)	grad_norm 31.4184 (29.7448)	mem 4879MB
[2022-05-31 02:52:58 MetaFG_0] (main.py 265): INFO Train: [32/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2932 (0.3060)	loss 1.7346 (1.4471)	grad_norm 38.3855 (29.7331)	mem 4879MB
[2022-05-31 02:53:01 MetaFG_0] (main.py 265): INFO Train: [32/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2938 (0.3059)	loss 1.5978 (1.4477)	grad_norm 33.6212 (29.6959)	mem 4879MB
[2022-05-31 02:53:04 MetaFG_0] (main.py 265): INFO Train: [32/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2985 (0.3059)	loss 1.6577 (1.4476)	grad_norm 40.4957 (29.6949)	mem 4879MB
[2022-05-31 02:53:07 MetaFG_0] (main.py 265): INFO Train: [32/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3437 (0.3060)	loss 1.5246 (1.4459)	grad_norm 26.6234 (29.6971)	mem 4879MB
[2022-05-31 02:53:10 MetaFG_0] (main.py 265): INFO Train: [32/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2935 (0.3062)	loss 1.5363 (1.4441)	grad_norm 21.4772 (29.7088)	mem 4879MB
[2022-05-31 02:53:13 MetaFG_0] (main.py 265): INFO Train: [32/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2975 (0.3062)	loss 1.1922 (1.4434)	grad_norm 17.8550 (29.6443)	mem 4879MB
[2022-05-31 02:53:16 MetaFG_0] (main.py 265): INFO Train: [32/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2985 (0.3062)	loss 1.2750 (1.4423)	grad_norm 21.3957 (29.6405)	mem 4879MB
[2022-05-31 02:53:19 MetaFG_0] (main.py 265): INFO Train: [32/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.3007 (0.3062)	loss 1.2761 (1.4413)	grad_norm 51.0045 (29.6298)	mem 4879MB
[2022-05-31 02:53:22 MetaFG_0] (main.py 265): INFO Train: [32/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2990 (0.3062)	loss 1.5498 (1.4406)	grad_norm 21.2045 (29.6063)	mem 4879MB
[2022-05-31 02:53:25 MetaFG_0] (main.py 265): INFO Train: [32/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.3091 (0.3062)	loss 1.6374 (1.4419)	grad_norm 19.6357 (29.6206)	mem 4879MB
[2022-05-31 02:53:28 MetaFG_0] (main.py 265): INFO Train: [32/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2933 (0.3062)	loss 1.6422 (1.4410)	grad_norm 26.1597 (29.6205)	mem 4879MB
[2022-05-31 02:53:32 MetaFG_0] (main.py 265): INFO Train: [32/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2926 (0.3062)	loss 1.6267 (1.4427)	grad_norm 50.2598 (29.6344)	mem 4879MB
[2022-05-31 02:53:35 MetaFG_0] (main.py 265): INFO Train: [32/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2917 (0.3062)	loss 1.6617 (1.4438)	grad_norm 18.3130 (29.6525)	mem 4879MB
[2022-05-31 02:53:38 MetaFG_0] (main.py 265): INFO Train: [32/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3007 (0.3061)	loss 1.5395 (1.4445)	grad_norm 45.5782 (29.6591)	mem 4879MB
[2022-05-31 02:53:41 MetaFG_0] (main.py 265): INFO Train: [32/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2984 (0.3061)	loss 1.6695 (1.4454)	grad_norm 26.1132 (29.6596)	mem 4879MB
[2022-05-31 02:53:44 MetaFG_0] (main.py 265): INFO Train: [32/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2983 (0.3061)	loss 1.3170 (1.4457)	grad_norm 25.6089 (29.6561)	mem 4879MB
[2022-05-31 02:53:47 MetaFG_0] (main.py 265): INFO Train: [32/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2960 (0.3061)	loss 1.3464 (1.4469)	grad_norm 26.7633 (29.7405)	mem 4879MB
[2022-05-31 02:53:50 MetaFG_0] (main.py 265): INFO Train: [32/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2940 (0.3060)	loss 1.4803 (1.4469)	grad_norm 19.3389 (29.7185)	mem 4879MB
[2022-05-31 02:53:53 MetaFG_0] (main.py 265): INFO Train: [32/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2947 (0.3060)	loss 1.0397 (1.4471)	grad_norm 40.5122 (29.8538)	mem 4879MB
[2022-05-31 02:53:56 MetaFG_0] (main.py 265): INFO Train: [32/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2927 (0.3060)	loss 1.3659 (1.4480)	grad_norm 20.8434 (29.9191)	mem 4879MB
[2022-05-31 02:53:59 MetaFG_0] (main.py 265): INFO Train: [32/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2986 (0.3060)	loss 1.7410 (1.4488)	grad_norm 26.0805 (29.9255)	mem 4879MB
[2022-05-31 02:54:02 MetaFG_0] (main.py 265): INFO Train: [32/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2921 (0.3060)	loss 1.3264 (1.4485)	grad_norm 18.8674 (29.8530)	mem 4879MB
[2022-05-31 02:54:05 MetaFG_0] (main.py 265): INFO Train: [32/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2996 (0.3059)	loss 1.3307 (1.4490)	grad_norm 20.5779 (29.8268)	mem 4879MB
[2022-05-31 02:54:08 MetaFG_0] (main.py 265): INFO Train: [32/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2956 (0.3060)	loss 1.6207 (1.4489)	grad_norm 37.9370 (29.8543)	mem 4879MB
[2022-05-31 02:54:11 MetaFG_0] (main.py 265): INFO Train: [32/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2932 (0.3060)	loss 1.4385 (1.4493)	grad_norm 23.3690 (29.8162)	mem 4879MB
[2022-05-31 02:54:14 MetaFG_0] (main.py 265): INFO Train: [32/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2932 (0.3059)	loss 1.3502 (1.4495)	grad_norm 32.2892 (29.8753)	mem 4879MB
[2022-05-31 02:54:17 MetaFG_0] (main.py 265): INFO Train: [32/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2923 (0.3059)	loss 1.4037 (1.4493)	grad_norm 24.4189 (29.8512)	mem 4879MB
[2022-05-31 02:54:20 MetaFG_0] (main.py 265): INFO Train: [32/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2980 (0.3059)	loss 1.5907 (1.4483)	grad_norm 22.4495 (29.8229)	mem 4879MB
[2022-05-31 02:54:23 MetaFG_0] (main.py 265): INFO Train: [32/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2992 (0.3059)	loss 1.8240 (1.4483)	grad_norm 29.2644 (29.8108)	mem 4879MB
[2022-05-31 02:54:26 MetaFG_0] (main.py 265): INFO Train: [32/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2926 (0.3059)	loss 1.4848 (1.4472)	grad_norm 33.0804 (29.8007)	mem 4879MB
[2022-05-31 02:54:29 MetaFG_0] (main.py 265): INFO Train: [32/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2978 (0.3058)	loss 1.5051 (1.4470)	grad_norm 17.3918 (29.8041)	mem 4879MB
[2022-05-31 02:54:32 MetaFG_0] (main.py 265): INFO Train: [32/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2990 (0.3058)	loss 1.6472 (1.4468)	grad_norm 27.6011 (29.7674)	mem 4879MB
[2022-05-31 02:54:35 MetaFG_0] (main.py 265): INFO Train: [32/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2987 (0.3058)	loss 1.2264 (1.4477)	grad_norm 34.6120 (29.7523)	mem 4879MB
[2022-05-31 02:54:38 MetaFG_0] (main.py 265): INFO Train: [32/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2935 (0.3058)	loss 1.4529 (1.4472)	grad_norm 38.7467 (29.7537)	mem 4879MB
[2022-05-31 02:54:41 MetaFG_0] (main.py 265): INFO Train: [32/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2934 (0.3057)	loss 1.1492 (1.4475)	grad_norm 26.3552 (29.7475)	mem 4879MB
[2022-05-31 02:54:45 MetaFG_0] (main.py 265): INFO Train: [32/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2926 (0.3057)	loss 1.4273 (1.4474)	grad_norm 37.1315 (29.7751)	mem 4879MB
[2022-05-31 02:54:48 MetaFG_0] (main.py 265): INFO Train: [32/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2992 (0.3057)	loss 1.2107 (1.4472)	grad_norm 26.0922 (29.7549)	mem 4879MB
[2022-05-31 02:54:51 MetaFG_0] (main.py 265): INFO Train: [32/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2985 (0.3057)	loss 1.5083 (1.4478)	grad_norm 27.8224 (29.7539)	mem 4879MB
[2022-05-31 02:54:54 MetaFG_0] (main.py 265): INFO Train: [32/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2980 (0.3057)	loss 1.8449 (1.4477)	grad_norm 19.7561 (29.8149)	mem 4879MB
[2022-05-31 02:54:57 MetaFG_0] (main.py 265): INFO Train: [32/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2926 (0.3057)	loss 1.3205 (1.4478)	grad_norm 19.3255 (29.7760)	mem 4879MB
[2022-05-31 02:55:00 MetaFG_0] (main.py 265): INFO Train: [32/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2980 (0.3057)	loss 1.7026 (1.4478)	grad_norm 19.1681 (29.8221)	mem 4879MB
[2022-05-31 02:55:03 MetaFG_0] (main.py 265): INFO Train: [32/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.3016 (0.3057)	loss 1.6143 (1.4465)	grad_norm 23.2405 (29.8408)	mem 4879MB
[2022-05-31 02:55:06 MetaFG_0] (main.py 265): INFO Train: [32/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2990 (0.3057)	loss 1.4590 (1.4458)	grad_norm 23.1306 (29.8897)	mem 4879MB
[2022-05-31 02:55:09 MetaFG_0] (main.py 265): INFO Train: [32/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2921 (0.3057)	loss 1.5137 (1.4464)	grad_norm 20.9250 (29.8524)	mem 4879MB
[2022-05-31 02:55:12 MetaFG_0] (main.py 265): INFO Train: [32/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2923 (0.3057)	loss 1.5711 (1.4474)	grad_norm 40.6950 (29.9021)	mem 4879MB
[2022-05-31 02:55:15 MetaFG_0] (main.py 265): INFO Train: [32/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2927 (0.3057)	loss 1.0155 (1.4475)	grad_norm 36.8111 (29.9063)	mem 4879MB
[2022-05-31 02:55:18 MetaFG_0] (main.py 265): INFO Train: [32/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2921 (0.3057)	loss 1.5911 (1.4477)	grad_norm 18.5181 (29.9390)	mem 4879MB
[2022-05-31 02:55:21 MetaFG_0] (main.py 265): INFO Train: [32/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3002 (0.3057)	loss 1.4041 (1.4477)	grad_norm 28.2815 (29.9375)	mem 4879MB
[2022-05-31 02:55:24 MetaFG_0] (main.py 265): INFO Train: [32/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2982 (0.3057)	loss 1.8451 (1.4473)	grad_norm 28.7369 (29.9528)	mem 4879MB
[2022-05-31 02:55:27 MetaFG_0] (main.py 265): INFO Train: [32/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2991 (0.3057)	loss 0.9033 (1.4460)	grad_norm 24.6508 (29.9328)	mem 4879MB
[2022-05-31 02:55:30 MetaFG_0] (main.py 265): INFO Train: [32/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2932 (0.3057)	loss 0.9803 (1.4456)	grad_norm 27.9391 (29.9005)	mem 4879MB
[2022-05-31 02:55:33 MetaFG_0] (main.py 265): INFO Train: [32/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2978 (0.3057)	loss 1.4020 (1.4448)	grad_norm 16.4981 (29.8613)	mem 4879MB
[2022-05-31 02:55:36 MetaFG_0] (main.py 265): INFO Train: [32/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2920 (0.3057)	loss 1.7650 (1.4445)	grad_norm 18.0763 (29.8163)	mem 4879MB
[2022-05-31 02:55:39 MetaFG_0] (main.py 265): INFO Train: [32/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2927 (0.3056)	loss 1.6131 (1.4432)	grad_norm 24.5956 (29.8048)	mem 4879MB
[2022-05-31 02:55:42 MetaFG_0] (main.py 265): INFO Train: [32/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.3011 (0.3056)	loss 1.4820 (1.4427)	grad_norm 31.4437 (29.7765)	mem 4879MB
[2022-05-31 02:55:46 MetaFG_0] (main.py 265): INFO Train: [32/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2994 (0.3056)	loss 1.0825 (1.4424)	grad_norm 25.2238 (29.7882)	mem 4879MB
[2022-05-31 02:55:49 MetaFG_0] (main.py 265): INFO Train: [32/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2935 (0.3056)	loss 1.1180 (1.4424)	grad_norm 27.9015 (29.7938)	mem 4879MB
[2022-05-31 02:55:52 MetaFG_0] (main.py 265): INFO Train: [32/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2982 (0.3056)	loss 1.5529 (1.4434)	grad_norm 31.5920 (29.7976)	mem 4879MB
[2022-05-31 02:55:55 MetaFG_0] (main.py 265): INFO Train: [32/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2996 (0.3056)	loss 1.7208 (1.4434)	grad_norm 31.6523 (29.8066)	mem 4879MB
[2022-05-31 02:55:58 MetaFG_0] (main.py 265): INFO Train: [32/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.3050 (0.3056)	loss 1.2029 (1.4427)	grad_norm 31.0109 (29.8005)	mem 4879MB
[2022-05-31 02:56:01 MetaFG_0] (main.py 265): INFO Train: [32/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2987 (0.3056)	loss 1.5391 (1.4414)	grad_norm 21.0540 (29.7785)	mem 4879MB
[2022-05-31 02:56:04 MetaFG_0] (main.py 265): INFO Train: [32/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2975 (0.3056)	loss 1.4849 (1.4416)	grad_norm 19.8426 (29.7553)	mem 4879MB
[2022-05-31 02:56:07 MetaFG_0] (main.py 265): INFO Train: [32/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2998 (0.3056)	loss 1.4712 (1.4420)	grad_norm 40.3458 (29.7477)	mem 4879MB
[2022-05-31 02:56:10 MetaFG_0] (main.py 265): INFO Train: [32/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2985 (0.3056)	loss 1.2683 (1.4414)	grad_norm 37.2270 (29.7626)	mem 4879MB
[2022-05-31 02:56:13 MetaFG_0] (main.py 265): INFO Train: [32/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2925 (0.3056)	loss 1.4367 (1.4419)	grad_norm 28.2491 (29.8449)	mem 4879MB
[2022-05-31 02:56:16 MetaFG_0] (main.py 265): INFO Train: [32/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3001 (0.3056)	loss 1.5669 (1.4421)	grad_norm 27.6680 (29.8752)	mem 4879MB
[2022-05-31 02:56:19 MetaFG_0] (main.py 265): INFO Train: [32/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2944 (0.3056)	loss 1.5232 (1.4416)	grad_norm 38.4498 (29.9269)	mem 4879MB
[2022-05-31 02:56:22 MetaFG_0] (main.py 265): INFO Train: [32/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2991 (0.3056)	loss 1.5669 (1.4419)	grad_norm 35.8708 (29.9226)	mem 4879MB
[2022-05-31 02:56:25 MetaFG_0] (main.py 265): INFO Train: [32/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2922 (0.3056)	loss 1.4225 (1.4417)	grad_norm 25.9935 (29.8915)	mem 4879MB
[2022-05-31 02:56:28 MetaFG_0] (main.py 265): INFO Train: [32/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2922 (0.3056)	loss 1.5552 (1.4417)	grad_norm 26.7323 (29.8874)	mem 4879MB
[2022-05-31 02:56:31 MetaFG_0] (main.py 265): INFO Train: [32/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2959 (0.3056)	loss 1.6394 (1.4423)	grad_norm 18.6167 (29.8620)	mem 4879MB
[2022-05-31 02:56:34 MetaFG_0] (main.py 265): INFO Train: [32/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2984 (0.3055)	loss 1.2696 (1.4424)	grad_norm 40.2527 (29.8540)	mem 4879MB
[2022-05-31 02:56:37 MetaFG_0] (main.py 265): INFO Train: [32/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2943 (0.3055)	loss 1.4918 (1.4431)	grad_norm 26.4287 (29.8361)	mem 4879MB
[2022-05-31 02:56:40 MetaFG_0] (main.py 265): INFO Train: [32/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2989 (0.3055)	loss 1.1838 (1.4426)	grad_norm 26.3439 (29.8187)	mem 4879MB
[2022-05-31 02:56:43 MetaFG_0] (main.py 265): INFO Train: [32/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2922 (0.3055)	loss 1.5144 (1.4426)	grad_norm 22.1573 (29.8583)	mem 4879MB
[2022-05-31 02:56:46 MetaFG_0] (main.py 265): INFO Train: [32/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2925 (0.3055)	loss 1.4756 (1.4420)	grad_norm 14.8667 (29.8324)	mem 4879MB
[2022-05-31 02:56:50 MetaFG_0] (main.py 265): INFO Train: [32/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2998 (0.3055)	loss 1.6125 (1.4407)	grad_norm 43.0169 (29.8359)	mem 4879MB
[2022-05-31 02:56:53 MetaFG_0] (main.py 265): INFO Train: [32/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2940 (0.3055)	loss 1.6369 (1.4413)	grad_norm 18.4384 (29.8011)	mem 4879MB
[2022-05-31 02:56:56 MetaFG_0] (main.py 265): INFO Train: [32/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3023 (0.3055)	loss 1.4751 (1.4411)	grad_norm 36.2979 (29.7965)	mem 4879MB
[2022-05-31 02:56:59 MetaFG_0] (main.py 265): INFO Train: [32/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.3032 (0.3055)	loss 1.0589 (1.4413)	grad_norm 33.6061 (29.8077)	mem 4879MB
[2022-05-31 02:57:02 MetaFG_0] (main.py 265): INFO Train: [32/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3066 (0.3055)	loss 1.6285 (1.4414)	grad_norm 16.4615 (29.7943)	mem 4879MB
[2022-05-31 02:57:05 MetaFG_0] (main.py 265): INFO Train: [32/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2925 (0.3055)	loss 1.3203 (1.4419)	grad_norm 26.2367 (29.7641)	mem 4879MB
[2022-05-31 02:57:08 MetaFG_0] (main.py 265): INFO Train: [32/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2928 (0.3055)	loss 1.4328 (1.4424)	grad_norm 43.3823 (29.7667)	mem 4879MB
[2022-05-31 02:57:08 MetaFG_0] (main.py 272): INFO EPOCH 32 training takes 0:07:57
[2022-05-31 02:57:08 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_32.pth saving......
[2022-05-31 02:57:09 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_32.pth saved !!!
[2022-05-31 02:57:09 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 02:57:10 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 02:57:10 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 02:57:11 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.607 (0.607)	Loss 0.7433 (0.7433)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 02:57:12 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.113 (0.146)	Loss 0.7521 (0.7603)	Acc@1 87.500 (82.670)	Acc@5 96.875 (98.864)	Mem 4879MB
[2022-05-31 02:57:13 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.121)	Loss 0.8484 (0.7365)	Acc@1 78.125 (84.524)	Acc@5 100.000 (98.363)	Mem 4879MB
[2022-05-31 02:57:14 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.112)	Loss 0.7373 (0.7374)	Acc@1 84.375 (84.879)	Acc@5 100.000 (98.085)	Mem 4879MB
[2022-05-31 02:57:15 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.108)	Loss 0.5365 (0.7305)	Acc@1 93.750 (84.070)	Acc@5 100.000 (98.247)	Mem 4879MB
[2022-05-31 02:57:16 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.106)	Loss 0.7313 (0.7318)	Acc@1 81.250 (83.885)	Acc@5 96.875 (98.284)	Mem 4879MB
[2022-05-31 02:57:17 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.104)	Loss 0.7414 (0.7397)	Acc@1 81.250 (83.607)	Acc@5 96.875 (98.053)	Mem 4879MB
[2022-05-31 02:57:18 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.102)	Loss 0.4439 (0.7415)	Acc@1 90.625 (83.363)	Acc@5 100.000 (98.063)	Mem 4879MB
[2022-05-31 02:57:19 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.092 (0.101)	Loss 0.4389 (0.7360)	Acc@1 96.875 (83.835)	Acc@5 100.000 (98.110)	Mem 4879MB
[2022-05-31 02:57:20 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.088 (0.100)	Loss 0.9143 (0.7432)	Acc@1 71.875 (83.448)	Acc@5 100.000 (98.111)	Mem 4879MB
[2022-05-31 02:57:21 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 0.7270 (0.7394)	Acc@1 84.375 (83.447)	Acc@5 96.875 (98.144)	Mem 4879MB
[2022-05-31 02:57:21 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.099)	Loss 0.8139 (0.7445)	Acc@1 81.250 (83.193)	Acc@5 96.875 (98.142)	Mem 4879MB
[2022-05-31 02:57:22 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.088 (0.098)	Loss 1.1265 (0.7478)	Acc@1 71.875 (83.187)	Acc@5 96.875 (98.115)	Mem 4879MB
[2022-05-31 02:57:23 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.098)	Loss 0.9038 (0.7458)	Acc@1 84.375 (83.182)	Acc@5 100.000 (98.092)	Mem 4879MB
[2022-05-31 02:57:24 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.099 (0.098)	Loss 0.5320 (0.7452)	Acc@1 87.500 (83.112)	Acc@5 100.000 (98.072)	Mem 4879MB
[2022-05-31 02:57:25 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.100 (0.097)	Loss 0.9519 (0.7442)	Acc@1 81.250 (83.133)	Acc@5 96.875 (98.096)	Mem 4879MB
[2022-05-31 02:57:26 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.097)	Loss 0.8719 (0.7457)	Acc@1 75.000 (82.997)	Acc@5 100.000 (98.137)	Mem 4879MB
[2022-05-31 02:57:27 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.097)	Loss 0.8224 (0.7441)	Acc@1 78.125 (83.023)	Acc@5 100.000 (98.136)	Mem 4879MB
[2022-05-31 02:57:28 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.097)	Loss 0.7588 (0.7424)	Acc@1 78.125 (83.011)	Acc@5 100.000 (98.135)	Mem 4879MB
[2022-05-31 02:57:29 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.097)	Loss 1.2508 (0.7480)	Acc@1 71.875 (82.870)	Acc@5 90.625 (98.069)	Mem 4879MB
[2022-05-31 02:57:30 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.096)	Loss 0.9557 (0.7472)	Acc@1 75.000 (82.882)	Acc@5 93.750 (98.057)	Mem 4879MB
[2022-05-31 02:57:31 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.098 (0.096)	Loss 0.8114 (0.7462)	Acc@1 84.375 (82.909)	Acc@5 96.875 (98.060)	Mem 4879MB
[2022-05-31 02:57:32 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.099 (0.096)	Loss 0.7386 (0.7421)	Acc@1 75.000 (83.032)	Acc@5 100.000 (98.063)	Mem 4879MB
[2022-05-31 02:57:33 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.096)	Loss 0.5340 (0.7459)	Acc@1 96.875 (82.887)	Acc@5 100.000 (98.052)	Mem 4879MB
[2022-05-31 02:57:34 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 0.6639 (0.7433)	Acc@1 81.250 (83.026)	Acc@5 100.000 (98.081)	Mem 4879MB
[2022-05-31 02:57:35 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.096)	Loss 0.8823 (0.7450)	Acc@1 81.250 (83.030)	Acc@5 100.000 (98.070)	Mem 4879MB
[2022-05-31 02:57:35 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 1.0107 (0.7476)	Acc@1 81.250 (82.938)	Acc@5 90.625 (98.012)	Mem 4879MB
[2022-05-31 02:57:36 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.099 (0.096)	Loss 0.8012 (0.7478)	Acc@1 71.875 (82.876)	Acc@5 100.000 (98.028)	Mem 4879MB
[2022-05-31 02:57:37 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.088 (0.096)	Loss 0.7367 (0.7484)	Acc@1 81.250 (82.896)	Acc@5 100.000 (98.020)	Mem 4879MB
[2022-05-31 02:57:38 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.096)	Loss 0.7223 (0.7497)	Acc@1 87.500 (82.904)	Acc@5 100.000 (98.024)	Mem 4879MB
[2022-05-31 02:57:39 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.118 (0.096)	Loss 0.6738 (0.7493)	Acc@1 81.250 (82.870)	Acc@5 100.000 (98.069)	Mem 4879MB
[2022-05-31 02:57:41 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.8716 (0.7505)	Acc@1 87.500 (82.858)	Acc@5 93.750 (98.051)	Mem 4879MB
[2022-05-31 02:57:41 MetaFG_0] (main.py 330): INFO  * Acc@1 82.840 Acc@5 98.050
[2022-05-31 02:57:41 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 82.8%
[2022-05-31 02:57:41 MetaFG_0] (main.py 171): INFO Max accuracy: 82.95%
[2022-05-31 02:57:42 MetaFG_0] (main.py 265): INFO Train: [33/300][0/1562]	eta 0:24:20 lr 0.000006	time 0.9349 (0.9349)	loss 1.4509 (1.4509)	grad_norm 40.3148 (40.3148)	mem 4879MB
[2022-05-31 02:57:45 MetaFG_0] (main.py 265): INFO Train: [33/300][10/1562]	eta 0:09:37 lr 0.000006	time 0.2937 (0.3723)	loss 1.5853 (1.5004)	grad_norm 26.9693 (27.0021)	mem 4879MB
[2022-05-31 02:57:48 MetaFG_0] (main.py 265): INFO Train: [33/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2995 (0.3403)	loss 1.2368 (1.4295)	grad_norm 23.5776 (28.6211)	mem 4879MB
[2022-05-31 02:57:51 MetaFG_0] (main.py 265): INFO Train: [33/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2982 (0.3294)	loss 1.7792 (1.4196)	grad_norm 19.6019 (27.6445)	mem 4879MB
[2022-05-31 02:57:54 MetaFG_0] (main.py 265): INFO Train: [33/300][40/1562]	eta 0:08:11 lr 0.000006	time 0.2959 (0.3231)	loss 1.0222 (1.4082)	grad_norm 42.1294 (28.0274)	mem 4879MB
[2022-05-31 02:57:57 MetaFG_0] (main.py 265): INFO Train: [33/300][50/1562]	eta 0:08:02 lr 0.000006	time 0.2989 (0.3191)	loss 1.3010 (1.4139)	grad_norm 19.3365 (28.5742)	mem 4879MB
[2022-05-31 02:58:00 MetaFG_0] (main.py 265): INFO Train: [33/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2993 (0.3168)	loss 1.8336 (1.4301)	grad_norm 28.9599 (28.4699)	mem 4879MB
[2022-05-31 02:58:03 MetaFG_0] (main.py 265): INFO Train: [33/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2986 (0.3153)	loss 1.4385 (1.4434)	grad_norm 31.1160 (28.5357)	mem 4879MB
[2022-05-31 02:58:06 MetaFG_0] (main.py 265): INFO Train: [33/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2935 (0.3141)	loss 1.2114 (1.4324)	grad_norm 23.5354 (28.1087)	mem 4879MB
[2022-05-31 02:58:09 MetaFG_0] (main.py 265): INFO Train: [33/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2945 (0.3133)	loss 1.3936 (1.4362)	grad_norm 24.2872 (28.0396)	mem 4879MB
[2022-05-31 02:58:12 MetaFG_0] (main.py 265): INFO Train: [33/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2935 (0.3124)	loss 1.1420 (1.4447)	grad_norm 20.6985 (27.7686)	mem 4879MB
[2022-05-31 02:58:15 MetaFG_0] (main.py 265): INFO Train: [33/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2991 (0.3120)	loss 1.4461 (1.4370)	grad_norm 20.6534 (27.5676)	mem 4879MB
[2022-05-31 02:58:18 MetaFG_0] (main.py 265): INFO Train: [33/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2974 (0.3113)	loss 1.6703 (1.4381)	grad_norm 31.0945 (27.6074)	mem 4879MB
[2022-05-31 02:58:22 MetaFG_0] (main.py 265): INFO Train: [33/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2980 (0.3108)	loss 1.6656 (1.4431)	grad_norm 24.3631 (27.2805)	mem 4879MB
[2022-05-31 02:58:25 MetaFG_0] (main.py 265): INFO Train: [33/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2934 (0.3105)	loss 1.5928 (1.4447)	grad_norm 19.0899 (27.4764)	mem 4879MB
[2022-05-31 02:58:28 MetaFG_0] (main.py 265): INFO Train: [33/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2926 (0.3100)	loss 1.7077 (1.4388)	grad_norm 29.5971 (27.8550)	mem 4879MB
[2022-05-31 02:58:31 MetaFG_0] (main.py 265): INFO Train: [33/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2925 (0.3097)	loss 1.4954 (1.4337)	grad_norm 49.7024 (28.1880)	mem 4879MB
[2022-05-31 02:58:34 MetaFG_0] (main.py 265): INFO Train: [33/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2987 (0.3094)	loss 1.3213 (1.4301)	grad_norm 26.5063 (28.3575)	mem 4879MB
[2022-05-31 02:58:37 MetaFG_0] (main.py 265): INFO Train: [33/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2994 (0.3092)	loss 1.5094 (1.4306)	grad_norm 23.6392 (28.4500)	mem 4879MB
[2022-05-31 02:58:40 MetaFG_0] (main.py 265): INFO Train: [33/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2919 (0.3089)	loss 1.3640 (1.4259)	grad_norm 20.7095 (28.2606)	mem 4879MB
[2022-05-31 02:58:43 MetaFG_0] (main.py 265): INFO Train: [33/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2932 (0.3087)	loss 1.4666 (1.4342)	grad_norm 25.9665 (28.4056)	mem 4879MB
[2022-05-31 02:58:46 MetaFG_0] (main.py 265): INFO Train: [33/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2934 (0.3084)	loss 1.6010 (1.4294)	grad_norm 31.1402 (28.4505)	mem 4879MB
[2022-05-31 02:58:49 MetaFG_0] (main.py 265): INFO Train: [33/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2934 (0.3083)	loss 1.6629 (1.4262)	grad_norm 23.4728 (28.4613)	mem 4879MB
[2022-05-31 02:58:52 MetaFG_0] (main.py 265): INFO Train: [33/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2983 (0.3081)	loss 1.4051 (1.4250)	grad_norm 21.6680 (28.5745)	mem 4879MB
[2022-05-31 02:58:55 MetaFG_0] (main.py 265): INFO Train: [33/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2940 (0.3080)	loss 1.5404 (1.4255)	grad_norm 26.6563 (28.5498)	mem 4879MB
[2022-05-31 02:58:58 MetaFG_0] (main.py 265): INFO Train: [33/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2930 (0.3078)	loss 1.2745 (1.4231)	grad_norm 30.6849 (28.4416)	mem 4879MB
[2022-05-31 02:59:01 MetaFG_0] (main.py 265): INFO Train: [33/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2919 (0.3077)	loss 1.5470 (1.4252)	grad_norm 29.5800 (28.5483)	mem 4879MB
[2022-05-31 02:59:04 MetaFG_0] (main.py 265): INFO Train: [33/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2916 (0.3075)	loss 1.2846 (1.4232)	grad_norm 33.5671 (28.6143)	mem 4879MB
[2022-05-31 02:59:07 MetaFG_0] (main.py 265): INFO Train: [33/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2943 (0.3074)	loss 1.6722 (1.4248)	grad_norm 41.3962 (28.6145)	mem 4879MB
[2022-05-31 02:59:10 MetaFG_0] (main.py 265): INFO Train: [33/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2928 (0.3073)	loss 1.3664 (1.4224)	grad_norm 34.3749 (28.7129)	mem 4879MB
[2022-05-31 02:59:13 MetaFG_0] (main.py 265): INFO Train: [33/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.3011 (0.3072)	loss 1.5349 (1.4230)	grad_norm 21.5267 (28.6000)	mem 4879MB
[2022-05-31 02:59:16 MetaFG_0] (main.py 265): INFO Train: [33/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2939 (0.3071)	loss 1.0295 (1.4201)	grad_norm 19.3168 (28.7835)	mem 4879MB
[2022-05-31 02:59:19 MetaFG_0] (main.py 265): INFO Train: [33/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2988 (0.3070)	loss 1.3091 (1.4199)	grad_norm 46.0946 (28.7802)	mem 4879MB
[2022-05-31 02:59:22 MetaFG_0] (main.py 265): INFO Train: [33/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2990 (0.3070)	loss 1.1994 (1.4172)	grad_norm 30.2475 (28.8166)	mem 4879MB
[2022-05-31 02:59:25 MetaFG_0] (main.py 265): INFO Train: [33/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2939 (0.3070)	loss 1.6809 (1.4202)	grad_norm 44.1821 (28.8043)	mem 4879MB
[2022-05-31 02:59:28 MetaFG_0] (main.py 265): INFO Train: [33/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2937 (0.3068)	loss 1.4867 (1.4228)	grad_norm 26.7150 (28.8498)	mem 4879MB
[2022-05-31 02:59:32 MetaFG_0] (main.py 265): INFO Train: [33/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2954 (0.3067)	loss 1.5681 (1.4210)	grad_norm 27.3738 (28.8689)	mem 4879MB
[2022-05-31 02:59:35 MetaFG_0] (main.py 265): INFO Train: [33/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2921 (0.3066)	loss 1.0942 (1.4241)	grad_norm 23.2712 (28.7282)	mem 4879MB
[2022-05-31 02:59:38 MetaFG_0] (main.py 265): INFO Train: [33/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2926 (0.3066)	loss 1.3302 (1.4257)	grad_norm 29.7350 (28.8539)	mem 4879MB
[2022-05-31 02:59:41 MetaFG_0] (main.py 265): INFO Train: [33/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.3005 (0.3066)	loss 1.5908 (1.4272)	grad_norm 39.2256 (28.9876)	mem 4879MB
[2022-05-31 02:59:44 MetaFG_0] (main.py 265): INFO Train: [33/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3014 (0.3065)	loss 0.8953 (1.4276)	grad_norm 35.8669 (28.9027)	mem 4879MB
[2022-05-31 02:59:47 MetaFG_0] (main.py 265): INFO Train: [33/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2920 (0.3065)	loss 1.7635 (1.4275)	grad_norm 55.2528 (29.0286)	mem 4879MB
[2022-05-31 02:59:50 MetaFG_0] (main.py 265): INFO Train: [33/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2926 (0.3065)	loss 1.5570 (1.4296)	grad_norm 26.7973 (29.0324)	mem 4879MB
[2022-05-31 02:59:53 MetaFG_0] (main.py 265): INFO Train: [33/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2937 (0.3064)	loss 1.6611 (1.4303)	grad_norm 35.6229 (28.9660)	mem 4879MB
[2022-05-31 02:59:56 MetaFG_0] (main.py 265): INFO Train: [33/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2935 (0.3064)	loss 1.1257 (1.4276)	grad_norm 32.0655 (29.0479)	mem 4879MB
[2022-05-31 02:59:59 MetaFG_0] (main.py 265): INFO Train: [33/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2930 (0.3063)	loss 1.4060 (1.4301)	grad_norm 37.3470 (29.0842)	mem 4879MB
[2022-05-31 03:00:02 MetaFG_0] (main.py 265): INFO Train: [33/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2918 (0.3063)	loss 1.0870 (1.4301)	grad_norm 35.0576 (29.1098)	mem 4879MB
[2022-05-31 03:00:05 MetaFG_0] (main.py 265): INFO Train: [33/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2942 (0.3062)	loss 1.7219 (1.4314)	grad_norm 15.1312 (29.0721)	mem 4879MB
[2022-05-31 03:00:08 MetaFG_0] (main.py 265): INFO Train: [33/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2925 (0.3062)	loss 1.4948 (1.4340)	grad_norm 16.2030 (29.0263)	mem 4879MB
[2022-05-31 03:00:11 MetaFG_0] (main.py 265): INFO Train: [33/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2992 (0.3061)	loss 1.7013 (1.4319)	grad_norm 21.3753 (29.0023)	mem 4879MB
[2022-05-31 03:00:14 MetaFG_0] (main.py 265): INFO Train: [33/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2920 (0.3061)	loss 1.3509 (1.4333)	grad_norm 36.0938 (29.0521)	mem 4879MB
[2022-05-31 03:00:17 MetaFG_0] (main.py 265): INFO Train: [33/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2997 (0.3061)	loss 1.3656 (1.4316)	grad_norm 26.1593 (29.0182)	mem 4879MB
[2022-05-31 03:00:20 MetaFG_0] (main.py 265): INFO Train: [33/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2921 (0.3060)	loss 1.6035 (1.4313)	grad_norm 36.8569 (29.1009)	mem 4879MB
[2022-05-31 03:00:23 MetaFG_0] (main.py 265): INFO Train: [33/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2921 (0.3060)	loss 1.6219 (1.4315)	grad_norm 44.2163 (29.1288)	mem 4879MB
[2022-05-31 03:00:26 MetaFG_0] (main.py 265): INFO Train: [33/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2981 (0.3060)	loss 1.1869 (1.4321)	grad_norm 22.5414 (29.1883)	mem 4879MB
[2022-05-31 03:00:29 MetaFG_0] (main.py 265): INFO Train: [33/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2955 (0.3060)	loss 1.6555 (1.4331)	grad_norm 33.7864 (29.1761)	mem 4879MB
[2022-05-31 03:00:32 MetaFG_0] (main.py 265): INFO Train: [33/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2985 (0.3060)	loss 1.3611 (1.4317)	grad_norm 31.6553 (29.1099)	mem 4879MB
[2022-05-31 03:00:35 MetaFG_0] (main.py 265): INFO Train: [33/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.3016 (0.3060)	loss 1.4670 (1.4334)	grad_norm 28.5428 (29.0647)	mem 4879MB
[2022-05-31 03:00:39 MetaFG_0] (main.py 265): INFO Train: [33/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2977 (0.3059)	loss 1.5449 (1.4361)	grad_norm 20.8055 (29.0625)	mem 4879MB
[2022-05-31 03:00:42 MetaFG_0] (main.py 265): INFO Train: [33/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2975 (0.3059)	loss 1.5525 (1.4383)	grad_norm 26.1276 (29.0725)	mem 4879MB
[2022-05-31 03:00:45 MetaFG_0] (main.py 265): INFO Train: [33/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2920 (0.3059)	loss 1.7443 (1.4394)	grad_norm 14.7600 (29.0785)	mem 4879MB
[2022-05-31 03:00:48 MetaFG_0] (main.py 265): INFO Train: [33/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2987 (0.3059)	loss 1.5661 (1.4399)	grad_norm 32.6189 (29.0187)	mem 4879MB
[2022-05-31 03:00:51 MetaFG_0] (main.py 265): INFO Train: [33/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2938 (0.3058)	loss 1.3259 (1.4407)	grad_norm 44.0462 (29.0726)	mem 4879MB
[2022-05-31 03:00:54 MetaFG_0] (main.py 265): INFO Train: [33/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3027 (0.3058)	loss 1.2633 (1.4405)	grad_norm 17.8508 (29.0254)	mem 4879MB
[2022-05-31 03:00:57 MetaFG_0] (main.py 265): INFO Train: [33/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2947 (0.3057)	loss 1.1269 (1.4404)	grad_norm 44.2024 (29.0976)	mem 4879MB
[2022-05-31 03:01:00 MetaFG_0] (main.py 265): INFO Train: [33/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2986 (0.3057)	loss 1.5236 (1.4402)	grad_norm 18.9837 (29.0529)	mem 4879MB
[2022-05-31 03:01:03 MetaFG_0] (main.py 265): INFO Train: [33/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2986 (0.3057)	loss 1.4408 (1.4402)	grad_norm 26.3514 (29.1153)	mem 4879MB
[2022-05-31 03:01:06 MetaFG_0] (main.py 265): INFO Train: [33/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.3015 (0.3057)	loss 1.2141 (1.4398)	grad_norm 36.5471 (29.1607)	mem 4879MB
[2022-05-31 03:01:09 MetaFG_0] (main.py 265): INFO Train: [33/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2935 (0.3057)	loss 1.6421 (1.4414)	grad_norm 25.3510 (29.1462)	mem 4879MB
[2022-05-31 03:01:12 MetaFG_0] (main.py 265): INFO Train: [33/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2925 (0.3057)	loss 1.6461 (1.4427)	grad_norm 34.3797 (29.1912)	mem 4879MB
[2022-05-31 03:01:15 MetaFG_0] (main.py 265): INFO Train: [33/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2932 (0.3057)	loss 1.6117 (1.4436)	grad_norm 40.8128 (29.1743)	mem 4879MB
[2022-05-31 03:01:18 MetaFG_0] (main.py 265): INFO Train: [33/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2922 (0.3057)	loss 1.0907 (1.4440)	grad_norm 30.7268 (29.2581)	mem 4879MB
[2022-05-31 03:01:21 MetaFG_0] (main.py 265): INFO Train: [33/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2930 (0.3056)	loss 1.3847 (1.4439)	grad_norm 23.6984 (29.2190)	mem 4879MB
[2022-05-31 03:01:24 MetaFG_0] (main.py 265): INFO Train: [33/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2991 (0.3056)	loss 1.4236 (1.4438)	grad_norm 24.1929 (29.2442)	mem 4879MB
[2022-05-31 03:01:27 MetaFG_0] (main.py 265): INFO Train: [33/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2934 (0.3055)	loss 1.7012 (1.4440)	grad_norm 23.1211 (29.2314)	mem 4879MB
[2022-05-31 03:01:30 MetaFG_0] (main.py 265): INFO Train: [33/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2934 (0.3055)	loss 1.1321 (1.4425)	grad_norm 41.9277 (29.3190)	mem 4879MB
[2022-05-31 03:01:33 MetaFG_0] (main.py 265): INFO Train: [33/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2990 (0.3055)	loss 1.4851 (1.4433)	grad_norm 25.6497 (29.3072)	mem 4879MB
[2022-05-31 03:01:36 MetaFG_0] (main.py 265): INFO Train: [33/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2953 (0.3055)	loss 1.2713 (1.4437)	grad_norm 29.5712 (29.3171)	mem 4879MB
[2022-05-31 03:01:39 MetaFG_0] (main.py 265): INFO Train: [33/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2934 (0.3055)	loss 1.6467 (1.4441)	grad_norm 25.6007 (29.2874)	mem 4879MB
[2022-05-31 03:01:42 MetaFG_0] (main.py 265): INFO Train: [33/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2925 (0.3055)	loss 1.2928 (1.4436)	grad_norm 31.0787 (29.2480)	mem 4879MB
[2022-05-31 03:01:45 MetaFG_0] (main.py 265): INFO Train: [33/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2955 (0.3055)	loss 1.6627 (1.4428)	grad_norm 23.6873 (29.2886)	mem 4879MB
[2022-05-31 03:01:49 MetaFG_0] (main.py 265): INFO Train: [33/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2935 (0.3055)	loss 1.2949 (1.4409)	grad_norm 25.1025 (29.2944)	mem 4879MB
[2022-05-31 03:01:52 MetaFG_0] (main.py 265): INFO Train: [33/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2981 (0.3055)	loss 1.3641 (1.4413)	grad_norm 35.4572 (29.2500)	mem 4879MB
[2022-05-31 03:01:55 MetaFG_0] (main.py 265): INFO Train: [33/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2924 (0.3055)	loss 1.4729 (1.4419)	grad_norm 45.1681 (29.2962)	mem 4879MB
[2022-05-31 03:01:58 MetaFG_0] (main.py 265): INFO Train: [33/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2927 (0.3054)	loss 1.5917 (1.4437)	grad_norm 62.3045 (29.3562)	mem 4879MB
[2022-05-31 03:02:01 MetaFG_0] (main.py 265): INFO Train: [33/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2999 (0.3054)	loss 1.1087 (1.4432)	grad_norm 39.7252 (29.3922)	mem 4879MB
[2022-05-31 03:02:04 MetaFG_0] (main.py 265): INFO Train: [33/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2942 (0.3054)	loss 1.6564 (1.4439)	grad_norm 18.0064 (29.3741)	mem 4879MB
[2022-05-31 03:02:07 MetaFG_0] (main.py 265): INFO Train: [33/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2930 (0.3054)	loss 1.6135 (1.4450)	grad_norm 32.3254 (29.3396)	mem 4879MB
[2022-05-31 03:02:10 MetaFG_0] (main.py 265): INFO Train: [33/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2918 (0.3054)	loss 1.2195 (1.4447)	grad_norm 37.9260 (29.3754)	mem 4879MB
[2022-05-31 03:02:13 MetaFG_0] (main.py 265): INFO Train: [33/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2996 (0.3054)	loss 1.4636 (1.4442)	grad_norm 19.3146 (29.3698)	mem 4879MB
[2022-05-31 03:02:16 MetaFG_0] (main.py 265): INFO Train: [33/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2942 (0.3057)	loss 1.6482 (1.4447)	grad_norm 27.3717 (29.3337)	mem 4879MB
[2022-05-31 03:02:19 MetaFG_0] (main.py 265): INFO Train: [33/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2934 (0.3057)	loss 1.5267 (1.4445)	grad_norm 25.1247 (29.3727)	mem 4879MB
[2022-05-31 03:02:22 MetaFG_0] (main.py 265): INFO Train: [33/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2928 (0.3056)	loss 1.5575 (1.4452)	grad_norm 24.8564 (29.3316)	mem 4879MB
[2022-05-31 03:02:25 MetaFG_0] (main.py 265): INFO Train: [33/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2918 (0.3056)	loss 1.6252 (1.4452)	grad_norm 23.4233 (29.3685)	mem 4879MB
[2022-05-31 03:02:28 MetaFG_0] (main.py 265): INFO Train: [33/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2922 (0.3056)	loss 1.3741 (1.4443)	grad_norm 26.3872 (29.4452)	mem 4879MB
[2022-05-31 03:02:31 MetaFG_0] (main.py 265): INFO Train: [33/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2983 (0.3056)	loss 1.6766 (1.4441)	grad_norm 29.9377 (29.4669)	mem 4879MB
[2022-05-31 03:02:34 MetaFG_0] (main.py 265): INFO Train: [33/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2922 (0.3056)	loss 1.1132 (1.4440)	grad_norm 26.8903 (29.4621)	mem 4879MB
[2022-05-31 03:02:38 MetaFG_0] (main.py 265): INFO Train: [33/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2940 (0.3056)	loss 0.9592 (1.4449)	grad_norm 21.7307 (29.4617)	mem 4879MB
[2022-05-31 03:02:41 MetaFG_0] (main.py 265): INFO Train: [33/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2924 (0.3056)	loss 1.5855 (1.4450)	grad_norm 34.4442 (29.4712)	mem 4879MB
[2022-05-31 03:02:44 MetaFG_0] (main.py 265): INFO Train: [33/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2922 (0.3056)	loss 1.1048 (1.4450)	grad_norm 23.6867 (29.4141)	mem 4879MB
[2022-05-31 03:02:47 MetaFG_0] (main.py 265): INFO Train: [33/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2982 (0.3056)	loss 1.6491 (1.4458)	grad_norm 24.1965 (29.3832)	mem 4879MB
[2022-05-31 03:02:50 MetaFG_0] (main.py 265): INFO Train: [33/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2996 (0.3056)	loss 1.5301 (1.4466)	grad_norm 17.2006 (29.3559)	mem 4879MB
[2022-05-31 03:02:53 MetaFG_0] (main.py 265): INFO Train: [33/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2924 (0.3056)	loss 1.5925 (1.4467)	grad_norm 21.8208 (29.3631)	mem 4879MB
[2022-05-31 03:02:56 MetaFG_0] (main.py 265): INFO Train: [33/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2936 (0.3056)	loss 1.5522 (1.4474)	grad_norm 18.5638 (29.3798)	mem 4879MB
[2022-05-31 03:02:59 MetaFG_0] (main.py 265): INFO Train: [33/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2999 (0.3055)	loss 1.4875 (1.4482)	grad_norm 23.9500 (29.3824)	mem 4879MB
[2022-05-31 03:03:02 MetaFG_0] (main.py 265): INFO Train: [33/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2986 (0.3055)	loss 1.0656 (1.4477)	grad_norm 16.8158 (29.3244)	mem 4879MB
[2022-05-31 03:03:05 MetaFG_0] (main.py 265): INFO Train: [33/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2982 (0.3055)	loss 1.4680 (1.4475)	grad_norm 24.3137 (29.3419)	mem 4879MB
[2022-05-31 03:03:08 MetaFG_0] (main.py 265): INFO Train: [33/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2997 (0.3055)	loss 1.4060 (1.4473)	grad_norm 30.1034 (29.3471)	mem 4879MB
[2022-05-31 03:03:11 MetaFG_0] (main.py 265): INFO Train: [33/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2946 (0.3056)	loss 1.0365 (1.4463)	grad_norm 27.7075 (29.3441)	mem 4879MB
[2022-05-31 03:03:14 MetaFG_0] (main.py 265): INFO Train: [33/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2981 (0.3055)	loss 1.2851 (1.4463)	grad_norm 41.3827 (29.3332)	mem 4879MB
[2022-05-31 03:03:17 MetaFG_0] (main.py 265): INFO Train: [33/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2988 (0.3055)	loss 1.4564 (1.4464)	grad_norm 27.9940 (29.3439)	mem 4879MB
[2022-05-31 03:03:20 MetaFG_0] (main.py 265): INFO Train: [33/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2955 (0.3055)	loss 1.1656 (1.4454)	grad_norm 41.7949 (29.3616)	mem 4879MB
[2022-05-31 03:03:23 MetaFG_0] (main.py 265): INFO Train: [33/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2987 (0.3055)	loss 1.1440 (1.4444)	grad_norm 37.0920 (29.3752)	mem 4879MB
[2022-05-31 03:03:26 MetaFG_0] (main.py 265): INFO Train: [33/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2931 (0.3055)	loss 1.1429 (1.4429)	grad_norm 27.3653 (29.3798)	mem 4879MB
[2022-05-31 03:03:29 MetaFG_0] (main.py 265): INFO Train: [33/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2930 (0.3055)	loss 1.2210 (1.4423)	grad_norm 20.0284 (29.4599)	mem 4879MB
[2022-05-31 03:03:32 MetaFG_0] (main.py 265): INFO Train: [33/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3055)	loss 1.7685 (1.4434)	grad_norm 30.2037 (29.4511)	mem 4879MB
[2022-05-31 03:03:35 MetaFG_0] (main.py 265): INFO Train: [33/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2926 (0.3055)	loss 1.3036 (1.4429)	grad_norm 31.6889 (29.4640)	mem 4879MB
[2022-05-31 03:03:38 MetaFG_0] (main.py 265): INFO Train: [33/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2923 (0.3055)	loss 1.4309 (1.4427)	grad_norm 22.5987 (29.4143)	mem 4879MB
[2022-05-31 03:03:42 MetaFG_0] (main.py 265): INFO Train: [33/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3023 (0.3055)	loss 1.3425 (1.4424)	grad_norm 30.4265 (29.4407)	mem 4879MB
[2022-05-31 03:03:45 MetaFG_0] (main.py 265): INFO Train: [33/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2995 (0.3055)	loss 1.0733 (1.4423)	grad_norm 22.3761 (29.4325)	mem 4879MB
[2022-05-31 03:03:48 MetaFG_0] (main.py 265): INFO Train: [33/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.3002 (0.3055)	loss 1.4020 (1.4421)	grad_norm 25.2646 (29.4497)	mem 4879MB
[2022-05-31 03:03:51 MetaFG_0] (main.py 265): INFO Train: [33/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2997 (0.3055)	loss 1.6175 (1.4429)	grad_norm 17.9555 (29.4516)	mem 4879MB
[2022-05-31 03:03:54 MetaFG_0] (main.py 265): INFO Train: [33/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2934 (0.3055)	loss 1.4411 (1.4431)	grad_norm 21.9751 (29.4622)	mem 4879MB
[2022-05-31 03:03:57 MetaFG_0] (main.py 265): INFO Train: [33/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2931 (0.3055)	loss 1.1087 (1.4423)	grad_norm 23.6737 (29.4891)	mem 4879MB
[2022-05-31 03:04:00 MetaFG_0] (main.py 265): INFO Train: [33/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2980 (0.3054)	loss 1.4315 (1.4411)	grad_norm 20.2683 (29.4546)	mem 4879MB
[2022-05-31 03:04:03 MetaFG_0] (main.py 265): INFO Train: [33/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2939 (0.3054)	loss 1.4169 (1.4416)	grad_norm 30.6434 (29.4387)	mem 4879MB
[2022-05-31 03:04:06 MetaFG_0] (main.py 265): INFO Train: [33/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2934 (0.3054)	loss 1.5667 (1.4411)	grad_norm 29.8378 (29.4195)	mem 4879MB
[2022-05-31 03:04:09 MetaFG_0] (main.py 265): INFO Train: [33/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2933 (0.3054)	loss 1.3908 (1.4409)	grad_norm 21.7323 (29.4631)	mem 4879MB
[2022-05-31 03:04:12 MetaFG_0] (main.py 265): INFO Train: [33/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2930 (0.3054)	loss 1.6241 (1.4403)	grad_norm 23.6022 (29.4544)	mem 4879MB
[2022-05-31 03:04:15 MetaFG_0] (main.py 265): INFO Train: [33/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2922 (0.3054)	loss 0.9153 (1.4401)	grad_norm 21.7378 (29.4587)	mem 4879MB
[2022-05-31 03:04:18 MetaFG_0] (main.py 265): INFO Train: [33/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.3000 (0.3054)	loss 1.0845 (1.4396)	grad_norm 25.7316 (29.4629)	mem 4879MB
[2022-05-31 03:04:21 MetaFG_0] (main.py 265): INFO Train: [33/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.3017 (0.3054)	loss 1.2079 (1.4395)	grad_norm 14.1437 (29.4520)	mem 4879MB
[2022-05-31 03:04:24 MetaFG_0] (main.py 265): INFO Train: [33/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2919 (0.3054)	loss 1.4104 (1.4396)	grad_norm 27.6517 (29.4468)	mem 4879MB
[2022-05-31 03:04:27 MetaFG_0] (main.py 265): INFO Train: [33/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2918 (0.3054)	loss 1.5468 (1.4391)	grad_norm 17.4030 (29.4456)	mem 4879MB
[2022-05-31 03:04:30 MetaFG_0] (main.py 265): INFO Train: [33/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2916 (0.3054)	loss 1.7367 (1.4396)	grad_norm 29.5033 (29.4494)	mem 4879MB
[2022-05-31 03:04:33 MetaFG_0] (main.py 265): INFO Train: [33/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2934 (0.3054)	loss 1.4611 (1.4391)	grad_norm 23.5342 (29.4395)	mem 4879MB
[2022-05-31 03:04:36 MetaFG_0] (main.py 265): INFO Train: [33/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2976 (0.3054)	loss 1.0830 (1.4383)	grad_norm 36.2882 (29.4231)	mem 4879MB
[2022-05-31 03:04:39 MetaFG_0] (main.py 265): INFO Train: [33/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.3008 (0.3054)	loss 1.2337 (1.4383)	grad_norm 29.6706 (29.4487)	mem 4879MB
[2022-05-31 03:04:43 MetaFG_0] (main.py 265): INFO Train: [33/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2924 (0.3054)	loss 1.5011 (1.4380)	grad_norm 33.2042 (29.4352)	mem 4879MB
[2022-05-31 03:04:46 MetaFG_0] (main.py 265): INFO Train: [33/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3001 (0.3054)	loss 1.5406 (1.4383)	grad_norm 37.6603 (29.4361)	mem 4879MB
[2022-05-31 03:04:49 MetaFG_0] (main.py 265): INFO Train: [33/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2919 (0.3054)	loss 1.1002 (1.4384)	grad_norm 16.7748 (29.4167)	mem 4879MB
[2022-05-31 03:04:52 MetaFG_0] (main.py 265): INFO Train: [33/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2985 (0.3054)	loss 1.1845 (1.4384)	grad_norm 29.6417 (29.4712)	mem 4879MB
[2022-05-31 03:04:55 MetaFG_0] (main.py 265): INFO Train: [33/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2983 (0.3054)	loss 1.4526 (1.4378)	grad_norm 29.0752 (29.4621)	mem 4879MB
[2022-05-31 03:04:58 MetaFG_0] (main.py 265): INFO Train: [33/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2979 (0.3054)	loss 1.2320 (1.4378)	grad_norm 34.0280 (29.4419)	mem 4879MB
[2022-05-31 03:05:01 MetaFG_0] (main.py 265): INFO Train: [33/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2929 (0.3054)	loss 1.6655 (1.4382)	grad_norm 52.0417 (29.4496)	mem 4879MB
[2022-05-31 03:05:04 MetaFG_0] (main.py 265): INFO Train: [33/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2989 (0.3054)	loss 1.5011 (1.4388)	grad_norm 20.3548 (29.4250)	mem 4879MB
[2022-05-31 03:05:07 MetaFG_0] (main.py 265): INFO Train: [33/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2939 (0.3054)	loss 1.3111 (1.4386)	grad_norm 21.9315 (29.3998)	mem 4879MB
[2022-05-31 03:05:10 MetaFG_0] (main.py 265): INFO Train: [33/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2992 (0.3054)	loss 1.5839 (1.4385)	grad_norm 19.5417 (29.3841)	mem 4879MB
[2022-05-31 03:05:13 MetaFG_0] (main.py 265): INFO Train: [33/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2932 (0.3054)	loss 1.4476 (1.4383)	grad_norm 39.2265 (29.4010)	mem 4879MB
[2022-05-31 03:05:16 MetaFG_0] (main.py 265): INFO Train: [33/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2981 (0.3054)	loss 1.5857 (1.4385)	grad_norm 41.1444 (29.4112)	mem 4879MB
[2022-05-31 03:05:19 MetaFG_0] (main.py 265): INFO Train: [33/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2927 (0.3054)	loss 1.5186 (1.4379)	grad_norm 24.6135 (29.4320)	mem 4879MB
[2022-05-31 03:05:22 MetaFG_0] (main.py 265): INFO Train: [33/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2921 (0.3053)	loss 1.5105 (1.4383)	grad_norm 47.1413 (29.4585)	mem 4879MB
[2022-05-31 03:05:25 MetaFG_0] (main.py 265): INFO Train: [33/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2993 (0.3053)	loss 1.3711 (1.4386)	grad_norm 18.3162 (29.4464)	mem 4879MB
[2022-05-31 03:05:28 MetaFG_0] (main.py 265): INFO Train: [33/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2992 (0.3053)	loss 1.5499 (1.4381)	grad_norm 22.6080 (29.4895)	mem 4879MB
[2022-05-31 03:05:31 MetaFG_0] (main.py 265): INFO Train: [33/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2942 (0.3053)	loss 0.8207 (1.4380)	grad_norm 39.7103 (29.4835)	mem 4879MB
[2022-05-31 03:05:34 MetaFG_0] (main.py 265): INFO Train: [33/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2997 (0.3053)	loss 1.5865 (1.4373)	grad_norm 18.3608 (29.4819)	mem 4879MB
[2022-05-31 03:05:37 MetaFG_0] (main.py 265): INFO Train: [33/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2931 (0.3053)	loss 1.8033 (1.4379)	grad_norm 19.3390 (29.4538)	mem 4879MB
[2022-05-31 03:05:38 MetaFG_0] (main.py 272): INFO EPOCH 33 training takes 0:07:56
[2022-05-31 03:05:38 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_33.pth saving......
[2022-05-31 03:05:39 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_33.pth saved !!!
[2022-05-31 03:05:39 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:05:40 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:05:40 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:05:41 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.632 (0.632)	Loss 0.7195 (0.7195)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 03:05:42 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.143)	Loss 0.6915 (0.6622)	Acc@1 84.375 (86.080)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 03:05:43 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.120)	Loss 0.6610 (0.6291)	Acc@1 81.250 (86.607)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 03:05:44 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.114)	Loss 0.6716 (0.6683)	Acc@1 84.375 (85.383)	Acc@5 96.875 (98.488)	Mem 4879MB
[2022-05-31 03:05:44 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.109)	Loss 0.7710 (0.6841)	Acc@1 84.375 (84.909)	Acc@5 93.750 (98.399)	Mem 4879MB
[2022-05-31 03:05:45 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.091 (0.105)	Loss 0.5258 (0.6858)	Acc@1 90.625 (84.926)	Acc@5 100.000 (98.223)	Mem 4879MB
[2022-05-31 03:05:46 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.097 (0.103)	Loss 0.8383 (0.6853)	Acc@1 75.000 (84.529)	Acc@5 100.000 (98.412)	Mem 4879MB
[2022-05-31 03:05:47 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.102)	Loss 1.0518 (0.6982)	Acc@1 75.000 (84.463)	Acc@5 87.500 (97.975)	Mem 4879MB
[2022-05-31 03:05:48 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.088 (0.101)	Loss 0.7409 (0.7011)	Acc@1 81.250 (84.298)	Acc@5 96.875 (98.071)	Mem 4879MB
[2022-05-31 03:05:49 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.098 (0.101)	Loss 0.6883 (0.6916)	Acc@1 90.625 (84.650)	Acc@5 93.750 (98.146)	Mem 4879MB
[2022-05-31 03:05:50 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 0.7324 (0.6957)	Acc@1 84.375 (84.437)	Acc@5 96.875 (98.113)	Mem 4879MB
[2022-05-31 03:05:51 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.100)	Loss 0.5335 (0.6958)	Acc@1 87.500 (84.319)	Acc@5 96.875 (98.086)	Mem 4879MB
[2022-05-31 03:05:52 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.092 (0.099)	Loss 0.4884 (0.6984)	Acc@1 93.750 (84.220)	Acc@5 96.875 (98.063)	Mem 4879MB
[2022-05-31 03:05:53 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.6070 (0.7005)	Acc@1 93.750 (84.113)	Acc@5 96.875 (98.115)	Mem 4879MB
[2022-05-31 03:05:54 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.091 (0.099)	Loss 0.6607 (0.7015)	Acc@1 87.500 (84.065)	Acc@5 96.875 (98.072)	Mem 4879MB
[2022-05-31 03:05:55 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.5375 (0.7063)	Acc@1 90.625 (83.837)	Acc@5 100.000 (98.034)	Mem 4879MB
[2022-05-31 03:05:56 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.9922 (0.7146)	Acc@1 68.750 (83.599)	Acc@5 100.000 (97.962)	Mem 4879MB
[2022-05-31 03:05:57 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 0.9452 (0.7163)	Acc@1 68.750 (83.406)	Acc@5 100.000 (98.045)	Mem 4879MB
[2022-05-31 03:05:58 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.098)	Loss 0.7060 (0.7143)	Acc@1 84.375 (83.581)	Acc@5 96.875 (97.997)	Mem 4879MB
[2022-05-31 03:05:59 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.093 (0.097)	Loss 0.6195 (0.7130)	Acc@1 87.500 (83.622)	Acc@5 96.875 (98.020)	Mem 4879MB
[2022-05-31 03:06:00 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 1.0146 (0.7164)	Acc@1 68.750 (83.473)	Acc@5 100.000 (98.072)	Mem 4879MB
[2022-05-31 03:06:01 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.9271 (0.7153)	Acc@1 75.000 (83.605)	Acc@5 90.625 (98.030)	Mem 4879MB
[2022-05-31 03:06:01 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.100 (0.097)	Loss 0.5859 (0.7139)	Acc@1 87.500 (83.668)	Acc@5 100.000 (98.049)	Mem 4879MB
[2022-05-31 03:06:02 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 0.8358 (0.7134)	Acc@1 84.375 (83.536)	Acc@5 96.875 (98.052)	Mem 4879MB
[2022-05-31 03:06:03 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.097)	Loss 0.6661 (0.7099)	Acc@1 84.375 (83.558)	Acc@5 100.000 (98.107)	Mem 4879MB
[2022-05-31 03:06:04 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.097)	Loss 0.4492 (0.7114)	Acc@1 93.750 (83.516)	Acc@5 100.000 (98.120)	Mem 4879MB
[2022-05-31 03:06:05 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.7247 (0.7090)	Acc@1 81.250 (83.609)	Acc@5 96.875 (98.132)	Mem 4879MB
[2022-05-31 03:06:06 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.097)	Loss 0.6601 (0.7092)	Acc@1 78.125 (83.476)	Acc@5 100.000 (98.132)	Mem 4879MB
[2022-05-31 03:06:07 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.7928 (0.7111)	Acc@1 75.000 (83.385)	Acc@5 96.875 (98.109)	Mem 4879MB
[2022-05-31 03:06:08 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.089 (0.096)	Loss 0.7469 (0.7105)	Acc@1 81.250 (83.398)	Acc@5 96.875 (98.110)	Mem 4879MB
[2022-05-31 03:06:09 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 0.6548 (0.7115)	Acc@1 90.625 (83.358)	Acc@5 100.000 (98.121)	Mem 4879MB
[2022-05-31 03:06:10 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.4688 (0.7105)	Acc@1 87.500 (83.340)	Acc@5 100.000 (98.111)	Mem 4879MB
[2022-05-31 03:06:10 MetaFG_0] (main.py 330): INFO  * Acc@1 83.330 Acc@5 98.120
[2022-05-31 03:06:10 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 83.3%
[2022-05-31 03:06:10 MetaFG_0] (main.py 171): INFO Max accuracy: 83.33%
[2022-05-31 03:06:11 MetaFG_0] (main.py 265): INFO Train: [34/300][0/1562]	eta 0:22:22 lr 0.000006	time 0.8593 (0.8593)	loss 1.1919 (1.1919)	grad_norm 30.2636 (30.2636)	mem 4879MB
[2022-05-31 03:06:14 MetaFG_0] (main.py 265): INFO Train: [34/300][10/1562]	eta 0:09:26 lr 0.000006	time 0.2922 (0.3653)	loss 1.6930 (1.5215)	grad_norm 22.3434 (28.1715)	mem 4879MB
[2022-05-31 03:06:17 MetaFG_0] (main.py 265): INFO Train: [34/300][20/1562]	eta 0:08:38 lr 0.000006	time 0.3007 (0.3364)	loss 1.0444 (1.4336)	grad_norm 32.9192 (29.1493)	mem 4879MB
[2022-05-31 03:06:20 MetaFG_0] (main.py 265): INFO Train: [34/300][30/1562]	eta 0:08:20 lr 0.000006	time 0.2983 (0.3264)	loss 1.2048 (1.4273)	grad_norm 33.4940 (29.7058)	mem 4879MB
[2022-05-31 03:06:23 MetaFG_0] (main.py 265): INFO Train: [34/300][40/1562]	eta 0:08:07 lr 0.000006	time 0.2988 (0.3203)	loss 1.7750 (1.4547)	grad_norm 45.8295 (29.8126)	mem 4879MB
[2022-05-31 03:06:26 MetaFG_0] (main.py 265): INFO Train: [34/300][50/1562]	eta 0:07:59 lr 0.000006	time 0.2933 (0.3171)	loss 1.7804 (1.4677)	grad_norm 21.1686 (29.1785)	mem 4879MB
[2022-05-31 03:06:29 MetaFG_0] (main.py 265): INFO Train: [34/300][60/1562]	eta 0:07:53 lr 0.000006	time 0.2933 (0.3150)	loss 1.7103 (1.4521)	grad_norm 19.0077 (28.8584)	mem 4879MB
[2022-05-31 03:06:32 MetaFG_0] (main.py 265): INFO Train: [34/300][70/1562]	eta 0:07:47 lr 0.000006	time 0.2932 (0.3136)	loss 1.3513 (1.4295)	grad_norm 22.8138 (28.9238)	mem 4879MB
[2022-05-31 03:06:35 MetaFG_0] (main.py 265): INFO Train: [34/300][80/1562]	eta 0:07:42 lr 0.000006	time 0.2920 (0.3124)	loss 0.9756 (1.4358)	grad_norm 21.2525 (28.6516)	mem 4879MB
[2022-05-31 03:06:38 MetaFG_0] (main.py 265): INFO Train: [34/300][90/1562]	eta 0:07:38 lr 0.000006	time 0.2992 (0.3116)	loss 1.7835 (1.4378)	grad_norm 38.3554 (29.6086)	mem 4879MB
[2022-05-31 03:06:42 MetaFG_0] (main.py 265): INFO Train: [34/300][100/1562]	eta 0:07:34 lr 0.000006	time 0.2932 (0.3108)	loss 1.3212 (1.4285)	grad_norm 23.3270 (29.5859)	mem 4879MB
[2022-05-31 03:06:45 MetaFG_0] (main.py 265): INFO Train: [34/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2989 (0.3102)	loss 1.4682 (1.4173)	grad_norm 27.0741 (29.6906)	mem 4879MB
[2022-05-31 03:06:48 MetaFG_0] (main.py 265): INFO Train: [34/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2935 (0.3098)	loss 1.6203 (1.4186)	grad_norm 53.6501 (30.5404)	mem 4879MB
[2022-05-31 03:06:51 MetaFG_0] (main.py 265): INFO Train: [34/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.3193 (0.3101)	loss 1.0508 (1.4093)	grad_norm 24.1225 (30.1273)	mem 4879MB
[2022-05-31 03:06:54 MetaFG_0] (main.py 265): INFO Train: [34/300][140/1562]	eta 0:07:23 lr 0.000006	time 0.2983 (0.3116)	loss 1.6194 (1.4099)	grad_norm 59.5202 (30.3183)	mem 4879MB
[2022-05-31 03:06:57 MetaFG_0] (main.py 265): INFO Train: [34/300][150/1562]	eta 0:07:19 lr 0.000006	time 0.3004 (0.3110)	loss 1.5631 (1.4075)	grad_norm 30.2673 (30.2154)	mem 4879MB
[2022-05-31 03:07:00 MetaFG_0] (main.py 265): INFO Train: [34/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.2984 (0.3107)	loss 0.9557 (1.3999)	grad_norm 26.8840 (30.1602)	mem 4879MB
[2022-05-31 03:07:03 MetaFG_0] (main.py 265): INFO Train: [34/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2929 (0.3103)	loss 1.4430 (1.4068)	grad_norm 31.6346 (30.0203)	mem 4879MB
[2022-05-31 03:07:06 MetaFG_0] (main.py 265): INFO Train: [34/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2923 (0.3101)	loss 1.4921 (1.4088)	grad_norm 24.7954 (30.1808)	mem 4879MB
[2022-05-31 03:07:09 MetaFG_0] (main.py 265): INFO Train: [34/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2951 (0.3097)	loss 1.4697 (1.4098)	grad_norm 28.9874 (30.2656)	mem 4879MB
[2022-05-31 03:07:12 MetaFG_0] (main.py 265): INFO Train: [34/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2937 (0.3093)	loss 1.5535 (1.4111)	grad_norm 22.1354 (30.1790)	mem 4879MB
[2022-05-31 03:07:15 MetaFG_0] (main.py 265): INFO Train: [34/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.3034 (0.3091)	loss 1.2670 (1.4102)	grad_norm 28.0606 (30.0998)	mem 4879MB
[2022-05-31 03:07:18 MetaFG_0] (main.py 265): INFO Train: [34/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2953 (0.3088)	loss 1.3906 (1.4144)	grad_norm 37.2954 (30.1483)	mem 4879MB
[2022-05-31 03:07:21 MetaFG_0] (main.py 265): INFO Train: [34/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2979 (0.3087)	loss 1.5323 (1.4117)	grad_norm 39.4099 (30.1184)	mem 4879MB
[2022-05-31 03:07:24 MetaFG_0] (main.py 265): INFO Train: [34/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2944 (0.3084)	loss 1.5430 (1.4132)	grad_norm 21.1955 (30.1811)	mem 4879MB
[2022-05-31 03:07:28 MetaFG_0] (main.py 265): INFO Train: [34/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2926 (0.3083)	loss 1.4436 (1.4129)	grad_norm 30.1431 (30.4801)	mem 4879MB
[2022-05-31 03:07:31 MetaFG_0] (main.py 265): INFO Train: [34/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2933 (0.3081)	loss 1.0500 (1.4138)	grad_norm 37.3350 (30.4109)	mem 4879MB
[2022-05-31 03:07:34 MetaFG_0] (main.py 265): INFO Train: [34/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2920 (0.3080)	loss 1.2569 (1.4118)	grad_norm 17.2315 (30.4779)	mem 4879MB
[2022-05-31 03:07:37 MetaFG_0] (main.py 265): INFO Train: [34/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2939 (0.3079)	loss 1.3654 (1.4074)	grad_norm 29.0430 (30.5034)	mem 4879MB
[2022-05-31 03:07:40 MetaFG_0] (main.py 265): INFO Train: [34/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2976 (0.3079)	loss 1.3958 (1.4072)	grad_norm 26.4021 (30.3997)	mem 4879MB
[2022-05-31 03:07:43 MetaFG_0] (main.py 265): INFO Train: [34/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2923 (0.3078)	loss 1.5366 (1.4113)	grad_norm 40.6607 (30.2343)	mem 4879MB
[2022-05-31 03:07:46 MetaFG_0] (main.py 265): INFO Train: [34/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2926 (0.3077)	loss 1.0434 (1.4106)	grad_norm 21.5324 (30.1151)	mem 4879MB
[2022-05-31 03:07:49 MetaFG_0] (main.py 265): INFO Train: [34/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2934 (0.3077)	loss 1.2074 (1.4099)	grad_norm 48.6266 (30.0859)	mem 4879MB
[2022-05-31 03:07:52 MetaFG_0] (main.py 265): INFO Train: [34/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2924 (0.3076)	loss 1.5229 (1.4105)	grad_norm 20.3780 (30.1186)	mem 4879MB
[2022-05-31 03:07:55 MetaFG_0] (main.py 265): INFO Train: [34/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2962 (0.3076)	loss 1.2091 (1.4106)	grad_norm 28.2971 (30.2009)	mem 4879MB
[2022-05-31 03:07:58 MetaFG_0] (main.py 265): INFO Train: [34/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2922 (0.3075)	loss 1.2519 (1.4086)	grad_norm 24.9038 (30.3036)	mem 4879MB
[2022-05-31 03:08:01 MetaFG_0] (main.py 265): INFO Train: [34/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2939 (0.3074)	loss 1.2225 (1.4103)	grad_norm 28.6281 (30.4885)	mem 4879MB
[2022-05-31 03:08:04 MetaFG_0] (main.py 265): INFO Train: [34/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2921 (0.3073)	loss 1.2439 (1.4116)	grad_norm 25.8553 (30.3794)	mem 4879MB
[2022-05-31 03:08:07 MetaFG_0] (main.py 265): INFO Train: [34/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2955 (0.3072)	loss 1.4698 (1.4114)	grad_norm 42.4718 (30.3835)	mem 4879MB
[2022-05-31 03:08:10 MetaFG_0] (main.py 265): INFO Train: [34/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2995 (0.3072)	loss 1.4768 (1.4118)	grad_norm 18.3144 (30.3622)	mem 4879MB
[2022-05-31 03:08:13 MetaFG_0] (main.py 265): INFO Train: [34/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3001 (0.3071)	loss 1.3220 (1.4128)	grad_norm 24.5443 (30.2219)	mem 4879MB
[2022-05-31 03:08:16 MetaFG_0] (main.py 265): INFO Train: [34/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2934 (0.3071)	loss 1.8393 (1.4138)	grad_norm 32.3393 (30.2141)	mem 4879MB
[2022-05-31 03:08:19 MetaFG_0] (main.py 265): INFO Train: [34/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2920 (0.3070)	loss 1.6068 (1.4159)	grad_norm 26.1247 (30.2248)	mem 4879MB
[2022-05-31 03:08:22 MetaFG_0] (main.py 265): INFO Train: [34/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2946 (0.3070)	loss 1.5308 (1.4171)	grad_norm 38.1739 (30.2089)	mem 4879MB
[2022-05-31 03:08:25 MetaFG_0] (main.py 265): INFO Train: [34/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2942 (0.3069)	loss 1.2046 (1.4170)	grad_norm 41.2166 (30.2472)	mem 4879MB
[2022-05-31 03:08:29 MetaFG_0] (main.py 265): INFO Train: [34/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2923 (0.3069)	loss 1.5439 (1.4175)	grad_norm 29.6656 (30.2867)	mem 4879MB
[2022-05-31 03:08:32 MetaFG_0] (main.py 265): INFO Train: [34/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2993 (0.3069)	loss 1.3807 (1.4185)	grad_norm 18.9412 (30.2456)	mem 4879MB
[2022-05-31 03:08:35 MetaFG_0] (main.py 265): INFO Train: [34/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2924 (0.3068)	loss 1.4205 (1.4199)	grad_norm 32.5608 (30.2515)	mem 4879MB
[2022-05-31 03:08:38 MetaFG_0] (main.py 265): INFO Train: [34/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2954 (0.3068)	loss 1.7584 (1.4221)	grad_norm 31.4193 (30.2281)	mem 4879MB
[2022-05-31 03:08:41 MetaFG_0] (main.py 265): INFO Train: [34/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2927 (0.3067)	loss 1.0226 (1.4215)	grad_norm 28.7356 (30.2326)	mem 4879MB
[2022-05-31 03:08:44 MetaFG_0] (main.py 265): INFO Train: [34/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2990 (0.3067)	loss 1.0254 (1.4226)	grad_norm 37.6433 (30.2405)	mem 4879MB
[2022-05-31 03:08:47 MetaFG_0] (main.py 265): INFO Train: [34/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2948 (0.3066)	loss 1.6976 (1.4219)	grad_norm 25.0290 (30.1842)	mem 4879MB
[2022-05-31 03:08:50 MetaFG_0] (main.py 265): INFO Train: [34/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2918 (0.3066)	loss 1.4955 (1.4231)	grad_norm 29.9129 (30.2290)	mem 4879MB
[2022-05-31 03:08:53 MetaFG_0] (main.py 265): INFO Train: [34/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2934 (0.3066)	loss 1.1393 (1.4201)	grad_norm 30.5668 (30.2950)	mem 4879MB
[2022-05-31 03:08:56 MetaFG_0] (main.py 265): INFO Train: [34/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.3000 (0.3066)	loss 1.6047 (1.4192)	grad_norm 32.1873 (30.3077)	mem 4879MB
[2022-05-31 03:08:59 MetaFG_0] (main.py 265): INFO Train: [34/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2992 (0.3065)	loss 1.6049 (1.4190)	grad_norm 24.2643 (30.3202)	mem 4879MB
[2022-05-31 03:09:02 MetaFG_0] (main.py 265): INFO Train: [34/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2920 (0.3065)	loss 1.4159 (1.4171)	grad_norm 24.8843 (30.3567)	mem 4879MB
[2022-05-31 03:09:05 MetaFG_0] (main.py 265): INFO Train: [34/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2922 (0.3065)	loss 1.6091 (1.4178)	grad_norm 26.3543 (30.4007)	mem 4879MB
[2022-05-31 03:09:08 MetaFG_0] (main.py 265): INFO Train: [34/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2922 (0.3064)	loss 1.6120 (1.4179)	grad_norm 35.4875 (30.4608)	mem 4879MB
[2022-05-31 03:09:11 MetaFG_0] (main.py 265): INFO Train: [34/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2940 (0.3064)	loss 1.6977 (1.4187)	grad_norm 37.3247 (30.4718)	mem 4879MB
[2022-05-31 03:09:14 MetaFG_0] (main.py 265): INFO Train: [34/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2994 (0.3063)	loss 1.0043 (1.4196)	grad_norm 39.6917 (nan)	mem 4879MB
[2022-05-31 03:09:17 MetaFG_0] (main.py 265): INFO Train: [34/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2991 (0.3063)	loss 1.6111 (1.4202)	grad_norm 22.9810 (nan)	mem 4879MB
[2022-05-31 03:09:20 MetaFG_0] (main.py 265): INFO Train: [34/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2938 (0.3063)	loss 1.7277 (1.4216)	grad_norm 17.7401 (nan)	mem 4879MB
[2022-05-31 03:09:23 MetaFG_0] (main.py 265): INFO Train: [34/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2942 (0.3063)	loss 1.5154 (1.4212)	grad_norm 24.2136 (nan)	mem 4879MB
[2022-05-31 03:09:26 MetaFG_0] (main.py 265): INFO Train: [34/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2930 (0.3063)	loss 1.6372 (1.4206)	grad_norm 23.7452 (nan)	mem 4879MB
[2022-05-31 03:09:29 MetaFG_0] (main.py 265): INFO Train: [34/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2922 (0.3062)	loss 1.3951 (1.4204)	grad_norm 27.2308 (nan)	mem 4879MB
[2022-05-31 03:09:33 MetaFG_0] (main.py 265): INFO Train: [34/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2992 (0.3062)	loss 0.8930 (1.4196)	grad_norm 29.0420 (nan)	mem 4879MB
[2022-05-31 03:09:36 MetaFG_0] (main.py 265): INFO Train: [34/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2992 (0.3062)	loss 1.5838 (1.4201)	grad_norm 25.6269 (nan)	mem 4879MB
[2022-05-31 03:09:39 MetaFG_0] (main.py 265): INFO Train: [34/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2982 (0.3062)	loss 1.2478 (1.4216)	grad_norm 33.5986 (nan)	mem 4879MB
[2022-05-31 03:09:42 MetaFG_0] (main.py 265): INFO Train: [34/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2995 (0.3061)	loss 1.1262 (1.4202)	grad_norm 23.8641 (nan)	mem 4879MB
[2022-05-31 03:09:45 MetaFG_0] (main.py 265): INFO Train: [34/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2927 (0.3061)	loss 1.6095 (1.4211)	grad_norm 28.9615 (nan)	mem 4879MB
[2022-05-31 03:09:48 MetaFG_0] (main.py 265): INFO Train: [34/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.3045 (0.3061)	loss 1.4787 (1.4222)	grad_norm 44.1599 (nan)	mem 4879MB
[2022-05-31 03:09:51 MetaFG_0] (main.py 265): INFO Train: [34/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2932 (0.3061)	loss 1.5554 (1.4222)	grad_norm 46.9244 (nan)	mem 4879MB
[2022-05-31 03:09:54 MetaFG_0] (main.py 265): INFO Train: [34/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2937 (0.3061)	loss 1.5454 (1.4234)	grad_norm 27.0087 (nan)	mem 4879MB
[2022-05-31 03:09:57 MetaFG_0] (main.py 265): INFO Train: [34/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2974 (0.3061)	loss 1.7211 (1.4246)	grad_norm 24.7276 (nan)	mem 4879MB
[2022-05-31 03:10:00 MetaFG_0] (main.py 265): INFO Train: [34/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2933 (0.3061)	loss 1.2062 (1.4244)	grad_norm 27.6140 (nan)	mem 4879MB
[2022-05-31 03:10:03 MetaFG_0] (main.py 265): INFO Train: [34/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2929 (0.3061)	loss 0.9929 (1.4240)	grad_norm 18.9090 (nan)	mem 4879MB
[2022-05-31 03:10:06 MetaFG_0] (main.py 265): INFO Train: [34/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2992 (0.3061)	loss 1.4895 (1.4239)	grad_norm 16.0311 (nan)	mem 4879MB
[2022-05-31 03:10:09 MetaFG_0] (main.py 265): INFO Train: [34/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2947 (0.3061)	loss 1.4378 (1.4240)	grad_norm 27.7259 (nan)	mem 4879MB
[2022-05-31 03:10:12 MetaFG_0] (main.py 265): INFO Train: [34/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2919 (0.3060)	loss 1.6251 (1.4242)	grad_norm 22.8482 (nan)	mem 4879MB
[2022-05-31 03:10:15 MetaFG_0] (main.py 265): INFO Train: [34/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.3007 (0.3060)	loss 1.0865 (1.4253)	grad_norm 27.5435 (nan)	mem 4879MB
[2022-05-31 03:10:18 MetaFG_0] (main.py 265): INFO Train: [34/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2939 (0.3060)	loss 1.2906 (1.4240)	grad_norm 32.6084 (nan)	mem 4879MB
[2022-05-31 03:10:21 MetaFG_0] (main.py 265): INFO Train: [34/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.3003 (0.3060)	loss 1.5588 (1.4231)	grad_norm 23.9051 (nan)	mem 4879MB
[2022-05-31 03:10:24 MetaFG_0] (main.py 265): INFO Train: [34/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2918 (0.3060)	loss 1.4361 (1.4232)	grad_norm 35.0012 (nan)	mem 4879MB
[2022-05-31 03:10:27 MetaFG_0] (main.py 265): INFO Train: [34/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2927 (0.3060)	loss 1.5528 (1.4248)	grad_norm 23.8483 (nan)	mem 4879MB
[2022-05-31 03:10:30 MetaFG_0] (main.py 265): INFO Train: [34/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2991 (0.3060)	loss 1.6409 (1.4242)	grad_norm 15.6665 (nan)	mem 4879MB
[2022-05-31 03:10:34 MetaFG_0] (main.py 265): INFO Train: [34/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2947 (0.3059)	loss 1.2334 (1.4241)	grad_norm 29.7639 (nan)	mem 4879MB
[2022-05-31 03:10:37 MetaFG_0] (main.py 265): INFO Train: [34/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2923 (0.3059)	loss 1.6054 (1.4240)	grad_norm 25.5791 (nan)	mem 4879MB
[2022-05-31 03:10:40 MetaFG_0] (main.py 265): INFO Train: [34/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3002 (0.3059)	loss 1.5595 (1.4240)	grad_norm 26.6701 (nan)	mem 4879MB
[2022-05-31 03:10:43 MetaFG_0] (main.py 265): INFO Train: [34/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2981 (0.3059)	loss 1.2706 (1.4233)	grad_norm 22.1372 (nan)	mem 4879MB
[2022-05-31 03:10:46 MetaFG_0] (main.py 265): INFO Train: [34/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2980 (0.3059)	loss 1.4866 (1.4229)	grad_norm 23.7943 (nan)	mem 4879MB
[2022-05-31 03:10:49 MetaFG_0] (main.py 265): INFO Train: [34/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3052 (0.3059)	loss 1.2651 (1.4224)	grad_norm 44.9956 (nan)	mem 4879MB
[2022-05-31 03:10:52 MetaFG_0] (main.py 265): INFO Train: [34/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2997 (0.3059)	loss 1.5188 (1.4230)	grad_norm 27.7397 (nan)	mem 4879MB
[2022-05-31 03:10:55 MetaFG_0] (main.py 265): INFO Train: [34/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3033 (0.3059)	loss 1.6206 (1.4239)	grad_norm 22.1421 (nan)	mem 4879MB
[2022-05-31 03:10:58 MetaFG_0] (main.py 265): INFO Train: [34/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2924 (0.3058)	loss 1.5835 (1.4238)	grad_norm 18.5466 (nan)	mem 4879MB
[2022-05-31 03:11:01 MetaFG_0] (main.py 265): INFO Train: [34/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2999 (0.3058)	loss 1.4804 (1.4256)	grad_norm 23.3992 (nan)	mem 4879MB
[2022-05-31 03:11:04 MetaFG_0] (main.py 265): INFO Train: [34/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2959 (0.3058)	loss 1.5571 (1.4262)	grad_norm 42.1995 (nan)	mem 4879MB
[2022-05-31 03:11:07 MetaFG_0] (main.py 265): INFO Train: [34/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2924 (0.3058)	loss 1.5894 (1.4263)	grad_norm 15.3746 (nan)	mem 4879MB
[2022-05-31 03:11:10 MetaFG_0] (main.py 265): INFO Train: [34/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2939 (0.3058)	loss 1.4952 (1.4268)	grad_norm 21.8230 (nan)	mem 4879MB
[2022-05-31 03:11:13 MetaFG_0] (main.py 265): INFO Train: [34/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2969 (0.3058)	loss 1.7147 (1.4263)	grad_norm 20.6260 (nan)	mem 4879MB
[2022-05-31 03:11:16 MetaFG_0] (main.py 265): INFO Train: [34/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2936 (0.3058)	loss 1.6583 (1.4257)	grad_norm 25.4180 (nan)	mem 4879MB
[2022-05-31 03:11:19 MetaFG_0] (main.py 265): INFO Train: [34/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2966 (0.3057)	loss 1.3668 (1.4258)	grad_norm 21.4821 (nan)	mem 4879MB
[2022-05-31 03:11:22 MetaFG_0] (main.py 265): INFO Train: [34/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2930 (0.3057)	loss 1.2848 (1.4243)	grad_norm 16.7288 (nan)	mem 4879MB
[2022-05-31 03:11:25 MetaFG_0] (main.py 265): INFO Train: [34/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2986 (0.3057)	loss 1.0667 (1.4243)	grad_norm 32.9373 (nan)	mem 4879MB
[2022-05-31 03:11:28 MetaFG_0] (main.py 265): INFO Train: [34/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3009 (0.3057)	loss 1.1160 (1.4235)	grad_norm 60.3937 (nan)	mem 4879MB
[2022-05-31 03:11:31 MetaFG_0] (main.py 265): INFO Train: [34/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2980 (0.3057)	loss 0.8903 (1.4234)	grad_norm 32.1092 (nan)	mem 4879MB
[2022-05-31 03:11:34 MetaFG_0] (main.py 265): INFO Train: [34/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2992 (0.3057)	loss 1.2593 (1.4232)	grad_norm 42.0864 (nan)	mem 4879MB
[2022-05-31 03:11:37 MetaFG_0] (main.py 265): INFO Train: [34/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2962 (0.3056)	loss 1.0994 (1.4233)	grad_norm 41.1619 (nan)	mem 4879MB
[2022-05-31 03:11:41 MetaFG_0] (main.py 265): INFO Train: [34/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2936 (0.3056)	loss 1.6937 (1.4242)	grad_norm 21.9534 (nan)	mem 4879MB
[2022-05-31 03:11:44 MetaFG_0] (main.py 265): INFO Train: [34/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2982 (0.3056)	loss 1.3396 (1.4241)	grad_norm 43.3815 (nan)	mem 4879MB
[2022-05-31 03:11:47 MetaFG_0] (main.py 265): INFO Train: [34/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2992 (0.3056)	loss 1.1331 (1.4245)	grad_norm 52.7560 (nan)	mem 4879MB
[2022-05-31 03:11:50 MetaFG_0] (main.py 265): INFO Train: [34/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2939 (0.3057)	loss 1.2942 (1.4243)	grad_norm 17.0311 (nan)	mem 4879MB
[2022-05-31 03:11:53 MetaFG_0] (main.py 265): INFO Train: [34/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2930 (0.3056)	loss 1.0005 (1.4238)	grad_norm 30.0649 (nan)	mem 4879MB
[2022-05-31 03:11:56 MetaFG_0] (main.py 265): INFO Train: [34/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2982 (0.3056)	loss 1.7470 (1.4250)	grad_norm 33.1919 (nan)	mem 4879MB
[2022-05-31 03:11:59 MetaFG_0] (main.py 265): INFO Train: [34/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2925 (0.3056)	loss 1.5183 (1.4252)	grad_norm 24.0222 (nan)	mem 4879MB
[2022-05-31 03:12:02 MetaFG_0] (main.py 265): INFO Train: [34/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2939 (0.3056)	loss 1.5256 (1.4254)	grad_norm 25.7381 (nan)	mem 4879MB
[2022-05-31 03:12:05 MetaFG_0] (main.py 265): INFO Train: [34/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2930 (0.3056)	loss 1.5374 (1.4258)	grad_norm 17.1254 (nan)	mem 4879MB
[2022-05-31 03:12:08 MetaFG_0] (main.py 265): INFO Train: [34/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2917 (0.3056)	loss 1.2037 (1.4259)	grad_norm 28.8706 (nan)	mem 4879MB
[2022-05-31 03:12:11 MetaFG_0] (main.py 265): INFO Train: [34/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2925 (0.3055)	loss 1.3363 (1.4260)	grad_norm 36.8067 (nan)	mem 4879MB
[2022-05-31 03:12:14 MetaFG_0] (main.py 265): INFO Train: [34/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2917 (0.3055)	loss 1.1474 (1.4259)	grad_norm 24.0988 (nan)	mem 4879MB
[2022-05-31 03:12:17 MetaFG_0] (main.py 265): INFO Train: [34/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2940 (0.3055)	loss 1.1598 (1.4261)	grad_norm 26.6204 (nan)	mem 4879MB
[2022-05-31 03:12:20 MetaFG_0] (main.py 265): INFO Train: [34/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2991 (0.3055)	loss 0.9559 (1.4257)	grad_norm 25.0006 (nan)	mem 4879MB
[2022-05-31 03:12:23 MetaFG_0] (main.py 265): INFO Train: [34/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2949 (0.3055)	loss 1.3727 (1.4260)	grad_norm 38.2160 (nan)	mem 4879MB
[2022-05-31 03:12:26 MetaFG_0] (main.py 265): INFO Train: [34/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2920 (0.3055)	loss 1.4605 (1.4257)	grad_norm 19.6336 (nan)	mem 4879MB
[2022-05-31 03:12:29 MetaFG_0] (main.py 265): INFO Train: [34/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2993 (0.3055)	loss 1.6008 (1.4251)	grad_norm 26.6049 (nan)	mem 4879MB
[2022-05-31 03:12:32 MetaFG_0] (main.py 265): INFO Train: [34/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2993 (0.3055)	loss 1.4753 (1.4241)	grad_norm 19.6452 (nan)	mem 4879MB
[2022-05-31 03:12:35 MetaFG_0] (main.py 265): INFO Train: [34/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2930 (0.3054)	loss 1.2134 (1.4234)	grad_norm 38.1182 (nan)	mem 4879MB
[2022-05-31 03:12:38 MetaFG_0] (main.py 265): INFO Train: [34/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2923 (0.3055)	loss 1.8252 (1.4237)	grad_norm 66.8165 (nan)	mem 4879MB
[2022-05-31 03:12:41 MetaFG_0] (main.py 265): INFO Train: [34/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2998 (0.3055)	loss 1.6523 (1.4253)	grad_norm 28.5803 (nan)	mem 4879MB
[2022-05-31 03:12:44 MetaFG_0] (main.py 265): INFO Train: [34/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2928 (0.3054)	loss 1.4622 (1.4251)	grad_norm 16.9861 (nan)	mem 4879MB
[2022-05-31 03:12:48 MetaFG_0] (main.py 265): INFO Train: [34/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2941 (0.3055)	loss 0.9889 (1.4237)	grad_norm 48.2631 (nan)	mem 4879MB
[2022-05-31 03:12:51 MetaFG_0] (main.py 265): INFO Train: [34/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2926 (0.3054)	loss 1.3196 (1.4235)	grad_norm 24.1582 (nan)	mem 4879MB
[2022-05-31 03:12:54 MetaFG_0] (main.py 265): INFO Train: [34/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2925 (0.3054)	loss 1.6475 (1.4234)	grad_norm 19.7974 (nan)	mem 4879MB
[2022-05-31 03:12:57 MetaFG_0] (main.py 265): INFO Train: [34/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2944 (0.3054)	loss 1.2410 (1.4241)	grad_norm 19.4701 (nan)	mem 4879MB
[2022-05-31 03:13:00 MetaFG_0] (main.py 265): INFO Train: [34/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2976 (0.3054)	loss 1.1925 (1.4232)	grad_norm 29.3288 (nan)	mem 4879MB
[2022-05-31 03:13:03 MetaFG_0] (main.py 265): INFO Train: [34/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2934 (0.3054)	loss 1.1302 (1.4232)	grad_norm 39.0257 (nan)	mem 4879MB
[2022-05-31 03:13:06 MetaFG_0] (main.py 265): INFO Train: [34/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2938 (0.3054)	loss 1.2211 (1.4231)	grad_norm 38.1541 (nan)	mem 4879MB
[2022-05-31 03:13:09 MetaFG_0] (main.py 265): INFO Train: [34/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2980 (0.3054)	loss 1.4663 (1.4236)	grad_norm 29.1688 (nan)	mem 4879MB
[2022-05-31 03:13:12 MetaFG_0] (main.py 265): INFO Train: [34/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2976 (0.3054)	loss 1.4988 (1.4233)	grad_norm 30.7445 (nan)	mem 4879MB
[2022-05-31 03:13:15 MetaFG_0] (main.py 265): INFO Train: [34/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2948 (0.3054)	loss 1.3361 (1.4239)	grad_norm 32.2854 (nan)	mem 4879MB
[2022-05-31 03:13:18 MetaFG_0] (main.py 265): INFO Train: [34/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.3011 (0.3054)	loss 1.5677 (1.4240)	grad_norm 34.6910 (nan)	mem 4879MB
[2022-05-31 03:13:21 MetaFG_0] (main.py 265): INFO Train: [34/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2923 (0.3054)	loss 1.1632 (1.4239)	grad_norm 40.0753 (nan)	mem 4879MB
[2022-05-31 03:13:24 MetaFG_0] (main.py 265): INFO Train: [34/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2925 (0.3054)	loss 1.4733 (1.4242)	grad_norm 39.8357 (nan)	mem 4879MB
[2022-05-31 03:13:27 MetaFG_0] (main.py 265): INFO Train: [34/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2921 (0.3054)	loss 1.3714 (1.4238)	grad_norm 19.8161 (nan)	mem 4879MB
[2022-05-31 03:13:30 MetaFG_0] (main.py 265): INFO Train: [34/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2986 (0.3054)	loss 1.3576 (1.4236)	grad_norm 19.9775 (nan)	mem 4879MB
[2022-05-31 03:13:33 MetaFG_0] (main.py 265): INFO Train: [34/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2995 (0.3054)	loss 1.5245 (1.4239)	grad_norm 19.4302 (nan)	mem 4879MB
[2022-05-31 03:13:36 MetaFG_0] (main.py 265): INFO Train: [34/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2939 (0.3054)	loss 1.2678 (1.4246)	grad_norm 37.2259 (nan)	mem 4879MB
[2022-05-31 03:13:39 MetaFG_0] (main.py 265): INFO Train: [34/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2936 (0.3054)	loss 0.8829 (1.4247)	grad_norm 28.1937 (nan)	mem 4879MB
[2022-05-31 03:13:42 MetaFG_0] (main.py 265): INFO Train: [34/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2931 (0.3054)	loss 1.1597 (1.4251)	grad_norm 20.1860 (nan)	mem 4879MB
[2022-05-31 03:13:45 MetaFG_0] (main.py 265): INFO Train: [34/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3031 (0.3054)	loss 1.2576 (1.4256)	grad_norm 30.3784 (nan)	mem 4879MB
[2022-05-31 03:13:48 MetaFG_0] (main.py 265): INFO Train: [34/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2926 (0.3054)	loss 0.9058 (1.4256)	grad_norm 19.6349 (nan)	mem 4879MB
[2022-05-31 03:13:52 MetaFG_0] (main.py 265): INFO Train: [34/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2935 (0.3054)	loss 1.6739 (1.4255)	grad_norm 38.8112 (nan)	mem 4879MB
[2022-05-31 03:13:55 MetaFG_0] (main.py 265): INFO Train: [34/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2931 (0.3054)	loss 1.7625 (1.4257)	grad_norm 21.0009 (nan)	mem 4879MB
[2022-05-31 03:13:58 MetaFG_0] (main.py 265): INFO Train: [34/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2924 (0.3054)	loss 1.3916 (1.4257)	grad_norm 52.1538 (nan)	mem 4879MB
[2022-05-31 03:14:01 MetaFG_0] (main.py 265): INFO Train: [34/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2926 (0.3053)	loss 1.4146 (1.4253)	grad_norm 28.7082 (nan)	mem 4879MB
[2022-05-31 03:14:04 MetaFG_0] (main.py 265): INFO Train: [34/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2981 (0.3054)	loss 0.7833 (1.4251)	grad_norm 15.4239 (nan)	mem 4879MB
[2022-05-31 03:14:07 MetaFG_0] (main.py 265): INFO Train: [34/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2919 (0.3053)	loss 1.6878 (1.4250)	grad_norm 24.5596 (nan)	mem 4879MB
[2022-05-31 03:14:07 MetaFG_0] (main.py 272): INFO EPOCH 34 training takes 0:07:57
[2022-05-31 03:14:07 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_34.pth saving......
[2022-05-31 03:14:08 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_34.pth saved !!!
[2022-05-31 03:14:08 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:14:09 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:14:09 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:14:10 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.709 (0.709)	Loss 0.8119 (0.8119)	Acc@1 78.125 (78.125)	Acc@5 93.750 (93.750)	Mem 4879MB
[2022-05-31 03:14:11 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.097 (0.153)	Loss 0.9037 (0.6995)	Acc@1 71.875 (84.091)	Acc@5 96.875 (97.159)	Mem 4879MB
[2022-05-31 03:14:12 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.095 (0.124)	Loss 0.7905 (0.6837)	Acc@1 75.000 (83.333)	Acc@5 96.875 (98.065)	Mem 4879MB
[2022-05-31 03:14:13 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.090 (0.115)	Loss 0.4709 (0.6771)	Acc@1 93.750 (83.871)	Acc@5 100.000 (98.185)	Mem 4879MB
[2022-05-31 03:14:14 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.110)	Loss 0.8775 (0.6918)	Acc@1 78.125 (83.155)	Acc@5 96.875 (98.171)	Mem 4879MB
[2022-05-31 03:14:15 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.107)	Loss 0.5306 (0.6898)	Acc@1 87.500 (83.272)	Acc@5 100.000 (98.162)	Mem 4879MB
[2022-05-31 03:14:16 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.105)	Loss 0.5939 (0.6717)	Acc@1 87.500 (84.119)	Acc@5 100.000 (98.309)	Mem 4879MB
[2022-05-31 03:14:17 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.8593 (0.6603)	Acc@1 78.125 (84.771)	Acc@5 100.000 (98.327)	Mem 4879MB
[2022-05-31 03:14:18 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 0.7900 (0.6499)	Acc@1 81.250 (84.992)	Acc@5 96.875 (98.418)	Mem 4879MB
[2022-05-31 03:14:19 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 0.6583 (0.6544)	Acc@1 84.375 (84.821)	Acc@5 96.875 (98.489)	Mem 4879MB
[2022-05-31 03:14:20 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 0.6152 (0.6633)	Acc@1 84.375 (84.406)	Acc@5 100.000 (98.391)	Mem 4879MB
[2022-05-31 03:14:20 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 0.7116 (0.6707)	Acc@1 81.250 (84.037)	Acc@5 100.000 (98.255)	Mem 4879MB
[2022-05-31 03:14:21 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.7841 (0.6739)	Acc@1 84.375 (83.988)	Acc@5 96.875 (98.295)	Mem 4879MB
[2022-05-31 03:14:22 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 0.6069 (0.6814)	Acc@1 84.375 (83.635)	Acc@5 100.000 (98.330)	Mem 4879MB
[2022-05-31 03:14:23 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.8426 (0.6839)	Acc@1 75.000 (83.644)	Acc@5 93.750 (98.205)	Mem 4879MB
[2022-05-31 03:14:24 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.102 (0.098)	Loss 0.4460 (0.6759)	Acc@1 93.750 (83.961)	Acc@5 100.000 (98.282)	Mem 4879MB
[2022-05-31 03:14:25 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.093 (0.097)	Loss 0.9286 (0.6800)	Acc@1 81.250 (83.793)	Acc@5 93.750 (98.195)	Mem 4879MB
[2022-05-31 03:14:26 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.097)	Loss 1.2102 (0.6838)	Acc@1 68.750 (83.754)	Acc@5 90.625 (98.154)	Mem 4879MB
[2022-05-31 03:14:27 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.106 (0.097)	Loss 0.7202 (0.6872)	Acc@1 87.500 (83.581)	Acc@5 96.875 (98.135)	Mem 4879MB
[2022-05-31 03:14:28 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.7371 (0.6886)	Acc@1 84.375 (83.704)	Acc@5 100.000 (98.151)	Mem 4879MB
[2022-05-31 03:14:29 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 1.1056 (0.6909)	Acc@1 71.875 (83.629)	Acc@5 96.875 (98.181)	Mem 4879MB
[2022-05-31 03:14:30 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.7097 (0.6928)	Acc@1 84.375 (83.649)	Acc@5 96.875 (98.149)	Mem 4879MB
[2022-05-31 03:14:31 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.6600 (0.6951)	Acc@1 93.750 (83.569)	Acc@5 100.000 (98.133)	Mem 4879MB
[2022-05-31 03:14:32 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.099 (0.097)	Loss 0.6329 (0.6940)	Acc@1 84.375 (83.617)	Acc@5 93.750 (98.147)	Mem 4879MB
[2022-05-31 03:14:33 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 0.7497 (0.6961)	Acc@1 78.125 (83.545)	Acc@5 100.000 (98.107)	Mem 4879MB
[2022-05-31 03:14:34 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.096)	Loss 0.4945 (0.6974)	Acc@1 87.500 (83.566)	Acc@5 100.000 (98.045)	Mem 4879MB
[2022-05-31 03:14:35 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 0.7444 (0.6993)	Acc@1 81.250 (83.453)	Acc@5 96.875 (98.060)	Mem 4879MB
[2022-05-31 03:14:35 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.6493 (0.6963)	Acc@1 84.375 (83.510)	Acc@5 100.000 (98.086)	Mem 4879MB
[2022-05-31 03:14:36 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.6604 (0.6960)	Acc@1 81.250 (83.530)	Acc@5 100.000 (98.098)	Mem 4879MB
[2022-05-31 03:14:37 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 0.6436 (0.6928)	Acc@1 87.500 (83.645)	Acc@5 100.000 (98.121)	Mem 4879MB
[2022-05-31 03:14:38 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.4568 (0.6889)	Acc@1 87.500 (83.731)	Acc@5 100.000 (98.173)	Mem 4879MB
[2022-05-31 03:14:39 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7210 (0.6881)	Acc@1 81.250 (83.802)	Acc@5 96.875 (98.181)	Mem 4879MB
[2022-05-31 03:14:39 MetaFG_0] (main.py 330): INFO  * Acc@1 83.800 Acc@5 98.180
[2022-05-31 03:14:39 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 83.8%
[2022-05-31 03:14:39 MetaFG_0] (main.py 171): INFO Max accuracy: 83.80%
[2022-05-31 03:14:40 MetaFG_0] (main.py 265): INFO Train: [35/300][0/1562]	eta 0:26:19 lr 0.000006	time 1.0110 (1.0110)	loss 1.5956 (1.5956)	grad_norm 32.5610 (32.5610)	mem 4879MB
[2022-05-31 03:14:44 MetaFG_0] (main.py 265): INFO Train: [35/300][10/1562]	eta 0:09:44 lr 0.000006	time 0.2981 (0.3764)	loss 1.3425 (1.4872)	grad_norm 29.1513 (27.4735)	mem 4879MB
[2022-05-31 03:14:47 MetaFG_0] (main.py 265): INFO Train: [35/300][20/1562]	eta 0:08:47 lr 0.000006	time 0.2927 (0.3418)	loss 1.4886 (1.4424)	grad_norm 19.2379 (28.6407)	mem 4879MB
[2022-05-31 03:14:50 MetaFG_0] (main.py 265): INFO Train: [35/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2925 (0.3291)	loss 1.5868 (1.3767)	grad_norm 49.2182 (28.0242)	mem 4879MB
[2022-05-31 03:14:53 MetaFG_0] (main.py 265): INFO Train: [35/300][40/1562]	eta 0:08:11 lr 0.000006	time 0.2996 (0.3231)	loss 1.4745 (1.3653)	grad_norm 25.6795 (28.2111)	mem 4879MB
[2022-05-31 03:14:56 MetaFG_0] (main.py 265): INFO Train: [35/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2998 (0.3195)	loss 1.2640 (1.3864)	grad_norm 35.5860 (27.9425)	mem 4879MB
[2022-05-31 03:14:59 MetaFG_0] (main.py 265): INFO Train: [35/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2949 (0.3171)	loss 1.5924 (1.3801)	grad_norm 27.8764 (28.3711)	mem 4879MB
[2022-05-31 03:15:02 MetaFG_0] (main.py 265): INFO Train: [35/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2982 (0.3155)	loss 1.1722 (1.3839)	grad_norm 27.2988 (28.7399)	mem 4879MB
[2022-05-31 03:15:05 MetaFG_0] (main.py 265): INFO Train: [35/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.3005 (0.3142)	loss 1.8023 (1.3862)	grad_norm 43.7186 (29.1066)	mem 4879MB
[2022-05-31 03:15:08 MetaFG_0] (main.py 265): INFO Train: [35/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.3034 (0.3133)	loss 1.5687 (1.3926)	grad_norm 28.8803 (29.1586)	mem 4879MB
[2022-05-31 03:15:11 MetaFG_0] (main.py 265): INFO Train: [35/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2990 (0.3123)	loss 1.2234 (1.3871)	grad_norm 25.0645 (29.5514)	mem 4879MB
[2022-05-31 03:15:14 MetaFG_0] (main.py 265): INFO Train: [35/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2928 (0.3117)	loss 1.4620 (1.3903)	grad_norm 38.9757 (29.8500)	mem 4879MB
[2022-05-31 03:15:17 MetaFG_0] (main.py 265): INFO Train: [35/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2939 (0.3111)	loss 1.2276 (1.3873)	grad_norm 28.1104 (30.5277)	mem 4879MB
[2022-05-31 03:15:20 MetaFG_0] (main.py 265): INFO Train: [35/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2943 (0.3107)	loss 1.4987 (1.3904)	grad_norm 25.9037 (30.3560)	mem 4879MB
[2022-05-31 03:15:23 MetaFG_0] (main.py 265): INFO Train: [35/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2950 (0.3104)	loss 1.3734 (1.3889)	grad_norm 22.7524 (30.4861)	mem 4879MB
[2022-05-31 03:15:26 MetaFG_0] (main.py 265): INFO Train: [35/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2925 (0.3101)	loss 1.5532 (1.3902)	grad_norm 24.1869 (30.1444)	mem 4879MB
[2022-05-31 03:15:29 MetaFG_0] (main.py 265): INFO Train: [35/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2923 (0.3097)	loss 1.5755 (1.3936)	grad_norm 50.8736 (30.2184)	mem 4879MB
[2022-05-31 03:15:32 MetaFG_0] (main.py 265): INFO Train: [35/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2989 (0.3094)	loss 1.1528 (1.3910)	grad_norm 24.4722 (30.0993)	mem 4879MB
[2022-05-31 03:15:35 MetaFG_0] (main.py 265): INFO Train: [35/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2923 (0.3091)	loss 1.4313 (1.3991)	grad_norm 21.2729 (29.8402)	mem 4879MB
[2022-05-31 03:15:38 MetaFG_0] (main.py 265): INFO Train: [35/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2961 (0.3088)	loss 1.7336 (1.3940)	grad_norm 22.6967 (29.6667)	mem 4879MB
[2022-05-31 03:15:41 MetaFG_0] (main.py 265): INFO Train: [35/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.3003 (0.3085)	loss 1.2817 (1.3977)	grad_norm 17.8824 (29.5549)	mem 4879MB
[2022-05-31 03:15:45 MetaFG_0] (main.py 265): INFO Train: [35/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2978 (0.3083)	loss 1.6093 (1.4047)	grad_norm 26.6010 (29.7082)	mem 4879MB
[2022-05-31 03:15:48 MetaFG_0] (main.py 265): INFO Train: [35/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.3015 (0.3082)	loss 1.6322 (1.4065)	grad_norm 20.4692 (29.6545)	mem 4879MB
[2022-05-31 03:15:51 MetaFG_0] (main.py 265): INFO Train: [35/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2960 (0.3080)	loss 1.5287 (1.4142)	grad_norm 33.6638 (29.3878)	mem 4879MB
[2022-05-31 03:15:54 MetaFG_0] (main.py 265): INFO Train: [35/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2922 (0.3079)	loss 1.3911 (1.4164)	grad_norm 27.8940 (29.2925)	mem 4879MB
[2022-05-31 03:15:57 MetaFG_0] (main.py 265): INFO Train: [35/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2933 (0.3077)	loss 1.5280 (1.4154)	grad_norm 25.1635 (29.1819)	mem 4879MB
[2022-05-31 03:16:00 MetaFG_0] (main.py 265): INFO Train: [35/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2976 (0.3076)	loss 1.6598 (1.4169)	grad_norm 26.1341 (29.2129)	mem 4879MB
[2022-05-31 03:16:03 MetaFG_0] (main.py 265): INFO Train: [35/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2995 (0.3075)	loss 1.3702 (1.4166)	grad_norm 27.0418 (29.0016)	mem 4879MB
[2022-05-31 03:16:06 MetaFG_0] (main.py 265): INFO Train: [35/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2990 (0.3074)	loss 1.1498 (1.4152)	grad_norm 38.5715 (29.0159)	mem 4879MB
[2022-05-31 03:16:09 MetaFG_0] (main.py 265): INFO Train: [35/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2988 (0.3073)	loss 1.1509 (1.4157)	grad_norm 36.9860 (28.9629)	mem 4879MB
[2022-05-31 03:16:12 MetaFG_0] (main.py 265): INFO Train: [35/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2979 (0.3072)	loss 1.2306 (1.4109)	grad_norm 43.4348 (28.9980)	mem 4879MB
[2022-05-31 03:16:15 MetaFG_0] (main.py 265): INFO Train: [35/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2927 (0.3071)	loss 1.5136 (1.4119)	grad_norm 30.2220 (28.9515)	mem 4879MB
[2022-05-31 03:16:18 MetaFG_0] (main.py 265): INFO Train: [35/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2997 (0.3070)	loss 1.5728 (1.4128)	grad_norm 21.5555 (28.8703)	mem 4879MB
[2022-05-31 03:16:21 MetaFG_0] (main.py 265): INFO Train: [35/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.3009 (0.3069)	loss 1.3342 (1.4108)	grad_norm 26.5316 (28.9098)	mem 4879MB
[2022-05-31 03:16:24 MetaFG_0] (main.py 265): INFO Train: [35/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2994 (0.3068)	loss 1.2609 (1.4105)	grad_norm 35.2805 (28.8902)	mem 4879MB
[2022-05-31 03:16:27 MetaFG_0] (main.py 265): INFO Train: [35/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2945 (0.3067)	loss 1.8149 (1.4104)	grad_norm 57.7571 (29.1754)	mem 4879MB
[2022-05-31 03:16:30 MetaFG_0] (main.py 265): INFO Train: [35/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2925 (0.3067)	loss 1.5648 (1.4105)	grad_norm 14.2682 (29.0996)	mem 4879MB
[2022-05-31 03:16:33 MetaFG_0] (main.py 265): INFO Train: [35/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.3001 (0.3066)	loss 1.1674 (1.4087)	grad_norm 26.9000 (29.3201)	mem 4879MB
[2022-05-31 03:16:36 MetaFG_0] (main.py 265): INFO Train: [35/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2993 (0.3066)	loss 1.6673 (1.4092)	grad_norm 25.2320 (29.2248)	mem 4879MB
[2022-05-31 03:16:39 MetaFG_0] (main.py 265): INFO Train: [35/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2931 (0.3066)	loss 1.5538 (1.4114)	grad_norm 36.7977 (29.3897)	mem 4879MB
[2022-05-31 03:16:42 MetaFG_0] (main.py 265): INFO Train: [35/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2923 (0.3066)	loss 1.4658 (1.4121)	grad_norm 31.0026 (29.5092)	mem 4879MB
[2022-05-31 03:16:45 MetaFG_0] (main.py 265): INFO Train: [35/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2988 (0.3065)	loss 1.6921 (1.4137)	grad_norm 36.2011 (29.5963)	mem 4879MB
[2022-05-31 03:16:49 MetaFG_0] (main.py 265): INFO Train: [35/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2988 (0.3065)	loss 1.6014 (1.4138)	grad_norm 21.7318 (29.5349)	mem 4879MB
[2022-05-31 03:16:52 MetaFG_0] (main.py 265): INFO Train: [35/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2950 (0.3065)	loss 1.3713 (1.4132)	grad_norm 16.2599 (29.5990)	mem 4879MB
[2022-05-31 03:16:55 MetaFG_0] (main.py 265): INFO Train: [35/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2980 (0.3064)	loss 0.8735 (1.4133)	grad_norm 27.1742 (29.4851)	mem 4879MB
[2022-05-31 03:16:58 MetaFG_0] (main.py 265): INFO Train: [35/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2928 (0.3064)	loss 1.6677 (1.4148)	grad_norm 29.2589 (29.4946)	mem 4879MB
[2022-05-31 03:17:01 MetaFG_0] (main.py 265): INFO Train: [35/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2990 (0.3063)	loss 1.3163 (1.4148)	grad_norm 42.6247 (29.4557)	mem 4879MB
[2022-05-31 03:17:04 MetaFG_0] (main.py 265): INFO Train: [35/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2922 (0.3062)	loss 1.6120 (1.4148)	grad_norm 29.1030 (29.5588)	mem 4879MB
[2022-05-31 03:17:07 MetaFG_0] (main.py 265): INFO Train: [35/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2980 (0.3062)	loss 1.5144 (1.4150)	grad_norm 25.9842 (29.5844)	mem 4879MB
[2022-05-31 03:17:10 MetaFG_0] (main.py 265): INFO Train: [35/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2977 (0.3062)	loss 1.5839 (1.4131)	grad_norm 35.0784 (29.6523)	mem 4879MB
[2022-05-31 03:17:13 MetaFG_0] (main.py 265): INFO Train: [35/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2923 (0.3061)	loss 1.1254 (1.4130)	grad_norm 27.5080 (29.7338)	mem 4879MB
[2022-05-31 03:17:16 MetaFG_0] (main.py 265): INFO Train: [35/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2941 (0.3061)	loss 1.3098 (1.4141)	grad_norm 40.4028 (29.7248)	mem 4879MB
[2022-05-31 03:17:19 MetaFG_0] (main.py 265): INFO Train: [35/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2955 (0.3061)	loss 1.5769 (1.4142)	grad_norm 22.2782 (29.6518)	mem 4879MB
[2022-05-31 03:17:22 MetaFG_0] (main.py 265): INFO Train: [35/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2934 (0.3060)	loss 1.6010 (1.4146)	grad_norm 28.8127 (29.6412)	mem 4879MB
[2022-05-31 03:17:25 MetaFG_0] (main.py 265): INFO Train: [35/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2986 (0.3060)	loss 1.6943 (1.4155)	grad_norm 28.9369 (29.7237)	mem 4879MB
[2022-05-31 03:17:28 MetaFG_0] (main.py 265): INFO Train: [35/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2932 (0.3060)	loss 1.5551 (1.4166)	grad_norm 29.5111 (29.6478)	mem 4879MB
[2022-05-31 03:17:31 MetaFG_0] (main.py 265): INFO Train: [35/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2936 (0.3059)	loss 1.5096 (1.4176)	grad_norm 15.0647 (29.5651)	mem 4879MB
[2022-05-31 03:17:34 MetaFG_0] (main.py 265): INFO Train: [35/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2982 (0.3059)	loss 1.2618 (1.4177)	grad_norm 28.3697 (29.5567)	mem 4879MB
[2022-05-31 03:17:37 MetaFG_0] (main.py 265): INFO Train: [35/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2927 (0.3059)	loss 1.4674 (1.4167)	grad_norm 42.3130 (29.5490)	mem 4879MB
[2022-05-31 03:17:40 MetaFG_0] (main.py 265): INFO Train: [35/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2989 (0.3059)	loss 1.0061 (1.4157)	grad_norm 28.9034 (29.4841)	mem 4879MB
[2022-05-31 03:17:43 MetaFG_0] (main.py 265): INFO Train: [35/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2958 (0.3058)	loss 1.4521 (1.4159)	grad_norm 19.6889 (29.4711)	mem 4879MB
[2022-05-31 03:17:46 MetaFG_0] (main.py 265): INFO Train: [35/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2937 (0.3058)	loss 1.2023 (1.4154)	grad_norm 25.9035 (29.5424)	mem 4879MB
[2022-05-31 03:17:49 MetaFG_0] (main.py 265): INFO Train: [35/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3001 (0.3058)	loss 1.4540 (1.4176)	grad_norm 24.2548 (29.6983)	mem 4879MB
[2022-05-31 03:17:52 MetaFG_0] (main.py 265): INFO Train: [35/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3000 (0.3058)	loss 1.6396 (1.4153)	grad_norm 35.9578 (29.6063)	mem 4879MB
[2022-05-31 03:17:56 MetaFG_0] (main.py 265): INFO Train: [35/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2981 (0.3058)	loss 1.6373 (1.4166)	grad_norm 33.4452 (29.5923)	mem 4879MB
[2022-05-31 03:17:59 MetaFG_0] (main.py 265): INFO Train: [35/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2938 (0.3058)	loss 0.9145 (1.4160)	grad_norm 21.0970 (29.6491)	mem 4879MB
[2022-05-31 03:18:02 MetaFG_0] (main.py 265): INFO Train: [35/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.3005 (0.3058)	loss 1.1060 (1.4165)	grad_norm 31.7955 (29.6359)	mem 4879MB
[2022-05-31 03:18:05 MetaFG_0] (main.py 265): INFO Train: [35/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2937 (0.3058)	loss 1.4861 (1.4171)	grad_norm 12.9907 (29.5968)	mem 4879MB
[2022-05-31 03:18:08 MetaFG_0] (main.py 265): INFO Train: [35/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2955 (0.3058)	loss 1.5893 (1.4186)	grad_norm 39.3865 (29.5891)	mem 4879MB
[2022-05-31 03:18:11 MetaFG_0] (main.py 265): INFO Train: [35/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2937 (0.3058)	loss 1.7375 (1.4179)	grad_norm 31.3638 (29.6348)	mem 4879MB
[2022-05-31 03:18:14 MetaFG_0] (main.py 265): INFO Train: [35/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2921 (0.3058)	loss 1.7168 (1.4199)	grad_norm 33.5231 (29.6210)	mem 4879MB
[2022-05-31 03:18:17 MetaFG_0] (main.py 265): INFO Train: [35/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2933 (0.3058)	loss 1.3660 (1.4199)	grad_norm 18.7517 (29.7333)	mem 4879MB
[2022-05-31 03:18:20 MetaFG_0] (main.py 265): INFO Train: [35/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2985 (0.3058)	loss 1.5579 (1.4194)	grad_norm 23.5513 (29.7127)	mem 4879MB
[2022-05-31 03:18:23 MetaFG_0] (main.py 265): INFO Train: [35/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2918 (0.3058)	loss 1.3946 (1.4188)	grad_norm 22.2761 (29.7937)	mem 4879MB
[2022-05-31 03:18:26 MetaFG_0] (main.py 265): INFO Train: [35/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2992 (0.3058)	loss 1.5716 (1.4201)	grad_norm 22.3000 (29.7528)	mem 4879MB
[2022-05-31 03:18:29 MetaFG_0] (main.py 265): INFO Train: [35/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2929 (0.3057)	loss 1.6924 (1.4205)	grad_norm 24.3125 (29.7189)	mem 4879MB
[2022-05-31 03:18:32 MetaFG_0] (main.py 265): INFO Train: [35/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2929 (0.3057)	loss 1.5075 (1.4207)	grad_norm 25.4744 (29.6963)	mem 4879MB
[2022-05-31 03:18:35 MetaFG_0] (main.py 265): INFO Train: [35/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3018 (0.3057)	loss 1.5509 (1.4207)	grad_norm 29.5106 (29.6772)	mem 4879MB
[2022-05-31 03:18:38 MetaFG_0] (main.py 265): INFO Train: [35/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.3013 (0.3057)	loss 1.6662 (1.4214)	grad_norm 58.2983 (29.7010)	mem 4879MB
[2022-05-31 03:18:41 MetaFG_0] (main.py 265): INFO Train: [35/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2977 (0.3057)	loss 1.5035 (1.4225)	grad_norm 34.3535 (29.7721)	mem 4879MB
[2022-05-31 03:18:44 MetaFG_0] (main.py 265): INFO Train: [35/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2979 (0.3056)	loss 1.7442 (1.4211)	grad_norm 42.4696 (29.8230)	mem 4879MB
[2022-05-31 03:18:47 MetaFG_0] (main.py 265): INFO Train: [35/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2929 (0.3056)	loss 1.6458 (1.4230)	grad_norm 28.0500 (29.7956)	mem 4879MB
[2022-05-31 03:18:50 MetaFG_0] (main.py 265): INFO Train: [35/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2975 (0.3057)	loss 1.3771 (1.4235)	grad_norm 26.2168 (29.7834)	mem 4879MB
[2022-05-31 03:18:53 MetaFG_0] (main.py 265): INFO Train: [35/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2952 (0.3056)	loss 1.2202 (1.4235)	grad_norm 37.5520 (29.7472)	mem 4879MB
[2022-05-31 03:18:57 MetaFG_0] (main.py 265): INFO Train: [35/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2923 (0.3056)	loss 1.4668 (1.4241)	grad_norm 17.8748 (29.7385)	mem 4879MB
[2022-05-31 03:19:00 MetaFG_0] (main.py 265): INFO Train: [35/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2981 (0.3056)	loss 1.3617 (1.4247)	grad_norm 13.7283 (29.7142)	mem 4879MB
[2022-05-31 03:19:03 MetaFG_0] (main.py 265): INFO Train: [35/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2928 (0.3056)	loss 1.3884 (1.4257)	grad_norm 30.2794 (29.7177)	mem 4879MB
[2022-05-31 03:19:06 MetaFG_0] (main.py 265): INFO Train: [35/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2984 (0.3056)	loss 1.1060 (1.4244)	grad_norm 28.3092 (29.7270)	mem 4879MB
[2022-05-31 03:19:09 MetaFG_0] (main.py 265): INFO Train: [35/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3008 (0.3055)	loss 1.7063 (1.4239)	grad_norm 31.2105 (29.7747)	mem 4879MB
[2022-05-31 03:19:12 MetaFG_0] (main.py 265): INFO Train: [35/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2921 (0.3055)	loss 1.7250 (1.4248)	grad_norm 35.1560 (29.7691)	mem 4879MB
[2022-05-31 03:19:15 MetaFG_0] (main.py 265): INFO Train: [35/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2935 (0.3055)	loss 1.5295 (1.4251)	grad_norm 49.5653 (29.8251)	mem 4879MB
[2022-05-31 03:19:18 MetaFG_0] (main.py 265): INFO Train: [35/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2982 (0.3055)	loss 1.5601 (1.4235)	grad_norm 40.1969 (29.8180)	mem 4879MB
[2022-05-31 03:19:21 MetaFG_0] (main.py 265): INFO Train: [35/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2949 (0.3055)	loss 1.5091 (1.4242)	grad_norm 23.2972 (29.8052)	mem 4879MB
[2022-05-31 03:19:24 MetaFG_0] (main.py 265): INFO Train: [35/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3005 (0.3055)	loss 1.4607 (1.4245)	grad_norm 23.9357 (29.8696)	mem 4879MB
[2022-05-31 03:19:27 MetaFG_0] (main.py 265): INFO Train: [35/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2987 (0.3055)	loss 1.6728 (1.4245)	grad_norm 39.0127 (29.9088)	mem 4879MB
[2022-05-31 03:19:30 MetaFG_0] (main.py 265): INFO Train: [35/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2934 (0.3055)	loss 1.6612 (1.4249)	grad_norm 20.0133 (29.9273)	mem 4879MB
[2022-05-31 03:19:33 MetaFG_0] (main.py 265): INFO Train: [35/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.3007 (0.3055)	loss 1.2826 (1.4250)	grad_norm 27.7514 (29.8825)	mem 4879MB
[2022-05-31 03:19:36 MetaFG_0] (main.py 265): INFO Train: [35/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.3001 (0.3055)	loss 1.6476 (1.4244)	grad_norm 19.0274 (29.8615)	mem 4879MB
[2022-05-31 03:19:39 MetaFG_0] (main.py 265): INFO Train: [35/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3001 (0.3055)	loss 1.5476 (1.4252)	grad_norm 21.9532 (29.8826)	mem 4879MB
[2022-05-31 03:19:42 MetaFG_0] (main.py 265): INFO Train: [35/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2950 (0.3055)	loss 1.2468 (1.4237)	grad_norm 48.2637 (29.9982)	mem 4879MB
[2022-05-31 03:19:45 MetaFG_0] (main.py 265): INFO Train: [35/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2922 (0.3055)	loss 1.4130 (1.4236)	grad_norm 27.7890 (29.9848)	mem 4879MB
[2022-05-31 03:19:48 MetaFG_0] (main.py 265): INFO Train: [35/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2993 (0.3055)	loss 1.4682 (1.4232)	grad_norm 14.4934 (29.9965)	mem 4879MB
[2022-05-31 03:19:51 MetaFG_0] (main.py 265): INFO Train: [35/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2932 (0.3055)	loss 1.5012 (1.4230)	grad_norm 30.2828 (30.0252)	mem 4879MB
[2022-05-31 03:19:54 MetaFG_0] (main.py 265): INFO Train: [35/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2982 (0.3055)	loss 1.7367 (1.4244)	grad_norm 20.8765 (30.1526)	mem 4879MB
[2022-05-31 03:19:58 MetaFG_0] (main.py 265): INFO Train: [35/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2996 (0.3055)	loss 1.3136 (1.4249)	grad_norm 32.9608 (30.1886)	mem 4879MB
[2022-05-31 03:20:01 MetaFG_0] (main.py 265): INFO Train: [35/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2999 (0.3055)	loss 1.4174 (1.4237)	grad_norm 20.1703 (30.1740)	mem 4879MB
[2022-05-31 03:20:04 MetaFG_0] (main.py 265): INFO Train: [35/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2926 (0.3055)	loss 1.1082 (1.4228)	grad_norm 71.3429 (30.2141)	mem 4879MB
[2022-05-31 03:20:07 MetaFG_0] (main.py 265): INFO Train: [35/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2923 (0.3055)	loss 1.5182 (1.4233)	grad_norm 24.7764 (30.2155)	mem 4879MB
[2022-05-31 03:20:10 MetaFG_0] (main.py 265): INFO Train: [35/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2942 (0.3055)	loss 1.4409 (1.4239)	grad_norm 20.1865 (30.2352)	mem 4879MB
[2022-05-31 03:20:13 MetaFG_0] (main.py 265): INFO Train: [35/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2998 (0.3055)	loss 1.0706 (1.4230)	grad_norm 25.8400 (30.2055)	mem 4879MB
[2022-05-31 03:20:16 MetaFG_0] (main.py 265): INFO Train: [35/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2929 (0.3054)	loss 1.4036 (1.4229)	grad_norm 24.4826 (30.2360)	mem 4879MB
[2022-05-31 03:20:19 MetaFG_0] (main.py 265): INFO Train: [35/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2942 (0.3054)	loss 1.4591 (1.4225)	grad_norm 20.0520 (30.2135)	mem 4879MB
[2022-05-31 03:20:22 MetaFG_0] (main.py 265): INFO Train: [35/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.3011 (0.3054)	loss 1.7998 (1.4237)	grad_norm 36.7432 (30.2121)	mem 4879MB
[2022-05-31 03:20:25 MetaFG_0] (main.py 265): INFO Train: [35/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2931 (0.3054)	loss 1.2906 (1.4227)	grad_norm 21.2307 (30.2176)	mem 4879MB
[2022-05-31 03:20:28 MetaFG_0] (main.py 265): INFO Train: [35/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2996 (0.3054)	loss 1.4040 (1.4226)	grad_norm 18.8905 (30.2059)	mem 4879MB
[2022-05-31 03:20:31 MetaFG_0] (main.py 265): INFO Train: [35/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2933 (0.3054)	loss 1.3513 (1.4221)	grad_norm 36.3736 (30.2218)	mem 4879MB
[2022-05-31 03:20:34 MetaFG_0] (main.py 265): INFO Train: [35/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2946 (0.3054)	loss 1.4825 (1.4209)	grad_norm 30.5229 (30.2431)	mem 4879MB
[2022-05-31 03:20:37 MetaFG_0] (main.py 265): INFO Train: [35/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2951 (0.3054)	loss 1.5646 (1.4206)	grad_norm 29.5681 (30.2345)	mem 4879MB
[2022-05-31 03:20:40 MetaFG_0] (main.py 265): INFO Train: [35/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2989 (0.3054)	loss 0.9676 (1.4197)	grad_norm 22.0243 (30.2028)	mem 4879MB
[2022-05-31 03:20:43 MetaFG_0] (main.py 265): INFO Train: [35/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2941 (0.3054)	loss 1.3682 (1.4201)	grad_norm 16.6027 (30.1582)	mem 4879MB
[2022-05-31 03:20:46 MetaFG_0] (main.py 265): INFO Train: [35/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2937 (0.3054)	loss 1.1031 (1.4207)	grad_norm 28.8249 (30.1398)	mem 4879MB
[2022-05-31 03:20:49 MetaFG_0] (main.py 265): INFO Train: [35/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3349 (0.3055)	loss 1.3786 (1.4208)	grad_norm 44.3336 (30.1533)	mem 4879MB
[2022-05-31 03:20:53 MetaFG_0] (main.py 265): INFO Train: [35/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2926 (0.3057)	loss 1.5639 (1.4204)	grad_norm 34.5837 (30.1260)	mem 4879MB
[2022-05-31 03:20:56 MetaFG_0] (main.py 265): INFO Train: [35/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2926 (0.3057)	loss 1.0796 (1.4205)	grad_norm 74.5523 (30.1560)	mem 4879MB
[2022-05-31 03:20:59 MetaFG_0] (main.py 265): INFO Train: [35/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2999 (0.3056)	loss 1.6153 (1.4204)	grad_norm 21.7773 (30.1463)	mem 4879MB
[2022-05-31 03:21:02 MetaFG_0] (main.py 265): INFO Train: [35/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2927 (0.3056)	loss 1.3900 (1.4206)	grad_norm 32.2077 (30.1386)	mem 4879MB
[2022-05-31 03:21:05 MetaFG_0] (main.py 265): INFO Train: [35/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2920 (0.3056)	loss 1.5433 (1.4211)	grad_norm 23.6067 (30.1391)	mem 4879MB
[2022-05-31 03:21:08 MetaFG_0] (main.py 265): INFO Train: [35/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2937 (0.3056)	loss 1.4570 (1.4213)	grad_norm 50.3412 (30.1660)	mem 4879MB
[2022-05-31 03:21:11 MetaFG_0] (main.py 265): INFO Train: [35/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2927 (0.3056)	loss 1.7224 (1.4218)	grad_norm 15.5899 (30.1513)	mem 4879MB
[2022-05-31 03:21:14 MetaFG_0] (main.py 265): INFO Train: [35/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2934 (0.3056)	loss 1.5779 (1.4218)	grad_norm 20.9816 (30.1310)	mem 4879MB
[2022-05-31 03:21:17 MetaFG_0] (main.py 265): INFO Train: [35/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2995 (0.3056)	loss 1.2485 (1.4217)	grad_norm 33.7346 (30.1304)	mem 4879MB
[2022-05-31 03:21:20 MetaFG_0] (main.py 265): INFO Train: [35/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2997 (0.3056)	loss 1.3093 (1.4228)	grad_norm 42.8190 (30.1558)	mem 4879MB
[2022-05-31 03:21:23 MetaFG_0] (main.py 265): INFO Train: [35/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2920 (0.3056)	loss 1.5317 (1.4226)	grad_norm 43.0327 (30.1425)	mem 4879MB
[2022-05-31 03:21:26 MetaFG_0] (main.py 265): INFO Train: [35/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2940 (0.3055)	loss 1.3454 (1.4227)	grad_norm 22.1330 (30.1149)	mem 4879MB
[2022-05-31 03:21:29 MetaFG_0] (main.py 265): INFO Train: [35/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2979 (0.3055)	loss 1.6539 (1.4229)	grad_norm 14.9586 (30.0746)	mem 4879MB
[2022-05-31 03:21:32 MetaFG_0] (main.py 265): INFO Train: [35/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2985 (0.3055)	loss 1.4521 (1.4230)	grad_norm 25.6746 (30.0658)	mem 4879MB
[2022-05-31 03:21:35 MetaFG_0] (main.py 265): INFO Train: [35/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2928 (0.3055)	loss 1.5886 (1.4226)	grad_norm 36.1788 (30.0579)	mem 4879MB
[2022-05-31 03:21:38 MetaFG_0] (main.py 265): INFO Train: [35/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2990 (0.3055)	loss 1.4650 (1.4228)	grad_norm 16.7524 (30.0454)	mem 4879MB
[2022-05-31 03:21:41 MetaFG_0] (main.py 265): INFO Train: [35/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2997 (0.3055)	loss 1.6971 (1.4229)	grad_norm 32.5103 (30.0173)	mem 4879MB
[2022-05-31 03:21:44 MetaFG_0] (main.py 265): INFO Train: [35/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3004 (0.3055)	loss 1.4568 (1.4230)	grad_norm 22.2152 (29.9644)	mem 4879MB
[2022-05-31 03:21:47 MetaFG_0] (main.py 265): INFO Train: [35/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2998 (0.3055)	loss 1.2173 (1.4226)	grad_norm 19.2391 (29.9971)	mem 4879MB
[2022-05-31 03:21:51 MetaFG_0] (main.py 265): INFO Train: [35/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2932 (0.3055)	loss 1.3946 (1.4227)	grad_norm 32.1600 (29.9851)	mem 4879MB
[2022-05-31 03:21:54 MetaFG_0] (main.py 265): INFO Train: [35/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2988 (0.3055)	loss 1.7158 (1.4228)	grad_norm 28.4680 (30.0345)	mem 4879MB
[2022-05-31 03:21:57 MetaFG_0] (main.py 265): INFO Train: [35/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2934 (0.3055)	loss 1.7277 (1.4227)	grad_norm 33.0637 (30.0332)	mem 4879MB
[2022-05-31 03:22:00 MetaFG_0] (main.py 265): INFO Train: [35/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.3060 (0.3055)	loss 1.4790 (1.4226)	grad_norm 14.5612 (30.0284)	mem 4879MB
[2022-05-31 03:22:03 MetaFG_0] (main.py 265): INFO Train: [35/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2993 (0.3055)	loss 1.2172 (1.4216)	grad_norm 27.3204 (30.0503)	mem 4879MB
[2022-05-31 03:22:06 MetaFG_0] (main.py 265): INFO Train: [35/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2991 (0.3055)	loss 1.3264 (1.4214)	grad_norm 39.3309 (30.0653)	mem 4879MB
[2022-05-31 03:22:09 MetaFG_0] (main.py 265): INFO Train: [35/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2933 (0.3055)	loss 1.3111 (1.4213)	grad_norm 31.2720 (30.1229)	mem 4879MB
[2022-05-31 03:22:12 MetaFG_0] (main.py 265): INFO Train: [35/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2984 (0.3055)	loss 1.4755 (1.4211)	grad_norm 16.9266 (30.1262)	mem 4879MB
[2022-05-31 03:22:15 MetaFG_0] (main.py 265): INFO Train: [35/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2925 (0.3054)	loss 1.5483 (1.4206)	grad_norm 18.8931 (30.1457)	mem 4879MB
[2022-05-31 03:22:18 MetaFG_0] (main.py 265): INFO Train: [35/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2992 (0.3054)	loss 1.3645 (1.4205)	grad_norm 37.6708 (30.1489)	mem 4879MB
[2022-05-31 03:22:21 MetaFG_0] (main.py 265): INFO Train: [35/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2924 (0.3055)	loss 1.5051 (1.4203)	grad_norm 20.9090 (30.1586)	mem 4879MB
[2022-05-31 03:22:24 MetaFG_0] (main.py 265): INFO Train: [35/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2984 (0.3054)	loss 1.3549 (1.4207)	grad_norm 33.2577 (30.1535)	mem 4879MB
[2022-05-31 03:22:27 MetaFG_0] (main.py 265): INFO Train: [35/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.3001 (0.3054)	loss 1.5655 (1.4205)	grad_norm 31.0271 (30.1844)	mem 4879MB
[2022-05-31 03:22:30 MetaFG_0] (main.py 265): INFO Train: [35/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2927 (0.3054)	loss 1.7336 (1.4202)	grad_norm 29.8259 (30.2312)	mem 4879MB
[2022-05-31 03:22:33 MetaFG_0] (main.py 265): INFO Train: [35/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2939 (0.3054)	loss 1.2122 (1.4207)	grad_norm 26.2480 (30.2249)	mem 4879MB
[2022-05-31 03:22:36 MetaFG_0] (main.py 265): INFO Train: [35/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2917 (0.3054)	loss 1.0929 (1.4200)	grad_norm 32.9420 (30.2576)	mem 4879MB
[2022-05-31 03:22:37 MetaFG_0] (main.py 272): INFO EPOCH 35 training takes 0:07:57
[2022-05-31 03:22:37 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_35.pth saving......
[2022-05-31 03:22:37 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_35.pth saved !!!
[2022-05-31 03:22:37 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:22:39 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:22:39 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:22:40 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.722 (0.722)	Loss 0.8250 (0.8250)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 03:22:41 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.151)	Loss 0.8665 (0.6672)	Acc@1 78.125 (84.375)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 03:22:41 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.095 (0.124)	Loss 0.6019 (0.6686)	Acc@1 90.625 (84.970)	Acc@5 96.875 (98.661)	Mem 4879MB
[2022-05-31 03:22:42 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.091 (0.114)	Loss 0.6119 (0.6695)	Acc@1 87.500 (84.778)	Acc@5 100.000 (98.488)	Mem 4879MB
[2022-05-31 03:22:43 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.105 (0.110)	Loss 0.4908 (0.6494)	Acc@1 90.625 (85.595)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 03:22:44 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.097 (0.107)	Loss 0.9336 (0.6615)	Acc@1 78.125 (85.049)	Acc@5 90.625 (98.223)	Mem 4879MB
[2022-05-31 03:22:45 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.100 (0.104)	Loss 0.6383 (0.6690)	Acc@1 84.375 (84.734)	Acc@5 100.000 (98.258)	Mem 4879MB
[2022-05-31 03:22:46 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.103)	Loss 1.0943 (0.6706)	Acc@1 62.500 (84.507)	Acc@5 96.875 (98.327)	Mem 4879MB
[2022-05-31 03:22:47 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.7877 (0.6731)	Acc@1 75.000 (84.414)	Acc@5 96.875 (98.302)	Mem 4879MB
[2022-05-31 03:22:48 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.7988 (0.6737)	Acc@1 81.250 (84.478)	Acc@5 96.875 (98.283)	Mem 4879MB
[2022-05-31 03:22:49 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.4802 (0.6746)	Acc@1 87.500 (84.499)	Acc@5 100.000 (98.236)	Mem 4879MB
[2022-05-31 03:22:50 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.097 (0.100)	Loss 0.6518 (0.6764)	Acc@1 84.375 (84.375)	Acc@5 100.000 (98.311)	Mem 4879MB
[2022-05-31 03:22:51 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.5673 (0.6820)	Acc@1 84.375 (84.375)	Acc@5 100.000 (98.244)	Mem 4879MB
[2022-05-31 03:22:52 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.7169 (0.6836)	Acc@1 78.125 (84.208)	Acc@5 100.000 (98.211)	Mem 4879MB
[2022-05-31 03:22:53 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.109 (0.099)	Loss 0.6559 (0.6857)	Acc@1 81.250 (84.176)	Acc@5 100.000 (98.094)	Mem 4879MB
[2022-05-31 03:22:54 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.099 (0.098)	Loss 0.5449 (0.6820)	Acc@1 87.500 (84.272)	Acc@5 96.875 (98.096)	Mem 4879MB
[2022-05-31 03:22:55 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.091 (0.098)	Loss 0.6290 (0.6732)	Acc@1 87.500 (84.589)	Acc@5 100.000 (98.214)	Mem 4879MB
[2022-05-31 03:22:56 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 0.7948 (0.6723)	Acc@1 75.000 (84.594)	Acc@5 100.000 (98.227)	Mem 4879MB
[2022-05-31 03:22:56 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.097)	Loss 0.4893 (0.6676)	Acc@1 93.750 (84.841)	Acc@5 100.000 (98.239)	Mem 4879MB
[2022-05-31 03:22:57 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.8300 (0.6689)	Acc@1 75.000 (84.784)	Acc@5 100.000 (98.298)	Mem 4879MB
[2022-05-31 03:22:58 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.7882 (0.6705)	Acc@1 78.125 (84.733)	Acc@5 100.000 (98.368)	Mem 4879MB
[2022-05-31 03:22:59 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.097)	Loss 1.0495 (0.6765)	Acc@1 78.125 (84.523)	Acc@5 90.625 (98.267)	Mem 4879MB
[2022-05-31 03:23:00 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.092 (0.097)	Loss 0.6806 (0.6799)	Acc@1 81.250 (84.389)	Acc@5 100.000 (98.289)	Mem 4879MB
[2022-05-31 03:23:01 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 0.5696 (0.6817)	Acc@1 84.375 (84.321)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 03:23:02 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.099 (0.097)	Loss 0.5861 (0.6807)	Acc@1 84.375 (84.349)	Acc@5 100.000 (98.314)	Mem 4879MB
[2022-05-31 03:23:03 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.7126 (0.6870)	Acc@1 84.375 (84.089)	Acc@5 100.000 (98.282)	Mem 4879MB
[2022-05-31 03:23:04 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.095 (0.096)	Loss 0.6254 (0.6870)	Acc@1 81.250 (84.052)	Acc@5 100.000 (98.312)	Mem 4879MB
[2022-05-31 03:23:05 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.4778 (0.6860)	Acc@1 90.625 (84.098)	Acc@5 100.000 (98.270)	Mem 4879MB
[2022-05-31 03:23:06 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.091 (0.096)	Loss 0.5173 (0.6856)	Acc@1 81.250 (83.986)	Acc@5 100.000 (98.298)	Mem 4879MB
[2022-05-31 03:23:07 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.098 (0.096)	Loss 0.6467 (0.6853)	Acc@1 84.375 (83.978)	Acc@5 100.000 (98.314)	Mem 4879MB
[2022-05-31 03:23:08 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.108 (0.096)	Loss 0.6566 (0.6853)	Acc@1 78.125 (83.929)	Acc@5 100.000 (98.297)	Mem 4879MB
[2022-05-31 03:23:09 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6840 (0.6859)	Acc@1 87.500 (83.933)	Acc@5 93.750 (98.262)	Mem 4879MB
[2022-05-31 03:23:09 MetaFG_0] (main.py 330): INFO  * Acc@1 83.950 Acc@5 98.270
[2022-05-31 03:23:09 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 84.0%
[2022-05-31 03:23:09 MetaFG_0] (main.py 171): INFO Max accuracy: 83.95%
[2022-05-31 03:23:10 MetaFG_0] (main.py 265): INFO Train: [36/300][0/1562]	eta 0:28:43 lr 0.000006	time 1.1034 (1.1034)	loss 1.7441 (1.7441)	grad_norm 34.1801 (34.1801)	mem 4879MB
[2022-05-31 03:23:13 MetaFG_0] (main.py 265): INFO Train: [36/300][10/1562]	eta 0:09:50 lr 0.000006	time 0.2933 (0.3806)	loss 1.5931 (1.5598)	grad_norm 18.0072 (27.3079)	mem 4879MB
[2022-05-31 03:23:16 MetaFG_0] (main.py 265): INFO Train: [36/300][20/1562]	eta 0:08:50 lr 0.000006	time 0.2933 (0.3440)	loss 1.0658 (1.4986)	grad_norm 45.8840 (27.3941)	mem 4879MB
[2022-05-31 03:23:19 MetaFG_0] (main.py 265): INFO Train: [36/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.3007 (0.3307)	loss 0.9540 (1.4443)	grad_norm 28.8229 (26.8399)	mem 4879MB
[2022-05-31 03:23:22 MetaFG_0] (main.py 265): INFO Train: [36/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.2948 (0.3241)	loss 1.6727 (1.4644)	grad_norm 30.7235 (28.0483)	mem 4879MB
[2022-05-31 03:23:25 MetaFG_0] (main.py 265): INFO Train: [36/300][50/1562]	eta 0:08:05 lr 0.000006	time 0.2992 (0.3211)	loss 1.4280 (1.4601)	grad_norm 22.7999 (28.7230)	mem 4879MB
[2022-05-31 03:23:28 MetaFG_0] (main.py 265): INFO Train: [36/300][60/1562]	eta 0:07:58 lr 0.000006	time 0.3009 (0.3183)	loss 1.2805 (1.4452)	grad_norm 22.7619 (28.2386)	mem 4879MB
[2022-05-31 03:23:31 MetaFG_0] (main.py 265): INFO Train: [36/300][70/1562]	eta 0:07:52 lr 0.000006	time 0.2996 (0.3165)	loss 1.3498 (1.4504)	grad_norm 26.2137 (28.8217)	mem 4879MB
[2022-05-31 03:23:34 MetaFG_0] (main.py 265): INFO Train: [36/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2980 (0.3148)	loss 1.6311 (1.4619)	grad_norm 27.8039 (29.5012)	mem 4879MB
[2022-05-31 03:23:37 MetaFG_0] (main.py 265): INFO Train: [36/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2989 (0.3136)	loss 1.5337 (1.4807)	grad_norm 33.2515 (29.5501)	mem 4879MB
[2022-05-31 03:23:40 MetaFG_0] (main.py 265): INFO Train: [36/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2994 (0.3126)	loss 1.7168 (1.4816)	grad_norm 41.7423 (29.2747)	mem 4879MB
[2022-05-31 03:23:43 MetaFG_0] (main.py 265): INFO Train: [36/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2949 (0.3118)	loss 1.5092 (1.4712)	grad_norm 33.3368 (29.5624)	mem 4879MB
[2022-05-31 03:23:47 MetaFG_0] (main.py 265): INFO Train: [36/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2928 (0.3112)	loss 1.8226 (1.4652)	grad_norm 20.2880 (29.7638)	mem 4879MB
[2022-05-31 03:23:50 MetaFG_0] (main.py 265): INFO Train: [36/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2935 (0.3105)	loss 1.3396 (1.4691)	grad_norm 37.6365 (30.1015)	mem 4879MB
[2022-05-31 03:23:53 MetaFG_0] (main.py 265): INFO Train: [36/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2935 (0.3101)	loss 1.6065 (1.4671)	grad_norm 29.5767 (29.8412)	mem 4879MB
[2022-05-31 03:23:56 MetaFG_0] (main.py 265): INFO Train: [36/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.3023 (0.3098)	loss 1.5729 (1.4761)	grad_norm 40.8441 (29.6674)	mem 4879MB
[2022-05-31 03:23:59 MetaFG_0] (main.py 265): INFO Train: [36/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3012 (0.3095)	loss 1.1401 (1.4726)	grad_norm 25.6294 (29.9111)	mem 4879MB
[2022-05-31 03:24:02 MetaFG_0] (main.py 265): INFO Train: [36/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.3057 (0.3094)	loss 1.6318 (1.4743)	grad_norm 25.1442 (29.8288)	mem 4879MB
[2022-05-31 03:24:05 MetaFG_0] (main.py 265): INFO Train: [36/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2954 (0.3091)	loss 1.4574 (1.4707)	grad_norm 25.8138 (29.7275)	mem 4879MB
[2022-05-31 03:24:08 MetaFG_0] (main.py 265): INFO Train: [36/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2956 (0.3089)	loss 1.4568 (1.4716)	grad_norm 19.2273 (29.9042)	mem 4879MB
[2022-05-31 03:24:11 MetaFG_0] (main.py 265): INFO Train: [36/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2978 (0.3088)	loss 1.0494 (1.4655)	grad_norm 43.0441 (29.8307)	mem 4879MB
[2022-05-31 03:24:14 MetaFG_0] (main.py 265): INFO Train: [36/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2983 (0.3085)	loss 1.6833 (1.4632)	grad_norm 24.7673 (nan)	mem 4879MB
[2022-05-31 03:24:17 MetaFG_0] (main.py 265): INFO Train: [36/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2948 (0.3082)	loss 1.4136 (1.4612)	grad_norm 25.2122 (nan)	mem 4879MB
[2022-05-31 03:24:20 MetaFG_0] (main.py 265): INFO Train: [36/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2925 (0.3081)	loss 1.6832 (1.4569)	grad_norm 38.9813 (nan)	mem 4879MB
[2022-05-31 03:24:23 MetaFG_0] (main.py 265): INFO Train: [36/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2926 (0.3079)	loss 1.5631 (1.4574)	grad_norm 33.8502 (nan)	mem 4879MB
[2022-05-31 03:24:26 MetaFG_0] (main.py 265): INFO Train: [36/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2920 (0.3078)	loss 1.4616 (1.4578)	grad_norm 25.9136 (nan)	mem 4879MB
[2022-05-31 03:24:29 MetaFG_0] (main.py 265): INFO Train: [36/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2931 (0.3076)	loss 1.3605 (1.4577)	grad_norm 52.3091 (nan)	mem 4879MB
[2022-05-31 03:24:32 MetaFG_0] (main.py 265): INFO Train: [36/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2925 (0.3075)	loss 1.3370 (1.4557)	grad_norm 34.6402 (nan)	mem 4879MB
[2022-05-31 03:24:35 MetaFG_0] (main.py 265): INFO Train: [36/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2993 (0.3074)	loss 1.6596 (1.4570)	grad_norm 38.1616 (nan)	mem 4879MB
[2022-05-31 03:24:38 MetaFG_0] (main.py 265): INFO Train: [36/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2934 (0.3073)	loss 1.2595 (1.4567)	grad_norm 50.1470 (nan)	mem 4879MB
[2022-05-31 03:24:41 MetaFG_0] (main.py 265): INFO Train: [36/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2935 (0.3071)	loss 1.0834 (1.4522)	grad_norm 22.5764 (nan)	mem 4879MB
[2022-05-31 03:24:44 MetaFG_0] (main.py 265): INFO Train: [36/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2995 (0.3071)	loss 1.2531 (1.4501)	grad_norm 23.2252 (nan)	mem 4879MB
[2022-05-31 03:24:47 MetaFG_0] (main.py 265): INFO Train: [36/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2936 (0.3070)	loss 1.3752 (1.4513)	grad_norm 40.1510 (nan)	mem 4879MB
[2022-05-31 03:24:50 MetaFG_0] (main.py 265): INFO Train: [36/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2981 (0.3069)	loss 1.2700 (1.4498)	grad_norm 23.8981 (nan)	mem 4879MB
[2022-05-31 03:24:54 MetaFG_0] (main.py 265): INFO Train: [36/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.3001 (0.3069)	loss 1.2176 (1.4456)	grad_norm 20.9204 (nan)	mem 4879MB
[2022-05-31 03:24:57 MetaFG_0] (main.py 265): INFO Train: [36/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2978 (0.3069)	loss 1.6862 (1.4453)	grad_norm 31.2596 (nan)	mem 4879MB
[2022-05-31 03:25:00 MetaFG_0] (main.py 265): INFO Train: [36/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2919 (0.3068)	loss 1.4044 (1.4446)	grad_norm 43.6098 (nan)	mem 4879MB
[2022-05-31 03:25:03 MetaFG_0] (main.py 265): INFO Train: [36/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2930 (0.3067)	loss 1.1035 (1.4450)	grad_norm 21.9771 (nan)	mem 4879MB
[2022-05-31 03:25:06 MetaFG_0] (main.py 265): INFO Train: [36/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2982 (0.3066)	loss 1.5324 (1.4451)	grad_norm 20.4356 (nan)	mem 4879MB
[2022-05-31 03:25:09 MetaFG_0] (main.py 265): INFO Train: [36/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2932 (0.3065)	loss 1.2695 (1.4456)	grad_norm 26.0876 (nan)	mem 4879MB
[2022-05-31 03:25:12 MetaFG_0] (main.py 265): INFO Train: [36/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2918 (0.3065)	loss 1.5382 (1.4447)	grad_norm 17.0418 (nan)	mem 4879MB
[2022-05-31 03:25:15 MetaFG_0] (main.py 265): INFO Train: [36/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2995 (0.3064)	loss 1.2652 (1.4448)	grad_norm 21.6170 (nan)	mem 4879MB
[2022-05-31 03:25:18 MetaFG_0] (main.py 265): INFO Train: [36/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2934 (0.3064)	loss 1.4130 (1.4457)	grad_norm 23.4124 (nan)	mem 4879MB
[2022-05-31 03:25:21 MetaFG_0] (main.py 265): INFO Train: [36/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2938 (0.3063)	loss 1.3584 (1.4444)	grad_norm 27.5116 (nan)	mem 4879MB
[2022-05-31 03:25:24 MetaFG_0] (main.py 265): INFO Train: [36/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2978 (0.3063)	loss 1.5836 (1.4435)	grad_norm 30.6279 (nan)	mem 4879MB
[2022-05-31 03:25:27 MetaFG_0] (main.py 265): INFO Train: [36/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2935 (0.3062)	loss 0.9347 (1.4439)	grad_norm 25.9926 (nan)	mem 4879MB
[2022-05-31 03:25:30 MetaFG_0] (main.py 265): INFO Train: [36/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2988 (0.3062)	loss 1.5861 (1.4433)	grad_norm 18.6670 (nan)	mem 4879MB
[2022-05-31 03:25:33 MetaFG_0] (main.py 265): INFO Train: [36/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2918 (0.3061)	loss 1.4609 (1.4421)	grad_norm 30.9386 (nan)	mem 4879MB
[2022-05-31 03:25:36 MetaFG_0] (main.py 265): INFO Train: [36/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2929 (0.3060)	loss 1.0567 (1.4410)	grad_norm 20.1510 (nan)	mem 4879MB
[2022-05-31 03:25:39 MetaFG_0] (main.py 265): INFO Train: [36/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2991 (0.3060)	loss 1.4299 (1.4409)	grad_norm 18.9471 (nan)	mem 4879MB
[2022-05-31 03:25:42 MetaFG_0] (main.py 265): INFO Train: [36/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.3006 (0.3060)	loss 1.6008 (1.4411)	grad_norm 34.6866 (nan)	mem 4879MB
[2022-05-31 03:25:45 MetaFG_0] (main.py 265): INFO Train: [36/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2987 (0.3059)	loss 1.4716 (1.4401)	grad_norm 38.1269 (nan)	mem 4879MB
[2022-05-31 03:25:48 MetaFG_0] (main.py 265): INFO Train: [36/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2932 (0.3059)	loss 1.3579 (1.4386)	grad_norm 42.1724 (nan)	mem 4879MB
[2022-05-31 03:25:51 MetaFG_0] (main.py 265): INFO Train: [36/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2992 (0.3059)	loss 1.6020 (1.4372)	grad_norm 20.4190 (nan)	mem 4879MB
[2022-05-31 03:25:54 MetaFG_0] (main.py 265): INFO Train: [36/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2922 (0.3059)	loss 1.6845 (1.4381)	grad_norm 28.4705 (nan)	mem 4879MB
[2022-05-31 03:25:57 MetaFG_0] (main.py 265): INFO Train: [36/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.3005 (0.3059)	loss 1.4561 (1.4384)	grad_norm 20.3959 (nan)	mem 4879MB
[2022-05-31 03:26:01 MetaFG_0] (main.py 265): INFO Train: [36/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2980 (0.3059)	loss 1.2189 (1.4381)	grad_norm 27.7521 (nan)	mem 4879MB
[2022-05-31 03:26:04 MetaFG_0] (main.py 265): INFO Train: [36/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2983 (0.3059)	loss 1.0827 (1.4381)	grad_norm 35.3794 (nan)	mem 4879MB
[2022-05-31 03:26:07 MetaFG_0] (main.py 265): INFO Train: [36/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2958 (0.3059)	loss 1.5838 (1.4383)	grad_norm 24.9342 (nan)	mem 4879MB
[2022-05-31 03:26:10 MetaFG_0] (main.py 265): INFO Train: [36/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2922 (0.3058)	loss 1.6606 (1.4372)	grad_norm 31.8658 (nan)	mem 4879MB
[2022-05-31 03:26:13 MetaFG_0] (main.py 265): INFO Train: [36/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2993 (0.3058)	loss 1.4534 (1.4382)	grad_norm 30.4409 (nan)	mem 4879MB
[2022-05-31 03:26:16 MetaFG_0] (main.py 265): INFO Train: [36/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.3000 (0.3058)	loss 1.6954 (1.4392)	grad_norm 25.2728 (nan)	mem 4879MB
[2022-05-31 03:26:19 MetaFG_0] (main.py 265): INFO Train: [36/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2947 (0.3058)	loss 1.0551 (1.4397)	grad_norm 21.3526 (nan)	mem 4879MB
[2022-05-31 03:26:22 MetaFG_0] (main.py 265): INFO Train: [36/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2987 (0.3057)	loss 1.2605 (1.4379)	grad_norm 33.6140 (nan)	mem 4879MB
[2022-05-31 03:26:25 MetaFG_0] (main.py 265): INFO Train: [36/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2948 (0.3057)	loss 1.0346 (1.4366)	grad_norm 56.3582 (nan)	mem 4879MB
[2022-05-31 03:26:28 MetaFG_0] (main.py 265): INFO Train: [36/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2924 (0.3057)	loss 1.6136 (1.4371)	grad_norm 19.8807 (nan)	mem 4879MB
[2022-05-31 03:26:31 MetaFG_0] (main.py 265): INFO Train: [36/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2932 (0.3056)	loss 1.0327 (1.4348)	grad_norm 28.9499 (nan)	mem 4879MB
[2022-05-31 03:26:34 MetaFG_0] (main.py 265): INFO Train: [36/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2982 (0.3056)	loss 1.6158 (1.4371)	grad_norm 25.9013 (nan)	mem 4879MB
[2022-05-31 03:26:37 MetaFG_0] (main.py 265): INFO Train: [36/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2977 (0.3056)	loss 1.3703 (1.4366)	grad_norm 23.7943 (nan)	mem 4879MB
[2022-05-31 03:26:40 MetaFG_0] (main.py 265): INFO Train: [36/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2926 (0.3056)	loss 1.7080 (1.4357)	grad_norm 28.8502 (nan)	mem 4879MB
[2022-05-31 03:26:43 MetaFG_0] (main.py 265): INFO Train: [36/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2948 (0.3056)	loss 1.1796 (1.4348)	grad_norm 24.8471 (nan)	mem 4879MB
[2022-05-31 03:26:46 MetaFG_0] (main.py 265): INFO Train: [36/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2986 (0.3055)	loss 1.8191 (1.4343)	grad_norm 28.1566 (nan)	mem 4879MB
[2022-05-31 03:26:49 MetaFG_0] (main.py 265): INFO Train: [36/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.3064 (0.3055)	loss 1.4344 (1.4341)	grad_norm 28.0720 (nan)	mem 4879MB
[2022-05-31 03:26:52 MetaFG_0] (main.py 265): INFO Train: [36/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2988 (0.3055)	loss 1.5558 (1.4344)	grad_norm 18.3133 (nan)	mem 4879MB
[2022-05-31 03:26:55 MetaFG_0] (main.py 265): INFO Train: [36/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2980 (0.3055)	loss 1.2421 (1.4343)	grad_norm 16.6762 (nan)	mem 4879MB
[2022-05-31 03:26:58 MetaFG_0] (main.py 265): INFO Train: [36/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2929 (0.3054)	loss 1.4278 (1.4349)	grad_norm 38.3481 (nan)	mem 4879MB
[2022-05-31 03:27:01 MetaFG_0] (main.py 265): INFO Train: [36/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2927 (0.3054)	loss 1.2872 (1.4347)	grad_norm 31.0245 (nan)	mem 4879MB
[2022-05-31 03:27:04 MetaFG_0] (main.py 265): INFO Train: [36/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2919 (0.3054)	loss 1.0238 (1.4338)	grad_norm 31.6560 (nan)	mem 4879MB
[2022-05-31 03:27:07 MetaFG_0] (main.py 265): INFO Train: [36/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.3002 (0.3054)	loss 1.3649 (1.4336)	grad_norm 23.2776 (nan)	mem 4879MB
[2022-05-31 03:27:10 MetaFG_0] (main.py 265): INFO Train: [36/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2984 (0.3054)	loss 1.2193 (1.4329)	grad_norm 23.3695 (nan)	mem 4879MB
[2022-05-31 03:27:14 MetaFG_0] (main.py 265): INFO Train: [36/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2919 (0.3054)	loss 1.7189 (1.4326)	grad_norm 35.1766 (nan)	mem 4879MB
[2022-05-31 03:27:17 MetaFG_0] (main.py 265): INFO Train: [36/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2989 (0.3054)	loss 1.3650 (1.4313)	grad_norm 33.3323 (nan)	mem 4879MB
[2022-05-31 03:27:20 MetaFG_0] (main.py 265): INFO Train: [36/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2916 (0.3054)	loss 1.6025 (1.4317)	grad_norm 21.3222 (nan)	mem 4879MB
[2022-05-31 03:27:23 MetaFG_0] (main.py 265): INFO Train: [36/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2984 (0.3054)	loss 1.4135 (1.4331)	grad_norm 46.6326 (nan)	mem 4879MB
[2022-05-31 03:27:26 MetaFG_0] (main.py 265): INFO Train: [36/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2919 (0.3054)	loss 1.5885 (1.4324)	grad_norm 16.9008 (nan)	mem 4879MB
[2022-05-31 03:27:29 MetaFG_0] (main.py 265): INFO Train: [36/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2993 (0.3054)	loss 1.5598 (1.4318)	grad_norm 15.9198 (nan)	mem 4879MB
[2022-05-31 03:27:32 MetaFG_0] (main.py 265): INFO Train: [36/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2936 (0.3054)	loss 1.2552 (1.4314)	grad_norm 21.9034 (nan)	mem 4879MB
[2022-05-31 03:27:35 MetaFG_0] (main.py 265): INFO Train: [36/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3002 (0.3054)	loss 1.3634 (1.4319)	grad_norm 16.6008 (nan)	mem 4879MB
[2022-05-31 03:27:38 MetaFG_0] (main.py 265): INFO Train: [36/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2934 (0.3054)	loss 1.5899 (1.4311)	grad_norm 24.4274 (nan)	mem 4879MB
[2022-05-31 03:27:41 MetaFG_0] (main.py 265): INFO Train: [36/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2928 (0.3053)	loss 1.7629 (1.4313)	grad_norm 24.4370 (nan)	mem 4879MB
[2022-05-31 03:27:44 MetaFG_0] (main.py 265): INFO Train: [36/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2930 (0.3053)	loss 1.1529 (1.4305)	grad_norm 52.9143 (nan)	mem 4879MB
[2022-05-31 03:27:47 MetaFG_0] (main.py 265): INFO Train: [36/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2934 (0.3053)	loss 1.2427 (1.4297)	grad_norm 40.9727 (nan)	mem 4879MB
[2022-05-31 03:27:50 MetaFG_0] (main.py 265): INFO Train: [36/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2929 (0.3053)	loss 1.4570 (1.4299)	grad_norm 42.4587 (nan)	mem 4879MB
[2022-05-31 03:27:53 MetaFG_0] (main.py 265): INFO Train: [36/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2936 (0.3053)	loss 1.2936 (1.4298)	grad_norm 15.0770 (nan)	mem 4879MB
[2022-05-31 03:27:56 MetaFG_0] (main.py 265): INFO Train: [36/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2986 (0.3053)	loss 1.5054 (1.4303)	grad_norm 21.1014 (nan)	mem 4879MB
[2022-05-31 03:27:59 MetaFG_0] (main.py 265): INFO Train: [36/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.3007 (0.3053)	loss 0.9800 (1.4287)	grad_norm 30.4109 (nan)	mem 4879MB
[2022-05-31 03:28:02 MetaFG_0] (main.py 265): INFO Train: [36/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2926 (0.3053)	loss 1.9003 (1.4280)	grad_norm 25.1886 (nan)	mem 4879MB
[2022-05-31 03:28:05 MetaFG_0] (main.py 265): INFO Train: [36/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2995 (0.3053)	loss 1.4991 (1.4286)	grad_norm 30.3851 (nan)	mem 4879MB
[2022-05-31 03:28:08 MetaFG_0] (main.py 265): INFO Train: [36/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2942 (0.3053)	loss 1.4662 (1.4281)	grad_norm 26.1226 (nan)	mem 4879MB
[2022-05-31 03:28:11 MetaFG_0] (main.py 265): INFO Train: [36/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2981 (0.3053)	loss 1.3646 (1.4281)	grad_norm 17.3590 (nan)	mem 4879MB
[2022-05-31 03:28:14 MetaFG_0] (main.py 265): INFO Train: [36/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2927 (0.3053)	loss 1.0022 (1.4279)	grad_norm 22.3349 (nan)	mem 4879MB
[2022-05-31 03:28:18 MetaFG_0] (main.py 265): INFO Train: [36/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2991 (0.3053)	loss 0.9415 (1.4261)	grad_norm 24.0054 (nan)	mem 4879MB
[2022-05-31 03:28:21 MetaFG_0] (main.py 265): INFO Train: [36/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2923 (0.3053)	loss 1.3570 (1.4256)	grad_norm 20.1596 (nan)	mem 4879MB
[2022-05-31 03:28:24 MetaFG_0] (main.py 265): INFO Train: [36/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2951 (0.3052)	loss 1.5044 (1.4262)	grad_norm 43.5644 (nan)	mem 4879MB
[2022-05-31 03:28:27 MetaFG_0] (main.py 265): INFO Train: [36/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2920 (0.3053)	loss 1.0212 (1.4261)	grad_norm 28.2311 (nan)	mem 4879MB
[2022-05-31 03:28:30 MetaFG_0] (main.py 265): INFO Train: [36/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2919 (0.3052)	loss 0.9531 (1.4263)	grad_norm 35.2814 (nan)	mem 4879MB
[2022-05-31 03:28:33 MetaFG_0] (main.py 265): INFO Train: [36/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2979 (0.3052)	loss 1.0777 (1.4252)	grad_norm 25.9940 (nan)	mem 4879MB
[2022-05-31 03:28:36 MetaFG_0] (main.py 265): INFO Train: [36/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2976 (0.3053)	loss 1.7154 (1.4257)	grad_norm 27.9320 (nan)	mem 4879MB
[2022-05-31 03:28:39 MetaFG_0] (main.py 265): INFO Train: [36/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2957 (0.3052)	loss 1.4728 (1.4263)	grad_norm 53.6780 (nan)	mem 4879MB
[2022-05-31 03:28:42 MetaFG_0] (main.py 265): INFO Train: [36/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2929 (0.3053)	loss 1.6459 (1.4269)	grad_norm 35.9127 (nan)	mem 4879MB
[2022-05-31 03:28:45 MetaFG_0] (main.py 265): INFO Train: [36/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.3004 (0.3052)	loss 1.4695 (1.4265)	grad_norm 24.2936 (nan)	mem 4879MB
[2022-05-31 03:28:48 MetaFG_0] (main.py 265): INFO Train: [36/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2924 (0.3052)	loss 1.6646 (1.4269)	grad_norm 26.6372 (nan)	mem 4879MB
[2022-05-31 03:28:51 MetaFG_0] (main.py 265): INFO Train: [36/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2991 (0.3053)	loss 1.2367 (1.4261)	grad_norm 23.4937 (nan)	mem 4879MB
[2022-05-31 03:28:54 MetaFG_0] (main.py 265): INFO Train: [36/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2981 (0.3052)	loss 1.3315 (1.4257)	grad_norm 23.8992 (nan)	mem 4879MB
[2022-05-31 03:28:57 MetaFG_0] (main.py 265): INFO Train: [36/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2928 (0.3052)	loss 1.4450 (1.4258)	grad_norm 50.2674 (nan)	mem 4879MB
[2022-05-31 03:29:00 MetaFG_0] (main.py 265): INFO Train: [36/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2937 (0.3052)	loss 1.5217 (1.4254)	grad_norm 26.6005 (nan)	mem 4879MB
[2022-05-31 03:29:03 MetaFG_0] (main.py 265): INFO Train: [36/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2928 (0.3052)	loss 1.3328 (1.4255)	grad_norm 15.7333 (nan)	mem 4879MB
[2022-05-31 03:29:06 MetaFG_0] (main.py 265): INFO Train: [36/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2919 (0.3052)	loss 1.3268 (1.4242)	grad_norm 41.5457 (nan)	mem 4879MB
[2022-05-31 03:29:09 MetaFG_0] (main.py 265): INFO Train: [36/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2984 (0.3052)	loss 1.7178 (1.4247)	grad_norm 29.4424 (nan)	mem 4879MB
[2022-05-31 03:29:12 MetaFG_0] (main.py 265): INFO Train: [36/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2931 (0.3052)	loss 1.5870 (1.4250)	grad_norm 40.6941 (nan)	mem 4879MB
[2022-05-31 03:29:15 MetaFG_0] (main.py 265): INFO Train: [36/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2936 (0.3052)	loss 1.3010 (1.4243)	grad_norm 39.9139 (nan)	mem 4879MB
[2022-05-31 03:29:18 MetaFG_0] (main.py 265): INFO Train: [36/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2932 (0.3052)	loss 1.0583 (1.4240)	grad_norm 53.2781 (nan)	mem 4879MB
[2022-05-31 03:29:22 MetaFG_0] (main.py 265): INFO Train: [36/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2994 (0.3052)	loss 1.6498 (1.4243)	grad_norm 26.9596 (nan)	mem 4879MB
[2022-05-31 03:29:25 MetaFG_0] (main.py 265): INFO Train: [36/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2997 (0.3052)	loss 1.1572 (1.4236)	grad_norm 38.4042 (nan)	mem 4879MB
[2022-05-31 03:29:28 MetaFG_0] (main.py 265): INFO Train: [36/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2946 (0.3052)	loss 1.5596 (1.4234)	grad_norm 17.8995 (nan)	mem 4879MB
[2022-05-31 03:29:31 MetaFG_0] (main.py 265): INFO Train: [36/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2991 (0.3052)	loss 1.5279 (1.4233)	grad_norm 23.4215 (nan)	mem 4879MB
[2022-05-31 03:29:34 MetaFG_0] (main.py 265): INFO Train: [36/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2978 (0.3051)	loss 1.4954 (1.4217)	grad_norm 24.8993 (nan)	mem 4879MB
[2022-05-31 03:29:37 MetaFG_0] (main.py 265): INFO Train: [36/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2936 (0.3051)	loss 1.2339 (1.4212)	grad_norm 28.8264 (nan)	mem 4879MB
[2022-05-31 03:29:40 MetaFG_0] (main.py 265): INFO Train: [36/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2945 (0.3051)	loss 1.4915 (1.4216)	grad_norm 20.8970 (nan)	mem 4879MB
[2022-05-31 03:29:43 MetaFG_0] (main.py 265): INFO Train: [36/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2924 (0.3051)	loss 1.3371 (1.4219)	grad_norm 18.3550 (nan)	mem 4879MB
[2022-05-31 03:29:46 MetaFG_0] (main.py 265): INFO Train: [36/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2936 (0.3051)	loss 1.3350 (1.4218)	grad_norm 33.5057 (nan)	mem 4879MB
[2022-05-31 03:29:49 MetaFG_0] (main.py 265): INFO Train: [36/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2932 (0.3051)	loss 1.4632 (1.4220)	grad_norm 24.4586 (nan)	mem 4879MB
[2022-05-31 03:29:52 MetaFG_0] (main.py 265): INFO Train: [36/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3033 (0.3050)	loss 1.4035 (1.4212)	grad_norm 51.4127 (nan)	mem 4879MB
[2022-05-31 03:29:55 MetaFG_0] (main.py 265): INFO Train: [36/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2999 (0.3050)	loss 1.0757 (1.4212)	grad_norm 39.3518 (nan)	mem 4879MB
[2022-05-31 03:29:58 MetaFG_0] (main.py 265): INFO Train: [36/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2931 (0.3051)	loss 1.3398 (1.4218)	grad_norm 20.1256 (nan)	mem 4879MB
[2022-05-31 03:30:01 MetaFG_0] (main.py 265): INFO Train: [36/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2979 (0.3051)	loss 1.7779 (1.4224)	grad_norm 46.6171 (nan)	mem 4879MB
[2022-05-31 03:30:04 MetaFG_0] (main.py 265): INFO Train: [36/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2930 (0.3051)	loss 1.1333 (1.4229)	grad_norm 31.8855 (nan)	mem 4879MB
[2022-05-31 03:30:07 MetaFG_0] (main.py 265): INFO Train: [36/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.3047 (0.3051)	loss 1.3597 (1.4233)	grad_norm 24.2279 (nan)	mem 4879MB
[2022-05-31 03:30:10 MetaFG_0] (main.py 265): INFO Train: [36/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3047 (0.3051)	loss 1.3969 (1.4236)	grad_norm 36.2740 (nan)	mem 4879MB
[2022-05-31 03:30:13 MetaFG_0] (main.py 265): INFO Train: [36/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2920 (0.3050)	loss 1.3337 (1.4241)	grad_norm 28.9082 (nan)	mem 4879MB
[2022-05-31 03:30:16 MetaFG_0] (main.py 265): INFO Train: [36/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2919 (0.3050)	loss 1.3949 (1.4245)	grad_norm 26.3117 (nan)	mem 4879MB
[2022-05-31 03:30:19 MetaFG_0] (main.py 265): INFO Train: [36/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2936 (0.3050)	loss 1.1858 (1.4243)	grad_norm 31.8770 (nan)	mem 4879MB
[2022-05-31 03:30:22 MetaFG_0] (main.py 265): INFO Train: [36/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3006 (0.3050)	loss 1.6630 (1.4243)	grad_norm 47.7985 (nan)	mem 4879MB
[2022-05-31 03:30:25 MetaFG_0] (main.py 265): INFO Train: [36/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2921 (0.3050)	loss 1.5656 (1.4240)	grad_norm 22.8977 (nan)	mem 4879MB
[2022-05-31 03:30:28 MetaFG_0] (main.py 265): INFO Train: [36/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2979 (0.3050)	loss 1.5713 (1.4239)	grad_norm 18.3468 (nan)	mem 4879MB
[2022-05-31 03:30:31 MetaFG_0] (main.py 265): INFO Train: [36/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2918 (0.3050)	loss 1.7862 (1.4242)	grad_norm 32.9345 (nan)	mem 4879MB
[2022-05-31 03:30:35 MetaFG_0] (main.py 265): INFO Train: [36/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2985 (0.3050)	loss 1.1824 (1.4246)	grad_norm 43.2786 (nan)	mem 4879MB
[2022-05-31 03:30:38 MetaFG_0] (main.py 265): INFO Train: [36/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.3010 (0.3050)	loss 0.9365 (1.4246)	grad_norm 28.1733 (nan)	mem 4879MB
[2022-05-31 03:30:41 MetaFG_0] (main.py 265): INFO Train: [36/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2926 (0.3050)	loss 1.3991 (1.4238)	grad_norm 68.8796 (nan)	mem 4879MB
[2022-05-31 03:30:44 MetaFG_0] (main.py 265): INFO Train: [36/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2977 (0.3050)	loss 1.4029 (1.4240)	grad_norm 35.0596 (nan)	mem 4879MB
[2022-05-31 03:30:47 MetaFG_0] (main.py 265): INFO Train: [36/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2989 (0.3050)	loss 1.0980 (1.4239)	grad_norm 67.6860 (nan)	mem 4879MB
[2022-05-31 03:30:50 MetaFG_0] (main.py 265): INFO Train: [36/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2945 (0.3050)	loss 1.3272 (1.4234)	grad_norm 20.7984 (nan)	mem 4879MB
[2022-05-31 03:30:53 MetaFG_0] (main.py 265): INFO Train: [36/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2935 (0.3050)	loss 1.3938 (1.4238)	grad_norm 22.0855 (nan)	mem 4879MB
[2022-05-31 03:30:56 MetaFG_0] (main.py 265): INFO Train: [36/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2920 (0.3050)	loss 1.6613 (1.4239)	grad_norm 30.7886 (nan)	mem 4879MB
[2022-05-31 03:30:59 MetaFG_0] (main.py 265): INFO Train: [36/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2983 (0.3050)	loss 1.3218 (1.4240)	grad_norm 16.8366 (nan)	mem 4879MB
[2022-05-31 03:31:02 MetaFG_0] (main.py 265): INFO Train: [36/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2931 (0.3050)	loss 0.9373 (1.4237)	grad_norm 43.3089 (nan)	mem 4879MB
[2022-05-31 03:31:05 MetaFG_0] (main.py 265): INFO Train: [36/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2927 (0.3050)	loss 1.5035 (1.4243)	grad_norm 27.2895 (nan)	mem 4879MB
[2022-05-31 03:31:05 MetaFG_0] (main.py 272): INFO EPOCH 36 training takes 0:07:56
[2022-05-31 03:31:05 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_36.pth saving......
[2022-05-31 03:31:06 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_36.pth saved !!!
[2022-05-31 03:31:06 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:31:08 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:31:08 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:31:08 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.698 (0.698)	Loss 0.8017 (0.8017)	Acc@1 75.000 (75.000)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 03:31:09 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.152)	Loss 0.5052 (0.6696)	Acc@1 90.625 (86.364)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 03:31:10 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.124)	Loss 0.6013 (0.6444)	Acc@1 87.500 (87.054)	Acc@5 96.875 (98.958)	Mem 4879MB
[2022-05-31 03:31:11 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.101 (0.114)	Loss 0.7642 (0.6888)	Acc@1 84.375 (85.383)	Acc@5 100.000 (98.790)	Mem 4879MB
[2022-05-31 03:31:12 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.103 (0.110)	Loss 0.6620 (0.6740)	Acc@1 84.375 (85.671)	Acc@5 100.000 (98.933)	Mem 4879MB
[2022-05-31 03:31:13 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.107)	Loss 0.7684 (0.6889)	Acc@1 84.375 (85.049)	Acc@5 96.875 (98.591)	Mem 4879MB
[2022-05-31 03:31:14 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.091 (0.104)	Loss 0.7232 (0.6953)	Acc@1 87.500 (84.580)	Acc@5 93.750 (98.566)	Mem 4879MB
[2022-05-31 03:31:15 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.100 (0.103)	Loss 0.7411 (0.7052)	Acc@1 90.625 (84.463)	Acc@5 100.000 (98.371)	Mem 4879MB
[2022-05-31 03:31:16 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.7007 (0.7012)	Acc@1 87.500 (84.761)	Acc@5 96.875 (98.341)	Mem 4879MB
[2022-05-31 03:31:17 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.093 (0.100)	Loss 1.0194 (0.7101)	Acc@1 78.125 (84.512)	Acc@5 93.750 (98.249)	Mem 4879MB
[2022-05-31 03:31:18 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.7269 (0.7051)	Acc@1 81.250 (84.561)	Acc@5 96.875 (98.360)	Mem 4879MB
[2022-05-31 03:31:19 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.099)	Loss 0.4865 (0.7014)	Acc@1 93.750 (84.713)	Acc@5 100.000 (98.311)	Mem 4879MB
[2022-05-31 03:31:20 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.088 (0.099)	Loss 0.7607 (0.7012)	Acc@1 81.250 (84.737)	Acc@5 100.000 (98.244)	Mem 4879MB
[2022-05-31 03:31:21 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.098)	Loss 0.8468 (0.6991)	Acc@1 78.125 (84.709)	Acc@5 93.750 (98.259)	Mem 4879MB
[2022-05-31 03:31:21 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.097 (0.098)	Loss 0.5594 (0.7001)	Acc@1 81.250 (84.597)	Acc@5 100.000 (98.227)	Mem 4879MB
[2022-05-31 03:31:22 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.098)	Loss 0.9796 (0.7007)	Acc@1 71.875 (84.499)	Acc@5 100.000 (98.303)	Mem 4879MB
[2022-05-31 03:31:23 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.8516 (0.7015)	Acc@1 81.250 (84.453)	Acc@5 96.875 (98.331)	Mem 4879MB
[2022-05-31 03:31:24 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.097)	Loss 0.6251 (0.6993)	Acc@1 78.125 (84.430)	Acc@5 100.000 (98.428)	Mem 4879MB
[2022-05-31 03:31:25 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.097)	Loss 0.6758 (0.6994)	Acc@1 78.125 (84.427)	Acc@5 100.000 (98.394)	Mem 4879MB
[2022-05-31 03:31:26 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.091 (0.097)	Loss 0.5641 (0.7008)	Acc@1 87.500 (84.342)	Acc@5 100.000 (98.331)	Mem 4879MB
[2022-05-31 03:31:27 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.097)	Loss 0.6709 (0.6997)	Acc@1 87.500 (84.406)	Acc@5 96.875 (98.368)	Mem 4879MB
[2022-05-31 03:31:28 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.097)	Loss 0.7239 (0.7014)	Acc@1 81.250 (84.257)	Acc@5 96.875 (98.341)	Mem 4879MB
[2022-05-31 03:31:29 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.097)	Loss 0.5956 (0.7034)	Acc@1 90.625 (84.191)	Acc@5 96.875 (98.346)	Mem 4879MB
[2022-05-31 03:31:30 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.103 (0.096)	Loss 0.7441 (0.7037)	Acc@1 71.875 (83.996)	Acc@5 100.000 (98.363)	Mem 4879MB
[2022-05-31 03:31:31 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.093 (0.097)	Loss 0.7530 (0.7015)	Acc@1 81.250 (84.025)	Acc@5 96.875 (98.353)	Mem 4879MB
[2022-05-31 03:31:32 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.101 (0.097)	Loss 0.5521 (0.7011)	Acc@1 87.500 (84.001)	Acc@5 100.000 (98.369)	Mem 4879MB
[2022-05-31 03:31:33 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 0.6818 (0.6983)	Acc@1 84.375 (84.076)	Acc@5 100.000 (98.384)	Mem 4879MB
[2022-05-31 03:31:34 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.097)	Loss 0.5934 (0.6991)	Acc@1 84.375 (84.029)	Acc@5 100.000 (98.409)	Mem 4879MB
[2022-05-31 03:31:35 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.097)	Loss 0.6483 (0.6988)	Acc@1 87.500 (84.041)	Acc@5 100.000 (98.410)	Mem 4879MB
[2022-05-31 03:31:36 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.096)	Loss 0.6856 (0.7008)	Acc@1 84.375 (84.031)	Acc@5 100.000 (98.400)	Mem 4879MB
[2022-05-31 03:31:37 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.088 (0.096)	Loss 0.7059 (0.7015)	Acc@1 90.625 (84.043)	Acc@5 96.875 (98.401)	Mem 4879MB
[2022-05-31 03:31:38 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.8884 (0.7020)	Acc@1 75.000 (83.983)	Acc@5 96.875 (98.392)	Mem 4879MB
[2022-05-31 03:31:38 MetaFG_0] (main.py 330): INFO  * Acc@1 83.970 Acc@5 98.370
[2022-05-31 03:31:38 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 84.0%
[2022-05-31 03:31:38 MetaFG_0] (main.py 171): INFO Max accuracy: 83.97%
[2022-05-31 03:31:39 MetaFG_0] (main.py 265): INFO Train: [37/300][0/1562]	eta 0:26:30 lr 0.000006	time 1.0181 (1.0181)	loss 1.2807 (1.2807)	grad_norm 35.1523 (35.1523)	mem 4879MB
[2022-05-31 03:31:42 MetaFG_0] (main.py 265): INFO Train: [37/300][10/1562]	eta 0:09:43 lr 0.000006	time 0.2977 (0.3759)	loss 1.3478 (1.4285)	grad_norm 21.6456 (25.3537)	mem 4879MB
[2022-05-31 03:31:45 MetaFG_0] (main.py 265): INFO Train: [37/300][20/1562]	eta 0:08:47 lr 0.000006	time 0.2980 (0.3419)	loss 1.2413 (1.3492)	grad_norm 31.3763 (26.7273)	mem 4879MB
[2022-05-31 03:31:48 MetaFG_0] (main.py 265): INFO Train: [37/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2930 (0.3296)	loss 1.0816 (1.3758)	grad_norm 28.5643 (27.9356)	mem 4879MB
[2022-05-31 03:31:51 MetaFG_0] (main.py 265): INFO Train: [37/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2937 (0.3237)	loss 1.5343 (1.3864)	grad_norm 25.9656 (27.3223)	mem 4879MB
[2022-05-31 03:31:54 MetaFG_0] (main.py 265): INFO Train: [37/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2921 (0.3196)	loss 1.0781 (1.3822)	grad_norm 32.5738 (28.1332)	mem 4879MB
[2022-05-31 03:31:57 MetaFG_0] (main.py 265): INFO Train: [37/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2990 (0.3170)	loss 1.3511 (1.3756)	grad_norm 35.6056 (27.9088)	mem 4879MB
[2022-05-31 03:32:00 MetaFG_0] (main.py 265): INFO Train: [37/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2916 (0.3152)	loss 1.3721 (1.3730)	grad_norm 29.0355 (28.7242)	mem 4879MB
[2022-05-31 03:32:03 MetaFG_0] (main.py 265): INFO Train: [37/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2988 (0.3139)	loss 1.6125 (1.3583)	grad_norm 29.6731 (29.5372)	mem 4879MB
[2022-05-31 03:32:06 MetaFG_0] (main.py 265): INFO Train: [37/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2934 (0.3129)	loss 1.6952 (1.3516)	grad_norm 47.2749 (30.2031)	mem 4879MB
[2022-05-31 03:32:09 MetaFG_0] (main.py 265): INFO Train: [37/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2995 (0.3122)	loss 1.4233 (1.3668)	grad_norm 27.2491 (30.1491)	mem 4879MB
[2022-05-31 03:32:12 MetaFG_0] (main.py 265): INFO Train: [37/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2919 (0.3114)	loss 1.5396 (1.3703)	grad_norm 33.8732 (29.8974)	mem 4879MB
[2022-05-31 03:32:15 MetaFG_0] (main.py 265): INFO Train: [37/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2924 (0.3108)	loss 1.3172 (1.3692)	grad_norm 24.0926 (30.0504)	mem 4879MB
[2022-05-31 03:32:18 MetaFG_0] (main.py 265): INFO Train: [37/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2978 (0.3101)	loss 1.1633 (1.3658)	grad_norm 22.2032 (30.1675)	mem 4879MB
[2022-05-31 03:32:21 MetaFG_0] (main.py 265): INFO Train: [37/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2923 (0.3096)	loss 1.5677 (1.3700)	grad_norm 20.1444 (29.9193)	mem 4879MB
[2022-05-31 03:32:25 MetaFG_0] (main.py 265): INFO Train: [37/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2916 (0.3092)	loss 1.5677 (1.3813)	grad_norm 26.1588 (30.0410)	mem 4879MB
[2022-05-31 03:32:28 MetaFG_0] (main.py 265): INFO Train: [37/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2988 (0.3088)	loss 1.4426 (1.3873)	grad_norm 37.9565 (29.9530)	mem 4879MB
[2022-05-31 03:32:31 MetaFG_0] (main.py 265): INFO Train: [37/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2930 (0.3086)	loss 1.2903 (1.3870)	grad_norm 23.3850 (30.1902)	mem 4879MB
[2022-05-31 03:32:34 MetaFG_0] (main.py 265): INFO Train: [37/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2931 (0.3084)	loss 1.6902 (1.3955)	grad_norm 21.2748 (30.0035)	mem 4879MB
[2022-05-31 03:32:37 MetaFG_0] (main.py 265): INFO Train: [37/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2932 (0.3082)	loss 1.7468 (1.3962)	grad_norm 21.7033 (29.8369)	mem 4879MB
[2022-05-31 03:32:40 MetaFG_0] (main.py 265): INFO Train: [37/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2920 (0.3080)	loss 1.4604 (1.3933)	grad_norm 19.6305 (29.7941)	mem 4879MB
[2022-05-31 03:32:43 MetaFG_0] (main.py 265): INFO Train: [37/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2922 (0.3077)	loss 1.1717 (1.3905)	grad_norm 35.1197 (29.6772)	mem 4879MB
[2022-05-31 03:32:46 MetaFG_0] (main.py 265): INFO Train: [37/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2995 (0.3076)	loss 1.3609 (1.3908)	grad_norm 33.8811 (29.6585)	mem 4879MB
[2022-05-31 03:32:49 MetaFG_0] (main.py 265): INFO Train: [37/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2919 (0.3074)	loss 1.4836 (1.3920)	grad_norm 42.1861 (29.7421)	mem 4879MB
[2022-05-31 03:32:52 MetaFG_0] (main.py 265): INFO Train: [37/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2923 (0.3072)	loss 1.4543 (1.3974)	grad_norm 27.7310 (29.8965)	mem 4879MB
[2022-05-31 03:32:55 MetaFG_0] (main.py 265): INFO Train: [37/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2926 (0.3071)	loss 1.6807 (1.4014)	grad_norm 24.7160 (29.8294)	mem 4879MB
[2022-05-31 03:32:58 MetaFG_0] (main.py 265): INFO Train: [37/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2977 (0.3069)	loss 1.3018 (1.4046)	grad_norm 43.9986 (29.9523)	mem 4879MB
[2022-05-31 03:33:01 MetaFG_0] (main.py 265): INFO Train: [37/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.3009 (0.3068)	loss 1.5671 (1.4068)	grad_norm 36.4553 (29.8147)	mem 4879MB
[2022-05-31 03:33:04 MetaFG_0] (main.py 265): INFO Train: [37/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2977 (0.3068)	loss 1.2924 (1.4057)	grad_norm 19.6176 (29.7102)	mem 4879MB
[2022-05-31 03:33:07 MetaFG_0] (main.py 265): INFO Train: [37/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2934 (0.3067)	loss 1.2708 (1.4038)	grad_norm 36.4074 (29.8575)	mem 4879MB
[2022-05-31 03:33:10 MetaFG_0] (main.py 265): INFO Train: [37/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2941 (0.3065)	loss 1.3868 (1.4048)	grad_norm 21.4602 (30.1885)	mem 4879MB
[2022-05-31 03:33:13 MetaFG_0] (main.py 265): INFO Train: [37/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2969 (0.3065)	loss 1.2911 (1.4063)	grad_norm 27.6571 (30.3855)	mem 4879MB
[2022-05-31 03:33:16 MetaFG_0] (main.py 265): INFO Train: [37/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2998 (0.3065)	loss 1.1622 (1.4097)	grad_norm 24.5834 (30.3094)	mem 4879MB
[2022-05-31 03:33:19 MetaFG_0] (main.py 265): INFO Train: [37/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.3025 (0.3064)	loss 1.4168 (1.4075)	grad_norm 21.6462 (30.1573)	mem 4879MB
[2022-05-31 03:33:22 MetaFG_0] (main.py 265): INFO Train: [37/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2982 (0.3063)	loss 1.0916 (1.4081)	grad_norm 36.7568 (30.2173)	mem 4879MB
[2022-05-31 03:33:25 MetaFG_0] (main.py 265): INFO Train: [37/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2953 (0.3063)	loss 1.4090 (1.4071)	grad_norm 23.8737 (30.2345)	mem 4879MB
[2022-05-31 03:33:28 MetaFG_0] (main.py 265): INFO Train: [37/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.3013 (0.3062)	loss 1.5419 (1.4019)	grad_norm 23.0646 (30.2968)	mem 4879MB
[2022-05-31 03:33:31 MetaFG_0] (main.py 265): INFO Train: [37/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2921 (0.3062)	loss 1.7152 (1.4042)	grad_norm 34.8302 (30.4502)	mem 4879MB
[2022-05-31 03:33:34 MetaFG_0] (main.py 265): INFO Train: [37/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2941 (0.3062)	loss 1.4927 (1.4041)	grad_norm 34.3695 (30.4371)	mem 4879MB
[2022-05-31 03:33:38 MetaFG_0] (main.py 265): INFO Train: [37/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2939 (0.3062)	loss 1.6114 (1.4056)	grad_norm 23.0784 (30.4554)	mem 4879MB
[2022-05-31 03:33:41 MetaFG_0] (main.py 265): INFO Train: [37/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2924 (0.3061)	loss 0.8771 (1.4045)	grad_norm 24.9201 (30.4123)	mem 4879MB
[2022-05-31 03:33:44 MetaFG_0] (main.py 265): INFO Train: [37/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2982 (0.3060)	loss 1.6465 (1.4071)	grad_norm 36.1296 (30.3602)	mem 4879MB
[2022-05-31 03:33:47 MetaFG_0] (main.py 265): INFO Train: [37/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.3022 (0.3060)	loss 1.6954 (1.4046)	grad_norm 35.4040 (30.3315)	mem 4879MB
[2022-05-31 03:33:50 MetaFG_0] (main.py 265): INFO Train: [37/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.3043 (0.3060)	loss 1.4463 (1.4048)	grad_norm 41.7499 (30.4283)	mem 4879MB
[2022-05-31 03:33:53 MetaFG_0] (main.py 265): INFO Train: [37/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.3057 (0.3060)	loss 1.5612 (1.4045)	grad_norm 16.6694 (30.2840)	mem 4879MB
[2022-05-31 03:33:56 MetaFG_0] (main.py 265): INFO Train: [37/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2996 (0.3059)	loss 1.2323 (1.4040)	grad_norm 18.0384 (30.2015)	mem 4879MB
[2022-05-31 03:33:59 MetaFG_0] (main.py 265): INFO Train: [37/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2919 (0.3059)	loss 1.3094 (1.4022)	grad_norm 28.6245 (30.3208)	mem 4879MB
[2022-05-31 03:34:02 MetaFG_0] (main.py 265): INFO Train: [37/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.2982 (0.3059)	loss 1.5093 (1.4038)	grad_norm 33.3532 (30.4095)	mem 4879MB
[2022-05-31 03:34:05 MetaFG_0] (main.py 265): INFO Train: [37/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2929 (0.3058)	loss 1.6128 (1.4045)	grad_norm 33.9073 (30.3926)	mem 4879MB
[2022-05-31 03:34:08 MetaFG_0] (main.py 265): INFO Train: [37/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2994 (0.3058)	loss 1.5454 (1.4053)	grad_norm 19.0322 (30.4330)	mem 4879MB
[2022-05-31 03:34:11 MetaFG_0] (main.py 265): INFO Train: [37/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.3000 (0.3058)	loss 1.0119 (1.4021)	grad_norm 17.1999 (30.4502)	mem 4879MB
[2022-05-31 03:34:14 MetaFG_0] (main.py 265): INFO Train: [37/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2993 (0.3058)	loss 1.4322 (1.4040)	grad_norm 24.2667 (30.4971)	mem 4879MB
[2022-05-31 03:34:17 MetaFG_0] (main.py 265): INFO Train: [37/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2921 (0.3058)	loss 1.5781 (1.4056)	grad_norm 31.1261 (30.4270)	mem 4879MB
[2022-05-31 03:34:20 MetaFG_0] (main.py 265): INFO Train: [37/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2979 (0.3057)	loss 1.1389 (1.4076)	grad_norm 34.8416 (30.4303)	mem 4879MB
[2022-05-31 03:34:23 MetaFG_0] (main.py 265): INFO Train: [37/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2935 (0.3057)	loss 1.5481 (1.4081)	grad_norm 31.2025 (30.4794)	mem 4879MB
[2022-05-31 03:34:26 MetaFG_0] (main.py 265): INFO Train: [37/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2932 (0.3057)	loss 1.3356 (1.4075)	grad_norm 35.0318 (30.5616)	mem 4879MB
[2022-05-31 03:34:29 MetaFG_0] (main.py 265): INFO Train: [37/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2997 (0.3057)	loss 1.1163 (1.4064)	grad_norm 44.6594 (30.5839)	mem 4879MB
[2022-05-31 03:34:32 MetaFG_0] (main.py 265): INFO Train: [37/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2924 (0.3056)	loss 1.3525 (1.4058)	grad_norm 27.5248 (30.5128)	mem 4879MB
[2022-05-31 03:34:35 MetaFG_0] (main.py 265): INFO Train: [37/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2951 (0.3056)	loss 1.4636 (1.4060)	grad_norm 23.1830 (30.4561)	mem 4879MB
[2022-05-31 03:34:38 MetaFG_0] (main.py 265): INFO Train: [37/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2950 (0.3056)	loss 1.3079 (1.4051)	grad_norm 27.6942 (30.4227)	mem 4879MB
[2022-05-31 03:34:41 MetaFG_0] (main.py 265): INFO Train: [37/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2998 (0.3055)	loss 1.2225 (1.4035)	grad_norm 34.9687 (30.3765)	mem 4879MB
[2022-05-31 03:34:44 MetaFG_0] (main.py 265): INFO Train: [37/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2978 (0.3055)	loss 1.1158 (1.4024)	grad_norm 28.2173 (30.2920)	mem 4879MB
[2022-05-31 03:34:48 MetaFG_0] (main.py 265): INFO Train: [37/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2990 (0.3055)	loss 0.8563 (1.4014)	grad_norm 54.7494 (30.3412)	mem 4879MB
[2022-05-31 03:34:51 MetaFG_0] (main.py 265): INFO Train: [37/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2924 (0.3055)	loss 1.1961 (1.4002)	grad_norm 30.3194 (30.3444)	mem 4879MB
[2022-05-31 03:34:54 MetaFG_0] (main.py 265): INFO Train: [37/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2953 (0.3055)	loss 1.4613 (1.4025)	grad_norm 18.6580 (30.3133)	mem 4879MB
[2022-05-31 03:34:57 MetaFG_0] (main.py 265): INFO Train: [37/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2934 (0.3054)	loss 1.4166 (1.4033)	grad_norm 22.0290 (30.3997)	mem 4879MB
[2022-05-31 03:35:00 MetaFG_0] (main.py 265): INFO Train: [37/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2985 (0.3054)	loss 1.3649 (1.4018)	grad_norm 25.5647 (30.2938)	mem 4879MB
[2022-05-31 03:35:03 MetaFG_0] (main.py 265): INFO Train: [37/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2940 (0.3054)	loss 1.3300 (1.4033)	grad_norm 46.2278 (30.3941)	mem 4879MB
[2022-05-31 03:35:06 MetaFG_0] (main.py 265): INFO Train: [37/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2934 (0.3054)	loss 1.2717 (1.4050)	grad_norm 27.1362 (30.4117)	mem 4879MB
[2022-05-31 03:35:09 MetaFG_0] (main.py 265): INFO Train: [37/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2993 (0.3054)	loss 1.3205 (1.4040)	grad_norm 19.6679 (30.3810)	mem 4879MB
[2022-05-31 03:35:12 MetaFG_0] (main.py 265): INFO Train: [37/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2933 (0.3054)	loss 1.6311 (1.4051)	grad_norm 36.2977 (30.4171)	mem 4879MB
[2022-05-31 03:35:15 MetaFG_0] (main.py 265): INFO Train: [37/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2918 (0.3054)	loss 1.5261 (1.4051)	grad_norm 36.7555 (30.4004)	mem 4879MB
[2022-05-31 03:35:18 MetaFG_0] (main.py 265): INFO Train: [37/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2920 (0.3053)	loss 1.0373 (1.4055)	grad_norm 35.8397 (30.4161)	mem 4879MB
[2022-05-31 03:35:21 MetaFG_0] (main.py 265): INFO Train: [37/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2919 (0.3053)	loss 1.5856 (1.4045)	grad_norm 25.4004 (30.3704)	mem 4879MB
[2022-05-31 03:35:24 MetaFG_0] (main.py 265): INFO Train: [37/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2921 (0.3053)	loss 1.2124 (1.4044)	grad_norm 35.0594 (30.4868)	mem 4879MB
[2022-05-31 03:35:27 MetaFG_0] (main.py 265): INFO Train: [37/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2920 (0.3053)	loss 1.2039 (1.4040)	grad_norm 32.4794 (30.5566)	mem 4879MB
[2022-05-31 03:35:30 MetaFG_0] (main.py 265): INFO Train: [37/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2935 (0.3053)	loss 1.5049 (1.4026)	grad_norm 23.2088 (30.5196)	mem 4879MB
[2022-05-31 03:35:33 MetaFG_0] (main.py 265): INFO Train: [37/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2930 (0.3052)	loss 1.2639 (1.4024)	grad_norm 24.7218 (30.5431)	mem 4879MB
[2022-05-31 03:35:36 MetaFG_0] (main.py 265): INFO Train: [37/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2922 (0.3052)	loss 1.4984 (1.4030)	grad_norm 27.8573 (30.5768)	mem 4879MB
[2022-05-31 03:35:39 MetaFG_0] (main.py 265): INFO Train: [37/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2931 (0.3052)	loss 1.5111 (1.4027)	grad_norm 17.2751 (30.5463)	mem 4879MB
[2022-05-31 03:35:42 MetaFG_0] (main.py 265): INFO Train: [37/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2934 (0.3052)	loss 1.6356 (1.4016)	grad_norm 29.4789 (30.5357)	mem 4879MB
[2022-05-31 03:35:45 MetaFG_0] (main.py 265): INFO Train: [37/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2923 (0.3052)	loss 1.7004 (1.4005)	grad_norm 57.6924 (30.5862)	mem 4879MB
[2022-05-31 03:35:48 MetaFG_0] (main.py 265): INFO Train: [37/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2947 (0.3052)	loss 1.1760 (1.4009)	grad_norm 29.7137 (30.5253)	mem 4879MB
[2022-05-31 03:35:51 MetaFG_0] (main.py 265): INFO Train: [37/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2948 (0.3052)	loss 1.0761 (1.3989)	grad_norm 27.7059 (30.5142)	mem 4879MB
[2022-05-31 03:35:54 MetaFG_0] (main.py 265): INFO Train: [37/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2995 (0.3052)	loss 1.6104 (1.3989)	grad_norm 27.8951 (30.4519)	mem 4879MB
[2022-05-31 03:35:58 MetaFG_0] (main.py 265): INFO Train: [37/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2930 (0.3052)	loss 1.2887 (1.3992)	grad_norm 44.2259 (30.5130)	mem 4879MB
[2022-05-31 03:36:01 MetaFG_0] (main.py 265): INFO Train: [37/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2998 (0.3052)	loss 1.5782 (1.3988)	grad_norm 28.7679 (30.5344)	mem 4879MB
[2022-05-31 03:36:04 MetaFG_0] (main.py 265): INFO Train: [37/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2923 (0.3052)	loss 1.6179 (1.3988)	grad_norm 34.5684 (30.5590)	mem 4879MB
[2022-05-31 03:36:07 MetaFG_0] (main.py 265): INFO Train: [37/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2978 (0.3052)	loss 1.5383 (1.4005)	grad_norm 27.2946 (30.5377)	mem 4879MB
[2022-05-31 03:36:10 MetaFG_0] (main.py 265): INFO Train: [37/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2986 (0.3052)	loss 1.4565 (1.4015)	grad_norm 24.2086 (30.5784)	mem 4879MB
[2022-05-31 03:36:13 MetaFG_0] (main.py 265): INFO Train: [37/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2942 (0.3051)	loss 1.2303 (1.4023)	grad_norm 31.2522 (30.5635)	mem 4879MB
[2022-05-31 03:36:16 MetaFG_0] (main.py 265): INFO Train: [37/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2925 (0.3051)	loss 1.4449 (1.4029)	grad_norm 22.1971 (30.5632)	mem 4879MB
[2022-05-31 03:36:19 MetaFG_0] (main.py 265): INFO Train: [37/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2953 (0.3051)	loss 1.4278 (1.4024)	grad_norm 36.3529 (30.5907)	mem 4879MB
[2022-05-31 03:36:22 MetaFG_0] (main.py 265): INFO Train: [37/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2951 (0.3051)	loss 0.9872 (1.4014)	grad_norm 35.8943 (30.5578)	mem 4879MB
[2022-05-31 03:36:25 MetaFG_0] (main.py 265): INFO Train: [37/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2990 (0.3051)	loss 1.4203 (1.4015)	grad_norm 30.4687 (30.5429)	mem 4879MB
[2022-05-31 03:36:28 MetaFG_0] (main.py 265): INFO Train: [37/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2990 (0.3051)	loss 1.1809 (1.4016)	grad_norm 20.5652 (30.4807)	mem 4879MB
[2022-05-31 03:36:31 MetaFG_0] (main.py 265): INFO Train: [37/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2947 (0.3051)	loss 1.5259 (1.4028)	grad_norm 29.3774 (30.4459)	mem 4879MB
[2022-05-31 03:36:34 MetaFG_0] (main.py 265): INFO Train: [37/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.3030 (0.3051)	loss 1.4584 (1.4028)	grad_norm 25.0957 (30.4399)	mem 4879MB
[2022-05-31 03:36:37 MetaFG_0] (main.py 265): INFO Train: [37/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2918 (0.3051)	loss 1.5058 (1.4031)	grad_norm 30.6472 (30.3950)	mem 4879MB
[2022-05-31 03:36:40 MetaFG_0] (main.py 265): INFO Train: [37/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2932 (0.3051)	loss 1.2082 (1.4042)	grad_norm 35.5711 (30.3759)	mem 4879MB
[2022-05-31 03:36:43 MetaFG_0] (main.py 265): INFO Train: [37/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2937 (0.3050)	loss 1.4474 (1.4048)	grad_norm 23.6010 (30.3414)	mem 4879MB
[2022-05-31 03:36:46 MetaFG_0] (main.py 265): INFO Train: [37/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2933 (0.3050)	loss 1.9368 (1.4049)	grad_norm 32.0485 (30.3322)	mem 4879MB
[2022-05-31 03:36:49 MetaFG_0] (main.py 265): INFO Train: [37/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3001 (0.3050)	loss 1.4845 (1.4054)	grad_norm 25.1332 (30.3297)	mem 4879MB
[2022-05-31 03:36:52 MetaFG_0] (main.py 265): INFO Train: [37/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2961 (0.3050)	loss 1.4882 (1.4046)	grad_norm 18.6620 (30.2858)	mem 4879MB
[2022-05-31 03:36:55 MetaFG_0] (main.py 265): INFO Train: [37/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2941 (0.3050)	loss 1.6423 (1.4045)	grad_norm 39.3389 (inf)	mem 4879MB
[2022-05-31 03:36:58 MetaFG_0] (main.py 265): INFO Train: [37/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2916 (0.3050)	loss 1.4552 (1.4041)	grad_norm 26.1290 (inf)	mem 4879MB
[2022-05-31 03:37:01 MetaFG_0] (main.py 265): INFO Train: [37/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2978 (0.3050)	loss 1.5506 (1.4040)	grad_norm 39.2772 (inf)	mem 4879MB
[2022-05-31 03:37:04 MetaFG_0] (main.py 265): INFO Train: [37/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2921 (0.3050)	loss 1.5298 (1.4042)	grad_norm 41.2664 (inf)	mem 4879MB
[2022-05-31 03:37:08 MetaFG_0] (main.py 265): INFO Train: [37/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2938 (0.3050)	loss 1.6050 (1.4047)	grad_norm 39.6121 (inf)	mem 4879MB
[2022-05-31 03:37:11 MetaFG_0] (main.py 265): INFO Train: [37/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2979 (0.3050)	loss 1.5520 (1.4046)	grad_norm 18.1712 (inf)	mem 4879MB
[2022-05-31 03:37:14 MetaFG_0] (main.py 265): INFO Train: [37/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.3007 (0.3050)	loss 1.5796 (1.4055)	grad_norm 22.6354 (inf)	mem 4879MB
[2022-05-31 03:37:17 MetaFG_0] (main.py 265): INFO Train: [37/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2937 (0.3050)	loss 1.2713 (1.4047)	grad_norm 25.4679 (inf)	mem 4879MB
[2022-05-31 03:37:20 MetaFG_0] (main.py 265): INFO Train: [37/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2933 (0.3050)	loss 1.2502 (1.4035)	grad_norm 24.7347 (inf)	mem 4879MB
[2022-05-31 03:37:23 MetaFG_0] (main.py 265): INFO Train: [37/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2937 (0.3050)	loss 1.6265 (1.4042)	grad_norm 24.1954 (inf)	mem 4879MB
[2022-05-31 03:37:26 MetaFG_0] (main.py 265): INFO Train: [37/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2924 (0.3050)	loss 1.2615 (1.4043)	grad_norm 24.0409 (inf)	mem 4879MB
[2022-05-31 03:37:29 MetaFG_0] (main.py 265): INFO Train: [37/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2935 (0.3049)	loss 1.2452 (1.4041)	grad_norm 37.1859 (inf)	mem 4879MB
[2022-05-31 03:37:32 MetaFG_0] (main.py 265): INFO Train: [37/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2932 (0.3049)	loss 1.4466 (1.4052)	grad_norm 27.4444 (inf)	mem 4879MB
[2022-05-31 03:37:35 MetaFG_0] (main.py 265): INFO Train: [37/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2919 (0.3049)	loss 1.3531 (1.4064)	grad_norm 25.0556 (inf)	mem 4879MB
[2022-05-31 03:37:38 MetaFG_0] (main.py 265): INFO Train: [37/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2921 (0.3049)	loss 0.9463 (1.4059)	grad_norm 17.7713 (inf)	mem 4879MB
[2022-05-31 03:37:41 MetaFG_0] (main.py 265): INFO Train: [37/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2929 (0.3049)	loss 1.1926 (1.4066)	grad_norm 19.8989 (inf)	mem 4879MB
[2022-05-31 03:37:44 MetaFG_0] (main.py 265): INFO Train: [37/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2920 (0.3049)	loss 1.5072 (1.4066)	grad_norm 24.8489 (inf)	mem 4879MB
[2022-05-31 03:37:47 MetaFG_0] (main.py 265): INFO Train: [37/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3003 (0.3049)	loss 1.3882 (1.4060)	grad_norm 25.4986 (inf)	mem 4879MB
[2022-05-31 03:37:50 MetaFG_0] (main.py 265): INFO Train: [37/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2930 (0.3049)	loss 1.5284 (1.4066)	grad_norm 17.0019 (inf)	mem 4879MB
[2022-05-31 03:37:53 MetaFG_0] (main.py 265): INFO Train: [37/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2924 (0.3049)	loss 0.9427 (1.4064)	grad_norm 25.2217 (inf)	mem 4879MB
[2022-05-31 03:37:56 MetaFG_0] (main.py 265): INFO Train: [37/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2941 (0.3049)	loss 1.5560 (1.4058)	grad_norm 22.6864 (inf)	mem 4879MB
[2022-05-31 03:37:59 MetaFG_0] (main.py 265): INFO Train: [37/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2943 (0.3049)	loss 1.6281 (1.4069)	grad_norm 16.7815 (inf)	mem 4879MB
[2022-05-31 03:38:02 MetaFG_0] (main.py 265): INFO Train: [37/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2946 (0.3049)	loss 1.6464 (1.4065)	grad_norm 35.5692 (inf)	mem 4879MB
[2022-05-31 03:38:05 MetaFG_0] (main.py 265): INFO Train: [37/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2974 (0.3049)	loss 1.3471 (1.4061)	grad_norm 14.8710 (inf)	mem 4879MB
[2022-05-31 03:38:08 MetaFG_0] (main.py 265): INFO Train: [37/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2921 (0.3049)	loss 1.5019 (1.4061)	grad_norm 23.8418 (inf)	mem 4879MB
[2022-05-31 03:38:11 MetaFG_0] (main.py 265): INFO Train: [37/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2992 (0.3049)	loss 0.9966 (1.4065)	grad_norm 36.1235 (inf)	mem 4879MB
[2022-05-31 03:38:14 MetaFG_0] (main.py 265): INFO Train: [37/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2920 (0.3049)	loss 1.2227 (1.4055)	grad_norm 22.6895 (inf)	mem 4879MB
[2022-05-31 03:38:17 MetaFG_0] (main.py 265): INFO Train: [37/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2928 (0.3048)	loss 1.7254 (1.4062)	grad_norm 40.6911 (inf)	mem 4879MB
[2022-05-31 03:38:20 MetaFG_0] (main.py 265): INFO Train: [37/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2917 (0.3048)	loss 1.7046 (1.4069)	grad_norm 28.3471 (inf)	mem 4879MB
[2022-05-31 03:38:24 MetaFG_0] (main.py 265): INFO Train: [37/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2993 (0.3048)	loss 1.5108 (1.4070)	grad_norm 24.2706 (nan)	mem 4879MB
[2022-05-31 03:38:27 MetaFG_0] (main.py 265): INFO Train: [37/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2933 (0.3048)	loss 0.9069 (1.4068)	grad_norm 26.6704 (nan)	mem 4879MB
[2022-05-31 03:38:30 MetaFG_0] (main.py 265): INFO Train: [37/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3002 (0.3048)	loss 1.6370 (1.4063)	grad_norm 20.5910 (nan)	mem 4879MB
[2022-05-31 03:38:33 MetaFG_0] (main.py 265): INFO Train: [37/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2919 (0.3048)	loss 1.3334 (1.4060)	grad_norm 31.4946 (nan)	mem 4879MB
[2022-05-31 03:38:36 MetaFG_0] (main.py 265): INFO Train: [37/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2996 (0.3048)	loss 1.4218 (1.4062)	grad_norm 32.1952 (nan)	mem 4879MB
[2022-05-31 03:38:39 MetaFG_0] (main.py 265): INFO Train: [37/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2989 (0.3048)	loss 1.6322 (1.4067)	grad_norm 35.5391 (nan)	mem 4879MB
[2022-05-31 03:38:42 MetaFG_0] (main.py 265): INFO Train: [37/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2985 (0.3048)	loss 1.5158 (1.4059)	grad_norm 31.1942 (nan)	mem 4879MB
[2022-05-31 03:38:45 MetaFG_0] (main.py 265): INFO Train: [37/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2990 (0.3048)	loss 1.1109 (1.4061)	grad_norm 22.4105 (nan)	mem 4879MB
[2022-05-31 03:38:48 MetaFG_0] (main.py 265): INFO Train: [37/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2916 (0.3048)	loss 1.7848 (1.4065)	grad_norm 26.4680 (nan)	mem 4879MB
[2022-05-31 03:38:51 MetaFG_0] (main.py 265): INFO Train: [37/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2989 (0.3048)	loss 1.5271 (1.4071)	grad_norm 21.4686 (nan)	mem 4879MB
[2022-05-31 03:38:54 MetaFG_0] (main.py 265): INFO Train: [37/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2980 (0.3048)	loss 1.2718 (1.4078)	grad_norm 35.0276 (nan)	mem 4879MB
[2022-05-31 03:38:57 MetaFG_0] (main.py 265): INFO Train: [37/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2926 (0.3048)	loss 1.6236 (1.4074)	grad_norm 27.2658 (nan)	mem 4879MB
[2022-05-31 03:39:00 MetaFG_0] (main.py 265): INFO Train: [37/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2993 (0.3048)	loss 0.8676 (1.4066)	grad_norm 49.0907 (nan)	mem 4879MB
[2022-05-31 03:39:03 MetaFG_0] (main.py 265): INFO Train: [37/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2949 (0.3048)	loss 1.4083 (1.4061)	grad_norm 31.6739 (nan)	mem 4879MB
[2022-05-31 03:39:06 MetaFG_0] (main.py 265): INFO Train: [37/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2966 (0.3048)	loss 1.7358 (1.4064)	grad_norm 30.8349 (nan)	mem 4879MB
[2022-05-31 03:39:09 MetaFG_0] (main.py 265): INFO Train: [37/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2996 (0.3048)	loss 1.3205 (1.4053)	grad_norm 22.7873 (nan)	mem 4879MB
[2022-05-31 03:39:12 MetaFG_0] (main.py 265): INFO Train: [37/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3031 (0.3048)	loss 1.6890 (1.4052)	grad_norm 23.3103 (nan)	mem 4879MB
[2022-05-31 03:39:15 MetaFG_0] (main.py 265): INFO Train: [37/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2944 (0.3048)	loss 1.5053 (1.4053)	grad_norm 30.9149 (nan)	mem 4879MB
[2022-05-31 03:39:18 MetaFG_0] (main.py 265): INFO Train: [37/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2925 (0.3047)	loss 1.3337 (1.4052)	grad_norm 19.9340 (nan)	mem 4879MB
[2022-05-31 03:39:21 MetaFG_0] (main.py 265): INFO Train: [37/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3000 (0.3047)	loss 1.5239 (1.4055)	grad_norm 27.8109 (nan)	mem 4879MB
[2022-05-31 03:39:24 MetaFG_0] (main.py 265): INFO Train: [37/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2990 (0.3048)	loss 1.3780 (1.4066)	grad_norm 16.0181 (nan)	mem 4879MB
[2022-05-31 03:39:27 MetaFG_0] (main.py 265): INFO Train: [37/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3012 (0.3048)	loss 1.1495 (1.4060)	grad_norm 50.3978 (nan)	mem 4879MB
[2022-05-31 03:39:31 MetaFG_0] (main.py 265): INFO Train: [37/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2923 (0.3048)	loss 1.5885 (1.4068)	grad_norm 20.4762 (nan)	mem 4879MB
[2022-05-31 03:39:34 MetaFG_0] (main.py 265): INFO Train: [37/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2930 (0.3048)	loss 1.6252 (1.4080)	grad_norm 23.0288 (nan)	mem 4879MB
[2022-05-31 03:39:34 MetaFG_0] (main.py 272): INFO EPOCH 37 training takes 0:07:56
[2022-05-31 03:39:34 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_37.pth saving......
[2022-05-31 03:39:35 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_37.pth saved !!!
[2022-05-31 03:39:35 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:39:36 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:39:36 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:39:37 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.765 (0.765)	Loss 0.8889 (0.8889)	Acc@1 71.875 (71.875)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 03:39:38 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.159)	Loss 0.5773 (0.7149)	Acc@1 93.750 (85.227)	Acc@5 100.000 (98.580)	Mem 4879MB
[2022-05-31 03:39:39 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.109 (0.128)	Loss 0.4806 (0.6976)	Acc@1 93.750 (85.863)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 03:39:40 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.106 (0.117)	Loss 0.6814 (0.7046)	Acc@1 87.500 (85.282)	Acc@5 96.875 (98.589)	Mem 4879MB
[2022-05-31 03:39:41 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.112)	Loss 0.6882 (0.7113)	Acc@1 87.500 (84.832)	Acc@5 96.875 (98.399)	Mem 4879MB
[2022-05-31 03:39:42 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.092 (0.109)	Loss 0.6189 (0.7006)	Acc@1 84.375 (85.110)	Acc@5 100.000 (98.529)	Mem 4879MB
[2022-05-31 03:39:43 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.106)	Loss 0.6599 (0.6872)	Acc@1 87.500 (86.014)	Acc@5 96.875 (98.668)	Mem 4879MB
[2022-05-31 03:39:44 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.105)	Loss 0.8014 (0.6964)	Acc@1 84.375 (85.695)	Acc@5 96.875 (98.504)	Mem 4879MB
[2022-05-31 03:39:45 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.103)	Loss 0.7634 (0.7038)	Acc@1 87.500 (85.340)	Acc@5 100.000 (98.418)	Mem 4879MB
[2022-05-31 03:39:46 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.100 (0.102)	Loss 0.7139 (0.7120)	Acc@1 87.500 (85.199)	Acc@5 96.875 (98.317)	Mem 4879MB
[2022-05-31 03:39:47 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.102)	Loss 0.7254 (0.7093)	Acc@1 87.500 (85.365)	Acc@5 96.875 (98.267)	Mem 4879MB
[2022-05-31 03:39:47 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.101)	Loss 0.8048 (0.7067)	Acc@1 81.250 (85.389)	Acc@5 90.625 (98.226)	Mem 4879MB
[2022-05-31 03:39:48 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.100)	Loss 0.7801 (0.7107)	Acc@1 84.375 (85.408)	Acc@5 100.000 (98.140)	Mem 4879MB
[2022-05-31 03:39:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.100)	Loss 0.7025 (0.7124)	Acc@1 84.375 (85.305)	Acc@5 100.000 (98.163)	Mem 4879MB
[2022-05-31 03:39:50 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.100)	Loss 0.6378 (0.7180)	Acc@1 87.500 (85.129)	Acc@5 100.000 (98.249)	Mem 4879MB
[2022-05-31 03:39:51 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.099)	Loss 1.0385 (0.7242)	Acc@1 71.875 (84.748)	Acc@5 96.875 (98.241)	Mem 4879MB
[2022-05-31 03:39:52 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.099)	Loss 0.7422 (0.7258)	Acc@1 81.250 (84.686)	Acc@5 100.000 (98.253)	Mem 4879MB
[2022-05-31 03:39:53 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.088 (0.099)	Loss 0.7525 (0.7268)	Acc@1 87.500 (84.759)	Acc@5 100.000 (98.227)	Mem 4879MB
[2022-05-31 03:39:54 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.099)	Loss 1.0490 (0.7292)	Acc@1 75.000 (84.669)	Acc@5 93.750 (98.256)	Mem 4879MB
[2022-05-31 03:39:55 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.098)	Loss 0.7173 (0.7303)	Acc@1 84.375 (84.571)	Acc@5 93.750 (98.266)	Mem 4879MB
[2022-05-31 03:39:56 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.098)	Loss 0.9174 (0.7324)	Acc@1 78.125 (84.499)	Acc@5 100.000 (98.290)	Mem 4879MB
[2022-05-31 03:39:57 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.098)	Loss 0.9397 (0.7347)	Acc@1 81.250 (84.405)	Acc@5 93.750 (98.282)	Mem 4879MB
[2022-05-31 03:39:58 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.098)	Loss 0.7509 (0.7379)	Acc@1 78.125 (84.262)	Acc@5 100.000 (98.275)	Mem 4879MB
[2022-05-31 03:39:59 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.102 (0.098)	Loss 0.6047 (0.7377)	Acc@1 93.750 (84.334)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 03:40:00 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.7897 (0.7354)	Acc@1 84.375 (84.375)	Acc@5 93.750 (98.262)	Mem 4879MB
[2022-05-31 03:40:01 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.6737 (0.7333)	Acc@1 90.625 (84.475)	Acc@5 96.875 (98.294)	Mem 4879MB
[2022-05-31 03:40:02 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.088 (0.097)	Loss 0.7573 (0.7322)	Acc@1 81.250 (84.471)	Acc@5 100.000 (98.312)	Mem 4879MB
[2022-05-31 03:40:03 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.097)	Loss 0.9657 (0.7343)	Acc@1 78.125 (84.410)	Acc@5 93.750 (98.305)	Mem 4879MB
[2022-05-31 03:40:04 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 1.0200 (0.7368)	Acc@1 71.875 (84.319)	Acc@5 96.875 (98.298)	Mem 4879MB
[2022-05-31 03:40:04 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.097)	Loss 0.6407 (0.7351)	Acc@1 87.500 (84.407)	Acc@5 100.000 (98.303)	Mem 4879MB
[2022-05-31 03:40:05 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.090 (0.097)	Loss 0.9198 (0.7364)	Acc@1 71.875 (84.282)	Acc@5 100.000 (98.308)	Mem 4879MB
[2022-05-31 03:40:06 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.097)	Loss 0.8387 (0.7382)	Acc@1 81.250 (84.174)	Acc@5 100.000 (98.302)	Mem 4879MB
[2022-05-31 03:40:07 MetaFG_0] (main.py 330): INFO  * Acc@1 84.190 Acc@5 98.300
[2022-05-31 03:40:07 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 84.2%
[2022-05-31 03:40:07 MetaFG_0] (main.py 171): INFO Max accuracy: 84.19%
[2022-05-31 03:40:08 MetaFG_0] (main.py 265): INFO Train: [38/300][0/1562]	eta 0:27:27 lr 0.000006	time 1.0549 (1.0549)	loss 1.3451 (1.3451)	grad_norm 34.4777 (34.4777)	mem 4879MB
[2022-05-31 03:40:11 MetaFG_0] (main.py 265): INFO Train: [38/300][10/1562]	eta 0:09:48 lr 0.000006	time 0.2990 (0.3794)	loss 1.8009 (1.5180)	grad_norm 33.0645 (28.5369)	mem 4879MB
[2022-05-31 03:40:14 MetaFG_0] (main.py 265): INFO Train: [38/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.2925 (0.3428)	loss 1.4758 (1.4850)	grad_norm 32.8729 (30.2823)	mem 4879MB
[2022-05-31 03:40:17 MetaFG_0] (main.py 265): INFO Train: [38/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2994 (0.3298)	loss 1.0464 (1.4523)	grad_norm 30.0500 (29.9738)	mem 4879MB
[2022-05-31 03:40:20 MetaFG_0] (main.py 265): INFO Train: [38/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2986 (0.3235)	loss 1.5947 (1.4546)	grad_norm 21.1542 (30.0724)	mem 4879MB
[2022-05-31 03:40:23 MetaFG_0] (main.py 265): INFO Train: [38/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2921 (0.3196)	loss 1.3548 (1.4515)	grad_norm 27.9859 (29.1935)	mem 4879MB
[2022-05-31 03:40:26 MetaFG_0] (main.py 265): INFO Train: [38/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2980 (0.3170)	loss 1.6078 (1.4458)	grad_norm 30.3258 (28.6111)	mem 4879MB
[2022-05-31 03:40:29 MetaFG_0] (main.py 265): INFO Train: [38/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.2920 (0.3149)	loss 1.6816 (1.4392)	grad_norm 130.0615 (29.2344)	mem 4879MB
[2022-05-31 03:40:32 MetaFG_0] (main.py 265): INFO Train: [38/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2923 (0.3135)	loss 1.3571 (1.4235)	grad_norm 29.4291 (29.1720)	mem 4879MB
[2022-05-31 03:40:35 MetaFG_0] (main.py 265): INFO Train: [38/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2997 (0.3126)	loss 1.3400 (1.4270)	grad_norm 31.1067 (29.0737)	mem 4879MB
[2022-05-31 03:40:38 MetaFG_0] (main.py 265): INFO Train: [38/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2983 (0.3119)	loss 1.5552 (1.4122)	grad_norm 29.3067 (29.3208)	mem 4879MB
[2022-05-31 03:40:41 MetaFG_0] (main.py 265): INFO Train: [38/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2967 (0.3113)	loss 1.1461 (1.3993)	grad_norm 41.4549 (29.4702)	mem 4879MB
[2022-05-31 03:40:44 MetaFG_0] (main.py 265): INFO Train: [38/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2974 (0.3108)	loss 0.9298 (1.3935)	grad_norm 58.5923 (29.7537)	mem 4879MB
[2022-05-31 03:40:47 MetaFG_0] (main.py 265): INFO Train: [38/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2980 (0.3102)	loss 1.5731 (1.3944)	grad_norm 12.4254 (30.0468)	mem 4879MB
[2022-05-31 03:40:50 MetaFG_0] (main.py 265): INFO Train: [38/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2919 (0.3097)	loss 1.4989 (1.4000)	grad_norm 27.8599 (29.9254)	mem 4879MB
[2022-05-31 03:40:53 MetaFG_0] (main.py 265): INFO Train: [38/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2959 (0.3094)	loss 1.2453 (1.4073)	grad_norm 24.2630 (29.6823)	mem 4879MB
[2022-05-31 03:40:56 MetaFG_0] (main.py 265): INFO Train: [38/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3006 (0.3093)	loss 1.8237 (1.4140)	grad_norm 26.5664 (29.6728)	mem 4879MB
[2022-05-31 03:40:59 MetaFG_0] (main.py 265): INFO Train: [38/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2980 (0.3091)	loss 1.1348 (1.4210)	grad_norm 26.7234 (29.8035)	mem 4879MB
[2022-05-31 03:41:03 MetaFG_0] (main.py 265): INFO Train: [38/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2978 (0.3090)	loss 1.3978 (1.4182)	grad_norm 28.0309 (29.6602)	mem 4879MB
[2022-05-31 03:41:06 MetaFG_0] (main.py 265): INFO Train: [38/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2938 (0.3088)	loss 1.4681 (1.4083)	grad_norm 18.4822 (29.4138)	mem 4879MB
[2022-05-31 03:41:09 MetaFG_0] (main.py 265): INFO Train: [38/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2944 (0.3086)	loss 1.5386 (1.4175)	grad_norm 33.1459 (29.2479)	mem 4879MB
[2022-05-31 03:41:12 MetaFG_0] (main.py 265): INFO Train: [38/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2986 (0.3083)	loss 0.9797 (1.4136)	grad_norm 37.3714 (29.1678)	mem 4879MB
[2022-05-31 03:41:15 MetaFG_0] (main.py 265): INFO Train: [38/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2940 (0.3081)	loss 1.4560 (1.4114)	grad_norm 15.8654 (29.2209)	mem 4879MB
[2022-05-31 03:41:18 MetaFG_0] (main.py 265): INFO Train: [38/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2945 (0.3081)	loss 1.2894 (1.4107)	grad_norm 34.0270 (29.1841)	mem 4879MB
[2022-05-31 03:41:21 MetaFG_0] (main.py 265): INFO Train: [38/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.3059 (0.3080)	loss 1.2874 (1.4144)	grad_norm 25.5342 (29.1242)	mem 4879MB
[2022-05-31 03:41:24 MetaFG_0] (main.py 265): INFO Train: [38/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.3004 (0.3079)	loss 0.8931 (1.4164)	grad_norm 30.6382 (29.2272)	mem 4879MB
[2022-05-31 03:41:27 MetaFG_0] (main.py 265): INFO Train: [38/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.3043 (0.3078)	loss 1.7093 (1.4174)	grad_norm 19.9557 (29.2145)	mem 4879MB
[2022-05-31 03:41:30 MetaFG_0] (main.py 265): INFO Train: [38/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2980 (0.3077)	loss 1.4947 (1.4175)	grad_norm 28.9098 (29.1658)	mem 4879MB
[2022-05-31 03:41:33 MetaFG_0] (main.py 265): INFO Train: [38/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2986 (0.3075)	loss 1.5389 (1.4159)	grad_norm 17.6735 (29.1581)	mem 4879MB
[2022-05-31 03:41:36 MetaFG_0] (main.py 265): INFO Train: [38/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2918 (0.3075)	loss 1.3355 (1.4159)	grad_norm 31.6808 (29.2025)	mem 4879MB
[2022-05-31 03:41:39 MetaFG_0] (main.py 265): INFO Train: [38/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2919 (0.3073)	loss 1.1743 (1.4117)	grad_norm 37.9794 (29.3415)	mem 4879MB
[2022-05-31 03:41:42 MetaFG_0] (main.py 265): INFO Train: [38/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2924 (0.3072)	loss 1.1237 (1.4089)	grad_norm 20.3908 (29.3107)	mem 4879MB
[2022-05-31 03:41:45 MetaFG_0] (main.py 265): INFO Train: [38/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2920 (0.3071)	loss 0.8705 (1.4088)	grad_norm 26.8624 (29.2632)	mem 4879MB
[2022-05-31 03:41:48 MetaFG_0] (main.py 265): INFO Train: [38/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2926 (0.3070)	loss 1.8577 (1.4096)	grad_norm 112.9027 (29.4714)	mem 4879MB
[2022-05-31 03:41:51 MetaFG_0] (main.py 265): INFO Train: [38/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2919 (0.3069)	loss 1.2858 (1.4052)	grad_norm 38.7806 (29.5427)	mem 4879MB
[2022-05-31 03:41:54 MetaFG_0] (main.py 265): INFO Train: [38/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2927 (0.3069)	loss 1.5975 (1.4058)	grad_norm 35.0036 (29.7261)	mem 4879MB
[2022-05-31 03:41:57 MetaFG_0] (main.py 265): INFO Train: [38/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2925 (0.3068)	loss 1.6416 (1.4075)	grad_norm 14.2621 (29.4913)	mem 4879MB
[2022-05-31 03:42:00 MetaFG_0] (main.py 265): INFO Train: [38/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2932 (0.3067)	loss 1.6230 (1.4074)	grad_norm 30.8156 (29.5027)	mem 4879MB
[2022-05-31 03:42:03 MetaFG_0] (main.py 265): INFO Train: [38/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2925 (0.3066)	loss 1.1555 (1.4025)	grad_norm 26.5665 (29.4763)	mem 4879MB
[2022-05-31 03:42:06 MetaFG_0] (main.py 265): INFO Train: [38/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2941 (0.3066)	loss 1.5796 (1.4067)	grad_norm 45.1662 (29.5815)	mem 4879MB
[2022-05-31 03:42:10 MetaFG_0] (main.py 265): INFO Train: [38/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3012 (0.3065)	loss 1.3443 (1.4054)	grad_norm 18.3866 (29.5893)	mem 4879MB
[2022-05-31 03:42:13 MetaFG_0] (main.py 265): INFO Train: [38/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.3006 (0.3065)	loss 1.2655 (1.4058)	grad_norm 16.3863 (29.5370)	mem 4879MB
[2022-05-31 03:42:16 MetaFG_0] (main.py 265): INFO Train: [38/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2924 (0.3065)	loss 1.3148 (1.4048)	grad_norm 40.7261 (29.6219)	mem 4879MB
[2022-05-31 03:42:19 MetaFG_0] (main.py 265): INFO Train: [38/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2920 (0.3064)	loss 1.5490 (1.4045)	grad_norm 18.4762 (29.6786)	mem 4879MB
[2022-05-31 03:42:22 MetaFG_0] (main.py 265): INFO Train: [38/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2965 (0.3063)	loss 1.3807 (1.4028)	grad_norm 35.4577 (29.6112)	mem 4879MB
[2022-05-31 03:42:25 MetaFG_0] (main.py 265): INFO Train: [38/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2920 (0.3062)	loss 1.7099 (1.4061)	grad_norm 16.9964 (29.5550)	mem 4879MB
[2022-05-31 03:42:28 MetaFG_0] (main.py 265): INFO Train: [38/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2933 (0.3062)	loss 1.3669 (1.4046)	grad_norm 20.2693 (29.4993)	mem 4879MB
[2022-05-31 03:42:31 MetaFG_0] (main.py 265): INFO Train: [38/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2925 (0.3061)	loss 1.5884 (1.4053)	grad_norm 24.2126 (29.4424)	mem 4879MB
[2022-05-31 03:42:34 MetaFG_0] (main.py 265): INFO Train: [38/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2998 (0.3061)	loss 1.5740 (1.4058)	grad_norm 31.6383 (29.4923)	mem 4879MB
[2022-05-31 03:42:37 MetaFG_0] (main.py 265): INFO Train: [38/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2986 (0.3061)	loss 1.4641 (1.4046)	grad_norm 20.1652 (29.5438)	mem 4879MB
[2022-05-31 03:42:40 MetaFG_0] (main.py 265): INFO Train: [38/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2994 (0.3060)	loss 1.6947 (1.4053)	grad_norm 23.0548 (29.7061)	mem 4879MB
[2022-05-31 03:42:43 MetaFG_0] (main.py 265): INFO Train: [38/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2941 (0.3060)	loss 1.6852 (1.4071)	grad_norm 28.4693 (29.6827)	mem 4879MB
[2022-05-31 03:42:46 MetaFG_0] (main.py 265): INFO Train: [38/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2920 (0.3060)	loss 1.1650 (1.4053)	grad_norm 35.9849 (29.6826)	mem 4879MB
[2022-05-31 03:42:49 MetaFG_0] (main.py 265): INFO Train: [38/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2923 (0.3059)	loss 1.5018 (1.4046)	grad_norm 32.8801 (29.7575)	mem 4879MB
[2022-05-31 03:42:52 MetaFG_0] (main.py 265): INFO Train: [38/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2933 (0.3059)	loss 1.6038 (1.4065)	grad_norm 21.8404 (29.7740)	mem 4879MB
[2022-05-31 03:42:55 MetaFG_0] (main.py 265): INFO Train: [38/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2959 (0.3060)	loss 1.1734 (1.4077)	grad_norm 41.0252 (29.8346)	mem 4879MB
[2022-05-31 03:42:58 MetaFG_0] (main.py 265): INFO Train: [38/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.3039 (0.3061)	loss 1.5591 (1.4069)	grad_norm 16.7748 (29.7618)	mem 4879MB
[2022-05-31 03:43:01 MetaFG_0] (main.py 265): INFO Train: [38/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2986 (0.3061)	loss 1.2849 (1.4056)	grad_norm 39.7887 (29.7794)	mem 4879MB
[2022-05-31 03:43:04 MetaFG_0] (main.py 265): INFO Train: [38/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2923 (0.3060)	loss 1.3803 (1.4046)	grad_norm 21.4930 (29.7584)	mem 4879MB
[2022-05-31 03:43:07 MetaFG_0] (main.py 265): INFO Train: [38/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2924 (0.3060)	loss 1.6073 (1.4058)	grad_norm 40.1693 (29.7152)	mem 4879MB
[2022-05-31 03:43:10 MetaFG_0] (main.py 265): INFO Train: [38/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2949 (0.3059)	loss 1.5085 (1.4052)	grad_norm 29.8657 (29.6828)	mem 4879MB
[2022-05-31 03:43:14 MetaFG_0] (main.py 265): INFO Train: [38/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2941 (0.3059)	loss 1.5499 (1.4061)	grad_norm 29.4772 (29.6798)	mem 4879MB
[2022-05-31 03:43:17 MetaFG_0] (main.py 265): INFO Train: [38/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3002 (0.3060)	loss 1.3569 (1.4073)	grad_norm 38.2668 (29.6405)	mem 4879MB
[2022-05-31 03:43:20 MetaFG_0] (main.py 265): INFO Train: [38/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3073 (0.3059)	loss 1.7402 (1.4066)	grad_norm 41.7146 (29.6207)	mem 4879MB
[2022-05-31 03:43:23 MetaFG_0] (main.py 265): INFO Train: [38/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2926 (0.3059)	loss 1.6088 (1.4072)	grad_norm 48.7443 (29.7492)	mem 4879MB
[2022-05-31 03:43:26 MetaFG_0] (main.py 265): INFO Train: [38/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2995 (0.3059)	loss 1.3547 (1.4058)	grad_norm 27.5479 (29.7421)	mem 4879MB
[2022-05-31 03:43:29 MetaFG_0] (main.py 265): INFO Train: [38/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2995 (0.3058)	loss 1.5368 (1.4066)	grad_norm 24.7228 (29.7350)	mem 4879MB
[2022-05-31 03:43:32 MetaFG_0] (main.py 265): INFO Train: [38/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2930 (0.3058)	loss 1.3125 (1.4062)	grad_norm 18.7664 (29.6916)	mem 4879MB
[2022-05-31 03:43:35 MetaFG_0] (main.py 265): INFO Train: [38/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.3021 (0.3058)	loss 1.5231 (1.4065)	grad_norm 19.3348 (29.7050)	mem 4879MB
[2022-05-31 03:43:38 MetaFG_0] (main.py 265): INFO Train: [38/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.3004 (0.3058)	loss 1.5710 (1.4077)	grad_norm 27.6194 (29.7526)	mem 4879MB
[2022-05-31 03:43:41 MetaFG_0] (main.py 265): INFO Train: [38/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2939 (0.3058)	loss 1.6127 (1.4091)	grad_norm 30.2018 (29.7806)	mem 4879MB
[2022-05-31 03:43:44 MetaFG_0] (main.py 265): INFO Train: [38/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2987 (0.3058)	loss 1.2168 (1.4092)	grad_norm 38.1057 (29.8427)	mem 4879MB
[2022-05-31 03:43:47 MetaFG_0] (main.py 265): INFO Train: [38/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2958 (0.3057)	loss 1.4253 (1.4091)	grad_norm 30.3668 (29.8914)	mem 4879MB
[2022-05-31 03:43:50 MetaFG_0] (main.py 265): INFO Train: [38/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2992 (0.3057)	loss 1.5328 (1.4101)	grad_norm 47.9508 (29.9025)	mem 4879MB
[2022-05-31 03:43:53 MetaFG_0] (main.py 265): INFO Train: [38/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2932 (0.3057)	loss 0.9778 (1.4097)	grad_norm 28.8429 (29.8776)	mem 4879MB
[2022-05-31 03:43:56 MetaFG_0] (main.py 265): INFO Train: [38/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2918 (0.3057)	loss 1.3965 (1.4092)	grad_norm 25.1124 (29.8365)	mem 4879MB
[2022-05-31 03:43:59 MetaFG_0] (main.py 265): INFO Train: [38/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2997 (0.3057)	loss 1.5495 (1.4096)	grad_norm 26.5225 (29.8584)	mem 4879MB
[2022-05-31 03:44:02 MetaFG_0] (main.py 265): INFO Train: [38/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2923 (0.3057)	loss 1.4679 (1.4098)	grad_norm 22.0464 (29.8040)	mem 4879MB
[2022-05-31 03:44:05 MetaFG_0] (main.py 265): INFO Train: [38/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2940 (0.3057)	loss 1.5183 (1.4104)	grad_norm 17.2527 (29.8066)	mem 4879MB
[2022-05-31 03:44:08 MetaFG_0] (main.py 265): INFO Train: [38/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2937 (0.3057)	loss 1.1105 (1.4102)	grad_norm 17.8127 (29.7487)	mem 4879MB
[2022-05-31 03:44:11 MetaFG_0] (main.py 265): INFO Train: [38/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2996 (0.3057)	loss 1.2184 (1.4111)	grad_norm 22.8323 (29.7625)	mem 4879MB
[2022-05-31 03:44:15 MetaFG_0] (main.py 265): INFO Train: [38/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2979 (0.3057)	loss 1.5804 (1.4106)	grad_norm 37.7064 (29.7769)	mem 4879MB
[2022-05-31 03:44:18 MetaFG_0] (main.py 265): INFO Train: [38/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2925 (0.3057)	loss 1.4748 (1.4105)	grad_norm 37.3966 (29.7603)	mem 4879MB
[2022-05-31 03:44:21 MetaFG_0] (main.py 265): INFO Train: [38/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2983 (0.3057)	loss 1.3952 (1.4107)	grad_norm 21.4514 (29.7128)	mem 4879MB
[2022-05-31 03:44:24 MetaFG_0] (main.py 265): INFO Train: [38/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2987 (0.3057)	loss 0.9552 (1.4098)	grad_norm 25.0454 (29.7280)	mem 4879MB
[2022-05-31 03:44:27 MetaFG_0] (main.py 265): INFO Train: [38/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2995 (0.3057)	loss 1.6081 (1.4109)	grad_norm 43.0052 (29.7674)	mem 4879MB
[2022-05-31 03:44:30 MetaFG_0] (main.py 265): INFO Train: [38/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.3035 (0.3057)	loss 1.4008 (1.4121)	grad_norm 33.0878 (29.7970)	mem 4879MB
[2022-05-31 03:44:33 MetaFG_0] (main.py 265): INFO Train: [38/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3008 (0.3057)	loss 1.0522 (1.4110)	grad_norm 51.7884 (29.8374)	mem 4879MB
[2022-05-31 03:44:36 MetaFG_0] (main.py 265): INFO Train: [38/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2916 (0.3056)	loss 1.2216 (1.4120)	grad_norm 43.4468 (29.9385)	mem 4879MB
[2022-05-31 03:44:39 MetaFG_0] (main.py 265): INFO Train: [38/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2983 (0.3056)	loss 1.5626 (1.4123)	grad_norm 40.1634 (29.9493)	mem 4879MB
[2022-05-31 03:44:42 MetaFG_0] (main.py 265): INFO Train: [38/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2917 (0.3056)	loss 1.1345 (1.4124)	grad_norm 32.9929 (29.9462)	mem 4879MB
[2022-05-31 03:44:45 MetaFG_0] (main.py 265): INFO Train: [38/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2920 (0.3056)	loss 1.4994 (1.4116)	grad_norm 17.1813 (29.9480)	mem 4879MB
[2022-05-31 03:44:48 MetaFG_0] (main.py 265): INFO Train: [38/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2999 (0.3056)	loss 1.3224 (1.4111)	grad_norm 24.5718 (29.9444)	mem 4879MB
[2022-05-31 03:44:51 MetaFG_0] (main.py 265): INFO Train: [38/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2932 (0.3055)	loss 1.3915 (1.4102)	grad_norm 36.0635 (29.9487)	mem 4879MB
[2022-05-31 03:44:54 MetaFG_0] (main.py 265): INFO Train: [38/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2932 (0.3056)	loss 1.4955 (1.4112)	grad_norm 27.1510 (29.9277)	mem 4879MB
[2022-05-31 03:44:57 MetaFG_0] (main.py 265): INFO Train: [38/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2985 (0.3056)	loss 1.5025 (1.4114)	grad_norm 28.1040 (29.9507)	mem 4879MB
[2022-05-31 03:45:00 MetaFG_0] (main.py 265): INFO Train: [38/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2919 (0.3055)	loss 1.5046 (1.4110)	grad_norm 27.0735 (29.9220)	mem 4879MB
[2022-05-31 03:45:03 MetaFG_0] (main.py 265): INFO Train: [38/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2933 (0.3055)	loss 1.6301 (1.4104)	grad_norm 45.2955 (29.9180)	mem 4879MB
[2022-05-31 03:45:06 MetaFG_0] (main.py 265): INFO Train: [38/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2982 (0.3055)	loss 1.0392 (1.4096)	grad_norm 22.0144 (29.8862)	mem 4879MB
[2022-05-31 03:45:09 MetaFG_0] (main.py 265): INFO Train: [38/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2982 (0.3055)	loss 1.7288 (1.4102)	grad_norm 25.7949 (29.8832)	mem 4879MB
[2022-05-31 03:45:12 MetaFG_0] (main.py 265): INFO Train: [38/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2931 (0.3055)	loss 1.1653 (1.4104)	grad_norm 34.9056 (29.8821)	mem 4879MB
[2022-05-31 03:45:15 MetaFG_0] (main.py 265): INFO Train: [38/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3002 (0.3055)	loss 1.4345 (1.4106)	grad_norm 29.7048 (29.8861)	mem 4879MB
[2022-05-31 03:45:18 MetaFG_0] (main.py 265): INFO Train: [38/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2978 (0.3055)	loss 1.6605 (1.4095)	grad_norm 23.9459 (29.8925)	mem 4879MB
[2022-05-31 03:45:22 MetaFG_0] (main.py 265): INFO Train: [38/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2987 (0.3055)	loss 1.7950 (1.4101)	grad_norm 35.6944 (29.9107)	mem 4879MB
[2022-05-31 03:45:25 MetaFG_0] (main.py 265): INFO Train: [38/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2929 (0.3054)	loss 1.0594 (1.4094)	grad_norm 45.1019 (29.8956)	mem 4879MB
[2022-05-31 03:45:28 MetaFG_0] (main.py 265): INFO Train: [38/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2918 (0.3054)	loss 1.5991 (1.4098)	grad_norm 21.6953 (29.9009)	mem 4879MB
[2022-05-31 03:45:31 MetaFG_0] (main.py 265): INFO Train: [38/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2978 (0.3054)	loss 1.5905 (1.4088)	grad_norm 18.2644 (29.9365)	mem 4879MB
[2022-05-31 03:45:34 MetaFG_0] (main.py 265): INFO Train: [38/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2926 (0.3054)	loss 1.2170 (1.4083)	grad_norm 33.2611 (29.9237)	mem 4879MB
[2022-05-31 03:45:37 MetaFG_0] (main.py 265): INFO Train: [38/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2921 (0.3053)	loss 1.1722 (1.4093)	grad_norm 33.5838 (29.9568)	mem 4879MB
[2022-05-31 03:45:40 MetaFG_0] (main.py 265): INFO Train: [38/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2932 (0.3053)	loss 1.4911 (1.4096)	grad_norm 27.8239 (29.8904)	mem 4879MB
[2022-05-31 03:45:43 MetaFG_0] (main.py 265): INFO Train: [38/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2991 (0.3053)	loss 1.1433 (1.4095)	grad_norm 18.7572 (29.8633)	mem 4879MB
[2022-05-31 03:45:46 MetaFG_0] (main.py 265): INFO Train: [38/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2924 (0.3053)	loss 1.5442 (1.4100)	grad_norm 36.8559 (29.8605)	mem 4879MB
[2022-05-31 03:45:49 MetaFG_0] (main.py 265): INFO Train: [38/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2986 (0.3053)	loss 1.3805 (1.4099)	grad_norm 23.5966 (29.8732)	mem 4879MB
[2022-05-31 03:45:52 MetaFG_0] (main.py 265): INFO Train: [38/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2940 (0.3053)	loss 1.0960 (1.4096)	grad_norm 25.2529 (29.8960)	mem 4879MB
[2022-05-31 03:45:55 MetaFG_0] (main.py 265): INFO Train: [38/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2982 (0.3052)	loss 1.6519 (1.4089)	grad_norm 46.5542 (29.8923)	mem 4879MB
[2022-05-31 03:45:58 MetaFG_0] (main.py 265): INFO Train: [38/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2982 (0.3052)	loss 0.9419 (1.4082)	grad_norm 39.6290 (29.8996)	mem 4879MB
[2022-05-31 03:46:01 MetaFG_0] (main.py 265): INFO Train: [38/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2919 (0.3052)	loss 1.4756 (1.4093)	grad_norm 22.7713 (29.8926)	mem 4879MB
[2022-05-31 03:46:04 MetaFG_0] (main.py 265): INFO Train: [38/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2934 (0.3052)	loss 1.6322 (1.4097)	grad_norm 25.1125 (29.8733)	mem 4879MB
[2022-05-31 03:46:07 MetaFG_0] (main.py 265): INFO Train: [38/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2926 (0.3052)	loss 1.4376 (1.4102)	grad_norm 26.9210 (29.8315)	mem 4879MB
[2022-05-31 03:46:10 MetaFG_0] (main.py 265): INFO Train: [38/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2919 (0.3052)	loss 1.4932 (1.4113)	grad_norm 23.4731 (29.8028)	mem 4879MB
[2022-05-31 03:46:13 MetaFG_0] (main.py 265): INFO Train: [38/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2964 (0.3052)	loss 1.7756 (1.4123)	grad_norm 34.8145 (29.8249)	mem 4879MB
[2022-05-31 03:46:16 MetaFG_0] (main.py 265): INFO Train: [38/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2932 (0.3052)	loss 0.9816 (1.4115)	grad_norm 26.5428 (29.7772)	mem 4879MB
[2022-05-31 03:46:19 MetaFG_0] (main.py 265): INFO Train: [38/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2978 (0.3051)	loss 1.7022 (1.4105)	grad_norm 36.3084 (29.8045)	mem 4879MB
[2022-05-31 03:46:22 MetaFG_0] (main.py 265): INFO Train: [38/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2922 (0.3051)	loss 1.7409 (1.4115)	grad_norm 36.0702 (29.7844)	mem 4879MB
[2022-05-31 03:46:25 MetaFG_0] (main.py 265): INFO Train: [38/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2987 (0.3051)	loss 1.0269 (1.4119)	grad_norm 25.4358 (29.8036)	mem 4879MB
[2022-05-31 03:46:28 MetaFG_0] (main.py 265): INFO Train: [38/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2987 (0.3051)	loss 1.3080 (1.4111)	grad_norm 31.8143 (29.8092)	mem 4879MB
[2022-05-31 03:46:31 MetaFG_0] (main.py 265): INFO Train: [38/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3056 (0.3051)	loss 1.2968 (1.4120)	grad_norm 23.3750 (29.8375)	mem 4879MB
[2022-05-31 03:46:34 MetaFG_0] (main.py 265): INFO Train: [38/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.3021 (0.3051)	loss 1.3006 (1.4117)	grad_norm 32.8506 (29.8312)	mem 4879MB
[2022-05-31 03:46:37 MetaFG_0] (main.py 265): INFO Train: [38/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.3023 (0.3051)	loss 1.3537 (1.4122)	grad_norm 19.3458 (29.9225)	mem 4879MB
[2022-05-31 03:46:40 MetaFG_0] (main.py 265): INFO Train: [38/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2927 (0.3051)	loss 1.2521 (1.4125)	grad_norm 67.4809 (29.9692)	mem 4879MB
[2022-05-31 03:46:43 MetaFG_0] (main.py 265): INFO Train: [38/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2926 (0.3050)	loss 1.3096 (1.4130)	grad_norm 46.0220 (29.9736)	mem 4879MB
[2022-05-31 03:46:46 MetaFG_0] (main.py 265): INFO Train: [38/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2917 (0.3050)	loss 1.7788 (1.4134)	grad_norm 35.3910 (29.9529)	mem 4879MB
[2022-05-31 03:46:50 MetaFG_0] (main.py 265): INFO Train: [38/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2920 (0.3050)	loss 1.4242 (1.4137)	grad_norm 35.9135 (29.9422)	mem 4879MB
[2022-05-31 03:46:53 MetaFG_0] (main.py 265): INFO Train: [38/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2974 (0.3050)	loss 1.5744 (1.4137)	grad_norm 19.2876 (29.9478)	mem 4879MB
[2022-05-31 03:46:56 MetaFG_0] (main.py 265): INFO Train: [38/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2986 (0.3050)	loss 1.4713 (1.4142)	grad_norm 18.9310 (29.9378)	mem 4879MB
[2022-05-31 03:46:59 MetaFG_0] (main.py 265): INFO Train: [38/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2983 (0.3050)	loss 1.4312 (1.4137)	grad_norm 23.7898 (29.9211)	mem 4879MB
[2022-05-31 03:47:02 MetaFG_0] (main.py 265): INFO Train: [38/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2928 (0.3050)	loss 1.5905 (1.4139)	grad_norm 47.2223 (29.8998)	mem 4879MB
[2022-05-31 03:47:05 MetaFG_0] (main.py 265): INFO Train: [38/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2935 (0.3050)	loss 1.5710 (1.4137)	grad_norm 31.3636 (29.9116)	mem 4879MB
[2022-05-31 03:47:08 MetaFG_0] (main.py 265): INFO Train: [38/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2977 (0.3050)	loss 1.8750 (1.4144)	grad_norm 29.6439 (29.8581)	mem 4879MB
[2022-05-31 03:47:11 MetaFG_0] (main.py 265): INFO Train: [38/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2920 (0.3049)	loss 1.6337 (1.4151)	grad_norm 26.2646 (30.0505)	mem 4879MB
[2022-05-31 03:47:14 MetaFG_0] (main.py 265): INFO Train: [38/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2920 (0.3049)	loss 1.7252 (1.4153)	grad_norm 38.5210 (30.0229)	mem 4879MB
[2022-05-31 03:47:17 MetaFG_0] (main.py 265): INFO Train: [38/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2993 (0.3049)	loss 1.1369 (1.4152)	grad_norm 32.5760 (30.0244)	mem 4879MB
[2022-05-31 03:47:20 MetaFG_0] (main.py 265): INFO Train: [38/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2937 (0.3049)	loss 1.4892 (1.4146)	grad_norm 18.6234 (30.0270)	mem 4879MB
[2022-05-31 03:47:23 MetaFG_0] (main.py 265): INFO Train: [38/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2930 (0.3049)	loss 1.1062 (1.4146)	grad_norm 27.6009 (30.0214)	mem 4879MB
[2022-05-31 03:47:26 MetaFG_0] (main.py 265): INFO Train: [38/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2994 (0.3049)	loss 1.6743 (1.4152)	grad_norm 49.0271 (30.0657)	mem 4879MB
[2022-05-31 03:47:29 MetaFG_0] (main.py 265): INFO Train: [38/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2981 (0.3049)	loss 1.3961 (1.4155)	grad_norm 15.1457 (30.0276)	mem 4879MB
[2022-05-31 03:47:32 MetaFG_0] (main.py 265): INFO Train: [38/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2926 (0.3049)	loss 0.9368 (1.4151)	grad_norm 32.1514 (29.9836)	mem 4879MB
[2022-05-31 03:47:35 MetaFG_0] (main.py 265): INFO Train: [38/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.3000 (0.3049)	loss 1.5461 (1.4155)	grad_norm 23.2806 (29.9578)	mem 4879MB
[2022-05-31 03:47:38 MetaFG_0] (main.py 265): INFO Train: [38/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2990 (0.3049)	loss 1.6504 (1.4161)	grad_norm 29.4946 (29.9312)	mem 4879MB
[2022-05-31 03:47:41 MetaFG_0] (main.py 265): INFO Train: [38/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2967 (0.3049)	loss 1.4264 (1.4166)	grad_norm 34.9121 (29.9320)	mem 4879MB
[2022-05-31 03:47:44 MetaFG_0] (main.py 265): INFO Train: [38/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2990 (0.3048)	loss 0.9140 (1.4159)	grad_norm 30.2360 (29.9238)	mem 4879MB
[2022-05-31 03:47:47 MetaFG_0] (main.py 265): INFO Train: [38/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2982 (0.3048)	loss 1.4515 (1.4150)	grad_norm 21.5642 (29.9005)	mem 4879MB
[2022-05-31 03:47:50 MetaFG_0] (main.py 265): INFO Train: [38/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2985 (0.3048)	loss 1.3548 (1.4150)	grad_norm 31.6381 (29.9043)	mem 4879MB
[2022-05-31 03:47:53 MetaFG_0] (main.py 265): INFO Train: [38/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2918 (0.3048)	loss 0.8762 (1.4148)	grad_norm 19.7761 (29.8887)	mem 4879MB
[2022-05-31 03:47:56 MetaFG_0] (main.py 265): INFO Train: [38/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2931 (0.3048)	loss 1.1066 (1.4146)	grad_norm 36.3802 (29.8704)	mem 4879MB
[2022-05-31 03:47:59 MetaFG_0] (main.py 265): INFO Train: [38/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2981 (0.3048)	loss 1.4383 (1.4149)	grad_norm 14.1284 (29.8478)	mem 4879MB
[2022-05-31 03:48:02 MetaFG_0] (main.py 265): INFO Train: [38/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2915 (0.3048)	loss 1.7445 (1.4153)	grad_norm 40.7756 (29.8535)	mem 4879MB
[2022-05-31 03:48:03 MetaFG_0] (main.py 272): INFO EPOCH 38 training takes 0:07:56
[2022-05-31 03:48:03 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_38.pth saving......
[2022-05-31 03:48:04 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_38.pth saved !!!
[2022-05-31 03:48:04 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:48:05 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:48:05 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:48:06 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.698 (0.698)	Loss 0.5797 (0.5797)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 03:48:07 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.148)	Loss 0.6140 (0.6482)	Acc@1 87.500 (84.659)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 03:48:08 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.089 (0.122)	Loss 0.7819 (0.6463)	Acc@1 81.250 (85.565)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 03:48:09 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.097 (0.113)	Loss 0.4821 (0.6347)	Acc@1 93.750 (86.593)	Acc@5 100.000 (98.690)	Mem 4879MB
[2022-05-31 03:48:10 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.108)	Loss 0.7657 (0.6550)	Acc@1 81.250 (85.976)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 03:48:10 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.105)	Loss 0.4963 (0.6556)	Acc@1 93.750 (86.029)	Acc@5 100.000 (98.468)	Mem 4879MB
[2022-05-31 03:48:11 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.104)	Loss 0.6281 (0.6597)	Acc@1 87.500 (85.861)	Acc@5 100.000 (98.463)	Mem 4879MB
[2022-05-31 03:48:12 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.7134 (0.6698)	Acc@1 84.375 (85.519)	Acc@5 96.875 (98.371)	Mem 4879MB
[2022-05-31 03:48:13 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.093 (0.101)	Loss 0.6585 (0.6779)	Acc@1 81.250 (85.185)	Acc@5 100.000 (98.341)	Mem 4879MB
[2022-05-31 03:48:14 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.099 (0.100)	Loss 0.6887 (0.6773)	Acc@1 84.375 (85.096)	Acc@5 100.000 (98.352)	Mem 4879MB
[2022-05-31 03:48:15 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.099)	Loss 0.9206 (0.6881)	Acc@1 78.125 (84.653)	Acc@5 93.750 (98.298)	Mem 4879MB
[2022-05-31 03:48:16 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.099)	Loss 0.5928 (0.6924)	Acc@1 87.500 (84.431)	Acc@5 96.875 (98.311)	Mem 4879MB
[2022-05-31 03:48:17 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.6522 (0.6898)	Acc@1 84.375 (84.401)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 03:48:18 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.098)	Loss 0.7460 (0.6891)	Acc@1 84.375 (84.303)	Acc@5 96.875 (98.259)	Mem 4879MB
[2022-05-31 03:48:19 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.098)	Loss 0.6468 (0.6894)	Acc@1 90.625 (84.397)	Acc@5 100.000 (98.249)	Mem 4879MB
[2022-05-31 03:48:20 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.100 (0.098)	Loss 0.7016 (0.6876)	Acc@1 87.500 (84.437)	Acc@5 96.875 (98.282)	Mem 4879MB
[2022-05-31 03:48:21 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.097)	Loss 0.6443 (0.6905)	Acc@1 81.250 (84.375)	Acc@5 100.000 (98.195)	Mem 4879MB
[2022-05-31 03:48:22 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.097)	Loss 0.4226 (0.6892)	Acc@1 93.750 (84.430)	Acc@5 100.000 (98.173)	Mem 4879MB
[2022-05-31 03:48:23 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.097)	Loss 0.5156 (0.6905)	Acc@1 87.500 (84.323)	Acc@5 96.875 (98.170)	Mem 4879MB
[2022-05-31 03:48:24 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.8054 (0.6925)	Acc@1 78.125 (84.228)	Acc@5 96.875 (98.102)	Mem 4879MB
[2022-05-31 03:48:24 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.7320 (0.6960)	Acc@1 84.375 (83.940)	Acc@5 100.000 (98.134)	Mem 4879MB
[2022-05-31 03:48:25 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.096)	Loss 0.8009 (0.6931)	Acc@1 87.500 (84.034)	Acc@5 96.875 (98.164)	Mem 4879MB
[2022-05-31 03:48:26 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.096)	Loss 0.8070 (0.6912)	Acc@1 78.125 (84.050)	Acc@5 96.875 (98.232)	Mem 4879MB
[2022-05-31 03:48:27 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 0.4871 (0.6871)	Acc@1 90.625 (84.104)	Acc@5 100.000 (98.255)	Mem 4879MB
[2022-05-31 03:48:28 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 0.5272 (0.6836)	Acc@1 90.625 (84.180)	Acc@5 96.875 (98.249)	Mem 4879MB
[2022-05-31 03:48:29 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.096)	Loss 0.8602 (0.6847)	Acc@1 75.000 (84.163)	Acc@5 96.875 (98.220)	Mem 4879MB
[2022-05-31 03:48:30 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.096)	Loss 0.7540 (0.6842)	Acc@1 87.500 (84.255)	Acc@5 96.875 (98.228)	Mem 4879MB
[2022-05-31 03:48:31 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.8947 (0.6827)	Acc@1 68.750 (84.225)	Acc@5 100.000 (98.270)	Mem 4879MB
[2022-05-31 03:48:32 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.095 (0.096)	Loss 0.8515 (0.6836)	Acc@1 81.250 (84.153)	Acc@5 96.875 (98.254)	Mem 4879MB
[2022-05-31 03:48:33 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.9091 (0.6833)	Acc@1 78.125 (84.117)	Acc@5 96.875 (98.239)	Mem 4879MB
[2022-05-31 03:48:34 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 0.6494 (0.6817)	Acc@1 78.125 (84.167)	Acc@5 100.000 (98.266)	Mem 4879MB
[2022-05-31 03:48:35 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6084 (0.6816)	Acc@1 87.500 (84.124)	Acc@5 100.000 (98.262)	Mem 4879MB
[2022-05-31 03:48:35 MetaFG_0] (main.py 330): INFO  * Acc@1 84.160 Acc@5 98.270
[2022-05-31 03:48:35 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 84.2%
[2022-05-31 03:48:35 MetaFG_0] (main.py 171): INFO Max accuracy: 84.19%
[2022-05-31 03:48:36 MetaFG_0] (main.py 265): INFO Train: [39/300][0/1562]	eta 0:26:15 lr 0.000006	time 1.0089 (1.0089)	loss 1.4160 (1.4160)	grad_norm 19.3008 (19.3008)	mem 4879MB
[2022-05-31 03:48:39 MetaFG_0] (main.py 265): INFO Train: [39/300][10/1562]	eta 0:09:39 lr 0.000006	time 0.2927 (0.3732)	loss 0.9061 (1.4923)	grad_norm 21.9771 (28.5336)	mem 4879MB
[2022-05-31 03:48:42 MetaFG_0] (main.py 265): INFO Train: [39/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2988 (0.3403)	loss 1.6328 (1.3945)	grad_norm 20.1507 (29.4429)	mem 4879MB
[2022-05-31 03:48:45 MetaFG_0] (main.py 265): INFO Train: [39/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2979 (0.3280)	loss 1.4949 (1.4017)	grad_norm 18.9831 (28.8747)	mem 4879MB
[2022-05-31 03:48:48 MetaFG_0] (main.py 265): INFO Train: [39/300][40/1562]	eta 0:08:09 lr 0.000006	time 0.2982 (0.3219)	loss 1.0909 (1.4039)	grad_norm 28.1683 (27.9087)	mem 4879MB
[2022-05-31 03:48:51 MetaFG_0] (main.py 265): INFO Train: [39/300][50/1562]	eta 0:08:00 lr 0.000006	time 0.2924 (0.3181)	loss 1.7271 (1.4055)	grad_norm 40.0723 (27.4999)	mem 4879MB
[2022-05-31 03:48:54 MetaFG_0] (main.py 265): INFO Train: [39/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.2924 (0.3157)	loss 1.3786 (1.4186)	grad_norm 28.7096 (27.0828)	mem 4879MB
[2022-05-31 03:48:57 MetaFG_0] (main.py 265): INFO Train: [39/300][70/1562]	eta 0:07:48 lr 0.000006	time 0.2990 (0.3140)	loss 1.3100 (1.4306)	grad_norm 19.4746 (27.3060)	mem 4879MB
[2022-05-31 03:49:00 MetaFG_0] (main.py 265): INFO Train: [39/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2921 (0.3126)	loss 1.5975 (1.4258)	grad_norm 29.9066 (27.7415)	mem 4879MB
[2022-05-31 03:49:03 MetaFG_0] (main.py 265): INFO Train: [39/300][90/1562]	eta 0:07:38 lr 0.000006	time 0.3012 (0.3116)	loss 0.9053 (1.4177)	grad_norm 18.3292 (28.1218)	mem 4879MB
[2022-05-31 03:49:06 MetaFG_0] (main.py 265): INFO Train: [39/300][100/1562]	eta 0:07:34 lr 0.000006	time 0.3000 (0.3108)	loss 1.7450 (1.4173)	grad_norm 29.4361 (28.4916)	mem 4879MB
[2022-05-31 03:49:10 MetaFG_0] (main.py 265): INFO Train: [39/300][110/1562]	eta 0:07:30 lr 0.000006	time 0.2940 (0.3102)	loss 1.6487 (1.4203)	grad_norm 36.7948 (28.6690)	mem 4879MB
[2022-05-31 03:49:13 MetaFG_0] (main.py 265): INFO Train: [39/300][120/1562]	eta 0:07:26 lr 0.000006	time 0.2993 (0.3096)	loss 1.7377 (1.4320)	grad_norm 46.7287 (29.8112)	mem 4879MB
[2022-05-31 03:49:16 MetaFG_0] (main.py 265): INFO Train: [39/300][130/1562]	eta 0:07:22 lr 0.000006	time 0.3048 (0.3091)	loss 1.2748 (1.4291)	grad_norm 22.0395 (29.7248)	mem 4879MB
[2022-05-31 03:49:19 MetaFG_0] (main.py 265): INFO Train: [39/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2992 (0.3108)	loss 1.6421 (1.4265)	grad_norm 25.0391 (29.4748)	mem 4879MB
[2022-05-31 03:49:22 MetaFG_0] (main.py 265): INFO Train: [39/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2924 (0.3104)	loss 1.4730 (1.4215)	grad_norm 18.6990 (29.6763)	mem 4879MB
[2022-05-31 03:49:25 MetaFG_0] (main.py 265): INFO Train: [39/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2978 (0.3100)	loss 1.7126 (1.4187)	grad_norm 21.6583 (29.8959)	mem 4879MB
[2022-05-31 03:49:28 MetaFG_0] (main.py 265): INFO Train: [39/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2917 (0.3095)	loss 1.1585 (1.4131)	grad_norm 29.2792 (30.0452)	mem 4879MB
[2022-05-31 03:49:31 MetaFG_0] (main.py 265): INFO Train: [39/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2994 (0.3092)	loss 1.3939 (1.4071)	grad_norm 40.3674 (30.0984)	mem 4879MB
[2022-05-31 03:49:34 MetaFG_0] (main.py 265): INFO Train: [39/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2930 (0.3090)	loss 1.4002 (1.4063)	grad_norm 22.7694 (29.9615)	mem 4879MB
[2022-05-31 03:49:37 MetaFG_0] (main.py 265): INFO Train: [39/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2960 (0.3088)	loss 1.5619 (1.4083)	grad_norm 26.7596 (29.7287)	mem 4879MB
[2022-05-31 03:49:40 MetaFG_0] (main.py 265): INFO Train: [39/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2936 (0.3085)	loss 1.5388 (1.4077)	grad_norm 26.1025 (29.5238)	mem 4879MB
[2022-05-31 03:49:43 MetaFG_0] (main.py 265): INFO Train: [39/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2930 (0.3082)	loss 1.2022 (1.4051)	grad_norm 33.0444 (29.4742)	mem 4879MB
[2022-05-31 03:49:46 MetaFG_0] (main.py 265): INFO Train: [39/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2937 (0.3080)	loss 1.3815 (1.4054)	grad_norm 15.8858 (29.1837)	mem 4879MB
[2022-05-31 03:49:49 MetaFG_0] (main.py 265): INFO Train: [39/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2923 (0.3079)	loss 1.2203 (1.4057)	grad_norm 41.5162 (29.1002)	mem 4879MB
[2022-05-31 03:49:52 MetaFG_0] (main.py 265): INFO Train: [39/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2921 (0.3078)	loss 1.1851 (1.4005)	grad_norm 28.0633 (29.4196)	mem 4879MB
[2022-05-31 03:49:55 MetaFG_0] (main.py 265): INFO Train: [39/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2984 (0.3076)	loss 1.4554 (1.4002)	grad_norm 23.2804 (29.7183)	mem 4879MB
[2022-05-31 03:49:58 MetaFG_0] (main.py 265): INFO Train: [39/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2981 (0.3075)	loss 1.2918 (1.3959)	grad_norm 23.5909 (29.7238)	mem 4879MB
[2022-05-31 03:50:01 MetaFG_0] (main.py 265): INFO Train: [39/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2971 (0.3073)	loss 1.5278 (1.3966)	grad_norm 37.2040 (29.6564)	mem 4879MB
[2022-05-31 03:50:04 MetaFG_0] (main.py 265): INFO Train: [39/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2922 (0.3071)	loss 1.5141 (1.3973)	grad_norm 39.1433 (29.7497)	mem 4879MB
[2022-05-31 03:50:07 MetaFG_0] (main.py 265): INFO Train: [39/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2930 (0.3070)	loss 1.6868 (1.4011)	grad_norm 34.1475 (29.7359)	mem 4879MB
[2022-05-31 03:50:11 MetaFG_0] (main.py 265): INFO Train: [39/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2922 (0.3069)	loss 1.6469 (1.4021)	grad_norm 26.0019 (29.8101)	mem 4879MB
[2022-05-31 03:50:14 MetaFG_0] (main.py 265): INFO Train: [39/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2979 (0.3068)	loss 1.4409 (1.4027)	grad_norm 28.6546 (29.8720)	mem 4879MB
[2022-05-31 03:50:17 MetaFG_0] (main.py 265): INFO Train: [39/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2933 (0.3067)	loss 1.2871 (1.4035)	grad_norm 28.3344 (29.9111)	mem 4879MB
[2022-05-31 03:50:20 MetaFG_0] (main.py 265): INFO Train: [39/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2976 (0.3066)	loss 1.3034 (1.4048)	grad_norm 27.7415 (29.9469)	mem 4879MB
[2022-05-31 03:50:23 MetaFG_0] (main.py 265): INFO Train: [39/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2932 (0.3065)	loss 1.6314 (1.4065)	grad_norm 15.2989 (29.9140)	mem 4879MB
[2022-05-31 03:50:26 MetaFG_0] (main.py 265): INFO Train: [39/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.3008 (0.3064)	loss 1.6539 (1.4098)	grad_norm 24.7608 (29.8647)	mem 4879MB
[2022-05-31 03:50:29 MetaFG_0] (main.py 265): INFO Train: [39/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2987 (0.3064)	loss 1.3796 (1.4064)	grad_norm 26.8855 (29.8669)	mem 4879MB
[2022-05-31 03:50:32 MetaFG_0] (main.py 265): INFO Train: [39/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2923 (0.3063)	loss 1.4788 (1.4059)	grad_norm 35.0675 (29.7617)	mem 4879MB
[2022-05-31 03:50:35 MetaFG_0] (main.py 265): INFO Train: [39/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2992 (0.3062)	loss 1.3606 (1.4070)	grad_norm 20.4001 (29.6940)	mem 4879MB
[2022-05-31 03:50:38 MetaFG_0] (main.py 265): INFO Train: [39/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2929 (0.3061)	loss 1.5760 (1.4069)	grad_norm 24.8431 (29.6738)	mem 4879MB
[2022-05-31 03:50:41 MetaFG_0] (main.py 265): INFO Train: [39/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2941 (0.3061)	loss 1.4220 (1.4050)	grad_norm 25.7866 (29.6122)	mem 4879MB
[2022-05-31 03:50:44 MetaFG_0] (main.py 265): INFO Train: [39/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2932 (0.3060)	loss 1.4482 (1.4047)	grad_norm 44.2678 (29.6068)	mem 4879MB
[2022-05-31 03:50:47 MetaFG_0] (main.py 265): INFO Train: [39/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2933 (0.3060)	loss 1.4078 (1.4079)	grad_norm 22.2955 (29.5902)	mem 4879MB
[2022-05-31 03:50:50 MetaFG_0] (main.py 265): INFO Train: [39/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2929 (0.3059)	loss 0.8933 (1.4037)	grad_norm 16.2753 (29.5655)	mem 4879MB
[2022-05-31 03:50:53 MetaFG_0] (main.py 265): INFO Train: [39/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2923 (0.3059)	loss 1.1122 (1.4037)	grad_norm 17.9793 (29.5881)	mem 4879MB
[2022-05-31 03:50:56 MetaFG_0] (main.py 265): INFO Train: [39/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2915 (0.3058)	loss 1.4173 (1.4033)	grad_norm 19.4480 (29.4840)	mem 4879MB
[2022-05-31 03:50:59 MetaFG_0] (main.py 265): INFO Train: [39/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.2920 (0.3057)	loss 1.0481 (1.4015)	grad_norm 40.0411 (29.5249)	mem 4879MB
[2022-05-31 03:51:02 MetaFG_0] (main.py 265): INFO Train: [39/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2982 (0.3057)	loss 1.4992 (1.4004)	grad_norm 22.3064 (29.4922)	mem 4879MB
[2022-05-31 03:51:05 MetaFG_0] (main.py 265): INFO Train: [39/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2984 (0.3057)	loss 1.1010 (1.3998)	grad_norm 41.1245 (29.4769)	mem 4879MB
[2022-05-31 03:51:08 MetaFG_0] (main.py 265): INFO Train: [39/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.3160 (0.3062)	loss 1.6066 (1.3996)	grad_norm 43.7698 (29.4284)	mem 4879MB
[2022-05-31 03:51:12 MetaFG_0] (main.py 265): INFO Train: [39/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2929 (0.3062)	loss 1.6724 (1.3986)	grad_norm 22.3314 (29.4083)	mem 4879MB
[2022-05-31 03:51:15 MetaFG_0] (main.py 265): INFO Train: [39/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2923 (0.3061)	loss 1.3193 (1.3981)	grad_norm 22.4941 (29.4355)	mem 4879MB
[2022-05-31 03:51:18 MetaFG_0] (main.py 265): INFO Train: [39/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2917 (0.3060)	loss 1.1450 (1.3969)	grad_norm 22.0316 (29.4798)	mem 4879MB
[2022-05-31 03:51:21 MetaFG_0] (main.py 265): INFO Train: [39/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2920 (0.3060)	loss 1.6135 (1.3969)	grad_norm 28.4332 (29.3971)	mem 4879MB
[2022-05-31 03:51:24 MetaFG_0] (main.py 265): INFO Train: [39/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2920 (0.3060)	loss 1.4932 (1.3952)	grad_norm 20.1759 (29.3687)	mem 4879MB
[2022-05-31 03:51:27 MetaFG_0] (main.py 265): INFO Train: [39/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2922 (0.3059)	loss 1.8045 (1.3959)	grad_norm 28.4777 (29.2829)	mem 4879MB
[2022-05-31 03:51:30 MetaFG_0] (main.py 265): INFO Train: [39/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2983 (0.3059)	loss 1.2261 (1.3943)	grad_norm 18.8433 (29.2411)	mem 4879MB
[2022-05-31 03:51:33 MetaFG_0] (main.py 265): INFO Train: [39/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2934 (0.3059)	loss 1.4212 (1.3927)	grad_norm 31.5078 (29.2109)	mem 4879MB
[2022-05-31 03:51:36 MetaFG_0] (main.py 265): INFO Train: [39/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2941 (0.3059)	loss 1.2393 (1.3914)	grad_norm 26.9840 (29.2123)	mem 4879MB
[2022-05-31 03:51:39 MetaFG_0] (main.py 265): INFO Train: [39/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2931 (0.3058)	loss 1.4473 (1.3908)	grad_norm 37.8075 (29.3222)	mem 4879MB
[2022-05-31 03:51:42 MetaFG_0] (main.py 265): INFO Train: [39/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2930 (0.3058)	loss 1.5214 (1.3917)	grad_norm 22.7549 (29.2495)	mem 4879MB
[2022-05-31 03:51:45 MetaFG_0] (main.py 265): INFO Train: [39/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2935 (0.3058)	loss 1.4869 (1.3936)	grad_norm 45.3671 (29.2818)	mem 4879MB
[2022-05-31 03:51:48 MetaFG_0] (main.py 265): INFO Train: [39/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2976 (0.3058)	loss 1.2981 (1.3931)	grad_norm 31.4037 (29.3141)	mem 4879MB
[2022-05-31 03:51:51 MetaFG_0] (main.py 265): INFO Train: [39/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2982 (0.3057)	loss 1.6213 (1.3938)	grad_norm 28.0664 (29.3775)	mem 4879MB
[2022-05-31 03:51:54 MetaFG_0] (main.py 265): INFO Train: [39/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2944 (0.3057)	loss 0.8412 (1.3931)	grad_norm 22.7231 (29.3244)	mem 4879MB
[2022-05-31 03:51:57 MetaFG_0] (main.py 265): INFO Train: [39/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2977 (0.3057)	loss 1.6029 (1.3933)	grad_norm 24.0367 (29.3295)	mem 4879MB
[2022-05-31 03:52:00 MetaFG_0] (main.py 265): INFO Train: [39/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2986 (0.3057)	loss 1.5066 (1.3927)	grad_norm 55.8768 (29.3413)	mem 4879MB
[2022-05-31 03:52:03 MetaFG_0] (main.py 265): INFO Train: [39/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2924 (0.3056)	loss 1.1372 (1.3921)	grad_norm 48.1534 (29.3537)	mem 4879MB
[2022-05-31 03:52:06 MetaFG_0] (main.py 265): INFO Train: [39/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2921 (0.3056)	loss 1.6902 (1.3926)	grad_norm 35.9365 (29.3318)	mem 4879MB
[2022-05-31 03:52:09 MetaFG_0] (main.py 265): INFO Train: [39/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2933 (0.3056)	loss 1.3226 (1.3926)	grad_norm 30.9263 (29.3738)	mem 4879MB
[2022-05-31 03:52:12 MetaFG_0] (main.py 265): INFO Train: [39/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2998 (0.3056)	loss 1.4669 (1.3928)	grad_norm 21.1433 (29.3812)	mem 4879MB
[2022-05-31 03:52:15 MetaFG_0] (main.py 265): INFO Train: [39/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2928 (0.3056)	loss 1.7378 (1.3934)	grad_norm 28.6875 (29.3441)	mem 4879MB
[2022-05-31 03:52:18 MetaFG_0] (main.py 265): INFO Train: [39/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3017 (0.3056)	loss 1.5233 (1.3922)	grad_norm 20.5065 (29.3314)	mem 4879MB
[2022-05-31 03:52:22 MetaFG_0] (main.py 265): INFO Train: [39/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.3002 (0.3056)	loss 1.5606 (1.3919)	grad_norm 29.4543 (29.3678)	mem 4879MB
[2022-05-31 03:52:25 MetaFG_0] (main.py 265): INFO Train: [39/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2976 (0.3055)	loss 1.0597 (1.3920)	grad_norm 43.3848 (29.3930)	mem 4879MB
[2022-05-31 03:52:28 MetaFG_0] (main.py 265): INFO Train: [39/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2923 (0.3055)	loss 1.6265 (1.3922)	grad_norm 24.6913 (29.3957)	mem 4879MB
[2022-05-31 03:52:31 MetaFG_0] (main.py 265): INFO Train: [39/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2944 (0.3055)	loss 1.1250 (1.3913)	grad_norm 25.1263 (29.3697)	mem 4879MB
[2022-05-31 03:52:34 MetaFG_0] (main.py 265): INFO Train: [39/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.3311 (0.3056)	loss 1.2113 (1.3921)	grad_norm 24.7118 (29.4202)	mem 4879MB
[2022-05-31 03:52:37 MetaFG_0] (main.py 265): INFO Train: [39/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2980 (0.3058)	loss 1.5874 (1.3923)	grad_norm 19.9948 (29.3992)	mem 4879MB
[2022-05-31 03:52:40 MetaFG_0] (main.py 265): INFO Train: [39/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2993 (0.3057)	loss 1.6220 (1.3924)	grad_norm 28.0246 (29.4452)	mem 4879MB
[2022-05-31 03:52:43 MetaFG_0] (main.py 265): INFO Train: [39/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2983 (0.3057)	loss 1.2510 (1.3923)	grad_norm 45.0924 (29.4546)	mem 4879MB
[2022-05-31 03:52:46 MetaFG_0] (main.py 265): INFO Train: [39/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2932 (0.3058)	loss 1.5400 (1.3936)	grad_norm 45.0707 (29.4433)	mem 4879MB
[2022-05-31 03:52:49 MetaFG_0] (main.py 265): INFO Train: [39/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2916 (0.3057)	loss 1.6696 (1.3947)	grad_norm 20.9304 (29.4350)	mem 4879MB
[2022-05-31 03:52:52 MetaFG_0] (main.py 265): INFO Train: [39/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2938 (0.3057)	loss 1.4723 (1.3942)	grad_norm 21.1371 (29.3869)	mem 4879MB
[2022-05-31 03:52:55 MetaFG_0] (main.py 265): INFO Train: [39/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2934 (0.3057)	loss 1.1888 (1.3932)	grad_norm 31.0151 (29.4481)	mem 4879MB
[2022-05-31 03:52:58 MetaFG_0] (main.py 265): INFO Train: [39/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2931 (0.3057)	loss 1.3004 (1.3931)	grad_norm 26.5870 (29.4953)	mem 4879MB
[2022-05-31 03:53:01 MetaFG_0] (main.py 265): INFO Train: [39/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2941 (0.3056)	loss 0.9413 (1.3942)	grad_norm 30.0098 (29.5053)	mem 4879MB
[2022-05-31 03:53:04 MetaFG_0] (main.py 265): INFO Train: [39/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2983 (0.3056)	loss 1.5624 (1.3939)	grad_norm 33.8825 (29.5231)	mem 4879MB
[2022-05-31 03:53:07 MetaFG_0] (main.py 265): INFO Train: [39/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.3015 (0.3056)	loss 1.2715 (1.3946)	grad_norm 15.4206 (29.4776)	mem 4879MB
[2022-05-31 03:53:10 MetaFG_0] (main.py 265): INFO Train: [39/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2931 (0.3056)	loss 1.4602 (1.3952)	grad_norm 32.1286 (29.5476)	mem 4879MB
[2022-05-31 03:53:13 MetaFG_0] (main.py 265): INFO Train: [39/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3013 (0.3056)	loss 1.5594 (1.3954)	grad_norm 20.3895 (29.5379)	mem 4879MB
[2022-05-31 03:53:16 MetaFG_0] (main.py 265): INFO Train: [39/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2922 (0.3055)	loss 1.5709 (1.3947)	grad_norm 49.3111 (29.6479)	mem 4879MB
[2022-05-31 03:53:20 MetaFG_0] (main.py 265): INFO Train: [39/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2918 (0.3055)	loss 1.1521 (1.3942)	grad_norm 21.1144 (29.6274)	mem 4879MB
[2022-05-31 03:53:23 MetaFG_0] (main.py 265): INFO Train: [39/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2921 (0.3055)	loss 1.1511 (1.3937)	grad_norm 39.6544 (29.6414)	mem 4879MB
[2022-05-31 03:53:26 MetaFG_0] (main.py 265): INFO Train: [39/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2924 (0.3055)	loss 1.3975 (1.3947)	grad_norm 24.7539 (29.6426)	mem 4879MB
[2022-05-31 03:53:29 MetaFG_0] (main.py 265): INFO Train: [39/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2922 (0.3055)	loss 1.7057 (1.3951)	grad_norm 25.5278 (29.6297)	mem 4879MB
[2022-05-31 03:53:32 MetaFG_0] (main.py 265): INFO Train: [39/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.3038 (0.3055)	loss 1.5304 (1.3951)	grad_norm 21.6983 (29.6216)	mem 4879MB
[2022-05-31 03:53:35 MetaFG_0] (main.py 265): INFO Train: [39/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2933 (0.3055)	loss 1.4072 (1.3956)	grad_norm 32.8827 (29.5774)	mem 4879MB
[2022-05-31 03:53:38 MetaFG_0] (main.py 265): INFO Train: [39/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.3089 (0.3055)	loss 1.5033 (1.3956)	grad_norm 20.0574 (29.5367)	mem 4879MB
[2022-05-31 03:53:41 MetaFG_0] (main.py 265): INFO Train: [39/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2991 (0.3057)	loss 1.4339 (1.3961)	grad_norm 20.1206 (29.5361)	mem 4879MB
[2022-05-31 03:53:44 MetaFG_0] (main.py 265): INFO Train: [39/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2984 (0.3057)	loss 1.5143 (1.3973)	grad_norm 37.1657 (29.4938)	mem 4879MB
[2022-05-31 03:53:47 MetaFG_0] (main.py 265): INFO Train: [39/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2920 (0.3057)	loss 1.4787 (1.3988)	grad_norm 34.9761 (29.4794)	mem 4879MB
[2022-05-31 03:53:50 MetaFG_0] (main.py 265): INFO Train: [39/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2981 (0.3057)	loss 1.6133 (1.3997)	grad_norm 15.7940 (29.4427)	mem 4879MB
[2022-05-31 03:53:53 MetaFG_0] (main.py 265): INFO Train: [39/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2924 (0.3056)	loss 1.1769 (1.3994)	grad_norm 30.3415 (29.4615)	mem 4879MB
[2022-05-31 03:53:56 MetaFG_0] (main.py 265): INFO Train: [39/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2946 (0.3056)	loss 0.9833 (1.3994)	grad_norm 26.1086 (29.4958)	mem 4879MB
[2022-05-31 03:53:59 MetaFG_0] (main.py 265): INFO Train: [39/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2920 (0.3056)	loss 1.5770 (1.3995)	grad_norm 17.0402 (29.5176)	mem 4879MB
[2022-05-31 03:54:02 MetaFG_0] (main.py 265): INFO Train: [39/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2975 (0.3056)	loss 1.0435 (1.3988)	grad_norm 23.4183 (29.4856)	mem 4879MB
[2022-05-31 03:54:05 MetaFG_0] (main.py 265): INFO Train: [39/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2964 (0.3056)	loss 1.5945 (1.3994)	grad_norm 26.1690 (29.5637)	mem 4879MB
[2022-05-31 03:54:08 MetaFG_0] (main.py 265): INFO Train: [39/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2951 (0.3056)	loss 1.6208 (1.3999)	grad_norm 28.0458 (29.5914)	mem 4879MB
[2022-05-31 03:54:12 MetaFG_0] (main.py 265): INFO Train: [39/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2946 (0.3056)	loss 1.4158 (1.3999)	grad_norm 23.7423 (29.5982)	mem 4879MB
[2022-05-31 03:54:15 MetaFG_0] (main.py 265): INFO Train: [39/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2930 (0.3056)	loss 1.5196 (1.4000)	grad_norm 25.4320 (29.5429)	mem 4879MB
[2022-05-31 03:54:18 MetaFG_0] (main.py 265): INFO Train: [39/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2945 (0.3056)	loss 1.5743 (1.3998)	grad_norm 23.9342 (29.5049)	mem 4879MB
[2022-05-31 03:54:21 MetaFG_0] (main.py 265): INFO Train: [39/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2922 (0.3055)	loss 1.4790 (1.4002)	grad_norm 29.6866 (29.5086)	mem 4879MB
[2022-05-31 03:54:24 MetaFG_0] (main.py 265): INFO Train: [39/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2940 (0.3055)	loss 1.2369 (1.4005)	grad_norm 19.6266 (29.4646)	mem 4879MB
[2022-05-31 03:54:27 MetaFG_0] (main.py 265): INFO Train: [39/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2988 (0.3056)	loss 1.4900 (1.4006)	grad_norm 24.5378 (29.4660)	mem 4879MB
[2022-05-31 03:54:30 MetaFG_0] (main.py 265): INFO Train: [39/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2982 (0.3056)	loss 1.2450 (1.4007)	grad_norm 16.3933 (29.4589)	mem 4879MB
[2022-05-31 03:54:33 MetaFG_0] (main.py 265): INFO Train: [39/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2915 (0.3055)	loss 1.5686 (1.4007)	grad_norm 29.7626 (29.4751)	mem 4879MB
[2022-05-31 03:54:36 MetaFG_0] (main.py 265): INFO Train: [39/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2981 (0.3055)	loss 1.2376 (1.4001)	grad_norm 25.8808 (29.4585)	mem 4879MB
[2022-05-31 03:54:39 MetaFG_0] (main.py 265): INFO Train: [39/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2931 (0.3055)	loss 1.4217 (1.3993)	grad_norm 25.5617 (29.4398)	mem 4879MB
[2022-05-31 03:54:42 MetaFG_0] (main.py 265): INFO Train: [39/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2935 (0.3055)	loss 1.3250 (1.3989)	grad_norm 24.1399 (29.3840)	mem 4879MB
[2022-05-31 03:54:45 MetaFG_0] (main.py 265): INFO Train: [39/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2935 (0.3055)	loss 1.0041 (1.3987)	grad_norm 26.0111 (29.3977)	mem 4879MB
[2022-05-31 03:54:48 MetaFG_0] (main.py 265): INFO Train: [39/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2944 (0.3055)	loss 0.8665 (1.3982)	grad_norm 15.6176 (29.4581)	mem 4879MB
[2022-05-31 03:54:51 MetaFG_0] (main.py 265): INFO Train: [39/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2977 (0.3055)	loss 1.4748 (1.3979)	grad_norm 36.2576 (29.4680)	mem 4879MB
[2022-05-31 03:54:54 MetaFG_0] (main.py 265): INFO Train: [39/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2979 (0.3054)	loss 1.6469 (1.3977)	grad_norm 17.3839 (29.4974)	mem 4879MB
[2022-05-31 03:54:57 MetaFG_0] (main.py 265): INFO Train: [39/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2980 (0.3054)	loss 1.4581 (1.3980)	grad_norm 45.2203 (29.5004)	mem 4879MB
[2022-05-31 03:55:00 MetaFG_0] (main.py 265): INFO Train: [39/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2926 (0.3054)	loss 1.4984 (1.3982)	grad_norm 51.7700 (29.5002)	mem 4879MB
[2022-05-31 03:55:03 MetaFG_0] (main.py 265): INFO Train: [39/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.3411 (0.3056)	loss 1.5709 (1.3987)	grad_norm 28.5404 (29.4859)	mem 4879MB
[2022-05-31 03:55:07 MetaFG_0] (main.py 265): INFO Train: [39/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2938 (0.3056)	loss 1.1583 (1.3984)	grad_norm 27.1784 (29.4761)	mem 4879MB
[2022-05-31 03:55:10 MetaFG_0] (main.py 265): INFO Train: [39/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2975 (0.3056)	loss 1.5857 (1.3980)	grad_norm 20.1428 (29.5800)	mem 4879MB
[2022-05-31 03:55:13 MetaFG_0] (main.py 265): INFO Train: [39/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2929 (0.3056)	loss 1.3664 (1.3980)	grad_norm 28.1797 (29.6506)	mem 4879MB
[2022-05-31 03:55:16 MetaFG_0] (main.py 265): INFO Train: [39/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2933 (0.3055)	loss 1.3306 (1.3980)	grad_norm 36.8265 (nan)	mem 4879MB
[2022-05-31 03:55:19 MetaFG_0] (main.py 265): INFO Train: [39/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2929 (0.3055)	loss 1.0118 (1.3979)	grad_norm 31.7948 (nan)	mem 4879MB
[2022-05-31 03:55:22 MetaFG_0] (main.py 265): INFO Train: [39/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2922 (0.3055)	loss 1.5856 (1.3981)	grad_norm 31.5894 (nan)	mem 4879MB
[2022-05-31 03:55:25 MetaFG_0] (main.py 265): INFO Train: [39/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2918 (0.3055)	loss 1.2383 (1.3976)	grad_norm 15.7951 (nan)	mem 4879MB
[2022-05-31 03:55:28 MetaFG_0] (main.py 265): INFO Train: [39/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2919 (0.3055)	loss 1.2662 (1.3974)	grad_norm 28.6047 (nan)	mem 4879MB
[2022-05-31 03:55:31 MetaFG_0] (main.py 265): INFO Train: [39/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2925 (0.3055)	loss 1.4933 (1.3964)	grad_norm 54.5058 (nan)	mem 4879MB
[2022-05-31 03:55:34 MetaFG_0] (main.py 265): INFO Train: [39/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2925 (0.3055)	loss 1.4267 (1.3962)	grad_norm 23.1421 (nan)	mem 4879MB
[2022-05-31 03:55:37 MetaFG_0] (main.py 265): INFO Train: [39/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2932 (0.3055)	loss 1.1224 (1.3968)	grad_norm 32.9414 (nan)	mem 4879MB
[2022-05-31 03:55:40 MetaFG_0] (main.py 265): INFO Train: [39/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2920 (0.3055)	loss 1.8778 (1.3974)	grad_norm 43.9974 (nan)	mem 4879MB
[2022-05-31 03:55:43 MetaFG_0] (main.py 265): INFO Train: [39/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2923 (0.3054)	loss 1.4134 (1.3976)	grad_norm 15.1647 (nan)	mem 4879MB
[2022-05-31 03:55:46 MetaFG_0] (main.py 265): INFO Train: [39/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2915 (0.3054)	loss 1.5785 (1.3972)	grad_norm 36.1136 (nan)	mem 4879MB
[2022-05-31 03:55:49 MetaFG_0] (main.py 265): INFO Train: [39/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2944 (0.3054)	loss 1.5849 (1.3976)	grad_norm 58.1585 (nan)	mem 4879MB
[2022-05-31 03:55:52 MetaFG_0] (main.py 265): INFO Train: [39/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2921 (0.3054)	loss 1.4667 (1.3977)	grad_norm 33.4572 (nan)	mem 4879MB
[2022-05-31 03:55:55 MetaFG_0] (main.py 265): INFO Train: [39/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2993 (0.3054)	loss 1.7556 (1.3981)	grad_norm 19.8563 (nan)	mem 4879MB
[2022-05-31 03:55:58 MetaFG_0] (main.py 265): INFO Train: [39/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2923 (0.3054)	loss 1.4885 (1.3988)	grad_norm 35.7574 (nan)	mem 4879MB
[2022-05-31 03:56:01 MetaFG_0] (main.py 265): INFO Train: [39/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2935 (0.3054)	loss 1.4672 (1.3977)	grad_norm 25.5606 (nan)	mem 4879MB
[2022-05-31 03:56:04 MetaFG_0] (main.py 265): INFO Train: [39/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2916 (0.3053)	loss 1.3437 (1.3979)	grad_norm 24.9330 (nan)	mem 4879MB
[2022-05-31 03:56:07 MetaFG_0] (main.py 265): INFO Train: [39/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2923 (0.3053)	loss 1.3353 (1.3983)	grad_norm 20.3537 (nan)	mem 4879MB
[2022-05-31 03:56:10 MetaFG_0] (main.py 265): INFO Train: [39/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2926 (0.3053)	loss 1.4780 (1.3983)	grad_norm 38.6538 (nan)	mem 4879MB
[2022-05-31 03:56:13 MetaFG_0] (main.py 265): INFO Train: [39/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2995 (0.3053)	loss 0.9187 (1.3976)	grad_norm 36.3851 (nan)	mem 4879MB
[2022-05-31 03:56:16 MetaFG_0] (main.py 265): INFO Train: [39/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2918 (0.3053)	loss 1.5416 (1.3978)	grad_norm 35.2477 (nan)	mem 4879MB
[2022-05-31 03:56:19 MetaFG_0] (main.py 265): INFO Train: [39/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3006 (0.3053)	loss 1.3081 (1.3978)	grad_norm 22.1765 (nan)	mem 4879MB
[2022-05-31 03:56:23 MetaFG_0] (main.py 265): INFO Train: [39/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2945 (0.3053)	loss 1.5450 (1.3982)	grad_norm 31.9601 (nan)	mem 4879MB
[2022-05-31 03:56:26 MetaFG_0] (main.py 265): INFO Train: [39/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2958 (0.3053)	loss 1.5558 (1.3992)	grad_norm 34.0278 (nan)	mem 4879MB
[2022-05-31 03:56:29 MetaFG_0] (main.py 265): INFO Train: [39/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2982 (0.3055)	loss 1.3603 (1.3988)	grad_norm 37.2571 (nan)	mem 4879MB
[2022-05-31 03:56:32 MetaFG_0] (main.py 265): INFO Train: [39/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2915 (0.3055)	loss 1.2820 (1.3989)	grad_norm 14.7392 (nan)	mem 4879MB
[2022-05-31 03:56:32 MetaFG_0] (main.py 272): INFO EPOCH 39 training takes 0:07:57
[2022-05-31 03:56:32 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_39.pth saving......
[2022-05-31 03:56:33 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_39.pth saved !!!
[2022-05-31 03:56:33 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 03:56:35 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 03:56:35 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 03:56:35 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.688 (0.688)	Loss 0.5541 (0.5541)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 03:56:36 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.103 (0.152)	Loss 0.5596 (0.6527)	Acc@1 84.375 (83.807)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 03:56:37 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.127)	Loss 0.5067 (0.6574)	Acc@1 90.625 (84.524)	Acc@5 100.000 (98.363)	Mem 4879MB
[2022-05-31 03:56:38 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.098 (0.118)	Loss 0.7987 (0.6601)	Acc@1 84.375 (84.778)	Acc@5 96.875 (98.589)	Mem 4879MB
[2022-05-31 03:56:39 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.112)	Loss 0.6421 (0.6709)	Acc@1 84.375 (84.146)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 03:56:40 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.109)	Loss 0.6386 (0.6704)	Acc@1 81.250 (84.069)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 03:56:41 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.091 (0.107)	Loss 0.6106 (0.6634)	Acc@1 84.375 (84.529)	Acc@5 100.000 (98.566)	Mem 4879MB
[2022-05-31 03:56:42 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.104)	Loss 0.8424 (0.6608)	Acc@1 81.250 (84.683)	Acc@5 96.875 (98.592)	Mem 4879MB
[2022-05-31 03:56:43 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.103)	Loss 0.5468 (0.6664)	Acc@1 93.750 (84.645)	Acc@5 96.875 (98.418)	Mem 4879MB
[2022-05-31 03:56:44 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.103 (0.102)	Loss 0.7947 (0.6726)	Acc@1 81.250 (84.684)	Acc@5 93.750 (98.352)	Mem 4879MB
[2022-05-31 03:56:45 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.101)	Loss 0.5606 (0.6782)	Acc@1 90.625 (84.561)	Acc@5 100.000 (98.360)	Mem 4879MB
[2022-05-31 03:56:46 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.7039 (0.6773)	Acc@1 84.375 (84.628)	Acc@5 96.875 (98.367)	Mem 4879MB
[2022-05-31 03:56:47 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.100)	Loss 0.6276 (0.6785)	Acc@1 90.625 (84.504)	Acc@5 100.000 (98.373)	Mem 4879MB
[2022-05-31 03:56:48 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.100)	Loss 0.7453 (0.6763)	Acc@1 87.500 (84.637)	Acc@5 100.000 (98.354)	Mem 4879MB
[2022-05-31 03:56:49 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 0.6452 (0.6774)	Acc@1 90.625 (84.619)	Acc@5 96.875 (98.338)	Mem 4879MB
[2022-05-31 03:56:50 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.092 (0.099)	Loss 0.5357 (0.6760)	Acc@1 87.500 (84.541)	Acc@5 100.000 (98.365)	Mem 4879MB
[2022-05-31 03:56:51 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.095 (0.099)	Loss 0.7107 (0.6736)	Acc@1 87.500 (84.647)	Acc@5 100.000 (98.408)	Mem 4879MB
[2022-05-31 03:56:52 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.099)	Loss 0.5758 (0.6761)	Acc@1 90.625 (84.576)	Acc@5 100.000 (98.410)	Mem 4879MB
[2022-05-31 03:56:52 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.095 (0.099)	Loss 0.5342 (0.6743)	Acc@1 90.625 (84.599)	Acc@5 100.000 (98.429)	Mem 4879MB
[2022-05-31 03:56:53 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.098)	Loss 0.6417 (0.6732)	Acc@1 84.375 (84.588)	Acc@5 100.000 (98.462)	Mem 4879MB
[2022-05-31 03:56:54 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.093 (0.098)	Loss 0.5781 (0.6723)	Acc@1 87.500 (84.546)	Acc@5 100.000 (98.492)	Mem 4879MB
[2022-05-31 03:56:55 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.098)	Loss 0.7003 (0.6711)	Acc@1 81.250 (84.597)	Acc@5 100.000 (98.519)	Mem 4879MB
[2022-05-31 03:56:56 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.106 (0.098)	Loss 0.5944 (0.6702)	Acc@1 87.500 (84.658)	Acc@5 96.875 (98.515)	Mem 4879MB
[2022-05-31 03:56:57 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.098)	Loss 0.8601 (0.6727)	Acc@1 81.250 (84.591)	Acc@5 93.750 (98.458)	Mem 4879MB
[2022-05-31 03:56:58 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.098)	Loss 0.8534 (0.6745)	Acc@1 71.875 (84.479)	Acc@5 96.875 (98.457)	Mem 4879MB
[2022-05-31 03:56:59 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.5504 (0.6727)	Acc@1 87.500 (84.500)	Acc@5 100.000 (98.456)	Mem 4879MB
[2022-05-31 03:57:00 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 0.5037 (0.6661)	Acc@1 90.625 (84.722)	Acc@5 100.000 (98.491)	Mem 4879MB
[2022-05-31 03:57:01 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.090 (0.097)	Loss 0.8903 (0.6635)	Acc@1 78.125 (84.848)	Acc@5 96.875 (98.478)	Mem 4879MB
[2022-05-31 03:57:02 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.097)	Loss 0.5857 (0.6619)	Acc@1 90.625 (84.887)	Acc@5 96.875 (98.476)	Mem 4879MB
[2022-05-31 03:57:03 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 0.7551 (0.6609)	Acc@1 81.250 (84.890)	Acc@5 100.000 (98.497)	Mem 4879MB
[2022-05-31 03:57:04 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.091 (0.097)	Loss 0.5808 (0.6618)	Acc@1 84.375 (84.863)	Acc@5 96.875 (98.432)	Mem 4879MB
[2022-05-31 03:57:05 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.7323 (0.6646)	Acc@1 75.000 (84.717)	Acc@5 100.000 (98.412)	Mem 4879MB
[2022-05-31 03:57:05 MetaFG_0] (main.py 330): INFO  * Acc@1 84.680 Acc@5 98.380
[2022-05-31 03:57:05 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 84.7%
[2022-05-31 03:57:05 MetaFG_0] (main.py 171): INFO Max accuracy: 84.68%
[2022-05-31 03:57:06 MetaFG_0] (main.py 265): INFO Train: [40/300][0/1562]	eta 0:28:30 lr 0.000006	time 1.0954 (1.0954)	loss 1.3671 (1.3671)	grad_norm 25.7069 (25.7069)	mem 4879MB
[2022-05-31 03:57:09 MetaFG_0] (main.py 265): INFO Train: [40/300][10/1562]	eta 0:09:46 lr 0.000006	time 0.2979 (0.3782)	loss 1.0063 (1.3331)	grad_norm 27.5939 (24.8076)	mem 4879MB
[2022-05-31 03:57:12 MetaFG_0] (main.py 265): INFO Train: [40/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.2989 (0.3428)	loss 1.2266 (1.3563)	grad_norm 24.0882 (25.4341)	mem 4879MB
[2022-05-31 03:57:15 MetaFG_0] (main.py 265): INFO Train: [40/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.2946 (0.3305)	loss 1.0584 (1.3752)	grad_norm 43.8495 (24.8994)	mem 4879MB
[2022-05-31 03:57:18 MetaFG_0] (main.py 265): INFO Train: [40/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2929 (0.3238)	loss 1.2403 (1.3424)	grad_norm 23.7458 (27.5819)	mem 4879MB
[2022-05-31 03:57:21 MetaFG_0] (main.py 265): INFO Train: [40/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2916 (0.3195)	loss 1.3837 (1.3567)	grad_norm 26.8466 (28.8608)	mem 4879MB
[2022-05-31 03:57:24 MetaFG_0] (main.py 265): INFO Train: [40/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2982 (0.3169)	loss 1.5174 (1.3812)	grad_norm 16.8653 (27.8945)	mem 4879MB
[2022-05-31 03:57:27 MetaFG_0] (main.py 265): INFO Train: [40/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2937 (0.3152)	loss 1.0809 (1.3854)	grad_norm 28.0368 (27.6008)	mem 4879MB
[2022-05-31 03:57:30 MetaFG_0] (main.py 265): INFO Train: [40/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2926 (0.3138)	loss 1.3279 (1.3805)	grad_norm 32.7827 (27.9826)	mem 4879MB
[2022-05-31 03:57:33 MetaFG_0] (main.py 265): INFO Train: [40/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2932 (0.3127)	loss 0.9463 (1.3787)	grad_norm 23.0318 (28.3369)	mem 4879MB
[2022-05-31 03:57:36 MetaFG_0] (main.py 265): INFO Train: [40/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2943 (0.3117)	loss 1.1659 (1.3883)	grad_norm 32.9824 (28.4747)	mem 4879MB
[2022-05-31 03:57:40 MetaFG_0] (main.py 265): INFO Train: [40/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2978 (0.3110)	loss 1.1634 (1.3852)	grad_norm 40.3778 (28.5809)	mem 4879MB
[2022-05-31 03:57:43 MetaFG_0] (main.py 265): INFO Train: [40/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2923 (0.3105)	loss 1.6105 (1.3877)	grad_norm 31.6302 (29.1797)	mem 4879MB
[2022-05-31 03:57:46 MetaFG_0] (main.py 265): INFO Train: [40/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2920 (0.3098)	loss 1.3946 (1.3945)	grad_norm 28.5936 (28.9852)	mem 4879MB
[2022-05-31 03:57:49 MetaFG_0] (main.py 265): INFO Train: [40/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2914 (0.3094)	loss 0.9279 (1.3891)	grad_norm 39.7875 (29.3281)	mem 4879MB
[2022-05-31 03:57:52 MetaFG_0] (main.py 265): INFO Train: [40/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2981 (0.3092)	loss 1.4222 (1.3866)	grad_norm 30.9620 (29.8690)	mem 4879MB
[2022-05-31 03:57:55 MetaFG_0] (main.py 265): INFO Train: [40/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3045 (0.3090)	loss 1.4886 (1.3923)	grad_norm 25.1995 (29.7790)	mem 4879MB
[2022-05-31 03:57:58 MetaFG_0] (main.py 265): INFO Train: [40/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2978 (0.3087)	loss 1.4660 (1.3867)	grad_norm 17.8251 (30.1062)	mem 4879MB
[2022-05-31 03:58:01 MetaFG_0] (main.py 265): INFO Train: [40/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2978 (0.3085)	loss 1.7177 (1.3897)	grad_norm 21.4159 (29.9992)	mem 4879MB
[2022-05-31 03:58:04 MetaFG_0] (main.py 265): INFO Train: [40/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2994 (0.3097)	loss 1.3143 (1.3966)	grad_norm 12.8861 (29.8174)	mem 4879MB
[2022-05-31 03:58:07 MetaFG_0] (main.py 265): INFO Train: [40/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2929 (0.3095)	loss 1.5112 (1.3938)	grad_norm 23.6735 (29.8329)	mem 4879MB
[2022-05-31 03:58:10 MetaFG_0] (main.py 265): INFO Train: [40/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2978 (0.3092)	loss 1.7421 (1.3994)	grad_norm 34.3344 (29.7724)	mem 4879MB
[2022-05-31 03:58:13 MetaFG_0] (main.py 265): INFO Train: [40/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2933 (0.3090)	loss 1.2604 (1.3999)	grad_norm 20.7544 (29.7808)	mem 4879MB
[2022-05-31 03:58:16 MetaFG_0] (main.py 265): INFO Train: [40/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2979 (0.3088)	loss 1.7057 (1.4017)	grad_norm 36.6477 (29.9195)	mem 4879MB
[2022-05-31 03:58:19 MetaFG_0] (main.py 265): INFO Train: [40/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2945 (0.3085)	loss 1.7019 (1.4053)	grad_norm 22.9757 (30.0808)	mem 4879MB
[2022-05-31 03:58:22 MetaFG_0] (main.py 265): INFO Train: [40/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2932 (0.3084)	loss 1.2359 (1.4042)	grad_norm 30.3455 (30.0572)	mem 4879MB
[2022-05-31 03:58:25 MetaFG_0] (main.py 265): INFO Train: [40/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2996 (0.3082)	loss 1.5504 (1.4038)	grad_norm 20.6336 (30.1593)	mem 4879MB
[2022-05-31 03:58:28 MetaFG_0] (main.py 265): INFO Train: [40/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2986 (0.3081)	loss 1.4500 (1.4000)	grad_norm 52.3207 (30.3149)	mem 4879MB
[2022-05-31 03:58:32 MetaFG_0] (main.py 265): INFO Train: [40/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2982 (0.3080)	loss 1.6936 (1.3991)	grad_norm 30.7968 (30.4569)	mem 4879MB
[2022-05-31 03:58:35 MetaFG_0] (main.py 265): INFO Train: [40/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2990 (0.3078)	loss 1.1186 (1.3995)	grad_norm 33.6513 (30.4965)	mem 4879MB
[2022-05-31 03:58:38 MetaFG_0] (main.py 265): INFO Train: [40/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2982 (0.3077)	loss 1.7077 (1.4022)	grad_norm 24.4762 (30.6698)	mem 4879MB
[2022-05-31 03:58:41 MetaFG_0] (main.py 265): INFO Train: [40/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2991 (0.3075)	loss 1.4625 (1.4042)	grad_norm 15.2806 (30.5733)	mem 4879MB
[2022-05-31 03:58:44 MetaFG_0] (main.py 265): INFO Train: [40/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2934 (0.3075)	loss 1.3839 (1.4035)	grad_norm 22.7996 (30.6491)	mem 4879MB
[2022-05-31 03:58:47 MetaFG_0] (main.py 265): INFO Train: [40/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2919 (0.3073)	loss 1.6626 (1.4042)	grad_norm 48.0150 (30.7180)	mem 4879MB
[2022-05-31 03:58:50 MetaFG_0] (main.py 265): INFO Train: [40/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2999 (0.3072)	loss 1.5757 (1.4050)	grad_norm 37.5211 (30.7443)	mem 4879MB
[2022-05-31 03:58:53 MetaFG_0] (main.py 265): INFO Train: [40/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2982 (0.3071)	loss 1.7589 (1.4059)	grad_norm 28.8107 (30.9025)	mem 4879MB
[2022-05-31 03:58:56 MetaFG_0] (main.py 265): INFO Train: [40/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2947 (0.3070)	loss 1.2723 (1.4042)	grad_norm 35.6559 (30.9186)	mem 4879MB
[2022-05-31 03:58:59 MetaFG_0] (main.py 265): INFO Train: [40/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2949 (0.3069)	loss 1.1111 (1.4011)	grad_norm 30.6462 (30.8893)	mem 4879MB
[2022-05-31 03:59:02 MetaFG_0] (main.py 265): INFO Train: [40/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2922 (0.3068)	loss 1.0261 (1.4036)	grad_norm 22.1934 (30.9510)	mem 4879MB
[2022-05-31 03:59:05 MetaFG_0] (main.py 265): INFO Train: [40/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2935 (0.3067)	loss 1.5270 (1.4061)	grad_norm 20.6667 (30.7399)	mem 4879MB
[2022-05-31 03:59:08 MetaFG_0] (main.py 265): INFO Train: [40/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2933 (0.3066)	loss 1.1916 (1.4080)	grad_norm 38.0930 (30.7031)	mem 4879MB
[2022-05-31 03:59:11 MetaFG_0] (main.py 265): INFO Train: [40/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.3053 (0.3066)	loss 1.6314 (1.4064)	grad_norm 25.4889 (30.6015)	mem 4879MB
[2022-05-31 03:59:14 MetaFG_0] (main.py 265): INFO Train: [40/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2921 (0.3065)	loss 1.2049 (1.4091)	grad_norm 27.3308 (30.4632)	mem 4879MB
[2022-05-31 03:59:17 MetaFG_0] (main.py 265): INFO Train: [40/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2987 (0.3064)	loss 0.9957 (1.4101)	grad_norm 103.2218 (30.5781)	mem 4879MB
[2022-05-31 03:59:20 MetaFG_0] (main.py 265): INFO Train: [40/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2969 (0.3063)	loss 1.2722 (1.4082)	grad_norm 22.3540 (30.5957)	mem 4879MB
[2022-05-31 03:59:23 MetaFG_0] (main.py 265): INFO Train: [40/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2917 (0.3062)	loss 1.2423 (1.4087)	grad_norm 27.8580 (30.5398)	mem 4879MB
[2022-05-31 03:59:26 MetaFG_0] (main.py 265): INFO Train: [40/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2988 (0.3062)	loss 1.5643 (1.4081)	grad_norm 28.6818 (30.5375)	mem 4879MB
[2022-05-31 03:59:29 MetaFG_0] (main.py 265): INFO Train: [40/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2983 (0.3062)	loss 1.3334 (1.4092)	grad_norm 20.4240 (30.4331)	mem 4879MB
[2022-05-31 03:59:32 MetaFG_0] (main.py 265): INFO Train: [40/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.3108 (0.3062)	loss 1.1554 (1.4086)	grad_norm 31.8171 (30.4628)	mem 4879MB
[2022-05-31 03:59:36 MetaFG_0] (main.py 265): INFO Train: [40/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2978 (0.3067)	loss 1.5134 (1.4100)	grad_norm 71.7519 (30.5196)	mem 4879MB
[2022-05-31 03:59:39 MetaFG_0] (main.py 265): INFO Train: [40/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2980 (0.3066)	loss 1.4531 (1.4121)	grad_norm 50.9352 (30.5380)	mem 4879MB
[2022-05-31 03:59:42 MetaFG_0] (main.py 265): INFO Train: [40/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.3010 (0.3066)	loss 1.5143 (1.4118)	grad_norm 35.4906 (30.5320)	mem 4879MB
[2022-05-31 03:59:45 MetaFG_0] (main.py 265): INFO Train: [40/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2975 (0.3066)	loss 1.2170 (1.4112)	grad_norm 27.7528 (30.4744)	mem 4879MB
[2022-05-31 03:59:48 MetaFG_0] (main.py 265): INFO Train: [40/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2921 (0.3065)	loss 1.7220 (1.4117)	grad_norm 31.8789 (30.4991)	mem 4879MB
[2022-05-31 03:59:51 MetaFG_0] (main.py 265): INFO Train: [40/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2934 (0.3065)	loss 1.5655 (1.4117)	grad_norm 26.2593 (30.4837)	mem 4879MB
[2022-05-31 03:59:54 MetaFG_0] (main.py 265): INFO Train: [40/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2923 (0.3064)	loss 1.3497 (1.4110)	grad_norm 21.9764 (30.4465)	mem 4879MB
[2022-05-31 03:59:57 MetaFG_0] (main.py 265): INFO Train: [40/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2938 (0.3064)	loss 1.4082 (1.4091)	grad_norm 22.9812 (30.5293)	mem 4879MB
[2022-05-31 04:00:00 MetaFG_0] (main.py 265): INFO Train: [40/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2917 (0.3063)	loss 1.4921 (1.4099)	grad_norm 32.0724 (30.4568)	mem 4879MB
[2022-05-31 04:00:03 MetaFG_0] (main.py 265): INFO Train: [40/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2928 (0.3063)	loss 1.6211 (1.4084)	grad_norm 39.1055 (30.4332)	mem 4879MB
[2022-05-31 04:00:06 MetaFG_0] (main.py 265): INFO Train: [40/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2922 (0.3063)	loss 1.5086 (1.4076)	grad_norm 15.2054 (30.3154)	mem 4879MB
[2022-05-31 04:00:09 MetaFG_0] (main.py 265): INFO Train: [40/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2938 (0.3063)	loss 1.4259 (1.4085)	grad_norm 33.0336 (30.3595)	mem 4879MB
[2022-05-31 04:00:12 MetaFG_0] (main.py 265): INFO Train: [40/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2992 (0.3062)	loss 1.5474 (1.4088)	grad_norm 23.1387 (30.2641)	mem 4879MB
[2022-05-31 04:00:15 MetaFG_0] (main.py 265): INFO Train: [40/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2964 (0.3062)	loss 1.4716 (1.4103)	grad_norm 27.7722 (30.2608)	mem 4879MB
[2022-05-31 04:00:18 MetaFG_0] (main.py 265): INFO Train: [40/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2933 (0.3061)	loss 0.8901 (1.4099)	grad_norm 31.0014 (30.2347)	mem 4879MB
[2022-05-31 04:00:21 MetaFG_0] (main.py 265): INFO Train: [40/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2959 (0.3061)	loss 1.0637 (1.4111)	grad_norm 73.4419 (30.3504)	mem 4879MB
[2022-05-31 04:00:24 MetaFG_0] (main.py 265): INFO Train: [40/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2922 (0.3060)	loss 1.4040 (1.4108)	grad_norm 32.8980 (30.3097)	mem 4879MB
[2022-05-31 04:00:27 MetaFG_0] (main.py 265): INFO Train: [40/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2934 (0.3060)	loss 1.2703 (1.4103)	grad_norm 18.5817 (30.3247)	mem 4879MB
[2022-05-31 04:00:30 MetaFG_0] (main.py 265): INFO Train: [40/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2943 (0.3060)	loss 1.4543 (1.4092)	grad_norm 26.7029 (30.3453)	mem 4879MB
[2022-05-31 04:00:33 MetaFG_0] (main.py 265): INFO Train: [40/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2935 (0.3060)	loss 1.6220 (1.4078)	grad_norm 42.7924 (30.3167)	mem 4879MB
[2022-05-31 04:00:36 MetaFG_0] (main.py 265): INFO Train: [40/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2975 (0.3059)	loss 1.4807 (1.4077)	grad_norm 37.0935 (30.3353)	mem 4879MB
[2022-05-31 04:00:39 MetaFG_0] (main.py 265): INFO Train: [40/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2940 (0.3059)	loss 1.3733 (1.4076)	grad_norm 33.7372 (30.3469)	mem 4879MB
[2022-05-31 04:00:42 MetaFG_0] (main.py 265): INFO Train: [40/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2929 (0.3058)	loss 1.1002 (1.4078)	grad_norm 25.1810 (30.3914)	mem 4879MB
[2022-05-31 04:00:46 MetaFG_0] (main.py 265): INFO Train: [40/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2984 (0.3058)	loss 1.1650 (1.4064)	grad_norm 24.0027 (30.3295)	mem 4879MB
[2022-05-31 04:00:49 MetaFG_0] (main.py 265): INFO Train: [40/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2985 (0.3058)	loss 1.6390 (1.4077)	grad_norm 28.2778 (30.2848)	mem 4879MB
[2022-05-31 04:00:52 MetaFG_0] (main.py 265): INFO Train: [40/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2921 (0.3058)	loss 1.5364 (1.4074)	grad_norm 24.5904 (30.2438)	mem 4879MB
[2022-05-31 04:00:55 MetaFG_0] (main.py 265): INFO Train: [40/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2924 (0.3058)	loss 1.1704 (1.4068)	grad_norm 68.0497 (30.2383)	mem 4879MB
[2022-05-31 04:00:58 MetaFG_0] (main.py 265): INFO Train: [40/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2972 (0.3058)	loss 1.1323 (1.4072)	grad_norm 35.4284 (30.1910)	mem 4879MB
[2022-05-31 04:01:01 MetaFG_0] (main.py 265): INFO Train: [40/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2936 (0.3057)	loss 1.2311 (1.4066)	grad_norm 25.8178 (30.1377)	mem 4879MB
[2022-05-31 04:01:04 MetaFG_0] (main.py 265): INFO Train: [40/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2925 (0.3057)	loss 1.5320 (1.4073)	grad_norm 21.8415 (30.1693)	mem 4879MB
[2022-05-31 04:01:07 MetaFG_0] (main.py 265): INFO Train: [40/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2923 (0.3057)	loss 1.5436 (1.4080)	grad_norm 29.2465 (30.1612)	mem 4879MB
[2022-05-31 04:01:10 MetaFG_0] (main.py 265): INFO Train: [40/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2927 (0.3056)	loss 1.3436 (1.4069)	grad_norm 20.8815 (30.1323)	mem 4879MB
[2022-05-31 04:01:13 MetaFG_0] (main.py 265): INFO Train: [40/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2925 (0.3056)	loss 1.5262 (1.4074)	grad_norm 37.1159 (30.1386)	mem 4879MB
[2022-05-31 04:01:16 MetaFG_0] (main.py 265): INFO Train: [40/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2935 (0.3056)	loss 0.8675 (1.4070)	grad_norm 38.4099 (30.1111)	mem 4879MB
[2022-05-31 04:01:19 MetaFG_0] (main.py 265): INFO Train: [40/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2976 (0.3056)	loss 1.2044 (1.4080)	grad_norm 18.6344 (30.0832)	mem 4879MB
[2022-05-31 04:01:22 MetaFG_0] (main.py 265): INFO Train: [40/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2927 (0.3056)	loss 1.5108 (1.4077)	grad_norm 42.2076 (30.0483)	mem 4879MB
[2022-05-31 04:01:25 MetaFG_0] (main.py 265): INFO Train: [40/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2916 (0.3056)	loss 1.5751 (1.4087)	grad_norm 69.2290 (30.1774)	mem 4879MB
[2022-05-31 04:01:28 MetaFG_0] (main.py 265): INFO Train: [40/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2970 (0.3056)	loss 1.4639 (1.4101)	grad_norm 21.9872 (30.1306)	mem 4879MB
[2022-05-31 04:01:31 MetaFG_0] (main.py 265): INFO Train: [40/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2976 (0.3055)	loss 1.0825 (1.4098)	grad_norm 35.1442 (30.1719)	mem 4879MB
[2022-05-31 04:01:34 MetaFG_0] (main.py 265): INFO Train: [40/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2979 (0.3055)	loss 1.4908 (1.4098)	grad_norm 43.3381 (30.1755)	mem 4879MB
[2022-05-31 04:01:37 MetaFG_0] (main.py 265): INFO Train: [40/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2973 (0.3055)	loss 1.4818 (1.4105)	grad_norm 39.4322 (30.2098)	mem 4879MB
[2022-05-31 04:01:40 MetaFG_0] (main.py 265): INFO Train: [40/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2929 (0.3055)	loss 1.1825 (1.4104)	grad_norm 32.3609 (30.1929)	mem 4879MB
[2022-05-31 04:01:43 MetaFG_0] (main.py 265): INFO Train: [40/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3029 (0.3055)	loss 1.3245 (1.4105)	grad_norm 40.7024 (30.2095)	mem 4879MB
[2022-05-31 04:01:46 MetaFG_0] (main.py 265): INFO Train: [40/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2955 (0.3055)	loss 1.3629 (1.4104)	grad_norm 18.4951 (30.1597)	mem 4879MB
[2022-05-31 04:01:49 MetaFG_0] (main.py 265): INFO Train: [40/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2959 (0.3055)	loss 1.0189 (1.4092)	grad_norm 24.4928 (30.1902)	mem 4879MB
[2022-05-31 04:01:52 MetaFG_0] (main.py 265): INFO Train: [40/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2978 (0.3055)	loss 1.3907 (1.4093)	grad_norm 20.8565 (30.2032)	mem 4879MB
[2022-05-31 04:01:55 MetaFG_0] (main.py 265): INFO Train: [40/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.3043 (0.3054)	loss 1.5243 (1.4102)	grad_norm 27.6453 (30.1783)	mem 4879MB
[2022-05-31 04:01:59 MetaFG_0] (main.py 265): INFO Train: [40/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.3009 (0.3054)	loss 1.6189 (1.4111)	grad_norm 46.5442 (30.1728)	mem 4879MB
[2022-05-31 04:02:02 MetaFG_0] (main.py 265): INFO Train: [40/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2928 (0.3054)	loss 1.6646 (1.4119)	grad_norm 16.4641 (30.1637)	mem 4879MB
[2022-05-31 04:02:05 MetaFG_0] (main.py 265): INFO Train: [40/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2926 (0.3054)	loss 1.4772 (1.4114)	grad_norm 17.0648 (30.1266)	mem 4879MB
[2022-05-31 04:02:08 MetaFG_0] (main.py 265): INFO Train: [40/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2915 (0.3054)	loss 1.1927 (1.4102)	grad_norm 32.2512 (30.0667)	mem 4879MB
[2022-05-31 04:02:11 MetaFG_0] (main.py 265): INFO Train: [40/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2984 (0.3054)	loss 1.2503 (1.4101)	grad_norm 21.6407 (30.0613)	mem 4879MB
[2022-05-31 04:02:14 MetaFG_0] (main.py 265): INFO Train: [40/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3007 (0.3054)	loss 1.3893 (1.4095)	grad_norm 28.8146 (30.0427)	mem 4879MB
[2022-05-31 04:02:17 MetaFG_0] (main.py 265): INFO Train: [40/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2921 (0.3053)	loss 1.4333 (1.4090)	grad_norm 18.4817 (30.0153)	mem 4879MB
[2022-05-31 04:02:20 MetaFG_0] (main.py 265): INFO Train: [40/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2919 (0.3053)	loss 1.8146 (1.4094)	grad_norm 54.6654 (30.0555)	mem 4879MB
[2022-05-31 04:02:23 MetaFG_0] (main.py 265): INFO Train: [40/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2983 (0.3053)	loss 1.5863 (1.4086)	grad_norm 24.0304 (30.0441)	mem 4879MB
[2022-05-31 04:02:26 MetaFG_0] (main.py 265): INFO Train: [40/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2942 (0.3053)	loss 1.1476 (1.4082)	grad_norm 27.5425 (30.0144)	mem 4879MB
[2022-05-31 04:02:29 MetaFG_0] (main.py 265): INFO Train: [40/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2925 (0.3053)	loss 1.5883 (1.4092)	grad_norm 25.3636 (30.0118)	mem 4879MB
[2022-05-31 04:02:32 MetaFG_0] (main.py 265): INFO Train: [40/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2919 (0.3053)	loss 1.1963 (1.4085)	grad_norm 58.0798 (30.1327)	mem 4879MB
[2022-05-31 04:02:35 MetaFG_0] (main.py 265): INFO Train: [40/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2940 (0.3053)	loss 1.0849 (1.4081)	grad_norm 21.5605 (30.0802)	mem 4879MB
[2022-05-31 04:02:38 MetaFG_0] (main.py 265): INFO Train: [40/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2932 (0.3052)	loss 1.3097 (1.4086)	grad_norm 30.0229 (30.0946)	mem 4879MB
[2022-05-31 04:02:41 MetaFG_0] (main.py 265): INFO Train: [40/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2928 (0.3052)	loss 1.5027 (1.4084)	grad_norm 26.4663 (30.1181)	mem 4879MB
[2022-05-31 04:02:44 MetaFG_0] (main.py 265): INFO Train: [40/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2986 (0.3052)	loss 1.1304 (1.4079)	grad_norm 30.7630 (30.1042)	mem 4879MB
[2022-05-31 04:02:47 MetaFG_0] (main.py 265): INFO Train: [40/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2918 (0.3052)	loss 1.4428 (1.4074)	grad_norm 28.5221 (30.0998)	mem 4879MB
[2022-05-31 04:02:50 MetaFG_0] (main.py 265): INFO Train: [40/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2920 (0.3052)	loss 1.2859 (1.4075)	grad_norm 22.6590 (30.1091)	mem 4879MB
[2022-05-31 04:02:53 MetaFG_0] (main.py 265): INFO Train: [40/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2920 (0.3052)	loss 1.2568 (1.4077)	grad_norm 23.6664 (30.0815)	mem 4879MB
[2022-05-31 04:02:56 MetaFG_0] (main.py 265): INFO Train: [40/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2979 (0.3052)	loss 1.7988 (1.4077)	grad_norm 30.5127 (30.1442)	mem 4879MB
[2022-05-31 04:02:59 MetaFG_0] (main.py 265): INFO Train: [40/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2927 (0.3052)	loss 1.5348 (1.4081)	grad_norm 20.4208 (30.1517)	mem 4879MB
[2022-05-31 04:03:02 MetaFG_0] (main.py 265): INFO Train: [40/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2926 (0.3052)	loss 1.3157 (1.4079)	grad_norm 23.7967 (30.2186)	mem 4879MB
[2022-05-31 04:03:05 MetaFG_0] (main.py 265): INFO Train: [40/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2931 (0.3051)	loss 1.6505 (1.4080)	grad_norm 28.7388 (30.1830)	mem 4879MB
[2022-05-31 04:03:08 MetaFG_0] (main.py 265): INFO Train: [40/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2920 (0.3051)	loss 1.2611 (1.4093)	grad_norm 25.0050 (30.1840)	mem 4879MB
[2022-05-31 04:03:11 MetaFG_0] (main.py 265): INFO Train: [40/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.3086 (0.3051)	loss 1.1432 (1.4087)	grad_norm 23.4533 (30.1557)	mem 4879MB
[2022-05-31 04:03:15 MetaFG_0] (main.py 265): INFO Train: [40/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2922 (0.3051)	loss 1.2753 (1.4083)	grad_norm 30.1312 (30.2293)	mem 4879MB
[2022-05-31 04:03:18 MetaFG_0] (main.py 265): INFO Train: [40/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2934 (0.3051)	loss 1.5529 (1.4079)	grad_norm 24.1896 (30.2065)	mem 4879MB
[2022-05-31 04:03:21 MetaFG_0] (main.py 265): INFO Train: [40/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2926 (0.3051)	loss 1.1716 (1.4073)	grad_norm 43.0531 (30.2252)	mem 4879MB
[2022-05-31 04:03:24 MetaFG_0] (main.py 265): INFO Train: [40/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2924 (0.3051)	loss 1.6077 (1.4075)	grad_norm 21.4820 (30.1915)	mem 4879MB
[2022-05-31 04:03:27 MetaFG_0] (main.py 265): INFO Train: [40/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2924 (0.3051)	loss 0.9962 (1.4071)	grad_norm 45.8974 (30.1744)	mem 4879MB
[2022-05-31 04:03:30 MetaFG_0] (main.py 265): INFO Train: [40/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2926 (0.3051)	loss 1.0886 (1.4075)	grad_norm 29.1073 (30.1437)	mem 4879MB
[2022-05-31 04:03:33 MetaFG_0] (main.py 265): INFO Train: [40/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2923 (0.3051)	loss 1.2467 (1.4075)	grad_norm 22.7298 (30.1619)	mem 4879MB
[2022-05-31 04:03:36 MetaFG_0] (main.py 265): INFO Train: [40/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2923 (0.3050)	loss 1.2554 (1.4078)	grad_norm 51.5412 (30.1374)	mem 4879MB
[2022-05-31 04:03:39 MetaFG_0] (main.py 265): INFO Train: [40/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2920 (0.3050)	loss 1.6492 (1.4083)	grad_norm 21.8430 (30.1197)	mem 4879MB
[2022-05-31 04:03:42 MetaFG_0] (main.py 265): INFO Train: [40/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2975 (0.3050)	loss 1.5768 (1.4080)	grad_norm 41.3633 (30.0941)	mem 4879MB
[2022-05-31 04:03:45 MetaFG_0] (main.py 265): INFO Train: [40/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2990 (0.3050)	loss 1.3885 (1.4084)	grad_norm 22.5109 (30.0459)	mem 4879MB
[2022-05-31 04:03:48 MetaFG_0] (main.py 265): INFO Train: [40/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2933 (0.3050)	loss 1.5173 (1.4085)	grad_norm 20.9360 (30.0635)	mem 4879MB
[2022-05-31 04:03:51 MetaFG_0] (main.py 265): INFO Train: [40/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2928 (0.3050)	loss 1.6015 (1.4084)	grad_norm 22.5976 (30.0352)	mem 4879MB
[2022-05-31 04:03:54 MetaFG_0] (main.py 265): INFO Train: [40/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2969 (0.3050)	loss 1.4957 (1.4084)	grad_norm 17.3553 (30.0361)	mem 4879MB
[2022-05-31 04:03:57 MetaFG_0] (main.py 265): INFO Train: [40/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3006 (0.3050)	loss 1.4771 (1.4086)	grad_norm 39.3502 (30.1003)	mem 4879MB
[2022-05-31 04:04:00 MetaFG_0] (main.py 265): INFO Train: [40/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2993 (0.3050)	loss 1.6025 (1.4081)	grad_norm 41.0919 (30.1134)	mem 4879MB
[2022-05-31 04:04:03 MetaFG_0] (main.py 265): INFO Train: [40/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2938 (0.3050)	loss 1.5926 (1.4089)	grad_norm 19.8725 (30.1197)	mem 4879MB
[2022-05-31 04:04:06 MetaFG_0] (main.py 265): INFO Train: [40/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2942 (0.3050)	loss 1.4929 (1.4082)	grad_norm 29.2795 (30.1381)	mem 4879MB
[2022-05-31 04:04:09 MetaFG_0] (main.py 265): INFO Train: [40/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3008 (0.3050)	loss 1.3774 (1.4086)	grad_norm 20.5578 (30.1221)	mem 4879MB
[2022-05-31 04:04:12 MetaFG_0] (main.py 265): INFO Train: [40/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2993 (0.3050)	loss 1.5809 (1.4093)	grad_norm 32.3808 (30.1369)	mem 4879MB
[2022-05-31 04:04:15 MetaFG_0] (main.py 265): INFO Train: [40/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2988 (0.3050)	loss 1.3182 (1.4095)	grad_norm 34.9042 (30.1659)	mem 4879MB
[2022-05-31 04:04:18 MetaFG_0] (main.py 265): INFO Train: [40/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2993 (0.3050)	loss 1.6263 (1.4103)	grad_norm 25.3915 (30.3985)	mem 4879MB
[2022-05-31 04:04:21 MetaFG_0] (main.py 265): INFO Train: [40/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2978 (0.3049)	loss 1.3701 (1.4104)	grad_norm 23.5345 (30.4150)	mem 4879MB
[2022-05-31 04:04:24 MetaFG_0] (main.py 265): INFO Train: [40/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2987 (0.3049)	loss 1.1278 (1.4105)	grad_norm 36.4305 (30.4239)	mem 4879MB
[2022-05-31 04:04:28 MetaFG_0] (main.py 265): INFO Train: [40/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2950 (0.3050)	loss 1.5967 (1.4108)	grad_norm 36.4714 (30.4421)	mem 4879MB
[2022-05-31 04:04:31 MetaFG_0] (main.py 265): INFO Train: [40/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2981 (0.3050)	loss 1.4995 (1.4100)	grad_norm 20.8334 (30.3955)	mem 4879MB
[2022-05-31 04:04:34 MetaFG_0] (main.py 265): INFO Train: [40/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2942 (0.3049)	loss 1.2974 (1.4088)	grad_norm 120.7159 (30.4252)	mem 4879MB
[2022-05-31 04:04:37 MetaFG_0] (main.py 265): INFO Train: [40/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2947 (0.3049)	loss 1.5134 (1.4096)	grad_norm 18.8482 (30.4408)	mem 4879MB
[2022-05-31 04:04:40 MetaFG_0] (main.py 265): INFO Train: [40/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2935 (0.3049)	loss 1.0644 (1.4092)	grad_norm 27.8806 (30.4328)	mem 4879MB
[2022-05-31 04:04:43 MetaFG_0] (main.py 265): INFO Train: [40/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3014 (0.3049)	loss 1.4081 (1.4097)	grad_norm 22.7095 (30.4685)	mem 4879MB
[2022-05-31 04:04:46 MetaFG_0] (main.py 265): INFO Train: [40/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.3005 (0.3049)	loss 1.3893 (1.4096)	grad_norm 25.9228 (30.4380)	mem 4879MB
[2022-05-31 04:04:49 MetaFG_0] (main.py 265): INFO Train: [40/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2995 (0.3049)	loss 1.0094 (1.4094)	grad_norm 24.5290 (30.4352)	mem 4879MB
[2022-05-31 04:04:52 MetaFG_0] (main.py 265): INFO Train: [40/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2918 (0.3049)	loss 0.9844 (1.4100)	grad_norm 30.9494 (30.4272)	mem 4879MB
[2022-05-31 04:04:55 MetaFG_0] (main.py 265): INFO Train: [40/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2996 (0.3049)	loss 1.2952 (1.4103)	grad_norm 32.9619 (30.4296)	mem 4879MB
[2022-05-31 04:04:58 MetaFG_0] (main.py 265): INFO Train: [40/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2931 (0.3049)	loss 1.6834 (1.4104)	grad_norm 37.3913 (30.4632)	mem 4879MB
[2022-05-31 04:05:01 MetaFG_0] (main.py 265): INFO Train: [40/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2916 (0.3049)	loss 1.6481 (1.4095)	grad_norm 24.3496 (30.4711)	mem 4879MB
[2022-05-31 04:05:01 MetaFG_0] (main.py 272): INFO EPOCH 40 training takes 0:07:56
[2022-05-31 04:05:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_40.pth saving......
[2022-05-31 04:05:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_40.pth saved !!!
[2022-05-31 04:05:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:05:04 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:05:04 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:05:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.644 (0.644)	Loss 0.6716 (0.6716)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 04:05:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.099 (0.148)	Loss 0.6019 (0.6503)	Acc@1 87.500 (85.795)	Acc@5 96.875 (98.011)	Mem 4879MB
[2022-05-31 04:05:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.122)	Loss 0.7199 (0.6485)	Acc@1 81.250 (85.565)	Acc@5 96.875 (98.363)	Mem 4879MB
[2022-05-31 04:05:07 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.102 (0.112)	Loss 0.5890 (0.6450)	Acc@1 84.375 (86.089)	Acc@5 96.875 (98.387)	Mem 4879MB
[2022-05-31 04:05:08 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.107)	Loss 0.6152 (0.6701)	Acc@1 84.375 (84.909)	Acc@5 100.000 (98.399)	Mem 4879MB
[2022-05-31 04:05:09 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.105)	Loss 0.6891 (0.6755)	Acc@1 87.500 (84.681)	Acc@5 100.000 (98.346)	Mem 4879MB
[2022-05-31 04:05:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.103)	Loss 0.6151 (0.6688)	Acc@1 87.500 (84.990)	Acc@5 100.000 (98.361)	Mem 4879MB
[2022-05-31 04:05:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.9815 (0.6749)	Acc@1 78.125 (84.815)	Acc@5 93.750 (98.239)	Mem 4879MB
[2022-05-31 04:05:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 0.8060 (0.6782)	Acc@1 81.250 (84.491)	Acc@5 100.000 (98.302)	Mem 4879MB
[2022-05-31 04:05:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.100)	Loss 0.6800 (0.6794)	Acc@1 87.500 (84.444)	Acc@5 100.000 (98.352)	Mem 4879MB
[2022-05-31 04:05:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.5363 (0.6779)	Acc@1 87.500 (84.406)	Acc@5 100.000 (98.298)	Mem 4879MB
[2022-05-31 04:05:15 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.099)	Loss 0.6841 (0.6640)	Acc@1 81.250 (85.023)	Acc@5 96.875 (98.395)	Mem 4879MB
[2022-05-31 04:05:16 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.6661 (0.6654)	Acc@1 81.250 (84.995)	Acc@5 100.000 (98.399)	Mem 4879MB
[2022-05-31 04:05:17 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.5466 (0.6678)	Acc@1 93.750 (84.995)	Acc@5 100.000 (98.330)	Mem 4879MB
[2022-05-31 04:05:18 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.4637 (0.6686)	Acc@1 93.750 (84.907)	Acc@5 100.000 (98.316)	Mem 4879MB
[2022-05-31 04:05:19 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.5339 (0.6657)	Acc@1 87.500 (84.975)	Acc@5 100.000 (98.324)	Mem 4879MB
[2022-05-31 04:05:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 0.7012 (0.6655)	Acc@1 90.625 (85.093)	Acc@5 96.875 (98.311)	Mem 4879MB
[2022-05-31 04:05:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.097)	Loss 0.3985 (0.6630)	Acc@1 93.750 (85.124)	Acc@5 100.000 (98.337)	Mem 4879MB
[2022-05-31 04:05:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.097)	Loss 0.5425 (0.6643)	Acc@1 87.500 (85.048)	Acc@5 100.000 (98.308)	Mem 4879MB
[2022-05-31 04:05:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.097)	Loss 0.5195 (0.6644)	Acc@1 87.500 (84.931)	Acc@5 96.875 (98.298)	Mem 4879MB
[2022-05-31 04:05:23 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.091 (0.097)	Loss 0.4756 (0.6640)	Acc@1 93.750 (85.012)	Acc@5 100.000 (98.321)	Mem 4879MB
[2022-05-31 04:05:24 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.091 (0.097)	Loss 0.3875 (0.6628)	Acc@1 93.750 (85.101)	Acc@5 100.000 (98.297)	Mem 4879MB
[2022-05-31 04:05:25 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 0.5229 (0.6596)	Acc@1 90.625 (85.209)	Acc@5 100.000 (98.346)	Mem 4879MB
[2022-05-31 04:05:26 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 0.5528 (0.6612)	Acc@1 90.625 (85.146)	Acc@5 96.875 (98.377)	Mem 4879MB
[2022-05-31 04:05:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.098 (0.096)	Loss 0.4653 (0.6588)	Acc@1 90.625 (85.270)	Acc@5 100.000 (98.392)	Mem 4879MB
[2022-05-31 04:05:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.097 (0.096)	Loss 0.6927 (0.6569)	Acc@1 87.500 (85.346)	Acc@5 96.875 (98.369)	Mem 4879MB
[2022-05-31 04:05:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.090 (0.096)	Loss 0.6798 (0.6578)	Acc@1 84.375 (85.249)	Acc@5 96.875 (98.360)	Mem 4879MB
[2022-05-31 04:05:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.5717 (0.6567)	Acc@1 90.625 (85.263)	Acc@5 100.000 (98.397)	Mem 4879MB
[2022-05-31 04:05:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.5619 (0.6578)	Acc@1 87.500 (85.187)	Acc@5 100.000 (98.399)	Mem 4879MB
[2022-05-31 04:05:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.089 (0.096)	Loss 0.5573 (0.6561)	Acc@1 87.500 (85.299)	Acc@5 100.000 (98.389)	Mem 4879MB
[2022-05-31 04:05:33 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.5760 (0.6558)	Acc@1 81.250 (85.268)	Acc@5 100.000 (98.391)	Mem 4879MB
[2022-05-31 04:05:33 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5357 (0.6581)	Acc@1 87.500 (85.129)	Acc@5 100.000 (98.392)	Mem 4879MB
[2022-05-31 04:05:34 MetaFG_0] (main.py 330): INFO  * Acc@1 85.110 Acc@5 98.390
[2022-05-31 04:05:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.1%
[2022-05-31 04:05:34 MetaFG_0] (main.py 171): INFO Max accuracy: 85.11%
[2022-05-31 04:05:35 MetaFG_0] (main.py 265): INFO Train: [41/300][0/1562]	eta 0:26:43 lr 0.000006	time 1.0263 (1.0263)	loss 1.3641 (1.3641)	grad_norm 22.6839 (22.6839)	mem 4879MB
[2022-05-31 04:05:38 MetaFG_0] (main.py 265): INFO Train: [41/300][10/1562]	eta 0:09:38 lr 0.000006	time 0.2930 (0.3730)	loss 1.2475 (1.3643)	grad_norm 40.4713 (29.6087)	mem 4879MB
[2022-05-31 04:05:41 MetaFG_0] (main.py 265): INFO Train: [41/300][20/1562]	eta 0:08:43 lr 0.000006	time 0.2931 (0.3396)	loss 1.7650 (1.4563)	grad_norm 28.3716 (30.2530)	mem 4879MB
[2022-05-31 04:05:44 MetaFG_0] (main.py 265): INFO Train: [41/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2934 (0.3281)	loss 1.4780 (1.4323)	grad_norm 36.7107 (30.4224)	mem 4879MB
[2022-05-31 04:05:47 MetaFG_0] (main.py 265): INFO Train: [41/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2976 (0.3222)	loss 1.7514 (1.4281)	grad_norm 21.1224 (29.4971)	mem 4879MB
[2022-05-31 04:05:50 MetaFG_0] (main.py 265): INFO Train: [41/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2924 (0.3184)	loss 1.4986 (1.4040)	grad_norm 27.0824 (31.9087)	mem 4879MB
[2022-05-31 04:05:53 MetaFG_0] (main.py 265): INFO Train: [41/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.3003 (0.3164)	loss 1.5731 (1.4225)	grad_norm 48.3689 (31.2575)	mem 4879MB
[2022-05-31 04:05:56 MetaFG_0] (main.py 265): INFO Train: [41/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.2985 (0.3147)	loss 1.4245 (1.4131)	grad_norm 27.8278 (30.1466)	mem 4879MB
[2022-05-31 04:05:59 MetaFG_0] (main.py 265): INFO Train: [41/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2956 (0.3134)	loss 1.1794 (1.3966)	grad_norm 25.9657 (29.4452)	mem 4879MB
[2022-05-31 04:06:02 MetaFG_0] (main.py 265): INFO Train: [41/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2928 (0.3122)	loss 1.1591 (1.3872)	grad_norm 23.6335 (29.4562)	mem 4879MB
[2022-05-31 04:06:05 MetaFG_0] (main.py 265): INFO Train: [41/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2990 (0.3113)	loss 1.3487 (1.3877)	grad_norm 25.8197 (29.3844)	mem 4879MB
[2022-05-31 04:06:08 MetaFG_0] (main.py 265): INFO Train: [41/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2989 (0.3108)	loss 1.8419 (1.3936)	grad_norm 44.5217 (29.2125)	mem 4879MB
[2022-05-31 04:06:11 MetaFG_0] (main.py 265): INFO Train: [41/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2917 (0.3102)	loss 0.9676 (1.3776)	grad_norm 27.6072 (29.3495)	mem 4879MB
[2022-05-31 04:06:14 MetaFG_0] (main.py 265): INFO Train: [41/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2980 (0.3097)	loss 1.4686 (1.3862)	grad_norm 16.8199 (29.2527)	mem 4879MB
[2022-05-31 04:06:17 MetaFG_0] (main.py 265): INFO Train: [41/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2984 (0.3093)	loss 1.2930 (1.3956)	grad_norm 61.9105 (29.5021)	mem 4879MB
[2022-05-31 04:06:20 MetaFG_0] (main.py 265): INFO Train: [41/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2926 (0.3090)	loss 1.3071 (1.3930)	grad_norm 28.8089 (29.3904)	mem 4879MB
[2022-05-31 04:06:23 MetaFG_0] (main.py 265): INFO Train: [41/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2947 (0.3086)	loss 1.5435 (1.3930)	grad_norm 28.9375 (29.3067)	mem 4879MB
[2022-05-31 04:06:26 MetaFG_0] (main.py 265): INFO Train: [41/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2941 (0.3084)	loss 1.2817 (1.3902)	grad_norm 33.2528 (29.4265)	mem 4879MB
[2022-05-31 04:06:30 MetaFG_0] (main.py 265): INFO Train: [41/300][180/1562]	eta 0:07:05 lr 0.000006	time 0.2961 (0.3082)	loss 1.2191 (1.3912)	grad_norm 17.2510 (29.2947)	mem 4879MB
[2022-05-31 04:06:33 MetaFG_0] (main.py 265): INFO Train: [41/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2925 (0.3080)	loss 1.0298 (1.3930)	grad_norm 25.5845 (29.2612)	mem 4879MB
[2022-05-31 04:06:36 MetaFG_0] (main.py 265): INFO Train: [41/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2961 (0.3078)	loss 1.5491 (1.3935)	grad_norm 19.2355 (28.9583)	mem 4879MB
[2022-05-31 04:06:39 MetaFG_0] (main.py 265): INFO Train: [41/300][210/1562]	eta 0:06:55 lr 0.000006	time 0.2922 (0.3077)	loss 1.1027 (1.3930)	grad_norm 35.2470 (29.2980)	mem 4879MB
[2022-05-31 04:06:42 MetaFG_0] (main.py 265): INFO Train: [41/300][220/1562]	eta 0:06:52 lr 0.000006	time 0.2983 (0.3075)	loss 1.2193 (1.3904)	grad_norm 25.2902 (29.5580)	mem 4879MB
[2022-05-31 04:06:45 MetaFG_0] (main.py 265): INFO Train: [41/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2987 (0.3073)	loss 1.5032 (1.3918)	grad_norm 52.6245 (29.8835)	mem 4879MB
[2022-05-31 04:06:48 MetaFG_0] (main.py 265): INFO Train: [41/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2933 (0.3072)	loss 1.1505 (1.3891)	grad_norm 32.2618 (29.8601)	mem 4879MB
[2022-05-31 04:06:51 MetaFG_0] (main.py 265): INFO Train: [41/300][250/1562]	eta 0:06:42 lr 0.000006	time 0.2927 (0.3071)	loss 1.5679 (1.3904)	grad_norm 32.2995 (29.8343)	mem 4879MB
[2022-05-31 04:06:54 MetaFG_0] (main.py 265): INFO Train: [41/300][260/1562]	eta 0:06:39 lr 0.000006	time 0.2942 (0.3070)	loss 1.2306 (1.3939)	grad_norm 33.0553 (29.7241)	mem 4879MB
[2022-05-31 04:06:57 MetaFG_0] (main.py 265): INFO Train: [41/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2933 (0.3069)	loss 1.6921 (1.3962)	grad_norm 39.8059 (29.5856)	mem 4879MB
[2022-05-31 04:07:00 MetaFG_0] (main.py 265): INFO Train: [41/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2917 (0.3068)	loss 1.1940 (1.3923)	grad_norm 49.7827 (29.7933)	mem 4879MB
[2022-05-31 04:07:03 MetaFG_0] (main.py 265): INFO Train: [41/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2917 (0.3067)	loss 1.5047 (1.3956)	grad_norm 30.8571 (29.9220)	mem 4879MB
[2022-05-31 04:07:06 MetaFG_0] (main.py 265): INFO Train: [41/300][300/1562]	eta 0:06:26 lr 0.000006	time 0.2995 (0.3065)	loss 1.2927 (1.3952)	grad_norm 32.6075 (30.0119)	mem 4879MB
[2022-05-31 04:07:09 MetaFG_0] (main.py 265): INFO Train: [41/300][310/1562]	eta 0:06:23 lr 0.000006	time 0.2926 (0.3064)	loss 1.2444 (1.3951)	grad_norm 29.4505 (29.9990)	mem 4879MB
[2022-05-31 04:07:12 MetaFG_0] (main.py 265): INFO Train: [41/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2922 (0.3063)	loss 1.0777 (1.3946)	grad_norm 28.6226 (30.1149)	mem 4879MB
[2022-05-31 04:07:15 MetaFG_0] (main.py 265): INFO Train: [41/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2976 (0.3063)	loss 1.3793 (1.3949)	grad_norm 22.5553 (29.9225)	mem 4879MB
[2022-05-31 04:07:18 MetaFG_0] (main.py 265): INFO Train: [41/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.3014 (0.3062)	loss 1.6501 (1.3954)	grad_norm 23.5955 (29.7497)	mem 4879MB
[2022-05-31 04:07:21 MetaFG_0] (main.py 265): INFO Train: [41/300][350/1562]	eta 0:06:10 lr 0.000006	time 0.2987 (0.3061)	loss 1.2187 (1.3941)	grad_norm 26.2931 (29.5239)	mem 4879MB
[2022-05-31 04:07:24 MetaFG_0] (main.py 265): INFO Train: [41/300][360/1562]	eta 0:06:07 lr 0.000006	time 0.2987 (0.3061)	loss 1.6706 (1.3938)	grad_norm 27.2558 (29.5694)	mem 4879MB
[2022-05-31 04:07:27 MetaFG_0] (main.py 265): INFO Train: [41/300][370/1562]	eta 0:06:04 lr 0.000006	time 0.2921 (0.3060)	loss 1.5481 (1.3961)	grad_norm 20.0026 (29.5776)	mem 4879MB
[2022-05-31 04:07:30 MetaFG_0] (main.py 265): INFO Train: [41/300][380/1562]	eta 0:06:01 lr 0.000006	time 0.2987 (0.3059)	loss 1.5183 (1.3961)	grad_norm 24.7732 (29.5077)	mem 4879MB
[2022-05-31 04:07:33 MetaFG_0] (main.py 265): INFO Train: [41/300][390/1562]	eta 0:05:58 lr 0.000006	time 0.2994 (0.3060)	loss 1.3848 (1.3921)	grad_norm 22.4001 (29.6482)	mem 4879MB
[2022-05-31 04:07:36 MetaFG_0] (main.py 265): INFO Train: [41/300][400/1562]	eta 0:05:55 lr 0.000006	time 0.2986 (0.3059)	loss 1.5831 (1.3912)	grad_norm 18.0122 (29.6404)	mem 4879MB
[2022-05-31 04:07:39 MetaFG_0] (main.py 265): INFO Train: [41/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.3001 (0.3059)	loss 1.6397 (1.3930)	grad_norm 20.3525 (29.5735)	mem 4879MB
[2022-05-31 04:07:42 MetaFG_0] (main.py 265): INFO Train: [41/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2930 (0.3059)	loss 1.4225 (1.3936)	grad_norm 23.3917 (29.5145)	mem 4879MB
[2022-05-31 04:07:46 MetaFG_0] (main.py 265): INFO Train: [41/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2990 (0.3058)	loss 1.2634 (1.3942)	grad_norm 20.3852 (29.4651)	mem 4879MB
[2022-05-31 04:07:49 MetaFG_0] (main.py 265): INFO Train: [41/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2982 (0.3058)	loss 1.4745 (1.3960)	grad_norm 21.5752 (29.5351)	mem 4879MB
[2022-05-31 04:07:52 MetaFG_0] (main.py 265): INFO Train: [41/300][450/1562]	eta 0:05:39 lr 0.000006	time 0.2916 (0.3057)	loss 1.5966 (1.3960)	grad_norm 23.7377 (29.5206)	mem 4879MB
[2022-05-31 04:07:55 MetaFG_0] (main.py 265): INFO Train: [41/300][460/1562]	eta 0:05:36 lr 0.000006	time 0.2918 (0.3057)	loss 1.3563 (1.3946)	grad_norm 22.8823 (29.4951)	mem 4879MB
[2022-05-31 04:07:58 MetaFG_0] (main.py 265): INFO Train: [41/300][470/1562]	eta 0:05:33 lr 0.000006	time 0.3003 (0.3057)	loss 1.0540 (1.3925)	grad_norm 31.3191 (29.6254)	mem 4879MB
[2022-05-31 04:08:01 MetaFG_0] (main.py 265): INFO Train: [41/300][480/1562]	eta 0:05:30 lr 0.000006	time 0.2935 (0.3057)	loss 1.5706 (1.3929)	grad_norm 27.5809 (29.5270)	mem 4879MB
[2022-05-31 04:08:04 MetaFG_0] (main.py 265): INFO Train: [41/300][490/1562]	eta 0:05:27 lr 0.000006	time 0.2925 (0.3057)	loss 0.9800 (1.3895)	grad_norm 36.0039 (29.5437)	mem 4879MB
[2022-05-31 04:08:07 MetaFG_0] (main.py 265): INFO Train: [41/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2923 (0.3056)	loss 1.3948 (1.3903)	grad_norm 24.1758 (29.5546)	mem 4879MB
[2022-05-31 04:08:10 MetaFG_0] (main.py 265): INFO Train: [41/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2988 (0.3056)	loss 1.3685 (1.3889)	grad_norm 39.2456 (29.6742)	mem 4879MB
[2022-05-31 04:08:13 MetaFG_0] (main.py 265): INFO Train: [41/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2930 (0.3056)	loss 1.6083 (1.3903)	grad_norm 48.4723 (29.7342)	mem 4879MB
[2022-05-31 04:08:16 MetaFG_0] (main.py 265): INFO Train: [41/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2978 (0.3055)	loss 1.4737 (1.3901)	grad_norm 28.2618 (29.6647)	mem 4879MB
[2022-05-31 04:08:19 MetaFG_0] (main.py 265): INFO Train: [41/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2940 (0.3055)	loss 1.3578 (1.3917)	grad_norm 30.1887 (29.7282)	mem 4879MB
[2022-05-31 04:08:22 MetaFG_0] (main.py 265): INFO Train: [41/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2918 (0.3055)	loss 1.3811 (1.3906)	grad_norm 22.1429 (29.7207)	mem 4879MB
[2022-05-31 04:08:25 MetaFG_0] (main.py 265): INFO Train: [41/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2990 (0.3055)	loss 1.6675 (1.3928)	grad_norm 37.9290 (29.7274)	mem 4879MB
[2022-05-31 04:08:28 MetaFG_0] (main.py 265): INFO Train: [41/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2924 (0.3055)	loss 1.6004 (1.3951)	grad_norm 28.3144 (29.7173)	mem 4879MB
[2022-05-31 04:08:31 MetaFG_0] (main.py 265): INFO Train: [41/300][580/1562]	eta 0:04:59 lr 0.000006	time 0.2941 (0.3055)	loss 1.3837 (1.3945)	grad_norm 25.8500 (29.7266)	mem 4879MB
[2022-05-31 04:08:34 MetaFG_0] (main.py 265): INFO Train: [41/300][590/1562]	eta 0:04:56 lr 0.000006	time 0.2985 (0.3055)	loss 1.2249 (1.3940)	grad_norm 25.8741 (29.6151)	mem 4879MB
[2022-05-31 04:08:37 MetaFG_0] (main.py 265): INFO Train: [41/300][600/1562]	eta 0:04:53 lr 0.000006	time 0.2977 (0.3054)	loss 1.2707 (1.3932)	grad_norm 39.8935 (29.6045)	mem 4879MB
[2022-05-31 04:08:40 MetaFG_0] (main.py 265): INFO Train: [41/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2956 (0.3054)	loss 1.2250 (1.3918)	grad_norm 15.2172 (29.6682)	mem 4879MB
[2022-05-31 04:08:43 MetaFG_0] (main.py 265): INFO Train: [41/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2936 (0.3054)	loss 1.0641 (1.3904)	grad_norm 44.5913 (29.7488)	mem 4879MB
[2022-05-31 04:08:46 MetaFG_0] (main.py 265): INFO Train: [41/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2932 (0.3054)	loss 1.2314 (1.3895)	grad_norm 45.9134 (29.8322)	mem 4879MB
[2022-05-31 04:08:49 MetaFG_0] (main.py 265): INFO Train: [41/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2938 (0.3054)	loss 1.4057 (1.3892)	grad_norm 17.2731 (29.8221)	mem 4879MB
[2022-05-31 04:08:53 MetaFG_0] (main.py 265): INFO Train: [41/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2941 (0.3054)	loss 1.2650 (1.3899)	grad_norm 39.4383 (29.8133)	mem 4879MB
[2022-05-31 04:08:56 MetaFG_0] (main.py 265): INFO Train: [41/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2929 (0.3053)	loss 1.3472 (1.3894)	grad_norm 30.4239 (29.8092)	mem 4879MB
[2022-05-31 04:08:59 MetaFG_0] (main.py 265): INFO Train: [41/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2927 (0.3053)	loss 1.2964 (1.3894)	grad_norm 61.2527 (29.8638)	mem 4879MB
[2022-05-31 04:09:02 MetaFG_0] (main.py 265): INFO Train: [41/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2995 (0.3053)	loss 1.6918 (1.3877)	grad_norm 36.6023 (30.0199)	mem 4879MB
[2022-05-31 04:09:05 MetaFG_0] (main.py 265): INFO Train: [41/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2930 (0.3053)	loss 1.2024 (1.3870)	grad_norm 19.2415 (29.9720)	mem 4879MB
[2022-05-31 04:09:08 MetaFG_0] (main.py 265): INFO Train: [41/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2976 (0.3053)	loss 1.5089 (1.3881)	grad_norm 28.4263 (29.9443)	mem 4879MB
[2022-05-31 04:09:11 MetaFG_0] (main.py 265): INFO Train: [41/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2922 (0.3052)	loss 1.4343 (1.3882)	grad_norm 24.8091 (29.9459)	mem 4879MB
[2022-05-31 04:09:14 MetaFG_0] (main.py 265): INFO Train: [41/300][720/1562]	eta 0:04:16 lr 0.000006	time 0.2983 (0.3052)	loss 1.3159 (1.3871)	grad_norm 31.1791 (29.9245)	mem 4879MB
[2022-05-31 04:09:17 MetaFG_0] (main.py 265): INFO Train: [41/300][730/1562]	eta 0:04:13 lr 0.000006	time 0.2934 (0.3052)	loss 1.3653 (1.3868)	grad_norm 22.8608 (29.9537)	mem 4879MB
[2022-05-31 04:09:20 MetaFG_0] (main.py 265): INFO Train: [41/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2937 (0.3052)	loss 1.6608 (1.3867)	grad_norm 17.4407 (30.0174)	mem 4879MB
[2022-05-31 04:09:23 MetaFG_0] (main.py 265): INFO Train: [41/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2929 (0.3052)	loss 1.0304 (1.3869)	grad_norm 22.1366 (30.0864)	mem 4879MB
[2022-05-31 04:09:26 MetaFG_0] (main.py 265): INFO Train: [41/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2940 (0.3052)	loss 1.3770 (1.3851)	grad_norm 14.8650 (30.0665)	mem 4879MB
[2022-05-31 04:09:29 MetaFG_0] (main.py 265): INFO Train: [41/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2996 (0.3052)	loss 1.1180 (1.3844)	grad_norm 47.2170 (30.0692)	mem 4879MB
[2022-05-31 04:09:32 MetaFG_0] (main.py 265): INFO Train: [41/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2979 (0.3052)	loss 1.0331 (1.3843)	grad_norm 37.6976 (30.0902)	mem 4879MB
[2022-05-31 04:09:35 MetaFG_0] (main.py 265): INFO Train: [41/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2979 (0.3052)	loss 1.4963 (1.3836)	grad_norm 20.7558 (30.0368)	mem 4879MB
[2022-05-31 04:09:38 MetaFG_0] (main.py 265): INFO Train: [41/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2924 (0.3052)	loss 1.3640 (1.3837)	grad_norm 31.4978 (30.0095)	mem 4879MB
[2022-05-31 04:09:41 MetaFG_0] (main.py 265): INFO Train: [41/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2918 (0.3052)	loss 1.5265 (1.3842)	grad_norm 40.7341 (29.9826)	mem 4879MB
[2022-05-31 04:09:44 MetaFG_0] (main.py 265): INFO Train: [41/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2944 (0.3051)	loss 1.7209 (1.3852)	grad_norm 29.5080 (30.0105)	mem 4879MB
[2022-05-31 04:09:47 MetaFG_0] (main.py 265): INFO Train: [41/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2979 (0.3052)	loss 1.4706 (1.3862)	grad_norm 30.9584 (29.9800)	mem 4879MB
[2022-05-31 04:09:50 MetaFG_0] (main.py 265): INFO Train: [41/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2983 (0.3051)	loss 1.3937 (1.3864)	grad_norm 42.8605 (29.9816)	mem 4879MB
[2022-05-31 04:09:53 MetaFG_0] (main.py 265): INFO Train: [41/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.3021 (0.3051)	loss 1.2751 (1.3869)	grad_norm 26.1176 (29.9434)	mem 4879MB
[2022-05-31 04:09:56 MetaFG_0] (main.py 265): INFO Train: [41/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2942 (0.3051)	loss 1.4058 (1.3880)	grad_norm 20.0567 (29.9544)	mem 4879MB
[2022-05-31 04:09:59 MetaFG_0] (main.py 265): INFO Train: [41/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2998 (0.3051)	loss 1.0828 (1.3872)	grad_norm 23.9670 (29.9651)	mem 4879MB
[2022-05-31 04:10:02 MetaFG_0] (main.py 265): INFO Train: [41/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2930 (0.3051)	loss 1.5424 (1.3879)	grad_norm 22.7764 (29.9262)	mem 4879MB
[2022-05-31 04:10:06 MetaFG_0] (main.py 265): INFO Train: [41/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2922 (0.3050)	loss 1.5815 (1.3888)	grad_norm 37.7356 (29.9108)	mem 4879MB
[2022-05-31 04:10:09 MetaFG_0] (main.py 265): INFO Train: [41/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2986 (0.3050)	loss 1.6531 (1.3886)	grad_norm 34.1657 (29.9316)	mem 4879MB
[2022-05-31 04:10:12 MetaFG_0] (main.py 265): INFO Train: [41/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.3047 (0.3050)	loss 1.3470 (1.3885)	grad_norm 23.3886 (29.8995)	mem 4879MB
[2022-05-31 04:10:15 MetaFG_0] (main.py 265): INFO Train: [41/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2922 (0.3050)	loss 1.5035 (1.3886)	grad_norm 17.1407 (29.9067)	mem 4879MB
[2022-05-31 04:10:18 MetaFG_0] (main.py 265): INFO Train: [41/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2924 (0.3050)	loss 1.6509 (1.3885)	grad_norm 26.8042 (29.9044)	mem 4879MB
[2022-05-31 04:10:21 MetaFG_0] (main.py 265): INFO Train: [41/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2922 (0.3050)	loss 1.6306 (1.3906)	grad_norm 25.7970 (29.9749)	mem 4879MB
[2022-05-31 04:10:24 MetaFG_0] (main.py 265): INFO Train: [41/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2984 (0.3050)	loss 1.7464 (1.3909)	grad_norm 15.9298 (29.9280)	mem 4879MB
[2022-05-31 04:10:27 MetaFG_0] (main.py 265): INFO Train: [41/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.3001 (0.3049)	loss 1.4676 (1.3903)	grad_norm 27.0338 (29.9101)	mem 4879MB
[2022-05-31 04:10:30 MetaFG_0] (main.py 265): INFO Train: [41/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2925 (0.3049)	loss 1.1516 (1.3898)	grad_norm 52.1669 (29.9880)	mem 4879MB
[2022-05-31 04:10:33 MetaFG_0] (main.py 265): INFO Train: [41/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2932 (0.3049)	loss 1.5055 (1.3905)	grad_norm 28.6730 (29.9454)	mem 4879MB
[2022-05-31 04:10:36 MetaFG_0] (main.py 265): INFO Train: [41/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2977 (0.3049)	loss 1.4302 (1.3908)	grad_norm 33.4084 (29.9809)	mem 4879MB
[2022-05-31 04:10:39 MetaFG_0] (main.py 265): INFO Train: [41/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2941 (0.3049)	loss 1.0499 (1.3902)	grad_norm 41.8553 (30.0289)	mem 4879MB
[2022-05-31 04:10:42 MetaFG_0] (main.py 265): INFO Train: [41/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2918 (0.3049)	loss 1.4189 (1.3910)	grad_norm 29.5322 (30.0235)	mem 4879MB
[2022-05-31 04:10:45 MetaFG_0] (main.py 265): INFO Train: [41/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2928 (0.3049)	loss 1.6093 (1.3912)	grad_norm 30.9615 (30.0044)	mem 4879MB
[2022-05-31 04:10:48 MetaFG_0] (main.py 265): INFO Train: [41/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2941 (0.3049)	loss 1.4324 (1.3912)	grad_norm 18.2549 (29.9645)	mem 4879MB
[2022-05-31 04:10:51 MetaFG_0] (main.py 265): INFO Train: [41/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2937 (0.3052)	loss 1.3631 (1.3913)	grad_norm 19.6762 (29.9138)	mem 4879MB
[2022-05-31 04:10:54 MetaFG_0] (main.py 265): INFO Train: [41/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2985 (0.3052)	loss 1.3172 (1.3920)	grad_norm 20.8607 (29.8864)	mem 4879MB
[2022-05-31 04:10:57 MetaFG_0] (main.py 265): INFO Train: [41/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2957 (0.3051)	loss 1.5129 (1.3919)	grad_norm 16.6461 (29.8562)	mem 4879MB
[2022-05-31 04:11:01 MetaFG_0] (main.py 265): INFO Train: [41/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2935 (0.3051)	loss 1.5595 (1.3926)	grad_norm 34.8785 (29.8611)	mem 4879MB
[2022-05-31 04:11:04 MetaFG_0] (main.py 265): INFO Train: [41/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2951 (0.3051)	loss 1.1948 (1.3926)	grad_norm 28.8548 (29.8693)	mem 4879MB
[2022-05-31 04:11:07 MetaFG_0] (main.py 265): INFO Train: [41/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.3003 (0.3051)	loss 1.1682 (1.3917)	grad_norm 28.1690 (29.8696)	mem 4879MB
[2022-05-31 04:11:10 MetaFG_0] (main.py 265): INFO Train: [41/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2976 (0.3051)	loss 1.3027 (1.3915)	grad_norm 28.3135 (29.8531)	mem 4879MB
[2022-05-31 04:11:13 MetaFG_0] (main.py 265): INFO Train: [41/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2922 (0.3051)	loss 1.7999 (1.3909)	grad_norm 28.8841 (29.8767)	mem 4879MB
[2022-05-31 04:11:16 MetaFG_0] (main.py 265): INFO Train: [41/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2945 (0.3051)	loss 1.6809 (1.3914)	grad_norm 31.9513 (29.8509)	mem 4879MB
[2022-05-31 04:11:19 MetaFG_0] (main.py 265): INFO Train: [41/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2922 (0.3051)	loss 1.6035 (1.3921)	grad_norm 14.9159 (29.8251)	mem 4879MB
[2022-05-31 04:11:22 MetaFG_0] (main.py 265): INFO Train: [41/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2915 (0.3051)	loss 1.6319 (1.3932)	grad_norm 38.6853 (29.8264)	mem 4879MB
[2022-05-31 04:11:25 MetaFG_0] (main.py 265): INFO Train: [41/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2921 (0.3050)	loss 1.3934 (1.3947)	grad_norm 32.9029 (29.8431)	mem 4879MB
[2022-05-31 04:11:28 MetaFG_0] (main.py 265): INFO Train: [41/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2988 (0.3050)	loss 1.5174 (1.3957)	grad_norm 17.9425 (29.8212)	mem 4879MB
[2022-05-31 04:11:31 MetaFG_0] (main.py 265): INFO Train: [41/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2916 (0.3050)	loss 1.3255 (1.3968)	grad_norm 16.2088 (29.8064)	mem 4879MB
[2022-05-31 04:11:34 MetaFG_0] (main.py 265): INFO Train: [41/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2921 (0.3050)	loss 1.4948 (1.3962)	grad_norm 28.7121 (29.7789)	mem 4879MB
[2022-05-31 04:11:37 MetaFG_0] (main.py 265): INFO Train: [41/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2928 (0.3050)	loss 1.5543 (1.3963)	grad_norm 29.2001 (29.7945)	mem 4879MB
[2022-05-31 04:11:40 MetaFG_0] (main.py 265): INFO Train: [41/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2928 (0.3050)	loss 1.5066 (1.3960)	grad_norm 25.8891 (29.7829)	mem 4879MB
[2022-05-31 04:11:43 MetaFG_0] (main.py 265): INFO Train: [41/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2921 (0.3050)	loss 1.3429 (1.3959)	grad_norm 30.7776 (29.7607)	mem 4879MB
[2022-05-31 04:11:46 MetaFG_0] (main.py 265): INFO Train: [41/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2940 (0.3050)	loss 1.1450 (1.3957)	grad_norm 22.3960 (29.8228)	mem 4879MB
[2022-05-31 04:11:49 MetaFG_0] (main.py 265): INFO Train: [41/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2965 (0.3050)	loss 1.3581 (1.3962)	grad_norm 25.5365 (29.8108)	mem 4879MB
[2022-05-31 04:11:52 MetaFG_0] (main.py 265): INFO Train: [41/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2942 (0.3050)	loss 1.5339 (1.3968)	grad_norm 47.5996 (29.8299)	mem 4879MB
[2022-05-31 04:11:55 MetaFG_0] (main.py 265): INFO Train: [41/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2923 (0.3049)	loss 1.5513 (1.3963)	grad_norm 25.5364 (29.8095)	mem 4879MB
[2022-05-31 04:11:58 MetaFG_0] (main.py 265): INFO Train: [41/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2914 (0.3049)	loss 1.3934 (1.3956)	grad_norm 32.4067 (29.8151)	mem 4879MB
[2022-05-31 04:12:01 MetaFG_0] (main.py 265): INFO Train: [41/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2918 (0.3049)	loss 1.7337 (1.3953)	grad_norm 29.4780 (29.8342)	mem 4879MB
[2022-05-31 04:12:04 MetaFG_0] (main.py 265): INFO Train: [41/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2922 (0.3049)	loss 0.9363 (1.3953)	grad_norm 26.5101 (29.7953)	mem 4879MB
[2022-05-31 04:12:07 MetaFG_0] (main.py 265): INFO Train: [41/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2982 (0.3049)	loss 1.3800 (1.3952)	grad_norm 45.5967 (29.7787)	mem 4879MB
[2022-05-31 04:12:10 MetaFG_0] (main.py 265): INFO Train: [41/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2920 (0.3049)	loss 1.7477 (1.3945)	grad_norm 40.1921 (29.7889)	mem 4879MB
[2022-05-31 04:12:14 MetaFG_0] (main.py 265): INFO Train: [41/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2994 (0.3049)	loss 1.5342 (1.3946)	grad_norm 17.5923 (29.8088)	mem 4879MB
[2022-05-31 04:12:17 MetaFG_0] (main.py 265): INFO Train: [41/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2930 (0.3049)	loss 1.5510 (1.3949)	grad_norm 27.4709 (29.8216)	mem 4879MB
[2022-05-31 04:12:20 MetaFG_0] (main.py 265): INFO Train: [41/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2927 (0.3050)	loss 1.6292 (1.3947)	grad_norm 27.7449 (29.8455)	mem 4879MB
[2022-05-31 04:12:23 MetaFG_0] (main.py 265): INFO Train: [41/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2926 (0.3049)	loss 1.4120 (1.3948)	grad_norm 42.9441 (29.8887)	mem 4879MB
[2022-05-31 04:12:26 MetaFG_0] (main.py 265): INFO Train: [41/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3007 (0.3049)	loss 1.6051 (1.3950)	grad_norm 28.2352 (29.9255)	mem 4879MB
[2022-05-31 04:12:29 MetaFG_0] (main.py 265): INFO Train: [41/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2931 (0.3049)	loss 1.7271 (1.3957)	grad_norm 31.6104 (29.9187)	mem 4879MB
[2022-05-31 04:12:32 MetaFG_0] (main.py 265): INFO Train: [41/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2944 (0.3049)	loss 1.5310 (1.3958)	grad_norm 20.6573 (29.9074)	mem 4879MB
[2022-05-31 04:12:35 MetaFG_0] (main.py 265): INFO Train: [41/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2926 (0.3049)	loss 1.2196 (1.3964)	grad_norm 28.4312 (29.8919)	mem 4879MB
[2022-05-31 04:12:38 MetaFG_0] (main.py 265): INFO Train: [41/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2942 (0.3049)	loss 1.0589 (1.3973)	grad_norm 48.5121 (29.9055)	mem 4879MB
[2022-05-31 04:12:41 MetaFG_0] (main.py 265): INFO Train: [41/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2930 (0.3049)	loss 1.1844 (1.3977)	grad_norm 21.5358 (29.8967)	mem 4879MB
[2022-05-31 04:12:44 MetaFG_0] (main.py 265): INFO Train: [41/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2916 (0.3049)	loss 1.5642 (1.3980)	grad_norm 21.1863 (29.9200)	mem 4879MB
[2022-05-31 04:12:47 MetaFG_0] (main.py 265): INFO Train: [41/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2925 (0.3049)	loss 1.6210 (1.3987)	grad_norm 24.2213 (29.9238)	mem 4879MB
[2022-05-31 04:12:50 MetaFG_0] (main.py 265): INFO Train: [41/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2945 (0.3049)	loss 1.0089 (1.3986)	grad_norm 104.0189 (29.9735)	mem 4879MB
[2022-05-31 04:12:53 MetaFG_0] (main.py 265): INFO Train: [41/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2976 (0.3049)	loss 1.5311 (1.3986)	grad_norm 32.4079 (29.9781)	mem 4879MB
[2022-05-31 04:12:56 MetaFG_0] (main.py 265): INFO Train: [41/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2919 (0.3049)	loss 1.4095 (1.3987)	grad_norm 19.1504 (29.9537)	mem 4879MB
[2022-05-31 04:12:59 MetaFG_0] (main.py 265): INFO Train: [41/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2920 (0.3048)	loss 1.6324 (1.3995)	grad_norm 31.6118 (29.9324)	mem 4879MB
[2022-05-31 04:13:02 MetaFG_0] (main.py 265): INFO Train: [41/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2925 (0.3048)	loss 1.0581 (1.4001)	grad_norm 33.3565 (29.9085)	mem 4879MB
[2022-05-31 04:13:05 MetaFG_0] (main.py 265): INFO Train: [41/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2933 (0.3048)	loss 1.5743 (1.4003)	grad_norm 26.3055 (29.9053)	mem 4879MB
[2022-05-31 04:13:08 MetaFG_0] (main.py 265): INFO Train: [41/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2930 (0.3048)	loss 1.6166 (1.4007)	grad_norm 25.7401 (29.9113)	mem 4879MB
[2022-05-31 04:13:11 MetaFG_0] (main.py 265): INFO Train: [41/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2946 (0.3048)	loss 0.8101 (1.4009)	grad_norm 22.1679 (29.8767)	mem 4879MB
[2022-05-31 04:13:14 MetaFG_0] (main.py 265): INFO Train: [41/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2933 (0.3048)	loss 1.6303 (1.4010)	grad_norm 28.1900 (29.8405)	mem 4879MB
[2022-05-31 04:13:17 MetaFG_0] (main.py 265): INFO Train: [41/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2982 (0.3048)	loss 1.4078 (1.4009)	grad_norm 37.5604 (29.8283)	mem 4879MB
[2022-05-31 04:13:20 MetaFG_0] (main.py 265): INFO Train: [41/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2920 (0.3048)	loss 1.5156 (1.4013)	grad_norm 21.0830 (29.7960)	mem 4879MB
[2022-05-31 04:13:23 MetaFG_0] (main.py 265): INFO Train: [41/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2978 (0.3048)	loss 1.5193 (1.4011)	grad_norm 39.8181 (29.8019)	mem 4879MB
[2022-05-31 04:13:26 MetaFG_0] (main.py 265): INFO Train: [41/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2999 (0.3048)	loss 0.9736 (1.4007)	grad_norm 39.2606 (29.8004)	mem 4879MB
[2022-05-31 04:13:29 MetaFG_0] (main.py 265): INFO Train: [41/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2926 (0.3048)	loss 1.4953 (1.4008)	grad_norm 30.1988 (29.7748)	mem 4879MB
[2022-05-31 04:13:30 MetaFG_0] (main.py 272): INFO EPOCH 41 training takes 0:07:56
[2022-05-31 04:13:30 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_41.pth saving......
[2022-05-31 04:13:31 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_41.pth saved !!!
[2022-05-31 04:13:31 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:13:32 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:13:32 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:13:33 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.754 (0.754)	Loss 0.6211 (0.6211)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 04:13:34 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.155)	Loss 0.6078 (0.6042)	Acc@1 90.625 (88.636)	Acc@5 100.000 (99.716)	Mem 4879MB
[2022-05-31 04:13:35 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.127)	Loss 0.7006 (0.6299)	Acc@1 81.250 (87.351)	Acc@5 100.000 (99.256)	Mem 4879MB
[2022-05-31 04:13:36 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.117)	Loss 0.6695 (0.6130)	Acc@1 84.375 (87.399)	Acc@5 100.000 (99.395)	Mem 4879MB
[2022-05-31 04:13:37 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.111)	Loss 1.1626 (0.6337)	Acc@1 68.750 (86.204)	Acc@5 96.875 (99.085)	Mem 4879MB
[2022-05-31 04:13:38 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.088 (0.108)	Loss 0.7127 (0.6358)	Acc@1 87.500 (86.397)	Acc@5 96.875 (99.020)	Mem 4879MB
[2022-05-31 04:13:39 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.106)	Loss 0.8077 (0.6285)	Acc@1 71.875 (86.424)	Acc@5 100.000 (99.027)	Mem 4879MB
[2022-05-31 04:13:40 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.105)	Loss 0.6084 (0.6356)	Acc@1 87.500 (86.136)	Acc@5 96.875 (98.988)	Mem 4879MB
[2022-05-31 04:13:41 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.103)	Loss 0.6568 (0.6408)	Acc@1 87.500 (86.111)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 04:13:41 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.100 (0.102)	Loss 0.8297 (0.6577)	Acc@1 75.000 (85.577)	Acc@5 100.000 (98.764)	Mem 4879MB
[2022-05-31 04:13:42 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.092 (0.101)	Loss 0.4487 (0.6733)	Acc@1 90.625 (84.963)	Acc@5 100.000 (98.484)	Mem 4879MB
[2022-05-31 04:13:43 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.8128 (0.6772)	Acc@1 90.625 (84.825)	Acc@5 93.750 (98.339)	Mem 4879MB
[2022-05-31 04:13:44 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.100)	Loss 0.4369 (0.6758)	Acc@1 96.875 (84.840)	Acc@5 100.000 (98.347)	Mem 4879MB
[2022-05-31 04:13:45 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.100 (0.100)	Loss 0.5942 (0.6762)	Acc@1 90.625 (84.828)	Acc@5 100.000 (98.330)	Mem 4879MB
[2022-05-31 04:13:46 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.5355 (0.6717)	Acc@1 93.750 (85.062)	Acc@5 100.000 (98.338)	Mem 4879MB
[2022-05-31 04:13:47 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.098 (0.099)	Loss 0.5160 (0.6717)	Acc@1 81.250 (84.913)	Acc@5 100.000 (98.324)	Mem 4879MB
[2022-05-31 04:13:48 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.099)	Loss 0.5767 (0.6697)	Acc@1 87.500 (85.113)	Acc@5 96.875 (98.273)	Mem 4879MB
[2022-05-31 04:13:49 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.093 (0.099)	Loss 0.7336 (0.6721)	Acc@1 81.250 (85.033)	Acc@5 96.875 (98.264)	Mem 4879MB
[2022-05-31 04:13:50 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.9343 (0.6689)	Acc@1 71.875 (85.048)	Acc@5 100.000 (98.308)	Mem 4879MB
[2022-05-31 04:13:51 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.098)	Loss 0.5748 (0.6675)	Acc@1 87.500 (85.079)	Acc@5 100.000 (98.331)	Mem 4879MB
[2022-05-31 04:13:52 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.098)	Loss 0.7029 (0.6691)	Acc@1 84.375 (85.059)	Acc@5 100.000 (98.336)	Mem 4879MB
[2022-05-31 04:13:53 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.098)	Loss 0.7938 (0.6714)	Acc@1 84.375 (84.834)	Acc@5 93.750 (98.326)	Mem 4879MB
[2022-05-31 04:13:54 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.8939 (0.6737)	Acc@1 78.125 (84.714)	Acc@5 96.875 (98.331)	Mem 4879MB
[2022-05-31 04:13:55 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 0.7170 (0.6734)	Acc@1 87.500 (84.808)	Acc@5 100.000 (98.323)	Mem 4879MB
[2022-05-31 04:13:56 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.095 (0.097)	Loss 0.7151 (0.6731)	Acc@1 81.250 (84.777)	Acc@5 100.000 (98.340)	Mem 4879MB
[2022-05-31 04:13:57 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.8069 (0.6732)	Acc@1 81.250 (84.699)	Acc@5 100.000 (98.357)	Mem 4879MB
[2022-05-31 04:13:57 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.8429 (0.6750)	Acc@1 81.250 (84.662)	Acc@5 93.750 (98.336)	Mem 4879MB
[2022-05-31 04:13:58 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.095 (0.097)	Loss 0.4513 (0.6745)	Acc@1 93.750 (84.675)	Acc@5 100.000 (98.363)	Mem 4879MB
[2022-05-31 04:13:59 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.6900 (0.6731)	Acc@1 81.250 (84.709)	Acc@5 100.000 (98.399)	Mem 4879MB
[2022-05-31 04:14:00 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.6676 (0.6729)	Acc@1 87.500 (84.729)	Acc@5 100.000 (98.411)	Mem 4879MB
[2022-05-31 04:14:01 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.5732 (0.6680)	Acc@1 87.500 (84.904)	Acc@5 96.875 (98.422)	Mem 4879MB
[2022-05-31 04:14:02 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.4676 (0.6676)	Acc@1 96.875 (84.948)	Acc@5 100.000 (98.412)	Mem 4879MB
[2022-05-31 04:14:02 MetaFG_0] (main.py 330): INFO  * Acc@1 84.970 Acc@5 98.410
[2022-05-31 04:14:02 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.0%
[2022-05-31 04:14:02 MetaFG_0] (main.py 171): INFO Max accuracy: 85.11%
[2022-05-31 04:14:03 MetaFG_0] (main.py 265): INFO Train: [42/300][0/1562]	eta 0:26:34 lr 0.000006	time 1.0206 (1.0206)	loss 1.2647 (1.2647)	grad_norm 15.1500 (15.1500)	mem 4879MB
[2022-05-31 04:14:06 MetaFG_0] (main.py 265): INFO Train: [42/300][10/1562]	eta 0:09:43 lr 0.000006	time 0.2934 (0.3758)	loss 1.2802 (1.3629)	grad_norm 57.8959 (27.6803)	mem 4879MB
[2022-05-31 04:14:09 MetaFG_0] (main.py 265): INFO Train: [42/300][20/1562]	eta 0:08:46 lr 0.000006	time 0.2952 (0.3414)	loss 1.3704 (1.3524)	grad_norm 22.2460 (27.2274)	mem 4879MB
[2022-05-31 04:14:13 MetaFG_0] (main.py 265): INFO Train: [42/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2929 (0.3296)	loss 0.9562 (1.3620)	grad_norm 23.5684 (28.5390)	mem 4879MB
[2022-05-31 04:14:16 MetaFG_0] (main.py 265): INFO Train: [42/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2980 (0.3233)	loss 1.4495 (1.3591)	grad_norm 19.4807 (28.1332)	mem 4879MB
[2022-05-31 04:14:19 MetaFG_0] (main.py 265): INFO Train: [42/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2990 (0.3200)	loss 1.8361 (1.3497)	grad_norm 40.1089 (28.8914)	mem 4879MB
[2022-05-31 04:14:22 MetaFG_0] (main.py 265): INFO Train: [42/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2940 (0.3174)	loss 1.5485 (1.3552)	grad_norm 21.5442 (28.9909)	mem 4879MB
[2022-05-31 04:14:25 MetaFG_0] (main.py 265): INFO Train: [42/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2997 (0.3156)	loss 1.4384 (1.3633)	grad_norm 20.3295 (29.2488)	mem 4879MB
[2022-05-31 04:14:28 MetaFG_0] (main.py 265): INFO Train: [42/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2937 (0.3144)	loss 1.2678 (1.3651)	grad_norm 28.7507 (29.0504)	mem 4879MB
[2022-05-31 04:14:31 MetaFG_0] (main.py 265): INFO Train: [42/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2985 (0.3134)	loss 1.5511 (1.3651)	grad_norm 22.3950 (28.9255)	mem 4879MB
[2022-05-31 04:14:34 MetaFG_0] (main.py 265): INFO Train: [42/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2933 (0.3124)	loss 1.5277 (1.3679)	grad_norm 27.8338 (28.6390)	mem 4879MB
[2022-05-31 04:14:37 MetaFG_0] (main.py 265): INFO Train: [42/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2998 (0.3118)	loss 1.0568 (1.3656)	grad_norm 30.4499 (29.0790)	mem 4879MB
[2022-05-31 04:14:40 MetaFG_0] (main.py 265): INFO Train: [42/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2921 (0.3112)	loss 1.3289 (1.3720)	grad_norm 31.0282 (28.7938)	mem 4879MB
[2022-05-31 04:14:43 MetaFG_0] (main.py 265): INFO Train: [42/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2986 (0.3107)	loss 1.5404 (1.3621)	grad_norm 23.1628 (29.4780)	mem 4879MB
[2022-05-31 04:14:46 MetaFG_0] (main.py 265): INFO Train: [42/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2943 (0.3104)	loss 1.1377 (1.3570)	grad_norm 24.4677 (29.2788)	mem 4879MB
[2022-05-31 04:14:49 MetaFG_0] (main.py 265): INFO Train: [42/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2941 (0.3101)	loss 1.6396 (1.3650)	grad_norm 29.8048 (29.2356)	mem 4879MB
[2022-05-31 04:14:52 MetaFG_0] (main.py 265): INFO Train: [42/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2945 (0.3098)	loss 1.5413 (1.3652)	grad_norm 19.8498 (29.3257)	mem 4879MB
[2022-05-31 04:14:55 MetaFG_0] (main.py 265): INFO Train: [42/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2998 (0.3095)	loss 0.9905 (1.3655)	grad_norm 40.2213 (29.3123)	mem 4879MB
[2022-05-31 04:14:58 MetaFG_0] (main.py 265): INFO Train: [42/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2968 (0.3092)	loss 1.4047 (1.3614)	grad_norm 45.1245 (29.5115)	mem 4879MB
[2022-05-31 04:15:01 MetaFG_0] (main.py 265): INFO Train: [42/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2933 (0.3089)	loss 1.2378 (1.3718)	grad_norm 29.0776 (29.7071)	mem 4879MB
[2022-05-31 04:15:04 MetaFG_0] (main.py 265): INFO Train: [42/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2939 (0.3087)	loss 1.7994 (1.3765)	grad_norm 50.8581 (30.0702)	mem 4879MB
[2022-05-31 04:15:07 MetaFG_0] (main.py 265): INFO Train: [42/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2941 (0.3084)	loss 1.6463 (1.3728)	grad_norm 42.8713 (30.1609)	mem 4879MB
[2022-05-31 04:15:10 MetaFG_0] (main.py 265): INFO Train: [42/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2967 (0.3084)	loss 1.5673 (1.3707)	grad_norm 30.6575 (30.1029)	mem 4879MB
[2022-05-31 04:15:14 MetaFG_0] (main.py 265): INFO Train: [42/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.3008 (0.3083)	loss 1.5300 (1.3658)	grad_norm 49.8126 (29.9945)	mem 4879MB
[2022-05-31 04:15:17 MetaFG_0] (main.py 265): INFO Train: [42/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2984 (0.3082)	loss 1.4076 (1.3654)	grad_norm 20.7844 (29.9324)	mem 4879MB
[2022-05-31 04:15:20 MetaFG_0] (main.py 265): INFO Train: [42/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2928 (0.3080)	loss 1.4331 (1.3666)	grad_norm 35.6257 (29.9300)	mem 4879MB
[2022-05-31 04:15:23 MetaFG_0] (main.py 265): INFO Train: [42/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2920 (0.3079)	loss 1.4287 (1.3664)	grad_norm 36.7318 (30.0733)	mem 4879MB
[2022-05-31 04:15:26 MetaFG_0] (main.py 265): INFO Train: [42/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2926 (0.3078)	loss 1.6406 (1.3688)	grad_norm 29.8122 (30.0658)	mem 4879MB
[2022-05-31 04:15:29 MetaFG_0] (main.py 265): INFO Train: [42/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2939 (0.3077)	loss 1.6830 (1.3700)	grad_norm 26.3427 (30.0113)	mem 4879MB
[2022-05-31 04:15:32 MetaFG_0] (main.py 265): INFO Train: [42/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.3003 (0.3076)	loss 1.4851 (1.3696)	grad_norm 18.3077 (29.9249)	mem 4879MB
[2022-05-31 04:15:35 MetaFG_0] (main.py 265): INFO Train: [42/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2992 (0.3075)	loss 1.6659 (1.3725)	grad_norm 26.2575 (30.0357)	mem 4879MB
[2022-05-31 04:15:38 MetaFG_0] (main.py 265): INFO Train: [42/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2919 (0.3074)	loss 1.6186 (1.3767)	grad_norm 28.3279 (29.8483)	mem 4879MB
[2022-05-31 04:15:41 MetaFG_0] (main.py 265): INFO Train: [42/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2921 (0.3073)	loss 1.3264 (1.3769)	grad_norm 31.7117 (29.8892)	mem 4879MB
[2022-05-31 04:15:44 MetaFG_0] (main.py 265): INFO Train: [42/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2930 (0.3072)	loss 1.6470 (1.3785)	grad_norm 31.3367 (29.9110)	mem 4879MB
[2022-05-31 04:15:47 MetaFG_0] (main.py 265): INFO Train: [42/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2987 (0.3072)	loss 1.5859 (1.3808)	grad_norm 38.3608 (29.8425)	mem 4879MB
[2022-05-31 04:15:50 MetaFG_0] (main.py 265): INFO Train: [42/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2925 (0.3071)	loss 1.4303 (1.3810)	grad_norm 23.9524 (29.8142)	mem 4879MB
[2022-05-31 04:15:53 MetaFG_0] (main.py 265): INFO Train: [42/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2921 (0.3070)	loss 1.2281 (1.3784)	grad_norm 31.7071 (29.7781)	mem 4879MB
[2022-05-31 04:15:56 MetaFG_0] (main.py 265): INFO Train: [42/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.3007 (0.3070)	loss 1.4742 (1.3778)	grad_norm 30.1576 (29.7194)	mem 4879MB
[2022-05-31 04:15:59 MetaFG_0] (main.py 265): INFO Train: [42/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2926 (0.3069)	loss 1.2403 (1.3763)	grad_norm 20.9228 (29.5599)	mem 4879MB
[2022-05-31 04:16:02 MetaFG_0] (main.py 265): INFO Train: [42/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2930 (0.3069)	loss 1.2845 (1.3791)	grad_norm 16.3623 (29.4968)	mem 4879MB
[2022-05-31 04:16:05 MetaFG_0] (main.py 265): INFO Train: [42/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3322 (0.3072)	loss 1.1524 (1.3789)	grad_norm 25.0680 (29.4173)	mem 4879MB
[2022-05-31 04:16:09 MetaFG_0] (main.py 265): INFO Train: [42/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2930 (0.3075)	loss 1.1837 (1.3794)	grad_norm 28.5248 (29.4934)	mem 4879MB
[2022-05-31 04:16:12 MetaFG_0] (main.py 265): INFO Train: [42/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2937 (0.3074)	loss 1.3865 (1.3799)	grad_norm 35.2344 (29.4605)	mem 4879MB
[2022-05-31 04:16:15 MetaFG_0] (main.py 265): INFO Train: [42/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2994 (0.3074)	loss 1.0687 (1.3792)	grad_norm 40.0180 (29.4103)	mem 4879MB
[2022-05-31 04:16:18 MetaFG_0] (main.py 265): INFO Train: [42/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.3010 (0.3074)	loss 1.0719 (1.3793)	grad_norm 30.5087 (29.3534)	mem 4879MB
[2022-05-31 04:16:21 MetaFG_0] (main.py 265): INFO Train: [42/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2966 (0.3074)	loss 1.5529 (1.3809)	grad_norm 53.2402 (29.4285)	mem 4879MB
[2022-05-31 04:16:24 MetaFG_0] (main.py 265): INFO Train: [42/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2943 (0.3074)	loss 1.6379 (1.3816)	grad_norm 37.2401 (29.5184)	mem 4879MB
[2022-05-31 04:16:27 MetaFG_0] (main.py 265): INFO Train: [42/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2955 (0.3074)	loss 1.5086 (1.3815)	grad_norm 39.6151 (29.5287)	mem 4879MB
[2022-05-31 04:16:30 MetaFG_0] (main.py 265): INFO Train: [42/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2941 (0.3073)	loss 1.3232 (1.3851)	grad_norm 41.7223 (29.6369)	mem 4879MB
[2022-05-31 04:16:33 MetaFG_0] (main.py 265): INFO Train: [42/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.3021 (0.3073)	loss 1.7125 (1.3871)	grad_norm 30.4610 (29.7593)	mem 4879MB
[2022-05-31 04:16:36 MetaFG_0] (main.py 265): INFO Train: [42/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2927 (0.3073)	loss 1.7142 (1.3877)	grad_norm 19.9999 (29.7850)	mem 4879MB
[2022-05-31 04:16:39 MetaFG_0] (main.py 265): INFO Train: [42/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2957 (0.3073)	loss 1.2591 (1.3894)	grad_norm 38.0219 (29.7822)	mem 4879MB
[2022-05-31 04:16:42 MetaFG_0] (main.py 265): INFO Train: [42/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2938 (0.3072)	loss 1.5358 (1.3888)	grad_norm 46.2253 (29.8997)	mem 4879MB
[2022-05-31 04:16:45 MetaFG_0] (main.py 265): INFO Train: [42/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2986 (0.3072)	loss 1.3831 (1.3894)	grad_norm 27.7625 (29.8642)	mem 4879MB
[2022-05-31 04:16:48 MetaFG_0] (main.py 265): INFO Train: [42/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2928 (0.3071)	loss 1.6403 (1.3907)	grad_norm 25.5818 (29.8422)	mem 4879MB
[2022-05-31 04:16:52 MetaFG_0] (main.py 265): INFO Train: [42/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2928 (0.3071)	loss 1.4019 (1.3915)	grad_norm 30.0818 (29.8440)	mem 4879MB
[2022-05-31 04:16:55 MetaFG_0] (main.py 265): INFO Train: [42/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2940 (0.3071)	loss 1.4853 (1.3935)	grad_norm 16.7030 (29.9558)	mem 4879MB
[2022-05-31 04:16:58 MetaFG_0] (main.py 265): INFO Train: [42/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2933 (0.3070)	loss 1.4625 (1.3937)	grad_norm 21.4202 (29.9752)	mem 4879MB
[2022-05-31 04:17:01 MetaFG_0] (main.py 265): INFO Train: [42/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2998 (0.3070)	loss 1.1181 (1.3929)	grad_norm 21.8337 (29.9100)	mem 4879MB
[2022-05-31 04:17:04 MetaFG_0] (main.py 265): INFO Train: [42/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.3013 (0.3070)	loss 1.2684 (1.3936)	grad_norm 13.0841 (29.9212)	mem 4879MB
[2022-05-31 04:17:07 MetaFG_0] (main.py 265): INFO Train: [42/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2939 (0.3069)	loss 1.1315 (1.3940)	grad_norm 30.3305 (29.9530)	mem 4879MB
[2022-05-31 04:17:10 MetaFG_0] (main.py 265): INFO Train: [42/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2923 (0.3068)	loss 1.2396 (1.3949)	grad_norm 39.0882 (29.8964)	mem 4879MB
[2022-05-31 04:17:13 MetaFG_0] (main.py 265): INFO Train: [42/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2922 (0.3068)	loss 1.6899 (1.3939)	grad_norm 20.7635 (29.8720)	mem 4879MB
[2022-05-31 04:17:16 MetaFG_0] (main.py 265): INFO Train: [42/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2940 (0.3068)	loss 1.2338 (1.3949)	grad_norm 25.2598 (29.8234)	mem 4879MB
[2022-05-31 04:17:19 MetaFG_0] (main.py 265): INFO Train: [42/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3000 (0.3068)	loss 1.4184 (1.3959)	grad_norm 30.6189 (29.8271)	mem 4879MB
[2022-05-31 04:17:22 MetaFG_0] (main.py 265): INFO Train: [42/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2941 (0.3068)	loss 1.2027 (1.3955)	grad_norm 36.2419 (29.7991)	mem 4879MB
[2022-05-31 04:17:25 MetaFG_0] (main.py 265): INFO Train: [42/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2938 (0.3067)	loss 0.8260 (1.3947)	grad_norm 47.6492 (29.7939)	mem 4879MB
[2022-05-31 04:17:28 MetaFG_0] (main.py 265): INFO Train: [42/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2982 (0.3068)	loss 1.6443 (1.3945)	grad_norm 34.1553 (29.8039)	mem 4879MB
[2022-05-31 04:17:31 MetaFG_0] (main.py 265): INFO Train: [42/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2972 (0.3067)	loss 1.1840 (1.3945)	grad_norm 31.4999 (29.8048)	mem 4879MB
[2022-05-31 04:17:34 MetaFG_0] (main.py 265): INFO Train: [42/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2937 (0.3067)	loss 1.3715 (1.3934)	grad_norm 26.2277 (29.7550)	mem 4879MB
[2022-05-31 04:17:37 MetaFG_0] (main.py 265): INFO Train: [42/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2953 (0.3067)	loss 1.3139 (1.3942)	grad_norm 47.0778 (29.7678)	mem 4879MB
[2022-05-31 04:17:40 MetaFG_0] (main.py 265): INFO Train: [42/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2942 (0.3066)	loss 1.1453 (1.3917)	grad_norm 19.3380 (29.7202)	mem 4879MB
[2022-05-31 04:17:43 MetaFG_0] (main.py 265): INFO Train: [42/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2994 (0.3066)	loss 1.2791 (1.3908)	grad_norm 20.9655 (29.6819)	mem 4879MB
[2022-05-31 04:17:46 MetaFG_0] (main.py 265): INFO Train: [42/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2933 (0.3066)	loss 1.2673 (1.3912)	grad_norm 23.9927 (29.7111)	mem 4879MB
[2022-05-31 04:17:50 MetaFG_0] (main.py 265): INFO Train: [42/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2946 (0.3066)	loss 1.5405 (1.3917)	grad_norm 29.6555 (29.6386)	mem 4879MB
[2022-05-31 04:17:53 MetaFG_0] (main.py 265): INFO Train: [42/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2994 (0.3066)	loss 1.2547 (1.3923)	grad_norm 21.6416 (29.6336)	mem 4879MB
[2022-05-31 04:17:56 MetaFG_0] (main.py 265): INFO Train: [42/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2985 (0.3066)	loss 1.7981 (1.3936)	grad_norm 34.1377 (29.6549)	mem 4879MB
[2022-05-31 04:17:59 MetaFG_0] (main.py 265): INFO Train: [42/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2939 (0.3065)	loss 1.0007 (1.3939)	grad_norm 29.3446 (29.6448)	mem 4879MB
[2022-05-31 04:18:02 MetaFG_0] (main.py 265): INFO Train: [42/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2926 (0.3065)	loss 1.0006 (1.3929)	grad_norm 33.6045 (29.6110)	mem 4879MB
[2022-05-31 04:18:05 MetaFG_0] (main.py 265): INFO Train: [42/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2942 (0.3065)	loss 1.5622 (1.3942)	grad_norm 34.7310 (29.5787)	mem 4879MB
[2022-05-31 04:18:08 MetaFG_0] (main.py 265): INFO Train: [42/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2932 (0.3064)	loss 1.2149 (1.3949)	grad_norm 23.2104 (29.5246)	mem 4879MB
[2022-05-31 04:18:11 MetaFG_0] (main.py 265): INFO Train: [42/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2984 (0.3064)	loss 1.4926 (1.3946)	grad_norm 32.1434 (29.4626)	mem 4879MB
[2022-05-31 04:18:14 MetaFG_0] (main.py 265): INFO Train: [42/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2972 (0.3064)	loss 1.0530 (1.3928)	grad_norm 53.4085 (29.4596)	mem 4879MB
[2022-05-31 04:18:17 MetaFG_0] (main.py 265): INFO Train: [42/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.3002 (0.3063)	loss 1.4396 (1.3942)	grad_norm 22.3789 (29.3994)	mem 4879MB
[2022-05-31 04:18:20 MetaFG_0] (main.py 265): INFO Train: [42/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2935 (0.3064)	loss 1.3568 (1.3940)	grad_norm 30.0915 (29.3980)	mem 4879MB
[2022-05-31 04:18:23 MetaFG_0] (main.py 265): INFO Train: [42/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2988 (0.3064)	loss 1.5990 (1.3926)	grad_norm 37.2069 (29.4298)	mem 4879MB
[2022-05-31 04:18:26 MetaFG_0] (main.py 265): INFO Train: [42/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2995 (0.3064)	loss 1.3371 (1.3928)	grad_norm 20.2835 (29.4213)	mem 4879MB
[2022-05-31 04:18:29 MetaFG_0] (main.py 265): INFO Train: [42/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2924 (0.3063)	loss 1.6150 (1.3925)	grad_norm 35.1194 (29.5021)	mem 4879MB
[2022-05-31 04:18:32 MetaFG_0] (main.py 265): INFO Train: [42/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2944 (0.3063)	loss 1.6462 (1.3923)	grad_norm 38.1712 (29.5227)	mem 4879MB
[2022-05-31 04:18:35 MetaFG_0] (main.py 265): INFO Train: [42/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.3007 (0.3063)	loss 1.3854 (1.3919)	grad_norm 21.1045 (29.5182)	mem 4879MB
[2022-05-31 04:18:38 MetaFG_0] (main.py 265): INFO Train: [42/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2918 (0.3063)	loss 1.4188 (1.3916)	grad_norm 30.6290 (29.5970)	mem 4879MB
[2022-05-31 04:18:41 MetaFG_0] (main.py 265): INFO Train: [42/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2997 (0.3063)	loss 1.4437 (1.3920)	grad_norm 34.4686 (29.5848)	mem 4879MB
[2022-05-31 04:18:44 MetaFG_0] (main.py 265): INFO Train: [42/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2949 (0.3063)	loss 0.9522 (1.3928)	grad_norm 20.2372 (29.5750)	mem 4879MB
[2022-05-31 04:18:48 MetaFG_0] (main.py 265): INFO Train: [42/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2997 (0.3063)	loss 1.4109 (1.3929)	grad_norm 21.4542 (29.5919)	mem 4879MB
[2022-05-31 04:18:51 MetaFG_0] (main.py 265): INFO Train: [42/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2926 (0.3063)	loss 1.3450 (1.3928)	grad_norm 25.5121 (29.6404)	mem 4879MB
[2022-05-31 04:18:54 MetaFG_0] (main.py 265): INFO Train: [42/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2938 (0.3063)	loss 1.3057 (1.3924)	grad_norm 26.7502 (29.6357)	mem 4879MB
[2022-05-31 04:18:57 MetaFG_0] (main.py 265): INFO Train: [42/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2924 (0.3063)	loss 1.4685 (1.3931)	grad_norm 25.7328 (29.5992)	mem 4879MB
[2022-05-31 04:19:00 MetaFG_0] (main.py 265): INFO Train: [42/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2933 (0.3063)	loss 1.5197 (1.3930)	grad_norm 30.0173 (29.5834)	mem 4879MB
[2022-05-31 04:19:03 MetaFG_0] (main.py 265): INFO Train: [42/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.3003 (0.3062)	loss 1.4826 (1.3940)	grad_norm 25.9001 (29.6097)	mem 4879MB
[2022-05-31 04:19:06 MetaFG_0] (main.py 265): INFO Train: [42/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.3020 (0.3062)	loss 1.3687 (1.3930)	grad_norm 14.9972 (29.5681)	mem 4879MB
[2022-05-31 04:19:09 MetaFG_0] (main.py 265): INFO Train: [42/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.3045 (0.3062)	loss 1.3530 (1.3927)	grad_norm 25.4823 (29.5955)	mem 4879MB
[2022-05-31 04:19:12 MetaFG_0] (main.py 265): INFO Train: [42/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2982 (0.3062)	loss 1.1969 (1.3923)	grad_norm 31.0817 (29.5868)	mem 4879MB
[2022-05-31 04:19:15 MetaFG_0] (main.py 265): INFO Train: [42/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2983 (0.3062)	loss 1.1125 (1.3909)	grad_norm 21.2065 (29.5963)	mem 4879MB
[2022-05-31 04:19:18 MetaFG_0] (main.py 265): INFO Train: [42/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2986 (0.3062)	loss 1.0498 (1.3915)	grad_norm 50.6132 (29.5600)	mem 4879MB
[2022-05-31 04:19:21 MetaFG_0] (main.py 265): INFO Train: [42/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2922 (0.3062)	loss 1.2096 (1.3907)	grad_norm 23.8768 (29.5764)	mem 4879MB
[2022-05-31 04:19:24 MetaFG_0] (main.py 265): INFO Train: [42/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3000 (0.3062)	loss 1.7116 (1.3911)	grad_norm 20.1670 (29.5493)	mem 4879MB
[2022-05-31 04:19:27 MetaFG_0] (main.py 265): INFO Train: [42/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2946 (0.3061)	loss 1.4255 (1.3909)	grad_norm 18.9648 (29.5547)	mem 4879MB
[2022-05-31 04:19:30 MetaFG_0] (main.py 265): INFO Train: [42/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2982 (0.3061)	loss 1.6659 (1.3916)	grad_norm 44.7369 (29.6006)	mem 4879MB
[2022-05-31 04:19:33 MetaFG_0] (main.py 265): INFO Train: [42/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2939 (0.3061)	loss 1.7436 (1.3917)	grad_norm 47.6089 (29.5827)	mem 4879MB
[2022-05-31 04:19:36 MetaFG_0] (main.py 265): INFO Train: [42/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2926 (0.3061)	loss 1.4590 (1.3914)	grad_norm 22.3367 (29.5695)	mem 4879MB
[2022-05-31 04:19:39 MetaFG_0] (main.py 265): INFO Train: [42/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2932 (0.3061)	loss 1.7033 (1.3924)	grad_norm 37.8475 (29.5675)	mem 4879MB
[2022-05-31 04:19:42 MetaFG_0] (main.py 265): INFO Train: [42/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2992 (0.3061)	loss 1.6043 (1.3930)	grad_norm 40.0469 (29.5799)	mem 4879MB
[2022-05-31 04:19:45 MetaFG_0] (main.py 265): INFO Train: [42/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2940 (0.3061)	loss 1.0025 (1.3935)	grad_norm 29.4217 (29.6310)	mem 4879MB
[2022-05-31 04:19:49 MetaFG_0] (main.py 265): INFO Train: [42/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2943 (0.3061)	loss 1.6048 (1.3940)	grad_norm 22.7451 (29.6485)	mem 4879MB
[2022-05-31 04:19:52 MetaFG_0] (main.py 265): INFO Train: [42/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2926 (0.3062)	loss 1.3426 (1.3941)	grad_norm 18.6764 (29.6486)	mem 4879MB
[2022-05-31 04:19:55 MetaFG_0] (main.py 265): INFO Train: [42/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2985 (0.3061)	loss 1.0191 (1.3944)	grad_norm 37.9671 (29.6611)	mem 4879MB
[2022-05-31 04:19:58 MetaFG_0] (main.py 265): INFO Train: [42/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2993 (0.3061)	loss 1.1589 (1.3940)	grad_norm 25.9775 (29.6395)	mem 4879MB
[2022-05-31 04:20:01 MetaFG_0] (main.py 265): INFO Train: [42/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2987 (0.3062)	loss 1.0139 (1.3943)	grad_norm 15.6272 (29.6279)	mem 4879MB
[2022-05-31 04:20:04 MetaFG_0] (main.py 265): INFO Train: [42/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2930 (0.3062)	loss 1.3719 (1.3942)	grad_norm 25.4350 (29.6366)	mem 4879MB
[2022-05-31 04:20:07 MetaFG_0] (main.py 265): INFO Train: [42/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2987 (0.3062)	loss 1.5169 (1.3944)	grad_norm 38.3928 (29.6184)	mem 4879MB
[2022-05-31 04:20:10 MetaFG_0] (main.py 265): INFO Train: [42/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2986 (0.3062)	loss 1.3960 (1.3941)	grad_norm 38.1789 (29.6474)	mem 4879MB
[2022-05-31 04:20:13 MetaFG_0] (main.py 265): INFO Train: [42/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2924 (0.3062)	loss 0.9063 (1.3951)	grad_norm 12.9161 (29.6336)	mem 4879MB
[2022-05-31 04:20:16 MetaFG_0] (main.py 265): INFO Train: [42/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2934 (0.3062)	loss 1.4799 (1.3963)	grad_norm 17.8153 (29.6344)	mem 4879MB
[2022-05-31 04:20:19 MetaFG_0] (main.py 265): INFO Train: [42/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3006 (0.3062)	loss 1.5898 (1.3976)	grad_norm 16.2042 (29.6211)	mem 4879MB
[2022-05-31 04:20:22 MetaFG_0] (main.py 265): INFO Train: [42/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2938 (0.3062)	loss 1.2716 (1.3972)	grad_norm 27.5795 (29.6300)	mem 4879MB
[2022-05-31 04:20:25 MetaFG_0] (main.py 265): INFO Train: [42/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3000 (0.3062)	loss 1.3483 (1.3988)	grad_norm 17.7014 (29.6093)	mem 4879MB
[2022-05-31 04:20:28 MetaFG_0] (main.py 265): INFO Train: [42/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2958 (0.3061)	loss 1.1683 (1.3987)	grad_norm 36.2393 (29.6036)	mem 4879MB
[2022-05-31 04:20:31 MetaFG_0] (main.py 265): INFO Train: [42/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2953 (0.3061)	loss 1.6213 (1.3987)	grad_norm 28.6506 (29.5948)	mem 4879MB
[2022-05-31 04:20:34 MetaFG_0] (main.py 265): INFO Train: [42/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2953 (0.3061)	loss 1.5743 (1.3989)	grad_norm 27.9806 (29.6138)	mem 4879MB
[2022-05-31 04:20:38 MetaFG_0] (main.py 265): INFO Train: [42/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2985 (0.3061)	loss 0.9557 (1.3984)	grad_norm 26.2020 (29.6371)	mem 4879MB
[2022-05-31 04:20:41 MetaFG_0] (main.py 265): INFO Train: [42/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2937 (0.3061)	loss 0.9607 (1.3978)	grad_norm 38.5936 (29.6510)	mem 4879MB
[2022-05-31 04:20:44 MetaFG_0] (main.py 265): INFO Train: [42/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2938 (0.3061)	loss 1.4413 (1.3985)	grad_norm 27.4527 (29.6820)	mem 4879MB
[2022-05-31 04:20:47 MetaFG_0] (main.py 265): INFO Train: [42/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2987 (0.3061)	loss 1.0244 (1.3984)	grad_norm 27.9755 (29.6817)	mem 4879MB
[2022-05-31 04:20:50 MetaFG_0] (main.py 265): INFO Train: [42/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2929 (0.3061)	loss 1.1762 (1.3981)	grad_norm 30.0349 (29.6983)	mem 4879MB
[2022-05-31 04:20:53 MetaFG_0] (main.py 265): INFO Train: [42/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2932 (0.3061)	loss 1.1816 (1.3983)	grad_norm 24.6610 (29.7041)	mem 4879MB
[2022-05-31 04:20:56 MetaFG_0] (main.py 265): INFO Train: [42/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3194 (0.3061)	loss 0.7789 (1.3987)	grad_norm 26.9565 (29.7194)	mem 4879MB
[2022-05-31 04:20:59 MetaFG_0] (main.py 265): INFO Train: [42/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3003 (0.3062)	loss 1.7788 (1.3983)	grad_norm 30.9530 (29.7205)	mem 4879MB
[2022-05-31 04:21:02 MetaFG_0] (main.py 265): INFO Train: [42/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2996 (0.3063)	loss 1.1151 (1.3984)	grad_norm 30.8017 (29.7234)	mem 4879MB
[2022-05-31 04:21:05 MetaFG_0] (main.py 265): INFO Train: [42/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2941 (0.3063)	loss 1.3380 (1.3985)	grad_norm 57.2415 (29.7589)	mem 4879MB
[2022-05-31 04:21:08 MetaFG_0] (main.py 265): INFO Train: [42/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2940 (0.3062)	loss 1.2953 (1.3983)	grad_norm 23.3439 (29.7802)	mem 4879MB
[2022-05-31 04:21:11 MetaFG_0] (main.py 265): INFO Train: [42/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2942 (0.3062)	loss 1.1731 (1.3978)	grad_norm 28.3201 (29.7885)	mem 4879MB
[2022-05-31 04:21:14 MetaFG_0] (main.py 265): INFO Train: [42/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2930 (0.3062)	loss 1.4606 (1.3977)	grad_norm 35.5085 (29.7554)	mem 4879MB
[2022-05-31 04:21:17 MetaFG_0] (main.py 265): INFO Train: [42/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2949 (0.3062)	loss 1.0361 (1.3971)	grad_norm 48.3489 (29.7608)	mem 4879MB
[2022-05-31 04:21:20 MetaFG_0] (main.py 265): INFO Train: [42/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2939 (0.3062)	loss 1.4807 (1.3969)	grad_norm 22.6271 (29.8215)	mem 4879MB
[2022-05-31 04:21:24 MetaFG_0] (main.py 265): INFO Train: [42/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2939 (0.3062)	loss 1.1772 (1.3971)	grad_norm 33.4185 (29.8292)	mem 4879MB
[2022-05-31 04:21:27 MetaFG_0] (main.py 265): INFO Train: [42/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2928 (0.3062)	loss 1.6281 (1.3970)	grad_norm 27.7729 (29.8514)	mem 4879MB
[2022-05-31 04:21:30 MetaFG_0] (main.py 265): INFO Train: [42/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3022 (0.3062)	loss 1.6475 (1.3972)	grad_norm 33.5371 (29.8452)	mem 4879MB
[2022-05-31 04:21:33 MetaFG_0] (main.py 265): INFO Train: [42/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2989 (0.3062)	loss 1.3890 (1.3972)	grad_norm 29.6037 (29.8168)	mem 4879MB
[2022-05-31 04:21:36 MetaFG_0] (main.py 265): INFO Train: [42/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2944 (0.3062)	loss 1.5249 (1.3962)	grad_norm 21.0714 (29.8145)	mem 4879MB
[2022-05-31 04:21:39 MetaFG_0] (main.py 265): INFO Train: [42/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.3004 (0.3062)	loss 1.3311 (1.3960)	grad_norm 44.5808 (29.8262)	mem 4879MB
[2022-05-31 04:21:42 MetaFG_0] (main.py 265): INFO Train: [42/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3016 (0.3062)	loss 1.4386 (1.3958)	grad_norm 23.6019 (29.8328)	mem 4879MB
[2022-05-31 04:21:45 MetaFG_0] (main.py 265): INFO Train: [42/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2991 (0.3062)	loss 1.2393 (1.3945)	grad_norm 22.0828 (29.8220)	mem 4879MB
[2022-05-31 04:21:48 MetaFG_0] (main.py 265): INFO Train: [42/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2929 (0.3062)	loss 1.5750 (1.3945)	grad_norm 27.9260 (29.7957)	mem 4879MB
[2022-05-31 04:21:51 MetaFG_0] (main.py 265): INFO Train: [42/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.3011 (0.3062)	loss 1.3055 (1.3941)	grad_norm 33.1429 (29.8092)	mem 4879MB
[2022-05-31 04:21:54 MetaFG_0] (main.py 265): INFO Train: [42/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2980 (0.3062)	loss 1.6673 (1.3938)	grad_norm 25.0502 (29.8268)	mem 4879MB
[2022-05-31 04:21:57 MetaFG_0] (main.py 265): INFO Train: [42/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2939 (0.3061)	loss 1.1948 (1.3940)	grad_norm 17.9113 (29.8092)	mem 4879MB
[2022-05-31 04:22:00 MetaFG_0] (main.py 265): INFO Train: [42/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2920 (0.3061)	loss 1.3468 (1.3937)	grad_norm 18.8827 (29.8179)	mem 4879MB
[2022-05-31 04:22:01 MetaFG_0] (main.py 272): INFO EPOCH 42 training takes 0:07:58
[2022-05-31 04:22:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_42.pth saving......
[2022-05-31 04:22:01 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_42.pth saved !!!
[2022-05-31 04:22:01 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:22:03 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:22:03 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:22:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.642 (0.642)	Loss 0.5197 (0.5197)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 04:22:04 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.144)	Loss 0.7236 (0.6980)	Acc@1 90.625 (83.807)	Acc@5 100.000 (99.148)	Mem 4879MB
[2022-05-31 04:22:05 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.088 (0.120)	Loss 0.5675 (0.6622)	Acc@1 87.500 (86.012)	Acc@5 100.000 (99.256)	Mem 4879MB
[2022-05-31 04:22:06 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.112)	Loss 0.6352 (0.6685)	Acc@1 87.500 (85.383)	Acc@5 100.000 (99.294)	Mem 4879MB
[2022-05-31 04:22:07 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.107)	Loss 0.7009 (0.6580)	Acc@1 81.250 (85.747)	Acc@5 93.750 (99.162)	Mem 4879MB
[2022-05-31 04:22:08 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.088 (0.105)	Loss 0.5678 (0.6658)	Acc@1 90.625 (85.846)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 04:22:09 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.092 (0.103)	Loss 0.7484 (0.6581)	Acc@1 84.375 (86.168)	Acc@5 96.875 (98.975)	Mem 4879MB
[2022-05-31 04:22:10 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.8198 (0.6731)	Acc@1 81.250 (85.607)	Acc@5 96.875 (98.988)	Mem 4879MB
[2022-05-31 04:22:11 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 0.7170 (0.6847)	Acc@1 87.500 (85.571)	Acc@5 96.875 (98.804)	Mem 4879MB
[2022-05-31 04:22:12 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.100 (0.100)	Loss 0.7648 (0.6811)	Acc@1 75.000 (85.474)	Acc@5 100.000 (98.832)	Mem 4879MB
[2022-05-31 04:22:13 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.100)	Loss 0.5392 (0.6840)	Acc@1 90.625 (85.118)	Acc@5 100.000 (98.793)	Mem 4879MB
[2022-05-31 04:22:14 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.093 (0.099)	Loss 0.8123 (0.6846)	Acc@1 81.250 (85.107)	Acc@5 96.875 (98.789)	Mem 4879MB
[2022-05-31 04:22:15 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.7567 (0.6864)	Acc@1 84.375 (84.917)	Acc@5 96.875 (98.812)	Mem 4879MB
[2022-05-31 04:22:16 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.8392 (0.6882)	Acc@1 87.500 (84.924)	Acc@5 96.875 (98.783)	Mem 4879MB
[2022-05-31 04:22:17 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.3626 (0.6866)	Acc@1 100.000 (85.040)	Acc@5 100.000 (98.737)	Mem 4879MB
[2022-05-31 04:22:18 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.7447 (0.6835)	Acc@1 81.250 (85.161)	Acc@5 96.875 (98.738)	Mem 4879MB
[2022-05-31 04:22:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.098 (0.098)	Loss 0.7965 (0.6825)	Acc@1 81.250 (85.151)	Acc@5 96.875 (98.758)	Mem 4879MB
[2022-05-31 04:22:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.9204 (0.6884)	Acc@1 81.250 (85.051)	Acc@5 93.750 (98.684)	Mem 4879MB
[2022-05-31 04:22:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 0.6297 (0.6880)	Acc@1 84.375 (85.031)	Acc@5 100.000 (98.705)	Mem 4879MB
[2022-05-31 04:22:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 0.8511 (0.6923)	Acc@1 81.250 (84.915)	Acc@5 96.875 (98.675)	Mem 4879MB
[2022-05-31 04:22:22 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.5913 (0.6923)	Acc@1 84.375 (84.919)	Acc@5 100.000 (98.694)	Mem 4879MB
[2022-05-31 04:22:23 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.8182 (0.6926)	Acc@1 78.125 (84.938)	Acc@5 96.875 (98.623)	Mem 4879MB
[2022-05-31 04:22:24 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 0.4924 (0.6925)	Acc@1 93.750 (84.983)	Acc@5 100.000 (98.600)	Mem 4879MB
[2022-05-31 04:22:25 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.7793 (0.6924)	Acc@1 84.375 (84.997)	Acc@5 96.875 (98.593)	Mem 4879MB
[2022-05-31 04:22:26 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.096)	Loss 0.4137 (0.6889)	Acc@1 96.875 (85.140)	Acc@5 100.000 (98.600)	Mem 4879MB
[2022-05-31 04:22:27 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.096)	Loss 0.8927 (0.6892)	Acc@1 75.000 (85.147)	Acc@5 96.875 (98.581)	Mem 4879MB
[2022-05-31 04:22:28 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.092 (0.096)	Loss 0.6878 (0.6920)	Acc@1 84.375 (85.022)	Acc@5 100.000 (98.563)	Mem 4879MB
[2022-05-31 04:22:29 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.6491 (0.6935)	Acc@1 87.500 (85.044)	Acc@5 100.000 (98.536)	Mem 4879MB
[2022-05-31 04:22:30 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.5914 (0.6948)	Acc@1 87.500 (84.976)	Acc@5 100.000 (98.543)	Mem 4879MB
[2022-05-31 04:22:31 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.096)	Loss 0.4623 (0.6967)	Acc@1 93.750 (84.901)	Acc@5 100.000 (98.507)	Mem 4879MB
[2022-05-31 04:22:32 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 0.5584 (0.6944)	Acc@1 81.250 (84.925)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 04:22:33 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8645 (0.6926)	Acc@1 84.375 (85.048)	Acc@5 96.875 (98.513)	Mem 4879MB
[2022-05-31 04:22:33 MetaFG_0] (main.py 330): INFO  * Acc@1 85.000 Acc@5 98.490
[2022-05-31 04:22:33 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.0%
[2022-05-31 04:22:33 MetaFG_0] (main.py 171): INFO Max accuracy: 85.11%
[2022-05-31 04:22:34 MetaFG_0] (main.py 265): INFO Train: [43/300][0/1562]	eta 0:28:42 lr 0.000006	time 1.1027 (1.1027)	loss 1.3227 (1.3227)	grad_norm 29.4141 (29.4141)	mem 4879MB
[2022-05-31 04:22:37 MetaFG_0] (main.py 265): INFO Train: [43/300][10/1562]	eta 0:09:51 lr 0.000006	time 0.2923 (0.3809)	loss 1.5884 (1.4632)	grad_norm 30.7277 (30.0759)	mem 4879MB
[2022-05-31 04:22:40 MetaFG_0] (main.py 265): INFO Train: [43/300][20/1562]	eta 0:08:51 lr 0.000006	time 0.2932 (0.3449)	loss 1.6848 (1.4498)	grad_norm 31.2861 (30.6147)	mem 4879MB
[2022-05-31 04:22:43 MetaFG_0] (main.py 265): INFO Train: [43/300][30/1562]	eta 0:08:29 lr 0.000006	time 0.2989 (0.3326)	loss 1.5394 (1.3786)	grad_norm 30.3438 (31.1741)	mem 4879MB
[2022-05-31 04:22:46 MetaFG_0] (main.py 265): INFO Train: [43/300][40/1562]	eta 0:08:16 lr 0.000006	time 0.2925 (0.3264)	loss 1.3567 (1.3761)	grad_norm 16.2265 (30.8115)	mem 4879MB
[2022-05-31 04:22:49 MetaFG_0] (main.py 265): INFO Train: [43/300][50/1562]	eta 0:08:07 lr 0.000006	time 0.2934 (0.3222)	loss 1.4759 (1.3844)	grad_norm 58.5234 (30.8339)	mem 4879MB
[2022-05-31 04:22:52 MetaFG_0] (main.py 265): INFO Train: [43/300][60/1562]	eta 0:07:59 lr 0.000006	time 0.2928 (0.3193)	loss 1.3067 (1.3826)	grad_norm 19.9466 (30.7591)	mem 4879MB
[2022-05-31 04:22:56 MetaFG_0] (main.py 265): INFO Train: [43/300][70/1562]	eta 0:07:53 lr 0.000006	time 0.2924 (0.3173)	loss 1.5276 (1.3872)	grad_norm 39.1593 (30.1181)	mem 4879MB
[2022-05-31 04:22:59 MetaFG_0] (main.py 265): INFO Train: [43/300][80/1562]	eta 0:07:47 lr 0.000006	time 0.2995 (0.3157)	loss 1.5285 (1.3716)	grad_norm 50.1786 (31.5262)	mem 4879MB
[2022-05-31 04:23:02 MetaFG_0] (main.py 265): INFO Train: [43/300][90/1562]	eta 0:07:42 lr 0.000006	time 0.2933 (0.3145)	loss 1.5673 (1.3647)	grad_norm 47.1962 (31.9429)	mem 4879MB
[2022-05-31 04:23:05 MetaFG_0] (main.py 265): INFO Train: [43/300][100/1562]	eta 0:07:38 lr 0.000006	time 0.2945 (0.3136)	loss 1.1757 (1.3552)	grad_norm 44.8703 (32.3268)	mem 4879MB
[2022-05-31 04:23:08 MetaFG_0] (main.py 265): INFO Train: [43/300][110/1562]	eta 0:07:34 lr 0.000006	time 0.2993 (0.3127)	loss 1.3233 (1.3556)	grad_norm 35.0322 (32.2284)	mem 4879MB
[2022-05-31 04:23:11 MetaFG_0] (main.py 265): INFO Train: [43/300][120/1562]	eta 0:07:30 lr 0.000006	time 0.2986 (0.3122)	loss 1.4879 (1.3665)	grad_norm 18.2538 (31.8030)	mem 4879MB
[2022-05-31 04:23:14 MetaFG_0] (main.py 265): INFO Train: [43/300][130/1562]	eta 0:07:26 lr 0.000006	time 0.2928 (0.3116)	loss 1.6083 (1.3688)	grad_norm 37.7122 (31.8187)	mem 4879MB
[2022-05-31 04:23:17 MetaFG_0] (main.py 265): INFO Train: [43/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.3034 (0.3113)	loss 1.6326 (1.3687)	grad_norm 29.2377 (32.0594)	mem 4879MB
[2022-05-31 04:23:20 MetaFG_0] (main.py 265): INFO Train: [43/300][150/1562]	eta 0:07:19 lr 0.000006	time 0.2992 (0.3109)	loss 1.4987 (1.3672)	grad_norm 22.4218 (31.7234)	mem 4879MB
[2022-05-31 04:23:23 MetaFG_0] (main.py 265): INFO Train: [43/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.3038 (0.3106)	loss 1.1771 (1.3604)	grad_norm 22.8711 (31.5581)	mem 4879MB
[2022-05-31 04:23:26 MetaFG_0] (main.py 265): INFO Train: [43/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2930 (0.3102)	loss 1.1809 (1.3630)	grad_norm 24.1018 (31.3203)	mem 4879MB
[2022-05-31 04:23:29 MetaFG_0] (main.py 265): INFO Train: [43/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2983 (0.3100)	loss 1.4370 (1.3733)	grad_norm 60.3367 (31.4891)	mem 4879MB
[2022-05-31 04:23:32 MetaFG_0] (main.py 265): INFO Train: [43/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2928 (0.3098)	loss 1.6807 (1.3720)	grad_norm 33.5104 (31.1798)	mem 4879MB
[2022-05-31 04:23:35 MetaFG_0] (main.py 265): INFO Train: [43/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.3008 (0.3094)	loss 1.0008 (1.3746)	grad_norm 35.1942 (31.0854)	mem 4879MB
[2022-05-31 04:23:38 MetaFG_0] (main.py 265): INFO Train: [43/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2925 (0.3092)	loss 1.3917 (1.3714)	grad_norm 18.9435 (30.8266)	mem 4879MB
[2022-05-31 04:23:41 MetaFG_0] (main.py 265): INFO Train: [43/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.3028 (0.3091)	loss 1.2325 (1.3713)	grad_norm 20.1663 (30.8020)	mem 4879MB
[2022-05-31 04:23:44 MetaFG_0] (main.py 265): INFO Train: [43/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.3018 (0.3090)	loss 1.0760 (1.3720)	grad_norm 26.5151 (30.5813)	mem 4879MB
[2022-05-31 04:23:47 MetaFG_0] (main.py 265): INFO Train: [43/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2941 (0.3089)	loss 1.5331 (1.3705)	grad_norm 39.5320 (30.8499)	mem 4879MB
[2022-05-31 04:23:50 MetaFG_0] (main.py 265): INFO Train: [43/300][250/1562]	eta 0:06:45 lr 0.000006	time 0.3021 (0.3088)	loss 1.4431 (1.3706)	grad_norm 16.8147 (30.8921)	mem 4879MB
[2022-05-31 04:23:54 MetaFG_0] (main.py 265): INFO Train: [43/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.3008 (0.3087)	loss 1.6130 (1.3710)	grad_norm 32.1374 (30.7152)	mem 4879MB
[2022-05-31 04:23:57 MetaFG_0] (main.py 265): INFO Train: [43/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2926 (0.3085)	loss 1.5517 (1.3710)	grad_norm 34.5168 (30.6052)	mem 4879MB
[2022-05-31 04:24:00 MetaFG_0] (main.py 265): INFO Train: [43/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.2930 (0.3084)	loss 1.7317 (1.3712)	grad_norm 24.3025 (30.4955)	mem 4879MB
[2022-05-31 04:24:03 MetaFG_0] (main.py 265): INFO Train: [43/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.3004 (0.3083)	loss 1.6723 (1.3712)	grad_norm 32.5795 (30.5877)	mem 4879MB
[2022-05-31 04:24:06 MetaFG_0] (main.py 265): INFO Train: [43/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2968 (0.3082)	loss 1.6033 (1.3709)	grad_norm 12.6003 (30.7223)	mem 4879MB
[2022-05-31 04:24:09 MetaFG_0] (main.py 265): INFO Train: [43/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2991 (0.3082)	loss 0.9939 (1.3707)	grad_norm 31.2502 (30.7633)	mem 4879MB
[2022-05-31 04:24:12 MetaFG_0] (main.py 265): INFO Train: [43/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2943 (0.3080)	loss 0.9002 (1.3716)	grad_norm 28.6609 (30.7281)	mem 4879MB
[2022-05-31 04:24:15 MetaFG_0] (main.py 265): INFO Train: [43/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2932 (0.3079)	loss 1.2480 (1.3714)	grad_norm 31.0491 (30.8901)	mem 4879MB
[2022-05-31 04:24:18 MetaFG_0] (main.py 265): INFO Train: [43/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2942 (0.3078)	loss 1.3947 (1.3752)	grad_norm 24.8246 (30.7861)	mem 4879MB
[2022-05-31 04:24:21 MetaFG_0] (main.py 265): INFO Train: [43/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2995 (0.3078)	loss 0.7380 (1.3744)	grad_norm 17.3337 (30.8896)	mem 4879MB
[2022-05-31 04:24:24 MetaFG_0] (main.py 265): INFO Train: [43/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2941 (0.3077)	loss 1.4258 (1.3736)	grad_norm 19.9063 (30.6841)	mem 4879MB
[2022-05-31 04:24:27 MetaFG_0] (main.py 265): INFO Train: [43/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.3027 (0.3077)	loss 1.2155 (1.3693)	grad_norm 34.4844 (30.7299)	mem 4879MB
[2022-05-31 04:24:30 MetaFG_0] (main.py 265): INFO Train: [43/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2928 (0.3076)	loss 1.1459 (1.3691)	grad_norm 32.5096 (30.8989)	mem 4879MB
[2022-05-31 04:24:33 MetaFG_0] (main.py 265): INFO Train: [43/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2928 (0.3075)	loss 1.3867 (1.3653)	grad_norm 42.6860 (31.0025)	mem 4879MB
[2022-05-31 04:24:36 MetaFG_0] (main.py 265): INFO Train: [43/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.3001 (0.3074)	loss 1.3033 (1.3678)	grad_norm 41.4748 (30.9611)	mem 4879MB
[2022-05-31 04:24:39 MetaFG_0] (main.py 265): INFO Train: [43/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2989 (0.3073)	loss 1.4406 (1.3699)	grad_norm 30.2831 (30.9654)	mem 4879MB
[2022-05-31 04:24:42 MetaFG_0] (main.py 265): INFO Train: [43/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2940 (0.3073)	loss 1.5420 (1.3694)	grad_norm 22.5975 (31.1174)	mem 4879MB
[2022-05-31 04:24:45 MetaFG_0] (main.py 265): INFO Train: [43/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.3040 (0.3072)	loss 1.5977 (1.3683)	grad_norm 18.2936 (31.0421)	mem 4879MB
[2022-05-31 04:24:48 MetaFG_0] (main.py 265): INFO Train: [43/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.3026 (0.3072)	loss 1.3554 (1.3660)	grad_norm 35.7093 (31.0320)	mem 4879MB
[2022-05-31 04:24:51 MetaFG_0] (main.py 265): INFO Train: [43/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.3045 (0.3071)	loss 1.6992 (1.3698)	grad_norm 40.3541 (30.9792)	mem 4879MB
[2022-05-31 04:24:55 MetaFG_0] (main.py 265): INFO Train: [43/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2924 (0.3071)	loss 1.1938 (1.3705)	grad_norm 44.8492 (30.9847)	mem 4879MB
[2022-05-31 04:24:58 MetaFG_0] (main.py 265): INFO Train: [43/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2942 (0.3070)	loss 1.5364 (1.3722)	grad_norm 51.0906 (30.8929)	mem 4879MB
[2022-05-31 04:25:01 MetaFG_0] (main.py 265): INFO Train: [43/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2935 (0.3070)	loss 1.5828 (1.3737)	grad_norm 42.1050 (30.8622)	mem 4879MB
[2022-05-31 04:25:04 MetaFG_0] (main.py 265): INFO Train: [43/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2931 (0.3069)	loss 1.4310 (1.3732)	grad_norm 15.2765 (30.7509)	mem 4879MB
[2022-05-31 04:25:07 MetaFG_0] (main.py 265): INFO Train: [43/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.3010 (0.3069)	loss 1.5378 (1.3752)	grad_norm 19.5283 (30.7130)	mem 4879MB
[2022-05-31 04:25:10 MetaFG_0] (main.py 265): INFO Train: [43/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.3004 (0.3068)	loss 1.3588 (1.3755)	grad_norm 28.9771 (30.7035)	mem 4879MB
[2022-05-31 04:25:13 MetaFG_0] (main.py 265): INFO Train: [43/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2922 (0.3068)	loss 1.5577 (1.3778)	grad_norm 29.7085 (30.6909)	mem 4879MB
[2022-05-31 04:25:16 MetaFG_0] (main.py 265): INFO Train: [43/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2948 (0.3068)	loss 1.5087 (1.3787)	grad_norm 24.7607 (30.6946)	mem 4879MB
[2022-05-31 04:25:19 MetaFG_0] (main.py 265): INFO Train: [43/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2984 (0.3068)	loss 1.5078 (1.3795)	grad_norm 23.5595 (30.6443)	mem 4879MB
[2022-05-31 04:25:22 MetaFG_0] (main.py 265): INFO Train: [43/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2924 (0.3068)	loss 1.2482 (1.3798)	grad_norm 17.6690 (30.6952)	mem 4879MB
[2022-05-31 04:25:25 MetaFG_0] (main.py 265): INFO Train: [43/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2991 (0.3067)	loss 1.3249 (1.3786)	grad_norm 22.9984 (30.6368)	mem 4879MB
[2022-05-31 04:25:28 MetaFG_0] (main.py 265): INFO Train: [43/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2929 (0.3067)	loss 1.6268 (1.3801)	grad_norm 15.1468 (30.5570)	mem 4879MB
[2022-05-31 04:25:31 MetaFG_0] (main.py 265): INFO Train: [43/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2955 (0.3067)	loss 1.5237 (1.3793)	grad_norm 23.7572 (inf)	mem 4879MB
[2022-05-31 04:25:34 MetaFG_0] (main.py 265): INFO Train: [43/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2929 (0.3066)	loss 1.2035 (1.3789)	grad_norm 29.2172 (inf)	mem 4879MB
[2022-05-31 04:25:37 MetaFG_0] (main.py 265): INFO Train: [43/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2930 (0.3066)	loss 1.3325 (1.3785)	grad_norm 27.1273 (inf)	mem 4879MB
[2022-05-31 04:25:40 MetaFG_0] (main.py 265): INFO Train: [43/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2950 (0.3066)	loss 1.6521 (1.3784)	grad_norm 35.3837 (inf)	mem 4879MB
[2022-05-31 04:25:43 MetaFG_0] (main.py 265): INFO Train: [43/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2962 (0.3066)	loss 1.1807 (1.3761)	grad_norm 31.6975 (inf)	mem 4879MB
[2022-05-31 04:25:46 MetaFG_0] (main.py 265): INFO Train: [43/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2941 (0.3065)	loss 1.7532 (1.3776)	grad_norm 33.5602 (inf)	mem 4879MB
[2022-05-31 04:25:50 MetaFG_0] (main.py 265): INFO Train: [43/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3420 (0.3069)	loss 1.4926 (1.3798)	grad_norm 27.3742 (inf)	mem 4879MB
[2022-05-31 04:25:53 MetaFG_0] (main.py 265): INFO Train: [43/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2998 (0.3069)	loss 1.8043 (1.3817)	grad_norm 23.9158 (inf)	mem 4879MB
[2022-05-31 04:25:56 MetaFG_0] (main.py 265): INFO Train: [43/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2997 (0.3070)	loss 1.3003 (1.3814)	grad_norm 22.2442 (inf)	mem 4879MB
[2022-05-31 04:25:59 MetaFG_0] (main.py 265): INFO Train: [43/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2933 (0.3069)	loss 1.3315 (1.3813)	grad_norm 33.6261 (inf)	mem 4879MB
[2022-05-31 04:26:02 MetaFG_0] (main.py 265): INFO Train: [43/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2935 (0.3069)	loss 1.1695 (1.3806)	grad_norm 36.1466 (inf)	mem 4879MB
[2022-05-31 04:26:05 MetaFG_0] (main.py 265): INFO Train: [43/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2997 (0.3068)	loss 1.4116 (1.3794)	grad_norm 32.4785 (inf)	mem 4879MB
[2022-05-31 04:26:08 MetaFG_0] (main.py 265): INFO Train: [43/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2987 (0.3068)	loss 0.8838 (1.3770)	grad_norm 35.4166 (inf)	mem 4879MB
[2022-05-31 04:26:11 MetaFG_0] (main.py 265): INFO Train: [43/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2935 (0.3068)	loss 1.6883 (1.3777)	grad_norm 18.6059 (inf)	mem 4879MB
[2022-05-31 04:26:14 MetaFG_0] (main.py 265): INFO Train: [43/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2924 (0.3067)	loss 1.5129 (1.3784)	grad_norm 27.8134 (inf)	mem 4879MB
[2022-05-31 04:26:17 MetaFG_0] (main.py 265): INFO Train: [43/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2926 (0.3067)	loss 1.0666 (1.3781)	grad_norm 22.4732 (inf)	mem 4879MB
[2022-05-31 04:26:20 MetaFG_0] (main.py 265): INFO Train: [43/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2932 (0.3066)	loss 0.9436 (1.3787)	grad_norm 33.8512 (inf)	mem 4879MB
[2022-05-31 04:26:23 MetaFG_0] (main.py 265): INFO Train: [43/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.3001 (0.3066)	loss 1.6114 (1.3789)	grad_norm 20.7445 (inf)	mem 4879MB
[2022-05-31 04:26:26 MetaFG_0] (main.py 265): INFO Train: [43/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2944 (0.3066)	loss 1.5500 (1.3791)	grad_norm 19.1183 (inf)	mem 4879MB
[2022-05-31 04:26:29 MetaFG_0] (main.py 265): INFO Train: [43/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3020 (0.3067)	loss 1.6194 (1.3784)	grad_norm 36.1462 (inf)	mem 4879MB
[2022-05-31 04:26:32 MetaFG_0] (main.py 265): INFO Train: [43/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2929 (0.3066)	loss 1.8719 (1.3784)	grad_norm 25.1817 (inf)	mem 4879MB
[2022-05-31 04:26:36 MetaFG_0] (main.py 265): INFO Train: [43/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2928 (0.3066)	loss 1.1588 (1.3795)	grad_norm 34.6593 (inf)	mem 4879MB
[2022-05-31 04:26:39 MetaFG_0] (main.py 265): INFO Train: [43/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2983 (0.3066)	loss 1.3705 (1.3802)	grad_norm 27.0104 (inf)	mem 4879MB
[2022-05-31 04:26:42 MetaFG_0] (main.py 265): INFO Train: [43/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2932 (0.3066)	loss 1.6191 (1.3796)	grad_norm 51.7459 (inf)	mem 4879MB
[2022-05-31 04:26:45 MetaFG_0] (main.py 265): INFO Train: [43/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2986 (0.3066)	loss 1.1192 (1.3798)	grad_norm 32.0589 (inf)	mem 4879MB
[2022-05-31 04:26:48 MetaFG_0] (main.py 265): INFO Train: [43/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2931 (0.3066)	loss 1.4029 (1.3790)	grad_norm 27.7513 (inf)	mem 4879MB
[2022-05-31 04:26:51 MetaFG_0] (main.py 265): INFO Train: [43/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2929 (0.3066)	loss 1.5304 (1.3800)	grad_norm 37.1952 (inf)	mem 4879MB
[2022-05-31 04:26:54 MetaFG_0] (main.py 265): INFO Train: [43/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2989 (0.3066)	loss 1.0790 (1.3791)	grad_norm 25.0488 (inf)	mem 4879MB
[2022-05-31 04:26:57 MetaFG_0] (main.py 265): INFO Train: [43/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2996 (0.3066)	loss 1.4354 (1.3786)	grad_norm 29.6940 (inf)	mem 4879MB
[2022-05-31 04:27:00 MetaFG_0] (main.py 265): INFO Train: [43/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.2992 (0.3065)	loss 1.3913 (1.3795)	grad_norm 32.7597 (inf)	mem 4879MB
[2022-05-31 04:27:03 MetaFG_0] (main.py 265): INFO Train: [43/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2923 (0.3065)	loss 1.0372 (1.3780)	grad_norm 22.7415 (inf)	mem 4879MB
[2022-05-31 04:27:06 MetaFG_0] (main.py 265): INFO Train: [43/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2936 (0.3065)	loss 1.2636 (1.3779)	grad_norm 52.1495 (inf)	mem 4879MB
[2022-05-31 04:27:09 MetaFG_0] (main.py 265): INFO Train: [43/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2926 (0.3065)	loss 1.1118 (1.3774)	grad_norm 22.2122 (inf)	mem 4879MB
[2022-05-31 04:27:12 MetaFG_0] (main.py 265): INFO Train: [43/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2920 (0.3064)	loss 1.0682 (1.3773)	grad_norm 39.6070 (inf)	mem 4879MB
[2022-05-31 04:27:15 MetaFG_0] (main.py 265): INFO Train: [43/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2936 (0.3064)	loss 1.5756 (1.3776)	grad_norm 29.1883 (inf)	mem 4879MB
[2022-05-31 04:27:18 MetaFG_0] (main.py 265): INFO Train: [43/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3002 (0.3064)	loss 1.4951 (1.3781)	grad_norm 16.5545 (inf)	mem 4879MB
[2022-05-31 04:27:21 MetaFG_0] (main.py 265): INFO Train: [43/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3010 (0.3064)	loss 1.2481 (1.3786)	grad_norm 30.6923 (inf)	mem 4879MB
[2022-05-31 04:27:24 MetaFG_0] (main.py 265): INFO Train: [43/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2943 (0.3064)	loss 1.1473 (1.3792)	grad_norm 32.8850 (inf)	mem 4879MB
[2022-05-31 04:27:27 MetaFG_0] (main.py 265): INFO Train: [43/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2936 (0.3064)	loss 1.6268 (1.3789)	grad_norm 26.9193 (inf)	mem 4879MB
[2022-05-31 04:27:30 MetaFG_0] (main.py 265): INFO Train: [43/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2920 (0.3063)	loss 1.4219 (1.3806)	grad_norm 16.4825 (inf)	mem 4879MB
[2022-05-31 04:27:33 MetaFG_0] (main.py 265): INFO Train: [43/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.3000 (0.3063)	loss 1.1572 (1.3791)	grad_norm 37.5042 (inf)	mem 4879MB
[2022-05-31 04:27:37 MetaFG_0] (main.py 265): INFO Train: [43/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2991 (0.3063)	loss 0.9119 (1.3791)	grad_norm 20.6093 (inf)	mem 4879MB
[2022-05-31 04:27:40 MetaFG_0] (main.py 265): INFO Train: [43/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2950 (0.3063)	loss 1.5468 (1.3800)	grad_norm 26.3994 (inf)	mem 4879MB
[2022-05-31 04:27:43 MetaFG_0] (main.py 265): INFO Train: [43/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2941 (0.3063)	loss 1.5536 (1.3812)	grad_norm 18.5702 (inf)	mem 4879MB
[2022-05-31 04:27:46 MetaFG_0] (main.py 265): INFO Train: [43/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2950 (0.3063)	loss 1.3011 (1.3819)	grad_norm 16.9526 (inf)	mem 4879MB
[2022-05-31 04:27:49 MetaFG_0] (main.py 265): INFO Train: [43/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2981 (0.3063)	loss 1.4719 (1.3825)	grad_norm 25.5422 (inf)	mem 4879MB
[2022-05-31 04:27:52 MetaFG_0] (main.py 265): INFO Train: [43/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2995 (0.3063)	loss 1.7137 (1.3828)	grad_norm 31.3992 (inf)	mem 4879MB
[2022-05-31 04:27:55 MetaFG_0] (main.py 265): INFO Train: [43/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2925 (0.3062)	loss 1.5375 (1.3826)	grad_norm 22.8886 (inf)	mem 4879MB
[2022-05-31 04:27:58 MetaFG_0] (main.py 265): INFO Train: [43/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2930 (0.3062)	loss 1.2592 (1.3829)	grad_norm 22.0146 (inf)	mem 4879MB
[2022-05-31 04:28:01 MetaFG_0] (main.py 265): INFO Train: [43/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.3005 (0.3063)	loss 1.1150 (1.3835)	grad_norm 33.1080 (inf)	mem 4879MB
[2022-05-31 04:28:04 MetaFG_0] (main.py 265): INFO Train: [43/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2926 (0.3062)	loss 0.8874 (1.3835)	grad_norm 30.8613 (inf)	mem 4879MB
[2022-05-31 04:28:07 MetaFG_0] (main.py 265): INFO Train: [43/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2929 (0.3062)	loss 1.2713 (1.3844)	grad_norm 17.3799 (inf)	mem 4879MB
[2022-05-31 04:28:10 MetaFG_0] (main.py 265): INFO Train: [43/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.3056 (0.3062)	loss 1.5894 (1.3852)	grad_norm 19.3886 (inf)	mem 4879MB
[2022-05-31 04:28:13 MetaFG_0] (main.py 265): INFO Train: [43/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2931 (0.3062)	loss 1.5949 (1.3862)	grad_norm 23.6413 (inf)	mem 4879MB
[2022-05-31 04:28:16 MetaFG_0] (main.py 265): INFO Train: [43/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.3064 (0.3062)	loss 1.7432 (1.3854)	grad_norm 44.4543 (inf)	mem 4879MB
[2022-05-31 04:28:19 MetaFG_0] (main.py 265): INFO Train: [43/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2967 (0.3062)	loss 1.5134 (1.3850)	grad_norm 30.3528 (inf)	mem 4879MB
[2022-05-31 04:28:22 MetaFG_0] (main.py 265): INFO Train: [43/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2938 (0.3062)	loss 1.2821 (1.3857)	grad_norm 35.5474 (inf)	mem 4879MB
[2022-05-31 04:28:25 MetaFG_0] (main.py 265): INFO Train: [43/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2934 (0.3062)	loss 1.5153 (1.3866)	grad_norm 19.0015 (inf)	mem 4879MB
[2022-05-31 04:28:28 MetaFG_0] (main.py 265): INFO Train: [43/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.3025 (0.3062)	loss 1.6937 (1.3870)	grad_norm 41.7728 (inf)	mem 4879MB
[2022-05-31 04:28:31 MetaFG_0] (main.py 265): INFO Train: [43/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2948 (0.3062)	loss 1.6695 (1.3875)	grad_norm 26.9085 (inf)	mem 4879MB
[2022-05-31 04:28:35 MetaFG_0] (main.py 265): INFO Train: [43/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2932 (0.3061)	loss 1.7562 (1.3885)	grad_norm 22.6981 (inf)	mem 4879MB
[2022-05-31 04:28:38 MetaFG_0] (main.py 265): INFO Train: [43/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2933 (0.3061)	loss 1.5236 (1.3887)	grad_norm 24.5930 (inf)	mem 4879MB
[2022-05-31 04:28:41 MetaFG_0] (main.py 265): INFO Train: [43/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2933 (0.3061)	loss 1.3966 (1.3891)	grad_norm 26.6746 (inf)	mem 4879MB
[2022-05-31 04:28:44 MetaFG_0] (main.py 265): INFO Train: [43/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3013 (0.3061)	loss 1.4740 (1.3899)	grad_norm 32.9739 (inf)	mem 4879MB
[2022-05-31 04:28:47 MetaFG_0] (main.py 265): INFO Train: [43/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3008 (0.3061)	loss 1.2678 (1.3901)	grad_norm 41.5862 (inf)	mem 4879MB
[2022-05-31 04:28:50 MetaFG_0] (main.py 265): INFO Train: [43/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2990 (0.3061)	loss 1.7412 (1.3899)	grad_norm 25.1644 (inf)	mem 4879MB
[2022-05-31 04:28:53 MetaFG_0] (main.py 265): INFO Train: [43/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2942 (0.3060)	loss 1.5818 (1.3907)	grad_norm 23.9975 (inf)	mem 4879MB
[2022-05-31 04:28:56 MetaFG_0] (main.py 265): INFO Train: [43/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2936 (0.3060)	loss 1.6539 (1.3907)	grad_norm 41.4294 (inf)	mem 4879MB
[2022-05-31 04:28:59 MetaFG_0] (main.py 265): INFO Train: [43/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2925 (0.3060)	loss 1.4957 (1.3908)	grad_norm 16.0290 (inf)	mem 4879MB
[2022-05-31 04:29:02 MetaFG_0] (main.py 265): INFO Train: [43/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2928 (0.3060)	loss 1.4995 (1.3919)	grad_norm 31.4230 (inf)	mem 4879MB
[2022-05-31 04:29:05 MetaFG_0] (main.py 265): INFO Train: [43/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2934 (0.3060)	loss 1.5047 (1.3926)	grad_norm 23.5165 (inf)	mem 4879MB
[2022-05-31 04:29:08 MetaFG_0] (main.py 265): INFO Train: [43/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2927 (0.3060)	loss 1.0831 (1.3926)	grad_norm 31.4398 (inf)	mem 4879MB
[2022-05-31 04:29:11 MetaFG_0] (main.py 265): INFO Train: [43/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2928 (0.3060)	loss 1.6435 (1.3930)	grad_norm 29.1736 (inf)	mem 4879MB
[2022-05-31 04:29:14 MetaFG_0] (main.py 265): INFO Train: [43/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2934 (0.3060)	loss 1.5437 (1.3935)	grad_norm 22.7487 (inf)	mem 4879MB
[2022-05-31 04:29:17 MetaFG_0] (main.py 265): INFO Train: [43/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2945 (0.3060)	loss 1.6657 (1.3931)	grad_norm 19.3030 (inf)	mem 4879MB
[2022-05-31 04:29:20 MetaFG_0] (main.py 265): INFO Train: [43/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2997 (0.3060)	loss 1.0261 (1.3926)	grad_norm 41.6395 (inf)	mem 4879MB
[2022-05-31 04:29:23 MetaFG_0] (main.py 265): INFO Train: [43/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2926 (0.3060)	loss 1.0841 (1.3922)	grad_norm 21.2207 (inf)	mem 4879MB
[2022-05-31 04:29:26 MetaFG_0] (main.py 265): INFO Train: [43/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2986 (0.3060)	loss 1.1589 (1.3921)	grad_norm 28.8952 (inf)	mem 4879MB
[2022-05-31 04:29:29 MetaFG_0] (main.py 265): INFO Train: [43/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2943 (0.3060)	loss 1.4151 (1.3925)	grad_norm 71.9943 (inf)	mem 4879MB
[2022-05-31 04:29:32 MetaFG_0] (main.py 265): INFO Train: [43/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2966 (0.3060)	loss 1.3654 (1.3926)	grad_norm 56.0362 (inf)	mem 4879MB
[2022-05-31 04:29:36 MetaFG_0] (main.py 265): INFO Train: [43/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2944 (0.3060)	loss 1.6278 (1.3930)	grad_norm 29.3989 (inf)	mem 4879MB
[2022-05-31 04:29:39 MetaFG_0] (main.py 265): INFO Train: [43/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2952 (0.3060)	loss 1.4857 (1.3934)	grad_norm 26.8437 (inf)	mem 4879MB
[2022-05-31 04:29:42 MetaFG_0] (main.py 265): INFO Train: [43/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2997 (0.3060)	loss 1.3245 (1.3938)	grad_norm 37.7365 (inf)	mem 4879MB
[2022-05-31 04:29:45 MetaFG_0] (main.py 265): INFO Train: [43/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3011 (0.3060)	loss 1.1616 (1.3932)	grad_norm 35.1462 (inf)	mem 4879MB
[2022-05-31 04:29:48 MetaFG_0] (main.py 265): INFO Train: [43/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2936 (0.3059)	loss 1.1309 (1.3928)	grad_norm 21.5547 (inf)	mem 4879MB
[2022-05-31 04:29:51 MetaFG_0] (main.py 265): INFO Train: [43/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2987 (0.3059)	loss 1.2760 (1.3936)	grad_norm 25.8168 (inf)	mem 4879MB
[2022-05-31 04:29:54 MetaFG_0] (main.py 265): INFO Train: [43/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2939 (0.3059)	loss 1.3860 (1.3930)	grad_norm 23.3940 (inf)	mem 4879MB
[2022-05-31 04:29:57 MetaFG_0] (main.py 265): INFO Train: [43/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2943 (0.3059)	loss 1.6560 (1.3932)	grad_norm 24.9628 (inf)	mem 4879MB
[2022-05-31 04:30:00 MetaFG_0] (main.py 265): INFO Train: [43/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2938 (0.3059)	loss 1.7533 (1.3939)	grad_norm 23.0660 (inf)	mem 4879MB
[2022-05-31 04:30:03 MetaFG_0] (main.py 265): INFO Train: [43/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2987 (0.3059)	loss 0.9728 (1.3928)	grad_norm 30.2149 (inf)	mem 4879MB
[2022-05-31 04:30:06 MetaFG_0] (main.py 265): INFO Train: [43/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2989 (0.3059)	loss 1.3951 (1.3939)	grad_norm 32.4596 (inf)	mem 4879MB
[2022-05-31 04:30:09 MetaFG_0] (main.py 265): INFO Train: [43/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2937 (0.3059)	loss 1.4369 (1.3945)	grad_norm 30.3014 (inf)	mem 4879MB
[2022-05-31 04:30:12 MetaFG_0] (main.py 265): INFO Train: [43/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3008 (0.3059)	loss 1.4121 (1.3952)	grad_norm 18.7239 (inf)	mem 4879MB
[2022-05-31 04:30:15 MetaFG_0] (main.py 265): INFO Train: [43/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2950 (0.3059)	loss 1.2511 (1.3950)	grad_norm 22.7356 (inf)	mem 4879MB
[2022-05-31 04:30:18 MetaFG_0] (main.py 265): INFO Train: [43/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3001 (0.3059)	loss 0.9548 (1.3941)	grad_norm 18.9136 (inf)	mem 4879MB
[2022-05-31 04:30:21 MetaFG_0] (main.py 265): INFO Train: [43/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2923 (0.3059)	loss 1.1616 (1.3937)	grad_norm 23.6756 (inf)	mem 4879MB
[2022-05-31 04:30:24 MetaFG_0] (main.py 265): INFO Train: [43/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2970 (0.3059)	loss 1.6312 (1.3934)	grad_norm 32.3850 (inf)	mem 4879MB
[2022-05-31 04:30:27 MetaFG_0] (main.py 265): INFO Train: [43/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2933 (0.3058)	loss 1.6602 (1.3940)	grad_norm 22.0015 (inf)	mem 4879MB
[2022-05-31 04:30:30 MetaFG_0] (main.py 265): INFO Train: [43/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2917 (0.3058)	loss 1.6397 (1.3948)	grad_norm 50.7831 (inf)	mem 4879MB
[2022-05-31 04:30:31 MetaFG_0] (main.py 272): INFO EPOCH 43 training takes 0:07:57
[2022-05-31 04:30:31 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_43.pth saving......
[2022-05-31 04:30:32 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_43.pth saved !!!
[2022-05-31 04:30:32 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:30:33 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:30:33 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:30:34 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.690 (0.690)	Loss 0.5777 (0.5777)	Acc@1 93.750 (93.750)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 04:30:35 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.103 (0.151)	Loss 0.6965 (0.7391)	Acc@1 84.375 (84.375)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 04:30:36 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.125)	Loss 0.6481 (0.7042)	Acc@1 84.375 (86.012)	Acc@5 96.875 (97.768)	Mem 4879MB
[2022-05-31 04:30:37 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.116)	Loss 0.7450 (0.6851)	Acc@1 78.125 (86.089)	Acc@5 100.000 (98.185)	Mem 4879MB
[2022-05-31 04:30:38 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.111)	Loss 0.5113 (0.6815)	Acc@1 96.875 (85.976)	Acc@5 100.000 (98.171)	Mem 4879MB
[2022-05-31 04:30:39 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.112)	Loss 0.6439 (0.6670)	Acc@1 81.250 (85.907)	Acc@5 100.000 (98.468)	Mem 4879MB
[2022-05-31 04:30:40 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.114)	Loss 1.0177 (0.6597)	Acc@1 75.000 (86.527)	Acc@5 100.000 (98.566)	Mem 4879MB
[2022-05-31 04:30:41 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.111)	Loss 0.6419 (0.6594)	Acc@1 87.500 (86.488)	Acc@5 100.000 (98.592)	Mem 4879MB
[2022-05-31 04:30:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.093 (0.109)	Loss 0.5950 (0.6587)	Acc@1 93.750 (86.497)	Acc@5 100.000 (98.534)	Mem 4879MB
[2022-05-31 04:30:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.095 (0.107)	Loss 1.2222 (0.6637)	Acc@1 68.750 (86.058)	Acc@5 93.750 (98.626)	Mem 4879MB
[2022-05-31 04:30:44 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.106)	Loss 0.7234 (0.6579)	Acc@1 81.250 (86.293)	Acc@5 93.750 (98.515)	Mem 4879MB
[2022-05-31 04:30:45 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.105)	Loss 0.7719 (0.6628)	Acc@1 81.250 (85.980)	Acc@5 100.000 (98.536)	Mem 4879MB
[2022-05-31 04:30:46 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.104)	Loss 0.7154 (0.6610)	Acc@1 84.375 (85.950)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 04:30:47 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.093 (0.103)	Loss 0.8660 (0.6616)	Acc@1 71.875 (85.854)	Acc@5 96.875 (98.545)	Mem 4879MB
[2022-05-31 04:30:48 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.102)	Loss 0.7903 (0.6654)	Acc@1 78.125 (85.660)	Acc@5 100.000 (98.537)	Mem 4879MB
[2022-05-31 04:30:49 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.102 (0.102)	Loss 0.8126 (0.6699)	Acc@1 81.250 (85.575)	Acc@5 100.000 (98.510)	Mem 4879MB
[2022-05-31 04:30:50 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.101)	Loss 0.3898 (0.6685)	Acc@1 100.000 (85.753)	Acc@5 100.000 (98.467)	Mem 4879MB
[2022-05-31 04:30:51 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.090 (0.101)	Loss 0.3798 (0.6692)	Acc@1 96.875 (85.581)	Acc@5 100.000 (98.520)	Mem 4879MB
[2022-05-31 04:30:51 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.100 (0.101)	Loss 1.1047 (0.6735)	Acc@1 71.875 (85.497)	Acc@5 96.875 (98.498)	Mem 4879MB
[2022-05-31 04:30:52 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.102 (0.100)	Loss 0.8152 (0.6760)	Acc@1 78.125 (85.373)	Acc@5 100.000 (98.478)	Mem 4879MB
[2022-05-31 04:30:53 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.092 (0.100)	Loss 0.4902 (0.6761)	Acc@1 96.875 (85.354)	Acc@5 100.000 (98.476)	Mem 4879MB
[2022-05-31 04:30:54 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.100)	Loss 0.7945 (0.6782)	Acc@1 78.125 (85.175)	Acc@5 100.000 (98.475)	Mem 4879MB
[2022-05-31 04:30:55 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.099)	Loss 0.5540 (0.6818)	Acc@1 87.500 (85.054)	Acc@5 100.000 (98.473)	Mem 4879MB
[2022-05-31 04:30:56 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.099)	Loss 0.5768 (0.6852)	Acc@1 84.375 (84.889)	Acc@5 100.000 (98.512)	Mem 4879MB
[2022-05-31 04:30:57 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.104 (0.099)	Loss 0.6603 (0.6868)	Acc@1 84.375 (84.829)	Acc@5 96.875 (98.496)	Mem 4879MB
[2022-05-31 04:30:58 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.099)	Loss 0.5301 (0.6848)	Acc@1 90.625 (84.923)	Acc@5 100.000 (98.506)	Mem 4879MB
[2022-05-31 04:30:59 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.099)	Loss 0.8525 (0.6855)	Acc@1 84.375 (84.926)	Acc@5 96.875 (98.515)	Mem 4879MB
[2022-05-31 04:31:00 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.089 (0.098)	Loss 0.5808 (0.6853)	Acc@1 93.750 (84.998)	Acc@5 96.875 (98.489)	Mem 4879MB
[2022-05-31 04:31:01 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.098)	Loss 0.6810 (0.6880)	Acc@1 93.750 (84.931)	Acc@5 100.000 (98.454)	Mem 4879MB
[2022-05-31 04:31:02 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.098)	Loss 0.8093 (0.6915)	Acc@1 84.375 (84.837)	Acc@5 96.875 (98.432)	Mem 4879MB
[2022-05-31 04:31:03 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.093 (0.098)	Loss 0.4142 (0.6851)	Acc@1 93.750 (85.050)	Acc@5 100.000 (98.474)	Mem 4879MB
[2022-05-31 04:31:04 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.098)	Loss 0.6671 (0.6839)	Acc@1 84.375 (85.058)	Acc@5 96.875 (98.463)	Mem 4879MB
[2022-05-31 04:31:04 MetaFG_0] (main.py 330): INFO  * Acc@1 85.020 Acc@5 98.470
[2022-05-31 04:31:04 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.0%
[2022-05-31 04:31:04 MetaFG_0] (main.py 171): INFO Max accuracy: 85.11%
[2022-05-31 04:31:05 MetaFG_0] (main.py 265): INFO Train: [44/300][0/1562]	eta 0:27:34 lr 0.000006	time 1.0594 (1.0594)	loss 1.4649 (1.4649)	grad_norm 28.9549 (28.9549)	mem 4879MB
[2022-05-31 04:31:08 MetaFG_0] (main.py 265): INFO Train: [44/300][10/1562]	eta 0:09:41 lr 0.000006	time 0.3010 (0.3744)	loss 1.4958 (1.3733)	grad_norm 32.3529 (27.2013)	mem 4879MB
[2022-05-31 04:31:11 MetaFG_0] (main.py 265): INFO Train: [44/300][20/1562]	eta 0:08:45 lr 0.000006	time 0.2934 (0.3410)	loss 1.0954 (1.3567)	grad_norm 16.8754 (25.6090)	mem 4879MB
[2022-05-31 04:31:14 MetaFG_0] (main.py 265): INFO Train: [44/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2931 (0.3297)	loss 1.0576 (1.3688)	grad_norm 20.9958 (26.3088)	mem 4879MB
[2022-05-31 04:31:17 MetaFG_0] (main.py 265): INFO Train: [44/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2922 (0.3238)	loss 1.0819 (1.3520)	grad_norm 45.1282 (27.8070)	mem 4879MB
[2022-05-31 04:31:20 MetaFG_0] (main.py 265): INFO Train: [44/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2939 (0.3204)	loss 1.6119 (1.3318)	grad_norm 38.7324 (28.7147)	mem 4879MB
[2022-05-31 04:31:23 MetaFG_0] (main.py 265): INFO Train: [44/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2991 (0.3180)	loss 1.3223 (1.3484)	grad_norm 27.6179 (28.2219)	mem 4879MB
[2022-05-31 04:31:26 MetaFG_0] (main.py 265): INFO Train: [44/300][70/1562]	eta 0:07:52 lr 0.000006	time 0.2988 (0.3164)	loss 1.4007 (1.3386)	grad_norm 33.0413 (29.6857)	mem 4879MB
[2022-05-31 04:31:29 MetaFG_0] (main.py 265): INFO Train: [44/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2922 (0.3150)	loss 1.4399 (1.3399)	grad_norm 24.6053 (29.3009)	mem 4879MB
[2022-05-31 04:31:32 MetaFG_0] (main.py 265): INFO Train: [44/300][90/1562]	eta 0:07:42 lr 0.000006	time 0.2928 (0.3139)	loss 1.4195 (1.3445)	grad_norm 33.1207 (29.2821)	mem 4879MB
[2022-05-31 04:31:35 MetaFG_0] (main.py 265): INFO Train: [44/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2991 (0.3130)	loss 1.5308 (1.3545)	grad_norm 27.8373 (29.4652)	mem 4879MB
[2022-05-31 04:31:39 MetaFG_0] (main.py 265): INFO Train: [44/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2945 (0.3125)	loss 1.0429 (1.3606)	grad_norm 25.5852 (29.2454)	mem 4879MB
[2022-05-31 04:31:42 MetaFG_0] (main.py 265): INFO Train: [44/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2932 (0.3116)	loss 1.5141 (1.3591)	grad_norm 21.4475 (28.8316)	mem 4879MB
[2022-05-31 04:31:45 MetaFG_0] (main.py 265): INFO Train: [44/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2992 (0.3112)	loss 1.5476 (1.3565)	grad_norm 34.9551 (28.9478)	mem 4879MB
[2022-05-31 04:31:48 MetaFG_0] (main.py 265): INFO Train: [44/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2994 (0.3108)	loss 1.5968 (1.3599)	grad_norm 30.7342 (28.9559)	mem 4879MB
[2022-05-31 04:31:51 MetaFG_0] (main.py 265): INFO Train: [44/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2994 (0.3105)	loss 1.2471 (1.3512)	grad_norm 40.3901 (29.3475)	mem 4879MB
[2022-05-31 04:31:54 MetaFG_0] (main.py 265): INFO Train: [44/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2994 (0.3102)	loss 1.4443 (1.3582)	grad_norm 20.9600 (29.1982)	mem 4879MB
[2022-05-31 04:31:57 MetaFG_0] (main.py 265): INFO Train: [44/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2929 (0.3099)	loss 1.4009 (1.3609)	grad_norm 36.7929 (29.2155)	mem 4879MB
[2022-05-31 04:32:00 MetaFG_0] (main.py 265): INFO Train: [44/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2978 (0.3097)	loss 1.1814 (1.3582)	grad_norm 21.0003 (29.4634)	mem 4879MB
[2022-05-31 04:32:03 MetaFG_0] (main.py 265): INFO Train: [44/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.3000 (0.3094)	loss 0.9483 (1.3647)	grad_norm 24.1273 (29.5332)	mem 4879MB
[2022-05-31 04:32:06 MetaFG_0] (main.py 265): INFO Train: [44/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2992 (0.3093)	loss 1.1716 (1.3681)	grad_norm 31.0236 (29.6771)	mem 4879MB
[2022-05-31 04:32:09 MetaFG_0] (main.py 265): INFO Train: [44/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2974 (0.3090)	loss 1.6020 (1.3732)	grad_norm 23.9964 (29.6290)	mem 4879MB
[2022-05-31 04:32:12 MetaFG_0] (main.py 265): INFO Train: [44/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2990 (0.3089)	loss 0.9568 (1.3731)	grad_norm 47.4319 (29.5362)	mem 4879MB
[2022-05-31 04:32:15 MetaFG_0] (main.py 265): INFO Train: [44/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.3093 (0.3088)	loss 1.3297 (1.3751)	grad_norm 42.1039 (29.5866)	mem 4879MB
[2022-05-31 04:32:18 MetaFG_0] (main.py 265): INFO Train: [44/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2934 (0.3086)	loss 1.7857 (1.3825)	grad_norm 32.4884 (29.7394)	mem 4879MB
[2022-05-31 04:32:21 MetaFG_0] (main.py 265): INFO Train: [44/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2990 (0.3085)	loss 1.5137 (1.3836)	grad_norm 45.4366 (29.7917)	mem 4879MB
[2022-05-31 04:32:24 MetaFG_0] (main.py 265): INFO Train: [44/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2929 (0.3084)	loss 1.5971 (1.3798)	grad_norm 25.3013 (29.7005)	mem 4879MB
[2022-05-31 04:32:27 MetaFG_0] (main.py 265): INFO Train: [44/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2920 (0.3082)	loss 1.3146 (1.3816)	grad_norm 28.2489 (29.5748)	mem 4879MB
[2022-05-31 04:32:30 MetaFG_0] (main.py 265): INFO Train: [44/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2937 (0.3081)	loss 1.4021 (1.3829)	grad_norm 31.6567 (29.5584)	mem 4879MB
[2022-05-31 04:32:33 MetaFG_0] (main.py 265): INFO Train: [44/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2929 (0.3079)	loss 1.3296 (1.3825)	grad_norm 35.3776 (29.7849)	mem 4879MB
[2022-05-31 04:32:37 MetaFG_0] (main.py 265): INFO Train: [44/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2929 (0.3078)	loss 1.3507 (1.3841)	grad_norm 44.3157 (29.8732)	mem 4879MB
[2022-05-31 04:32:40 MetaFG_0] (main.py 265): INFO Train: [44/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2928 (0.3077)	loss 1.0871 (1.3798)	grad_norm 45.3270 (30.2428)	mem 4879MB
[2022-05-31 04:32:43 MetaFG_0] (main.py 265): INFO Train: [44/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2931 (0.3076)	loss 1.5618 (1.3823)	grad_norm 16.1586 (30.1882)	mem 4879MB
[2022-05-31 04:32:46 MetaFG_0] (main.py 265): INFO Train: [44/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2989 (0.3075)	loss 1.6871 (1.3877)	grad_norm 19.8195 (30.1712)	mem 4879MB
[2022-05-31 04:32:49 MetaFG_0] (main.py 265): INFO Train: [44/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2932 (0.3074)	loss 1.5618 (1.3893)	grad_norm 19.1617 (30.2064)	mem 4879MB
[2022-05-31 04:32:52 MetaFG_0] (main.py 265): INFO Train: [44/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2963 (0.3074)	loss 1.1660 (1.3845)	grad_norm 28.0283 (30.1503)	mem 4879MB
[2022-05-31 04:32:55 MetaFG_0] (main.py 265): INFO Train: [44/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2944 (0.3073)	loss 0.8703 (1.3814)	grad_norm 33.4379 (30.1556)	mem 4879MB
[2022-05-31 04:32:58 MetaFG_0] (main.py 265): INFO Train: [44/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2950 (0.3073)	loss 1.7547 (1.3808)	grad_norm 55.6800 (30.1152)	mem 4879MB
[2022-05-31 04:33:01 MetaFG_0] (main.py 265): INFO Train: [44/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2954 (0.3072)	loss 1.3692 (1.3788)	grad_norm 24.1165 (29.9879)	mem 4879MB
[2022-05-31 04:33:04 MetaFG_0] (main.py 265): INFO Train: [44/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2933 (0.3071)	loss 0.9004 (1.3781)	grad_norm 22.1005 (29.9618)	mem 4879MB
[2022-05-31 04:33:07 MetaFG_0] (main.py 265): INFO Train: [44/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2942 (0.3071)	loss 1.0358 (1.3778)	grad_norm 29.6600 (29.9525)	mem 4879MB
[2022-05-31 04:33:10 MetaFG_0] (main.py 265): INFO Train: [44/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2948 (0.3071)	loss 1.3702 (1.3783)	grad_norm 23.9441 (29.9238)	mem 4879MB
[2022-05-31 04:33:13 MetaFG_0] (main.py 265): INFO Train: [44/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2925 (0.3070)	loss 1.6920 (1.3778)	grad_norm 33.4148 (29.8297)	mem 4879MB
[2022-05-31 04:33:16 MetaFG_0] (main.py 265): INFO Train: [44/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2929 (0.3069)	loss 1.0790 (1.3792)	grad_norm 47.3509 (29.8454)	mem 4879MB
[2022-05-31 04:33:19 MetaFG_0] (main.py 265): INFO Train: [44/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2990 (0.3070)	loss 1.0923 (1.3784)	grad_norm 27.0131 (29.9099)	mem 4879MB
[2022-05-31 04:33:22 MetaFG_0] (main.py 265): INFO Train: [44/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2921 (0.3069)	loss 1.5356 (1.3793)	grad_norm 35.3506 (30.0398)	mem 4879MB
[2022-05-31 04:33:25 MetaFG_0] (main.py 265): INFO Train: [44/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.3001 (0.3069)	loss 1.5553 (1.3798)	grad_norm 19.7274 (30.0333)	mem 4879MB
[2022-05-31 04:33:28 MetaFG_0] (main.py 265): INFO Train: [44/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2933 (0.3068)	loss 1.6637 (1.3800)	grad_norm 32.7129 (30.0080)	mem 4879MB
[2022-05-31 04:33:31 MetaFG_0] (main.py 265): INFO Train: [44/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2940 (0.3067)	loss 1.0390 (1.3785)	grad_norm 50.4204 (30.0728)	mem 4879MB
[2022-05-31 04:33:34 MetaFG_0] (main.py 265): INFO Train: [44/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2939 (0.3067)	loss 1.7580 (1.3813)	grad_norm 40.9584 (30.0095)	mem 4879MB
[2022-05-31 04:33:38 MetaFG_0] (main.py 265): INFO Train: [44/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2922 (0.3066)	loss 1.5205 (1.3821)	grad_norm 45.6063 (29.9420)	mem 4879MB
[2022-05-31 04:33:41 MetaFG_0] (main.py 265): INFO Train: [44/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2955 (0.3066)	loss 1.5452 (1.3815)	grad_norm 43.9497 (29.9547)	mem 4879MB
[2022-05-31 04:33:44 MetaFG_0] (main.py 265): INFO Train: [44/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2933 (0.3066)	loss 1.6298 (1.3805)	grad_norm 35.9913 (30.0960)	mem 4879MB
[2022-05-31 04:33:47 MetaFG_0] (main.py 265): INFO Train: [44/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.3024 (0.3065)	loss 1.5625 (1.3795)	grad_norm 33.9044 (30.1639)	mem 4879MB
[2022-05-31 04:33:50 MetaFG_0] (main.py 265): INFO Train: [44/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.3075 (0.3065)	loss 1.7616 (1.3787)	grad_norm 31.2998 (30.1450)	mem 4879MB
[2022-05-31 04:33:53 MetaFG_0] (main.py 265): INFO Train: [44/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.3001 (0.3065)	loss 1.5272 (1.3780)	grad_norm 12.6489 (30.0676)	mem 4879MB
[2022-05-31 04:33:56 MetaFG_0] (main.py 265): INFO Train: [44/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.3012 (0.3065)	loss 1.3092 (1.3779)	grad_norm 21.3306 (30.0902)	mem 4879MB
[2022-05-31 04:33:59 MetaFG_0] (main.py 265): INFO Train: [44/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2990 (0.3065)	loss 1.3981 (1.3788)	grad_norm 18.0660 (30.0262)	mem 4879MB
[2022-05-31 04:34:02 MetaFG_0] (main.py 265): INFO Train: [44/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2925 (0.3064)	loss 1.2917 (1.3777)	grad_norm 19.9207 (29.9126)	mem 4879MB
[2022-05-31 04:34:05 MetaFG_0] (main.py 265): INFO Train: [44/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2939 (0.3063)	loss 1.2189 (1.3773)	grad_norm 30.9098 (30.0119)	mem 4879MB
[2022-05-31 04:34:08 MetaFG_0] (main.py 265): INFO Train: [44/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2998 (0.3063)	loss 1.4297 (1.3777)	grad_norm 49.2349 (30.0517)	mem 4879MB
[2022-05-31 04:34:11 MetaFG_0] (main.py 265): INFO Train: [44/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2925 (0.3063)	loss 1.4309 (1.3798)	grad_norm 30.9732 (30.0940)	mem 4879MB
[2022-05-31 04:34:14 MetaFG_0] (main.py 265): INFO Train: [44/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2983 (0.3063)	loss 1.1829 (1.3796)	grad_norm 20.7393 (30.0920)	mem 4879MB
[2022-05-31 04:34:17 MetaFG_0] (main.py 265): INFO Train: [44/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2979 (0.3062)	loss 1.6135 (1.3814)	grad_norm 29.9934 (30.0712)	mem 4879MB
[2022-05-31 04:34:20 MetaFG_0] (main.py 265): INFO Train: [44/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3019 (0.3062)	loss 1.4152 (1.3813)	grad_norm 41.4299 (30.0388)	mem 4879MB
[2022-05-31 04:34:23 MetaFG_0] (main.py 265): INFO Train: [44/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2943 (0.3062)	loss 1.5149 (1.3824)	grad_norm 22.6647 (inf)	mem 4879MB
[2022-05-31 04:34:26 MetaFG_0] (main.py 265): INFO Train: [44/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2935 (0.3062)	loss 1.3145 (1.3826)	grad_norm 23.5106 (inf)	mem 4879MB
[2022-05-31 04:34:29 MetaFG_0] (main.py 265): INFO Train: [44/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.3008 (0.3061)	loss 1.4125 (1.3814)	grad_norm 32.1210 (inf)	mem 4879MB
[2022-05-31 04:34:32 MetaFG_0] (main.py 265): INFO Train: [44/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2941 (0.3061)	loss 1.1730 (1.3804)	grad_norm 23.1666 (inf)	mem 4879MB
[2022-05-31 04:34:35 MetaFG_0] (main.py 265): INFO Train: [44/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2988 (0.3061)	loss 1.6665 (1.3816)	grad_norm 37.9259 (inf)	mem 4879MB
[2022-05-31 04:34:38 MetaFG_0] (main.py 265): INFO Train: [44/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2923 (0.3061)	loss 1.4931 (1.3823)	grad_norm 24.4787 (inf)	mem 4879MB
[2022-05-31 04:34:41 MetaFG_0] (main.py 265): INFO Train: [44/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2948 (0.3060)	loss 1.3019 (1.3835)	grad_norm 21.8246 (inf)	mem 4879MB
[2022-05-31 04:34:45 MetaFG_0] (main.py 265): INFO Train: [44/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.3000 (0.3060)	loss 1.6661 (1.3839)	grad_norm 64.0578 (inf)	mem 4879MB
[2022-05-31 04:34:48 MetaFG_0] (main.py 265): INFO Train: [44/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2993 (0.3060)	loss 1.6352 (1.3839)	grad_norm 37.2923 (inf)	mem 4879MB
[2022-05-31 04:34:51 MetaFG_0] (main.py 265): INFO Train: [44/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.3048 (0.3060)	loss 1.3623 (1.3830)	grad_norm 22.4357 (inf)	mem 4879MB
[2022-05-31 04:34:54 MetaFG_0] (main.py 265): INFO Train: [44/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2930 (0.3060)	loss 1.5358 (1.3828)	grad_norm 27.9449 (inf)	mem 4879MB
[2022-05-31 04:34:57 MetaFG_0] (main.py 265): INFO Train: [44/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2946 (0.3060)	loss 1.6741 (1.3834)	grad_norm 28.9678 (inf)	mem 4879MB
[2022-05-31 04:35:00 MetaFG_0] (main.py 265): INFO Train: [44/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2921 (0.3060)	loss 1.4423 (1.3825)	grad_norm 23.7890 (inf)	mem 4879MB
[2022-05-31 04:35:03 MetaFG_0] (main.py 265): INFO Train: [44/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2936 (0.3059)	loss 1.2326 (1.3827)	grad_norm 30.7629 (inf)	mem 4879MB
[2022-05-31 04:35:06 MetaFG_0] (main.py 265): INFO Train: [44/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.3001 (0.3059)	loss 1.3196 (1.3816)	grad_norm 23.3038 (inf)	mem 4879MB
[2022-05-31 04:35:09 MetaFG_0] (main.py 265): INFO Train: [44/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2919 (0.3059)	loss 1.5678 (1.3814)	grad_norm 25.7962 (inf)	mem 4879MB
[2022-05-31 04:35:12 MetaFG_0] (main.py 265): INFO Train: [44/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2980 (0.3059)	loss 0.9752 (1.3813)	grad_norm 38.8220 (inf)	mem 4879MB
[2022-05-31 04:35:15 MetaFG_0] (main.py 265): INFO Train: [44/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2931 (0.3059)	loss 1.4297 (1.3823)	grad_norm 30.9347 (inf)	mem 4879MB
[2022-05-31 04:35:18 MetaFG_0] (main.py 265): INFO Train: [44/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2951 (0.3058)	loss 1.2486 (1.3811)	grad_norm 40.4906 (inf)	mem 4879MB
[2022-05-31 04:35:21 MetaFG_0] (main.py 265): INFO Train: [44/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2926 (0.3058)	loss 0.9350 (1.3805)	grad_norm 38.2191 (inf)	mem 4879MB
[2022-05-31 04:35:24 MetaFG_0] (main.py 265): INFO Train: [44/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.3014 (0.3059)	loss 1.6524 (1.3805)	grad_norm 26.5612 (inf)	mem 4879MB
[2022-05-31 04:35:27 MetaFG_0] (main.py 265): INFO Train: [44/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2940 (0.3059)	loss 1.3883 (1.3793)	grad_norm 55.0719 (inf)	mem 4879MB
[2022-05-31 04:35:30 MetaFG_0] (main.py 265): INFO Train: [44/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2987 (0.3059)	loss 1.3171 (1.3792)	grad_norm 32.7609 (inf)	mem 4879MB
[2022-05-31 04:35:33 MetaFG_0] (main.py 265): INFO Train: [44/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2991 (0.3058)	loss 1.6109 (1.3791)	grad_norm 18.0055 (inf)	mem 4879MB
[2022-05-31 04:35:36 MetaFG_0] (main.py 265): INFO Train: [44/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2939 (0.3058)	loss 1.1455 (1.3789)	grad_norm 33.2147 (inf)	mem 4879MB
[2022-05-31 04:35:40 MetaFG_0] (main.py 265): INFO Train: [44/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.3318 (0.3059)	loss 1.4335 (1.3786)	grad_norm 26.8881 (inf)	mem 4879MB
[2022-05-31 04:35:43 MetaFG_0] (main.py 265): INFO Train: [44/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2954 (0.3060)	loss 1.5965 (1.3783)	grad_norm 21.0528 (inf)	mem 4879MB
[2022-05-31 04:35:46 MetaFG_0] (main.py 265): INFO Train: [44/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.3010 (0.3060)	loss 1.1767 (1.3773)	grad_norm 47.4840 (inf)	mem 4879MB
[2022-05-31 04:35:49 MetaFG_0] (main.py 265): INFO Train: [44/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2935 (0.3060)	loss 1.6905 (1.3774)	grad_norm 24.5966 (inf)	mem 4879MB
[2022-05-31 04:35:52 MetaFG_0] (main.py 265): INFO Train: [44/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2925 (0.3060)	loss 1.1647 (1.3765)	grad_norm 29.7755 (inf)	mem 4879MB
[2022-05-31 04:35:55 MetaFG_0] (main.py 265): INFO Train: [44/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2989 (0.3060)	loss 1.3600 (1.3764)	grad_norm 22.1088 (inf)	mem 4879MB
[2022-05-31 04:35:58 MetaFG_0] (main.py 265): INFO Train: [44/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2942 (0.3060)	loss 1.6554 (1.3771)	grad_norm 34.8965 (inf)	mem 4879MB
[2022-05-31 04:36:01 MetaFG_0] (main.py 265): INFO Train: [44/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2945 (0.3060)	loss 1.4050 (1.3769)	grad_norm 23.3982 (inf)	mem 4879MB
[2022-05-31 04:36:04 MetaFG_0] (main.py 265): INFO Train: [44/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2920 (0.3060)	loss 1.2527 (1.3774)	grad_norm 12.5356 (inf)	mem 4879MB
[2022-05-31 04:36:07 MetaFG_0] (main.py 265): INFO Train: [44/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2935 (0.3060)	loss 0.9324 (1.3769)	grad_norm 22.2505 (inf)	mem 4879MB
[2022-05-31 04:36:10 MetaFG_0] (main.py 265): INFO Train: [44/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2935 (0.3059)	loss 1.7241 (1.3763)	grad_norm 29.6704 (inf)	mem 4879MB
[2022-05-31 04:36:13 MetaFG_0] (main.py 265): INFO Train: [44/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2931 (0.3059)	loss 1.6684 (1.3779)	grad_norm 16.7147 (inf)	mem 4879MB
[2022-05-31 04:36:16 MetaFG_0] (main.py 265): INFO Train: [44/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2999 (0.3059)	loss 1.4873 (1.3793)	grad_norm 37.2894 (inf)	mem 4879MB
[2022-05-31 04:36:19 MetaFG_0] (main.py 265): INFO Train: [44/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2933 (0.3059)	loss 1.4014 (1.3792)	grad_norm 18.8038 (inf)	mem 4879MB
[2022-05-31 04:36:22 MetaFG_0] (main.py 265): INFO Train: [44/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3001 (0.3059)	loss 0.9629 (1.3807)	grad_norm 45.4117 (inf)	mem 4879MB
[2022-05-31 04:36:25 MetaFG_0] (main.py 265): INFO Train: [44/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3001 (0.3059)	loss 1.4177 (1.3808)	grad_norm 20.1320 (inf)	mem 4879MB
[2022-05-31 04:36:28 MetaFG_0] (main.py 265): INFO Train: [44/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2937 (0.3059)	loss 1.4632 (1.3802)	grad_norm 34.0155 (inf)	mem 4879MB
[2022-05-31 04:36:31 MetaFG_0] (main.py 265): INFO Train: [44/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2933 (0.3059)	loss 1.5406 (1.3803)	grad_norm 23.8490 (inf)	mem 4879MB
[2022-05-31 04:36:35 MetaFG_0] (main.py 265): INFO Train: [44/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2935 (0.3059)	loss 1.3919 (1.3806)	grad_norm 28.4216 (inf)	mem 4879MB
[2022-05-31 04:36:38 MetaFG_0] (main.py 265): INFO Train: [44/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2932 (0.3059)	loss 1.5444 (1.3805)	grad_norm 24.8058 (inf)	mem 4879MB
[2022-05-31 04:36:41 MetaFG_0] (main.py 265): INFO Train: [44/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2930 (0.3059)	loss 1.3754 (1.3798)	grad_norm 25.1186 (inf)	mem 4879MB
[2022-05-31 04:36:44 MetaFG_0] (main.py 265): INFO Train: [44/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2934 (0.3059)	loss 1.3909 (1.3803)	grad_norm 21.7977 (inf)	mem 4879MB
[2022-05-31 04:36:47 MetaFG_0] (main.py 265): INFO Train: [44/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2941 (0.3058)	loss 1.4564 (1.3807)	grad_norm 32.8795 (inf)	mem 4879MB
[2022-05-31 04:36:50 MetaFG_0] (main.py 265): INFO Train: [44/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2940 (0.3058)	loss 1.4498 (1.3799)	grad_norm 22.4939 (inf)	mem 4879MB
[2022-05-31 04:36:53 MetaFG_0] (main.py 265): INFO Train: [44/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2927 (0.3058)	loss 1.2832 (1.3800)	grad_norm 29.1055 (inf)	mem 4879MB
[2022-05-31 04:36:56 MetaFG_0] (main.py 265): INFO Train: [44/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2941 (0.3058)	loss 1.7083 (1.3791)	grad_norm 31.2934 (inf)	mem 4879MB
[2022-05-31 04:36:59 MetaFG_0] (main.py 265): INFO Train: [44/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2927 (0.3058)	loss 1.5774 (1.3793)	grad_norm 18.1474 (inf)	mem 4879MB
[2022-05-31 04:37:02 MetaFG_0] (main.py 265): INFO Train: [44/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2987 (0.3058)	loss 1.5963 (1.3794)	grad_norm 23.2689 (inf)	mem 4879MB
[2022-05-31 04:37:05 MetaFG_0] (main.py 265): INFO Train: [44/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2991 (0.3058)	loss 1.6919 (1.3797)	grad_norm 26.0247 (inf)	mem 4879MB
[2022-05-31 04:37:08 MetaFG_0] (main.py 265): INFO Train: [44/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2928 (0.3057)	loss 1.0977 (1.3796)	grad_norm 27.7419 (inf)	mem 4879MB
[2022-05-31 04:37:11 MetaFG_0] (main.py 265): INFO Train: [44/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2929 (0.3057)	loss 1.5746 (1.3804)	grad_norm 30.3991 (inf)	mem 4879MB
[2022-05-31 04:37:14 MetaFG_0] (main.py 265): INFO Train: [44/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2935 (0.3057)	loss 1.6820 (1.3808)	grad_norm 30.0716 (inf)	mem 4879MB
[2022-05-31 04:37:17 MetaFG_0] (main.py 265): INFO Train: [44/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2930 (0.3057)	loss 1.4064 (1.3800)	grad_norm 21.0987 (inf)	mem 4879MB
[2022-05-31 04:37:20 MetaFG_0] (main.py 265): INFO Train: [44/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2982 (0.3057)	loss 1.0670 (1.3801)	grad_norm 25.6581 (inf)	mem 4879MB
[2022-05-31 04:37:23 MetaFG_0] (main.py 265): INFO Train: [44/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2926 (0.3057)	loss 1.1138 (1.3790)	grad_norm 21.5727 (inf)	mem 4879MB
[2022-05-31 04:37:26 MetaFG_0] (main.py 265): INFO Train: [44/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3037 (0.3057)	loss 1.1545 (1.3788)	grad_norm 29.4419 (inf)	mem 4879MB
[2022-05-31 04:37:29 MetaFG_0] (main.py 265): INFO Train: [44/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3003 (0.3057)	loss 0.9696 (1.3780)	grad_norm 19.8933 (inf)	mem 4879MB
[2022-05-31 04:37:32 MetaFG_0] (main.py 265): INFO Train: [44/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2950 (0.3057)	loss 1.6181 (1.3781)	grad_norm 37.0433 (inf)	mem 4879MB
[2022-05-31 04:37:35 MetaFG_0] (main.py 265): INFO Train: [44/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2941 (0.3057)	loss 1.4877 (1.3773)	grad_norm 30.1025 (inf)	mem 4879MB
[2022-05-31 04:37:38 MetaFG_0] (main.py 265): INFO Train: [44/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2925 (0.3057)	loss 1.4725 (1.3780)	grad_norm 19.9456 (inf)	mem 4879MB
[2022-05-31 04:37:42 MetaFG_0] (main.py 265): INFO Train: [44/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2926 (0.3056)	loss 1.3688 (1.3771)	grad_norm 23.6526 (inf)	mem 4879MB
[2022-05-31 04:37:45 MetaFG_0] (main.py 265): INFO Train: [44/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.3003 (0.3056)	loss 1.1565 (1.3773)	grad_norm 26.3737 (inf)	mem 4879MB
[2022-05-31 04:37:48 MetaFG_0] (main.py 265): INFO Train: [44/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3051 (0.3056)	loss 1.6693 (1.3782)	grad_norm 24.2282 (inf)	mem 4879MB
[2022-05-31 04:37:51 MetaFG_0] (main.py 265): INFO Train: [44/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2999 (0.3056)	loss 1.0661 (1.3780)	grad_norm 46.9124 (inf)	mem 4879MB
[2022-05-31 04:37:54 MetaFG_0] (main.py 265): INFO Train: [44/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2983 (0.3056)	loss 1.3866 (1.3780)	grad_norm 37.4801 (inf)	mem 4879MB
[2022-05-31 04:37:57 MetaFG_0] (main.py 265): INFO Train: [44/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2978 (0.3056)	loss 1.4390 (1.3788)	grad_norm 29.8950 (inf)	mem 4879MB
[2022-05-31 04:38:00 MetaFG_0] (main.py 265): INFO Train: [44/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2946 (0.3056)	loss 1.5775 (1.3788)	grad_norm 19.2817 (inf)	mem 4879MB
[2022-05-31 04:38:03 MetaFG_0] (main.py 265): INFO Train: [44/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2923 (0.3056)	loss 1.2383 (1.3777)	grad_norm 25.2434 (inf)	mem 4879MB
[2022-05-31 04:38:06 MetaFG_0] (main.py 265): INFO Train: [44/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2996 (0.3056)	loss 0.9376 (1.3777)	grad_norm 24.2283 (inf)	mem 4879MB
[2022-05-31 04:38:09 MetaFG_0] (main.py 265): INFO Train: [44/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2992 (0.3056)	loss 1.0795 (1.3780)	grad_norm 48.9611 (inf)	mem 4879MB
[2022-05-31 04:38:12 MetaFG_0] (main.py 265): INFO Train: [44/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2936 (0.3056)	loss 1.0314 (1.3780)	grad_norm 47.0442 (inf)	mem 4879MB
[2022-05-31 04:38:15 MetaFG_0] (main.py 265): INFO Train: [44/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2928 (0.3056)	loss 1.2535 (1.3775)	grad_norm 32.6532 (inf)	mem 4879MB
[2022-05-31 04:38:18 MetaFG_0] (main.py 265): INFO Train: [44/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2990 (0.3056)	loss 1.5649 (1.3778)	grad_norm 35.6602 (inf)	mem 4879MB
[2022-05-31 04:38:21 MetaFG_0] (main.py 265): INFO Train: [44/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2949 (0.3056)	loss 1.4190 (1.3785)	grad_norm 21.8967 (inf)	mem 4879MB
[2022-05-31 04:38:24 MetaFG_0] (main.py 265): INFO Train: [44/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2956 (0.3056)	loss 1.3224 (1.3780)	grad_norm 28.3531 (inf)	mem 4879MB
[2022-05-31 04:38:27 MetaFG_0] (main.py 265): INFO Train: [44/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2981 (0.3056)	loss 1.6718 (1.3774)	grad_norm 31.8501 (inf)	mem 4879MB
[2022-05-31 04:38:30 MetaFG_0] (main.py 265): INFO Train: [44/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2965 (0.3056)	loss 1.5978 (1.3778)	grad_norm 22.4227 (inf)	mem 4879MB
[2022-05-31 04:38:33 MetaFG_0] (main.py 265): INFO Train: [44/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.3044 (0.3056)	loss 1.2183 (1.3781)	grad_norm 25.4886 (inf)	mem 4879MB
[2022-05-31 04:38:36 MetaFG_0] (main.py 265): INFO Train: [44/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2990 (0.3056)	loss 1.6116 (1.3788)	grad_norm 30.1146 (inf)	mem 4879MB
[2022-05-31 04:38:39 MetaFG_0] (main.py 265): INFO Train: [44/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.3036 (0.3056)	loss 1.4371 (1.3784)	grad_norm 103.6718 (inf)	mem 4879MB
[2022-05-31 04:38:43 MetaFG_0] (main.py 265): INFO Train: [44/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2925 (0.3056)	loss 1.4536 (1.3783)	grad_norm 19.9087 (inf)	mem 4879MB
[2022-05-31 04:38:46 MetaFG_0] (main.py 265): INFO Train: [44/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2933 (0.3056)	loss 1.4613 (1.3784)	grad_norm 11.6575 (inf)	mem 4879MB
[2022-05-31 04:38:49 MetaFG_0] (main.py 265): INFO Train: [44/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2986 (0.3056)	loss 1.1259 (1.3782)	grad_norm 21.8702 (inf)	mem 4879MB
[2022-05-31 04:38:52 MetaFG_0] (main.py 265): INFO Train: [44/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2958 (0.3056)	loss 1.1427 (1.3773)	grad_norm 39.3059 (inf)	mem 4879MB
[2022-05-31 04:38:55 MetaFG_0] (main.py 265): INFO Train: [44/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2930 (0.3056)	loss 1.4187 (1.3777)	grad_norm 29.6237 (inf)	mem 4879MB
[2022-05-31 04:38:58 MetaFG_0] (main.py 265): INFO Train: [44/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2926 (0.3056)	loss 0.9672 (1.3773)	grad_norm 26.3511 (inf)	mem 4879MB
[2022-05-31 04:39:01 MetaFG_0] (main.py 265): INFO Train: [44/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2924 (0.3056)	loss 1.3305 (1.3775)	grad_norm 45.0589 (inf)	mem 4879MB
[2022-05-31 04:39:01 MetaFG_0] (main.py 272): INFO EPOCH 44 training takes 0:07:57
[2022-05-31 04:39:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_44.pth saving......
[2022-05-31 04:39:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_44.pth saved !!!
[2022-05-31 04:39:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:39:04 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:39:04 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:39:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.762 (0.762)	Loss 0.8864 (0.8864)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 04:39:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.157)	Loss 0.8095 (0.6854)	Acc@1 78.125 (82.386)	Acc@5 96.875 (98.295)	Mem 4879MB
[2022-05-31 04:39:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.126)	Loss 0.8147 (0.6861)	Acc@1 84.375 (83.780)	Acc@5 96.875 (98.363)	Mem 4879MB
[2022-05-31 04:39:07 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.089 (0.115)	Loss 0.7403 (0.6828)	Acc@1 78.125 (83.569)	Acc@5 100.000 (98.286)	Mem 4879MB
[2022-05-31 04:39:08 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.099 (0.110)	Loss 0.6897 (0.6735)	Acc@1 84.375 (83.918)	Acc@5 96.875 (98.323)	Mem 4879MB
[2022-05-31 04:39:09 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.107)	Loss 1.0265 (0.6842)	Acc@1 71.875 (83.824)	Acc@5 100.000 (98.223)	Mem 4879MB
[2022-05-31 04:39:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 0.4506 (0.6742)	Acc@1 90.625 (84.119)	Acc@5 100.000 (98.258)	Mem 4879MB
[2022-05-31 04:39:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.104)	Loss 0.6815 (0.6806)	Acc@1 87.500 (83.935)	Acc@5 100.000 (98.283)	Mem 4879MB
[2022-05-31 04:39:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.103 (0.103)	Loss 0.5783 (0.6799)	Acc@1 84.375 (83.912)	Acc@5 100.000 (98.225)	Mem 4879MB
[2022-05-31 04:39:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.102)	Loss 0.6642 (0.6778)	Acc@1 90.625 (84.135)	Acc@5 96.875 (98.180)	Mem 4879MB
[2022-05-31 04:39:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.101)	Loss 0.5548 (0.6734)	Acc@1 96.875 (84.313)	Acc@5 96.875 (98.175)	Mem 4879MB
[2022-05-31 04:39:15 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.100)	Loss 0.6886 (0.6674)	Acc@1 84.375 (84.516)	Acc@5 100.000 (98.311)	Mem 4879MB
[2022-05-31 04:39:16 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.090 (0.100)	Loss 0.6832 (0.6664)	Acc@1 87.500 (84.530)	Acc@5 96.875 (98.295)	Mem 4879MB
[2022-05-31 04:39:17 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.098 (0.100)	Loss 0.5942 (0.6619)	Acc@1 84.375 (84.733)	Acc@5 100.000 (98.282)	Mem 4879MB
[2022-05-31 04:39:18 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.100)	Loss 0.6535 (0.6625)	Acc@1 84.375 (84.752)	Acc@5 100.000 (98.316)	Mem 4879MB
[2022-05-31 04:39:19 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.098 (0.099)	Loss 0.4336 (0.6594)	Acc@1 90.625 (84.892)	Acc@5 100.000 (98.386)	Mem 4879MB
[2022-05-31 04:39:20 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.118 (0.099)	Loss 0.7758 (0.6625)	Acc@1 68.750 (84.744)	Acc@5 96.875 (98.370)	Mem 4879MB
[2022-05-31 04:39:21 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.099)	Loss 0.6527 (0.6630)	Acc@1 84.375 (84.814)	Acc@5 100.000 (98.355)	Mem 4879MB
[2022-05-31 04:39:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.100 (0.099)	Loss 0.5814 (0.6608)	Acc@1 84.375 (84.841)	Acc@5 100.000 (98.377)	Mem 4879MB
[2022-05-31 04:39:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.099)	Loss 0.3954 (0.6600)	Acc@1 96.875 (84.964)	Acc@5 100.000 (98.348)	Mem 4879MB
[2022-05-31 04:39:23 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.101 (0.098)	Loss 0.6268 (0.6604)	Acc@1 90.625 (84.950)	Acc@5 100.000 (98.368)	Mem 4879MB
[2022-05-31 04:39:24 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.098)	Loss 0.5174 (0.6590)	Acc@1 84.375 (84.923)	Acc@5 100.000 (98.371)	Mem 4879MB
[2022-05-31 04:39:25 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.089 (0.098)	Loss 0.6502 (0.6593)	Acc@1 78.125 (84.912)	Acc@5 96.875 (98.402)	Mem 4879MB
[2022-05-31 04:39:26 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.098)	Loss 0.6061 (0.6600)	Acc@1 84.375 (84.848)	Acc@5 100.000 (98.404)	Mem 4879MB
[2022-05-31 04:39:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.092 (0.098)	Loss 0.6012 (0.6594)	Acc@1 87.500 (84.829)	Acc@5 100.000 (98.431)	Mem 4879MB
[2022-05-31 04:39:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.098)	Loss 0.4902 (0.6571)	Acc@1 87.500 (84.873)	Acc@5 100.000 (98.481)	Mem 4879MB
[2022-05-31 04:39:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.106 (0.098)	Loss 0.8044 (0.6538)	Acc@1 87.500 (85.034)	Acc@5 96.875 (98.491)	Mem 4879MB
[2022-05-31 04:39:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.098)	Loss 0.6644 (0.6494)	Acc@1 87.500 (85.217)	Acc@5 96.875 (98.501)	Mem 4879MB
[2022-05-31 04:39:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.098)	Loss 0.5265 (0.6505)	Acc@1 90.625 (85.142)	Acc@5 96.875 (98.465)	Mem 4879MB
[2022-05-31 04:39:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.092 (0.097)	Loss 0.6587 (0.6510)	Acc@1 81.250 (85.105)	Acc@5 100.000 (98.443)	Mem 4879MB
[2022-05-31 04:39:33 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.097)	Loss 0.6517 (0.6510)	Acc@1 84.375 (85.133)	Acc@5 100.000 (98.443)	Mem 4879MB
[2022-05-31 04:39:34 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.5414 (0.6526)	Acc@1 87.500 (85.109)	Acc@5 100.000 (98.412)	Mem 4879MB
[2022-05-31 04:39:34 MetaFG_0] (main.py 330): INFO  * Acc@1 85.130 Acc@5 98.420
[2022-05-31 04:39:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.1%
[2022-05-31 04:39:34 MetaFG_0] (main.py 171): INFO Max accuracy: 85.13%
[2022-05-31 04:39:35 MetaFG_0] (main.py 265): INFO Train: [45/300][0/1562]	eta 0:25:56 lr 0.000006	time 0.9965 (0.9965)	loss 1.6465 (1.6465)	grad_norm 29.0531 (29.0531)	mem 4879MB
[2022-05-31 04:39:38 MetaFG_0] (main.py 265): INFO Train: [45/300][10/1562]	eta 0:09:40 lr 0.000006	time 0.2942 (0.3738)	loss 1.2116 (1.4994)	grad_norm 25.8598 (27.2093)	mem 4879MB
[2022-05-31 04:39:41 MetaFG_0] (main.py 265): INFO Train: [45/300][20/1562]	eta 0:08:46 lr 0.000006	time 0.2938 (0.3412)	loss 1.1192 (1.3869)	grad_norm 26.7712 (28.4324)	mem 4879MB
[2022-05-31 04:39:44 MetaFG_0] (main.py 265): INFO Train: [45/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2967 (0.3295)	loss 1.2286 (1.3891)	grad_norm 37.7195 (29.6247)	mem 4879MB
[2022-05-31 04:39:47 MetaFG_0] (main.py 265): INFO Train: [45/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2928 (0.3235)	loss 1.4784 (1.3934)	grad_norm 18.2937 (29.2990)	mem 4879MB
[2022-05-31 04:39:50 MetaFG_0] (main.py 265): INFO Train: [45/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2937 (0.3198)	loss 1.5166 (1.4073)	grad_norm 32.5806 (29.1921)	mem 4879MB
[2022-05-31 04:39:53 MetaFG_0] (main.py 265): INFO Train: [45/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2988 (0.3171)	loss 1.0789 (1.4141)	grad_norm 25.7303 (28.9098)	mem 4879MB
[2022-05-31 04:39:56 MetaFG_0] (main.py 265): INFO Train: [45/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.3002 (0.3157)	loss 0.9879 (1.4060)	grad_norm 34.8266 (28.5000)	mem 4879MB
[2022-05-31 04:40:00 MetaFG_0] (main.py 265): INFO Train: [45/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2951 (0.3143)	loss 1.5766 (1.3989)	grad_norm 23.8032 (28.3731)	mem 4879MB
[2022-05-31 04:40:03 MetaFG_0] (main.py 265): INFO Train: [45/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2952 (0.3133)	loss 1.3525 (1.4096)	grad_norm 28.8622 (28.4582)	mem 4879MB
[2022-05-31 04:40:06 MetaFG_0] (main.py 265): INFO Train: [45/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2996 (0.3127)	loss 1.4780 (1.4146)	grad_norm 44.2574 (28.2453)	mem 4879MB
[2022-05-31 04:40:09 MetaFG_0] (main.py 265): INFO Train: [45/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.3003 (0.3119)	loss 1.5459 (1.4085)	grad_norm 26.3538 (28.1533)	mem 4879MB
[2022-05-31 04:40:12 MetaFG_0] (main.py 265): INFO Train: [45/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2999 (0.3113)	loss 1.3527 (1.4115)	grad_norm 20.5421 (28.5734)	mem 4879MB
[2022-05-31 04:40:15 MetaFG_0] (main.py 265): INFO Train: [45/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2930 (0.3108)	loss 1.5602 (1.4122)	grad_norm 17.7106 (28.7693)	mem 4879MB
[2022-05-31 04:40:18 MetaFG_0] (main.py 265): INFO Train: [45/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2970 (0.3104)	loss 1.4819 (1.4038)	grad_norm 40.6727 (29.1706)	mem 4879MB
[2022-05-31 04:40:21 MetaFG_0] (main.py 265): INFO Train: [45/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.3023 (0.3100)	loss 1.3554 (1.4005)	grad_norm 26.1738 (29.1063)	mem 4879MB
[2022-05-31 04:40:24 MetaFG_0] (main.py 265): INFO Train: [45/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.3022 (0.3097)	loss 1.4574 (1.4039)	grad_norm 22.2903 (29.0327)	mem 4879MB
[2022-05-31 04:40:27 MetaFG_0] (main.py 265): INFO Train: [45/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.3000 (0.3096)	loss 1.4959 (1.4049)	grad_norm 21.1786 (28.9517)	mem 4879MB
[2022-05-31 04:40:30 MetaFG_0] (main.py 265): INFO Train: [45/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.3002 (0.3093)	loss 1.3612 (1.4028)	grad_norm 20.9503 (29.1651)	mem 4879MB
[2022-05-31 04:40:33 MetaFG_0] (main.py 265): INFO Train: [45/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.3008 (0.3090)	loss 1.4244 (1.4031)	grad_norm 25.9305 (29.2454)	mem 4879MB
[2022-05-31 04:40:36 MetaFG_0] (main.py 265): INFO Train: [45/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2934 (0.3088)	loss 1.6598 (1.3986)	grad_norm 24.7852 (29.2462)	mem 4879MB
[2022-05-31 04:40:39 MetaFG_0] (main.py 265): INFO Train: [45/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2967 (0.3087)	loss 1.2938 (1.3950)	grad_norm 38.7984 (29.6155)	mem 4879MB
[2022-05-31 04:40:42 MetaFG_0] (main.py 265): INFO Train: [45/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2940 (0.3085)	loss 0.9342 (1.3925)	grad_norm 21.9322 (29.4624)	mem 4879MB
[2022-05-31 04:40:45 MetaFG_0] (main.py 265): INFO Train: [45/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2988 (0.3085)	loss 0.6924 (1.3919)	grad_norm 42.7586 (29.4442)	mem 4879MB
[2022-05-31 04:40:48 MetaFG_0] (main.py 265): INFO Train: [45/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2947 (0.3083)	loss 1.4004 (1.3963)	grad_norm 31.9136 (29.4347)	mem 4879MB
[2022-05-31 04:40:51 MetaFG_0] (main.py 265): INFO Train: [45/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.3018 (0.3082)	loss 1.4638 (1.3993)	grad_norm 25.6114 (29.4865)	mem 4879MB
[2022-05-31 04:40:54 MetaFG_0] (main.py 265): INFO Train: [45/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2921 (0.3081)	loss 1.3351 (1.3966)	grad_norm 19.0125 (29.4739)	mem 4879MB
[2022-05-31 04:40:58 MetaFG_0] (main.py 265): INFO Train: [45/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.3003 (0.3080)	loss 1.5045 (1.3947)	grad_norm 35.1905 (29.5633)	mem 4879MB
[2022-05-31 04:41:01 MetaFG_0] (main.py 265): INFO Train: [45/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.3003 (0.3080)	loss 1.3221 (1.3947)	grad_norm 29.3324 (29.5242)	mem 4879MB
[2022-05-31 04:41:04 MetaFG_0] (main.py 265): INFO Train: [45/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2994 (0.3079)	loss 1.6115 (1.3935)	grad_norm 44.2874 (29.5397)	mem 4879MB
[2022-05-31 04:41:07 MetaFG_0] (main.py 265): INFO Train: [45/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2997 (0.3078)	loss 1.4190 (1.3930)	grad_norm 24.1451 (29.6040)	mem 4879MB
[2022-05-31 04:41:10 MetaFG_0] (main.py 265): INFO Train: [45/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2929 (0.3077)	loss 1.4979 (1.3956)	grad_norm 21.2658 (29.4972)	mem 4879MB
[2022-05-31 04:41:13 MetaFG_0] (main.py 265): INFO Train: [45/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2999 (0.3076)	loss 1.7020 (1.3954)	grad_norm 20.4615 (29.3915)	mem 4879MB
[2022-05-31 04:41:16 MetaFG_0] (main.py 265): INFO Train: [45/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2950 (0.3075)	loss 1.5827 (1.3944)	grad_norm 28.7352 (29.3089)	mem 4879MB
[2022-05-31 04:41:19 MetaFG_0] (main.py 265): INFO Train: [45/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2926 (0.3074)	loss 1.6290 (1.3957)	grad_norm 23.4525 (29.3866)	mem 4879MB
[2022-05-31 04:41:22 MetaFG_0] (main.py 265): INFO Train: [45/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2924 (0.3074)	loss 1.6284 (1.3954)	grad_norm 29.9204 (29.3387)	mem 4879MB
[2022-05-31 04:41:25 MetaFG_0] (main.py 265): INFO Train: [45/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2930 (0.3073)	loss 1.3966 (1.3934)	grad_norm 22.3330 (29.3558)	mem 4879MB
[2022-05-31 04:41:28 MetaFG_0] (main.py 265): INFO Train: [45/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.3018 (0.3072)	loss 1.6054 (1.3965)	grad_norm 28.6027 (29.2087)	mem 4879MB
[2022-05-31 04:41:31 MetaFG_0] (main.py 265): INFO Train: [45/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2930 (0.3071)	loss 1.4300 (1.3976)	grad_norm 25.7180 (29.0327)	mem 4879MB
[2022-05-31 04:41:34 MetaFG_0] (main.py 265): INFO Train: [45/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2930 (0.3070)	loss 1.2666 (1.3972)	grad_norm 43.2270 (29.0242)	mem 4879MB
[2022-05-31 04:41:37 MetaFG_0] (main.py 265): INFO Train: [45/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2936 (0.3070)	loss 1.3915 (1.3969)	grad_norm 48.7810 (29.1184)	mem 4879MB
[2022-05-31 04:41:40 MetaFG_0] (main.py 265): INFO Train: [45/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2940 (0.3069)	loss 1.4039 (1.3966)	grad_norm 15.6812 (29.0923)	mem 4879MB
[2022-05-31 04:41:43 MetaFG_0] (main.py 265): INFO Train: [45/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2935 (0.3068)	loss 1.4009 (1.3992)	grad_norm 22.4908 (28.9967)	mem 4879MB
[2022-05-31 04:41:46 MetaFG_0] (main.py 265): INFO Train: [45/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2996 (0.3068)	loss 1.3804 (1.3976)	grad_norm 27.7364 (29.0213)	mem 4879MB
[2022-05-31 04:41:49 MetaFG_0] (main.py 265): INFO Train: [45/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2924 (0.3068)	loss 1.6198 (1.3975)	grad_norm 26.2726 (28.9353)	mem 4879MB
[2022-05-31 04:41:52 MetaFG_0] (main.py 265): INFO Train: [45/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2927 (0.3068)	loss 1.5195 (1.3966)	grad_norm 23.1529 (28.8197)	mem 4879MB
[2022-05-31 04:41:55 MetaFG_0] (main.py 265): INFO Train: [45/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.3002 (0.3068)	loss 1.0949 (1.3956)	grad_norm 27.6813 (28.8720)	mem 4879MB
[2022-05-31 04:41:59 MetaFG_0] (main.py 265): INFO Train: [45/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2997 (0.3068)	loss 0.8971 (1.3926)	grad_norm 20.1164 (28.7970)	mem 4879MB
[2022-05-31 04:42:02 MetaFG_0] (main.py 265): INFO Train: [45/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2929 (0.3067)	loss 1.5027 (1.3914)	grad_norm 54.8257 (28.8520)	mem 4879MB
[2022-05-31 04:42:05 MetaFG_0] (main.py 265): INFO Train: [45/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2924 (0.3067)	loss 1.4206 (1.3899)	grad_norm 35.7363 (28.7842)	mem 4879MB
[2022-05-31 04:42:08 MetaFG_0] (main.py 265): INFO Train: [45/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2930 (0.3067)	loss 1.0231 (1.3888)	grad_norm 34.4402 (28.9132)	mem 4879MB
[2022-05-31 04:42:11 MetaFG_0] (main.py 265): INFO Train: [45/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2999 (0.3067)	loss 1.4687 (1.3884)	grad_norm 18.1424 (28.8646)	mem 4879MB
[2022-05-31 04:42:14 MetaFG_0] (main.py 265): INFO Train: [45/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2978 (0.3067)	loss 1.3507 (1.3876)	grad_norm 15.8134 (29.0038)	mem 4879MB
[2022-05-31 04:42:17 MetaFG_0] (main.py 265): INFO Train: [45/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2992 (0.3066)	loss 0.8595 (1.3857)	grad_norm 29.4210 (28.9453)	mem 4879MB
[2022-05-31 04:42:20 MetaFG_0] (main.py 265): INFO Train: [45/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2989 (0.3066)	loss 1.3859 (1.3862)	grad_norm 34.1731 (29.0152)	mem 4879MB
[2022-05-31 04:42:23 MetaFG_0] (main.py 265): INFO Train: [45/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2997 (0.3066)	loss 1.0463 (1.3857)	grad_norm 22.8022 (29.0800)	mem 4879MB
[2022-05-31 04:42:26 MetaFG_0] (main.py 265): INFO Train: [45/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2995 (0.3065)	loss 0.9412 (1.3857)	grad_norm 33.7888 (29.0125)	mem 4879MB
[2022-05-31 04:42:29 MetaFG_0] (main.py 265): INFO Train: [45/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2989 (0.3065)	loss 1.6079 (1.3864)	grad_norm 28.4999 (29.0423)	mem 4879MB
[2022-05-31 04:42:32 MetaFG_0] (main.py 265): INFO Train: [45/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2928 (0.3065)	loss 1.2414 (1.3870)	grad_norm 33.4253 (29.0124)	mem 4879MB
[2022-05-31 04:42:35 MetaFG_0] (main.py 265): INFO Train: [45/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2937 (0.3064)	loss 1.1227 (1.3842)	grad_norm 19.7904 (29.0550)	mem 4879MB
[2022-05-31 04:42:38 MetaFG_0] (main.py 265): INFO Train: [45/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2943 (0.3064)	loss 1.5030 (1.3854)	grad_norm 25.2493 (29.1782)	mem 4879MB
[2022-05-31 04:42:41 MetaFG_0] (main.py 265): INFO Train: [45/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2925 (0.3064)	loss 1.3954 (1.3849)	grad_norm 26.5212 (29.2129)	mem 4879MB
[2022-05-31 04:42:44 MetaFG_0] (main.py 265): INFO Train: [45/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2990 (0.3064)	loss 1.3197 (1.3851)	grad_norm 37.1383 (29.3947)	mem 4879MB
[2022-05-31 04:42:47 MetaFG_0] (main.py 265): INFO Train: [45/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2928 (0.3064)	loss 1.6001 (1.3845)	grad_norm 32.5460 (29.3970)	mem 4879MB
[2022-05-31 04:42:50 MetaFG_0] (main.py 265): INFO Train: [45/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2948 (0.3064)	loss 1.5627 (1.3836)	grad_norm 25.6771 (29.3532)	mem 4879MB
[2022-05-31 04:42:53 MetaFG_0] (main.py 265): INFO Train: [45/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2931 (0.3063)	loss 1.5332 (1.3819)	grad_norm 35.5698 (29.4382)	mem 4879MB
[2022-05-31 04:42:57 MetaFG_0] (main.py 265): INFO Train: [45/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2941 (0.3063)	loss 1.4700 (1.3845)	grad_norm 26.1603 (29.5094)	mem 4879MB
[2022-05-31 04:43:00 MetaFG_0] (main.py 265): INFO Train: [45/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2939 (0.3063)	loss 1.4573 (1.3867)	grad_norm 29.2460 (29.6326)	mem 4879MB
[2022-05-31 04:43:03 MetaFG_0] (main.py 265): INFO Train: [45/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2933 (0.3063)	loss 1.0318 (1.3874)	grad_norm 30.8051 (29.5956)	mem 4879MB
[2022-05-31 04:43:06 MetaFG_0] (main.py 265): INFO Train: [45/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2921 (0.3063)	loss 1.0767 (1.3870)	grad_norm 33.2675 (29.5236)	mem 4879MB
[2022-05-31 04:43:09 MetaFG_0] (main.py 265): INFO Train: [45/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2929 (0.3063)	loss 1.1346 (1.3867)	grad_norm 23.4583 (29.5026)	mem 4879MB
[2022-05-31 04:43:12 MetaFG_0] (main.py 265): INFO Train: [45/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.3009 (0.3063)	loss 1.3298 (1.3864)	grad_norm 14.7142 (29.4852)	mem 4879MB
[2022-05-31 04:43:15 MetaFG_0] (main.py 265): INFO Train: [45/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2995 (0.3062)	loss 1.2500 (1.3864)	grad_norm 37.2227 (29.4823)	mem 4879MB
[2022-05-31 04:43:18 MetaFG_0] (main.py 265): INFO Train: [45/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3002 (0.3063)	loss 1.2845 (1.3874)	grad_norm 38.2871 (29.6109)	mem 4879MB
[2022-05-31 04:43:21 MetaFG_0] (main.py 265): INFO Train: [45/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2998 (0.3062)	loss 1.1699 (1.3869)	grad_norm 27.1181 (29.6348)	mem 4879MB
[2022-05-31 04:43:24 MetaFG_0] (main.py 265): INFO Train: [45/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2926 (0.3062)	loss 1.1244 (1.3881)	grad_norm 19.3515 (29.5969)	mem 4879MB
[2022-05-31 04:43:27 MetaFG_0] (main.py 265): INFO Train: [45/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2956 (0.3062)	loss 1.6165 (1.3897)	grad_norm 48.0232 (29.5984)	mem 4879MB
[2022-05-31 04:43:30 MetaFG_0] (main.py 265): INFO Train: [45/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2941 (0.3062)	loss 1.2527 (1.3889)	grad_norm 26.3931 (29.6043)	mem 4879MB
[2022-05-31 04:43:33 MetaFG_0] (main.py 265): INFO Train: [45/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2997 (0.3062)	loss 0.9546 (1.3880)	grad_norm 39.3258 (29.6137)	mem 4879MB
[2022-05-31 04:43:36 MetaFG_0] (main.py 265): INFO Train: [45/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2938 (0.3062)	loss 1.3489 (1.3879)	grad_norm 21.0155 (29.6094)	mem 4879MB
[2022-05-31 04:43:39 MetaFG_0] (main.py 265): INFO Train: [45/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2929 (0.3062)	loss 1.3302 (1.3865)	grad_norm 38.8721 (29.6406)	mem 4879MB
[2022-05-31 04:43:42 MetaFG_0] (main.py 265): INFO Train: [45/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2938 (0.3061)	loss 1.4973 (1.3866)	grad_norm 18.6500 (29.6663)	mem 4879MB
[2022-05-31 04:43:45 MetaFG_0] (main.py 265): INFO Train: [45/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.3001 (0.3061)	loss 1.2697 (1.3864)	grad_norm 38.7696 (29.7015)	mem 4879MB
[2022-05-31 04:43:48 MetaFG_0] (main.py 265): INFO Train: [45/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2999 (0.3061)	loss 1.5220 (1.3870)	grad_norm 22.0095 (29.6610)	mem 4879MB
[2022-05-31 04:43:51 MetaFG_0] (main.py 265): INFO Train: [45/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.3010 (0.3061)	loss 1.3113 (1.3882)	grad_norm 42.3041 (29.6288)	mem 4879MB
[2022-05-31 04:43:55 MetaFG_0] (main.py 265): INFO Train: [45/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.3006 (0.3061)	loss 1.3989 (1.3880)	grad_norm 32.8204 (29.6603)	mem 4879MB
[2022-05-31 04:43:58 MetaFG_0] (main.py 265): INFO Train: [45/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.3005 (0.3061)	loss 1.4443 (1.3881)	grad_norm 31.5619 (29.6837)	mem 4879MB
[2022-05-31 04:44:01 MetaFG_0] (main.py 265): INFO Train: [45/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2990 (0.3061)	loss 1.5117 (1.3880)	grad_norm 25.6994 (29.6901)	mem 4879MB
[2022-05-31 04:44:04 MetaFG_0] (main.py 265): INFO Train: [45/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3005 (0.3061)	loss 1.6585 (1.3886)	grad_norm 30.0933 (29.6630)	mem 4879MB
[2022-05-31 04:44:07 MetaFG_0] (main.py 265): INFO Train: [45/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2931 (0.3060)	loss 1.4807 (1.3889)	grad_norm 31.2390 (29.6843)	mem 4879MB
[2022-05-31 04:44:10 MetaFG_0] (main.py 265): INFO Train: [45/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2921 (0.3060)	loss 1.1699 (1.3895)	grad_norm 38.6426 (29.6598)	mem 4879MB
[2022-05-31 04:44:13 MetaFG_0] (main.py 265): INFO Train: [45/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3022 (0.3060)	loss 0.8508 (1.3874)	grad_norm 45.8462 (29.6625)	mem 4879MB
[2022-05-31 04:44:16 MetaFG_0] (main.py 265): INFO Train: [45/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2930 (0.3060)	loss 1.1380 (1.3886)	grad_norm 48.7511 (29.7576)	mem 4879MB
[2022-05-31 04:44:19 MetaFG_0] (main.py 265): INFO Train: [45/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2997 (0.3060)	loss 1.6219 (1.3886)	grad_norm 21.6076 (29.8020)	mem 4879MB
[2022-05-31 04:44:22 MetaFG_0] (main.py 265): INFO Train: [45/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3013 (0.3060)	loss 1.4697 (1.3884)	grad_norm 22.5629 (29.7999)	mem 4879MB
[2022-05-31 04:44:25 MetaFG_0] (main.py 265): INFO Train: [45/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3000 (0.3061)	loss 1.8664 (1.3890)	grad_norm 40.3321 (29.7799)	mem 4879MB
[2022-05-31 04:44:28 MetaFG_0] (main.py 265): INFO Train: [45/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2923 (0.3061)	loss 0.9396 (1.3897)	grad_norm 44.7527 (29.8356)	mem 4879MB
[2022-05-31 04:44:31 MetaFG_0] (main.py 265): INFO Train: [45/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2935 (0.3061)	loss 1.2708 (1.3894)	grad_norm 53.3895 (29.9088)	mem 4879MB
[2022-05-31 04:44:34 MetaFG_0] (main.py 265): INFO Train: [45/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2925 (0.3060)	loss 1.5204 (1.3906)	grad_norm 20.8883 (29.8935)	mem 4879MB
[2022-05-31 04:44:37 MetaFG_0] (main.py 265): INFO Train: [45/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2931 (0.3060)	loss 1.2859 (1.3908)	grad_norm 35.9968 (29.8557)	mem 4879MB
[2022-05-31 04:44:40 MetaFG_0] (main.py 265): INFO Train: [45/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3001 (0.3060)	loss 1.6224 (1.3894)	grad_norm 25.4703 (29.8332)	mem 4879MB
[2022-05-31 04:44:43 MetaFG_0] (main.py 265): INFO Train: [45/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2952 (0.3060)	loss 1.3880 (1.3891)	grad_norm 19.1725 (29.8364)	mem 4879MB
[2022-05-31 04:44:46 MetaFG_0] (main.py 265): INFO Train: [45/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2936 (0.3060)	loss 1.3583 (1.3892)	grad_norm 19.3725 (29.8642)	mem 4879MB
[2022-05-31 04:44:50 MetaFG_0] (main.py 265): INFO Train: [45/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2985 (0.3060)	loss 0.9602 (1.3893)	grad_norm 31.0626 (29.8981)	mem 4879MB
[2022-05-31 04:44:53 MetaFG_0] (main.py 265): INFO Train: [45/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2929 (0.3060)	loss 1.0525 (1.3873)	grad_norm 34.4937 (29.9259)	mem 4879MB
[2022-05-31 04:44:56 MetaFG_0] (main.py 265): INFO Train: [45/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3214 (0.3060)	loss 1.3944 (1.3862)	grad_norm 28.1160 (29.9935)	mem 4879MB
[2022-05-31 04:44:59 MetaFG_0] (main.py 265): INFO Train: [45/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2926 (0.3063)	loss 1.1116 (1.3854)	grad_norm 44.5537 (30.0431)	mem 4879MB
[2022-05-31 04:45:02 MetaFG_0] (main.py 265): INFO Train: [45/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2930 (0.3062)	loss 1.4185 (1.3853)	grad_norm 20.0041 (30.1575)	mem 4879MB
[2022-05-31 04:45:05 MetaFG_0] (main.py 265): INFO Train: [45/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2922 (0.3062)	loss 0.9107 (1.3845)	grad_norm 35.8890 (30.1831)	mem 4879MB
[2022-05-31 04:45:08 MetaFG_0] (main.py 265): INFO Train: [45/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2989 (0.3062)	loss 1.4273 (1.3853)	grad_norm 30.0013 (30.1786)	mem 4879MB
[2022-05-31 04:45:11 MetaFG_0] (main.py 265): INFO Train: [45/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2933 (0.3062)	loss 1.7296 (1.3851)	grad_norm 27.9475 (30.1941)	mem 4879MB
[2022-05-31 04:45:14 MetaFG_0] (main.py 265): INFO Train: [45/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2984 (0.3062)	loss 1.5344 (1.3853)	grad_norm 39.3769 (30.1823)	mem 4879MB
[2022-05-31 04:45:17 MetaFG_0] (main.py 265): INFO Train: [45/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2929 (0.3062)	loss 1.6178 (1.3854)	grad_norm 25.0559 (30.1765)	mem 4879MB
[2022-05-31 04:45:20 MetaFG_0] (main.py 265): INFO Train: [45/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2942 (0.3062)	loss 0.9419 (1.3847)	grad_norm 65.4377 (30.2000)	mem 4879MB
[2022-05-31 04:45:23 MetaFG_0] (main.py 265): INFO Train: [45/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2920 (0.3062)	loss 1.1193 (1.3842)	grad_norm 37.1351 (30.2034)	mem 4879MB
[2022-05-31 04:45:26 MetaFG_0] (main.py 265): INFO Train: [45/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2931 (0.3062)	loss 1.4518 (1.3850)	grad_norm 16.8011 (30.2083)	mem 4879MB
[2022-05-31 04:45:30 MetaFG_0] (main.py 265): INFO Train: [45/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.3011 (0.3062)	loss 1.7513 (1.3860)	grad_norm 42.5653 (30.2041)	mem 4879MB
[2022-05-31 04:45:33 MetaFG_0] (main.py 265): INFO Train: [45/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2982 (0.3062)	loss 1.5915 (1.3860)	grad_norm 28.5996 (30.1930)	mem 4879MB
[2022-05-31 04:45:36 MetaFG_0] (main.py 265): INFO Train: [45/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3035 (0.3062)	loss 1.2059 (1.3862)	grad_norm 62.0831 (30.2455)	mem 4879MB
[2022-05-31 04:45:39 MetaFG_0] (main.py 265): INFO Train: [45/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2986 (0.3062)	loss 1.6607 (1.3872)	grad_norm 32.7790 (30.2330)	mem 4879MB
[2022-05-31 04:45:42 MetaFG_0] (main.py 265): INFO Train: [45/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2984 (0.3062)	loss 1.0578 (1.3875)	grad_norm 29.0553 (30.2400)	mem 4879MB
[2022-05-31 04:45:45 MetaFG_0] (main.py 265): INFO Train: [45/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2940 (0.3061)	loss 1.5331 (1.3875)	grad_norm 49.0597 (30.2402)	mem 4879MB
[2022-05-31 04:45:48 MetaFG_0] (main.py 265): INFO Train: [45/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2930 (0.3061)	loss 1.7580 (1.3875)	grad_norm 35.6628 (30.2292)	mem 4879MB
[2022-05-31 04:45:51 MetaFG_0] (main.py 265): INFO Train: [45/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3001 (0.3061)	loss 1.1773 (1.3867)	grad_norm 17.4497 (30.2425)	mem 4879MB
[2022-05-31 04:45:54 MetaFG_0] (main.py 265): INFO Train: [45/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2934 (0.3061)	loss 1.2524 (1.3863)	grad_norm 48.7735 (30.2482)	mem 4879MB
[2022-05-31 04:45:57 MetaFG_0] (main.py 265): INFO Train: [45/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2936 (0.3061)	loss 1.3206 (1.3857)	grad_norm 42.9399 (30.2890)	mem 4879MB
[2022-05-31 04:46:00 MetaFG_0] (main.py 265): INFO Train: [45/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2949 (0.3061)	loss 1.2908 (1.3861)	grad_norm 25.6043 (30.2967)	mem 4879MB
[2022-05-31 04:46:03 MetaFG_0] (main.py 265): INFO Train: [45/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2931 (0.3061)	loss 1.3540 (1.3861)	grad_norm 27.2894 (30.2598)	mem 4879MB
[2022-05-31 04:46:06 MetaFG_0] (main.py 265): INFO Train: [45/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2927 (0.3061)	loss 1.5595 (1.3861)	grad_norm 25.6359 (30.2554)	mem 4879MB
[2022-05-31 04:46:09 MetaFG_0] (main.py 265): INFO Train: [45/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2999 (0.3061)	loss 1.4009 (1.3857)	grad_norm 26.9129 (30.2495)	mem 4879MB
[2022-05-31 04:46:12 MetaFG_0] (main.py 265): INFO Train: [45/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2996 (0.3060)	loss 1.6026 (1.3859)	grad_norm 25.2964 (30.2527)	mem 4879MB
[2022-05-31 04:46:15 MetaFG_0] (main.py 265): INFO Train: [45/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2998 (0.3060)	loss 1.1446 (1.3855)	grad_norm 32.7032 (30.2504)	mem 4879MB
[2022-05-31 04:46:18 MetaFG_0] (main.py 265): INFO Train: [45/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2987 (0.3060)	loss 1.3986 (1.3863)	grad_norm 27.3441 (30.2908)	mem 4879MB
[2022-05-31 04:46:21 MetaFG_0] (main.py 265): INFO Train: [45/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2932 (0.3060)	loss 1.4339 (1.3861)	grad_norm 36.9622 (30.3380)	mem 4879MB
[2022-05-31 04:46:24 MetaFG_0] (main.py 265): INFO Train: [45/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2982 (0.3060)	loss 1.2858 (1.3870)	grad_norm 41.9629 (30.3530)	mem 4879MB
[2022-05-31 04:46:27 MetaFG_0] (main.py 265): INFO Train: [45/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2943 (0.3060)	loss 1.1276 (1.3865)	grad_norm 19.8647 (30.3561)	mem 4879MB
[2022-05-31 04:46:30 MetaFG_0] (main.py 265): INFO Train: [45/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2989 (0.3060)	loss 1.2907 (1.3858)	grad_norm 25.1784 (30.4218)	mem 4879MB
[2022-05-31 04:46:34 MetaFG_0] (main.py 265): INFO Train: [45/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.3025 (0.3060)	loss 1.4523 (1.3857)	grad_norm 15.2668 (30.3981)	mem 4879MB
[2022-05-31 04:46:37 MetaFG_0] (main.py 265): INFO Train: [45/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2931 (0.3060)	loss 1.5373 (1.3859)	grad_norm 49.3660 (30.3940)	mem 4879MB
[2022-05-31 04:46:40 MetaFG_0] (main.py 265): INFO Train: [45/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3007 (0.3059)	loss 1.2324 (1.3858)	grad_norm 30.8369 (30.4907)	mem 4879MB
[2022-05-31 04:46:43 MetaFG_0] (main.py 265): INFO Train: [45/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2984 (0.3059)	loss 1.2523 (1.3866)	grad_norm 18.1460 (30.4482)	mem 4879MB
[2022-05-31 04:46:46 MetaFG_0] (main.py 265): INFO Train: [45/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3008 (0.3059)	loss 1.7161 (1.3870)	grad_norm 19.3789 (30.4371)	mem 4879MB
[2022-05-31 04:46:49 MetaFG_0] (main.py 265): INFO Train: [45/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2984 (0.3059)	loss 1.2846 (1.3871)	grad_norm 20.8855 (30.3900)	mem 4879MB
[2022-05-31 04:46:52 MetaFG_0] (main.py 265): INFO Train: [45/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.3001 (0.3059)	loss 1.4836 (1.3866)	grad_norm 30.5106 (30.3616)	mem 4879MB
[2022-05-31 04:46:55 MetaFG_0] (main.py 265): INFO Train: [45/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2926 (0.3059)	loss 1.6070 (1.3865)	grad_norm 22.6151 (30.3709)	mem 4879MB
[2022-05-31 04:46:58 MetaFG_0] (main.py 265): INFO Train: [45/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.3026 (0.3059)	loss 1.1024 (1.3863)	grad_norm 33.7985 (30.3476)	mem 4879MB
[2022-05-31 04:47:01 MetaFG_0] (main.py 265): INFO Train: [45/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3002 (0.3059)	loss 1.5722 (1.3871)	grad_norm 22.6753 (30.3292)	mem 4879MB
[2022-05-31 04:47:04 MetaFG_0] (main.py 265): INFO Train: [45/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2932 (0.3059)	loss 1.2279 (1.3876)	grad_norm 47.4778 (30.3758)	mem 4879MB
[2022-05-31 04:47:07 MetaFG_0] (main.py 265): INFO Train: [45/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2935 (0.3059)	loss 1.6109 (1.3879)	grad_norm 31.5598 (30.3377)	mem 4879MB
[2022-05-31 04:47:10 MetaFG_0] (main.py 265): INFO Train: [45/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2921 (0.3059)	loss 1.0008 (1.3878)	grad_norm 28.2349 (30.3201)	mem 4879MB
[2022-05-31 04:47:13 MetaFG_0] (main.py 265): INFO Train: [45/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2925 (0.3059)	loss 1.1248 (1.3867)	grad_norm 25.8309 (30.3079)	mem 4879MB
[2022-05-31 04:47:16 MetaFG_0] (main.py 265): INFO Train: [45/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2956 (0.3059)	loss 0.9763 (1.3868)	grad_norm 16.8406 (30.2675)	mem 4879MB
[2022-05-31 04:47:19 MetaFG_0] (main.py 265): INFO Train: [45/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2998 (0.3059)	loss 1.6392 (1.3871)	grad_norm 22.1709 (30.2477)	mem 4879MB
[2022-05-31 04:47:22 MetaFG_0] (main.py 265): INFO Train: [45/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2987 (0.3059)	loss 1.1333 (1.3866)	grad_norm 57.4434 (30.2518)	mem 4879MB
[2022-05-31 04:47:25 MetaFG_0] (main.py 265): INFO Train: [45/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2985 (0.3059)	loss 1.3903 (1.3866)	grad_norm 23.1111 (30.3046)	mem 4879MB
[2022-05-31 04:47:28 MetaFG_0] (main.py 265): INFO Train: [45/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2936 (0.3059)	loss 1.1695 (1.3867)	grad_norm 31.9523 (30.3136)	mem 4879MB
[2022-05-31 04:47:31 MetaFG_0] (main.py 265): INFO Train: [45/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2924 (0.3058)	loss 1.2411 (1.3870)	grad_norm 20.9149 (30.3194)	mem 4879MB
[2022-05-31 04:47:32 MetaFG_0] (main.py 272): INFO EPOCH 45 training takes 0:07:57
[2022-05-31 04:47:32 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_45.pth saving......
[2022-05-31 04:47:33 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_45.pth saved !!!
[2022-05-31 04:47:33 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:47:34 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:47:34 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:47:35 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.728 (0.728)	Loss 0.6813 (0.6813)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 04:47:36 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.155)	Loss 0.9093 (0.6818)	Acc@1 78.125 (86.080)	Acc@5 96.875 (98.011)	Mem 4879MB
[2022-05-31 04:47:37 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.126)	Loss 0.3367 (0.6758)	Acc@1 100.000 (85.863)	Acc@5 100.000 (97.917)	Mem 4879MB
[2022-05-31 04:47:38 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.099 (0.116)	Loss 0.4382 (0.6752)	Acc@1 93.750 (86.190)	Acc@5 100.000 (98.085)	Mem 4879MB
[2022-05-31 04:47:39 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.088 (0.110)	Loss 0.8377 (0.6766)	Acc@1 81.250 (85.671)	Acc@5 100.000 (98.247)	Mem 4879MB
[2022-05-31 04:47:39 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.099 (0.106)	Loss 0.8528 (0.6691)	Acc@1 75.000 (85.723)	Acc@5 96.875 (98.346)	Mem 4879MB
[2022-05-31 04:47:40 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.091 (0.104)	Loss 0.6021 (0.6715)	Acc@1 84.375 (85.502)	Acc@5 100.000 (98.309)	Mem 4879MB
[2022-05-31 04:47:41 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.7820 (0.6688)	Acc@1 81.250 (85.343)	Acc@5 100.000 (98.415)	Mem 4879MB
[2022-05-31 04:47:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.102)	Loss 0.7123 (0.6694)	Acc@1 84.375 (85.571)	Acc@5 93.750 (98.418)	Mem 4879MB
[2022-05-31 04:47:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.103 (0.101)	Loss 0.9659 (0.6755)	Acc@1 75.000 (85.371)	Acc@5 93.750 (98.420)	Mem 4879MB
[2022-05-31 04:47:44 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.100)	Loss 0.7358 (0.6768)	Acc@1 78.125 (85.303)	Acc@5 100.000 (98.391)	Mem 4879MB
[2022-05-31 04:47:45 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 0.6996 (0.6807)	Acc@1 84.375 (85.163)	Acc@5 100.000 (98.311)	Mem 4879MB
[2022-05-31 04:47:46 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.100 (0.099)	Loss 0.6686 (0.6816)	Acc@1 84.375 (85.021)	Acc@5 100.000 (98.347)	Mem 4879MB
[2022-05-31 04:47:47 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.4420 (0.6767)	Acc@1 96.875 (85.258)	Acc@5 100.000 (98.402)	Mem 4879MB
[2022-05-31 04:47:48 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.099)	Loss 0.6751 (0.6728)	Acc@1 81.250 (85.372)	Acc@5 100.000 (98.426)	Mem 4879MB
[2022-05-31 04:47:49 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.8049 (0.6712)	Acc@1 84.375 (85.410)	Acc@5 100.000 (98.469)	Mem 4879MB
[2022-05-31 04:47:50 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.093 (0.098)	Loss 0.8140 (0.6678)	Acc@1 81.250 (85.481)	Acc@5 100.000 (98.467)	Mem 4879MB
[2022-05-31 04:47:51 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.098)	Loss 0.8077 (0.6703)	Acc@1 81.250 (85.380)	Acc@5 96.875 (98.428)	Mem 4879MB
[2022-05-31 04:47:52 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.088 (0.098)	Loss 0.8892 (0.6692)	Acc@1 81.250 (85.497)	Acc@5 96.875 (98.446)	Mem 4879MB
[2022-05-31 04:47:53 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.102 (0.097)	Loss 0.5673 (0.6691)	Acc@1 87.500 (85.422)	Acc@5 100.000 (98.413)	Mem 4879MB
[2022-05-31 04:47:54 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.092 (0.097)	Loss 0.8435 (0.6688)	Acc@1 84.375 (85.432)	Acc@5 90.625 (98.399)	Mem 4879MB
[2022-05-31 04:47:55 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 1.0135 (0.6692)	Acc@1 78.125 (85.352)	Acc@5 93.750 (98.371)	Mem 4879MB
[2022-05-31 04:47:55 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.092 (0.097)	Loss 0.4511 (0.6726)	Acc@1 93.750 (85.308)	Acc@5 100.000 (98.346)	Mem 4879MB
[2022-05-31 04:47:56 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.092 (0.097)	Loss 0.6355 (0.6735)	Acc@1 90.625 (85.349)	Acc@5 100.000 (98.336)	Mem 4879MB
[2022-05-31 04:47:57 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.088 (0.096)	Loss 0.7876 (0.6768)	Acc@1 81.250 (85.283)	Acc@5 96.875 (98.301)	Mem 4879MB
[2022-05-31 04:47:58 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.096)	Loss 0.6139 (0.6759)	Acc@1 90.625 (85.334)	Acc@5 100.000 (98.344)	Mem 4879MB
[2022-05-31 04:47:59 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 0.5312 (0.6764)	Acc@1 87.500 (85.273)	Acc@5 100.000 (98.372)	Mem 4879MB
[2022-05-31 04:48:00 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.093 (0.096)	Loss 0.6641 (0.6722)	Acc@1 84.375 (85.367)	Acc@5 100.000 (98.420)	Mem 4879MB
[2022-05-31 04:48:01 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.8729 (0.6702)	Acc@1 84.375 (85.431)	Acc@5 93.750 (98.432)	Mem 4879MB
[2022-05-31 04:48:02 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.9444 (0.6695)	Acc@1 78.125 (85.470)	Acc@5 96.875 (98.432)	Mem 4879MB
[2022-05-31 04:48:03 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.103 (0.096)	Loss 0.6949 (0.6681)	Acc@1 87.500 (85.475)	Acc@5 96.875 (98.443)	Mem 4879MB
[2022-05-31 04:48:04 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6670 (0.6669)	Acc@1 84.375 (85.490)	Acc@5 100.000 (98.473)	Mem 4879MB
[2022-05-31 04:48:04 MetaFG_0] (main.py 330): INFO  * Acc@1 85.500 Acc@5 98.480
[2022-05-31 04:48:04 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.5%
[2022-05-31 04:48:04 MetaFG_0] (main.py 171): INFO Max accuracy: 85.50%
[2022-05-31 04:48:05 MetaFG_0] (main.py 265): INFO Train: [46/300][0/1562]	eta 0:23:12 lr 0.000006	time 0.8917 (0.8917)	loss 1.0952 (1.0952)	grad_norm 36.3843 (36.3843)	mem 4879MB
[2022-05-31 04:48:08 MetaFG_0] (main.py 265): INFO Train: [46/300][10/1562]	eta 0:09:29 lr 0.000006	time 0.2925 (0.3669)	loss 1.5307 (1.3162)	grad_norm 36.5919 (31.7699)	mem 4879MB
[2022-05-31 04:48:11 MetaFG_0] (main.py 265): INFO Train: [46/300][20/1562]	eta 0:08:39 lr 0.000006	time 0.2989 (0.3370)	loss 0.8971 (1.3072)	grad_norm 29.9291 (31.4722)	mem 4879MB
[2022-05-31 04:48:14 MetaFG_0] (main.py 265): INFO Train: [46/300][30/1562]	eta 0:08:20 lr 0.000006	time 0.2950 (0.3267)	loss 1.3547 (1.3173)	grad_norm 23.7004 (32.2670)	mem 4879MB
[2022-05-31 04:48:17 MetaFG_0] (main.py 265): INFO Train: [46/300][40/1562]	eta 0:08:09 lr 0.000006	time 0.2939 (0.3214)	loss 1.0634 (1.3104)	grad_norm 32.3141 (30.7399)	mem 4879MB
[2022-05-31 04:48:20 MetaFG_0] (main.py 265): INFO Train: [46/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2929 (0.3181)	loss 1.6799 (1.3218)	grad_norm 34.8792 (30.0529)	mem 4879MB
[2022-05-31 04:48:23 MetaFG_0] (main.py 265): INFO Train: [46/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.2919 (0.3159)	loss 1.4267 (1.3215)	grad_norm 41.1614 (30.2695)	mem 4879MB
[2022-05-31 04:48:26 MetaFG_0] (main.py 265): INFO Train: [46/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.3002 (0.3146)	loss 1.5863 (1.3260)	grad_norm 24.1214 (30.5172)	mem 4879MB
[2022-05-31 04:48:30 MetaFG_0] (main.py 265): INFO Train: [46/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2990 (0.3135)	loss 1.4214 (1.3275)	grad_norm 16.5106 (30.2524)	mem 4879MB
[2022-05-31 04:48:33 MetaFG_0] (main.py 265): INFO Train: [46/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2926 (0.3126)	loss 1.6148 (1.3395)	grad_norm 40.2272 (29.8146)	mem 4879MB
[2022-05-31 04:48:36 MetaFG_0] (main.py 265): INFO Train: [46/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.3030 (0.3120)	loss 1.1633 (1.3403)	grad_norm 22.9747 (29.3887)	mem 4879MB
[2022-05-31 04:48:39 MetaFG_0] (main.py 265): INFO Train: [46/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2942 (0.3113)	loss 1.5377 (1.3488)	grad_norm 18.8833 (29.2075)	mem 4879MB
[2022-05-31 04:48:42 MetaFG_0] (main.py 265): INFO Train: [46/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2930 (0.3107)	loss 1.2521 (1.3481)	grad_norm 26.2471 (29.0728)	mem 4879MB
[2022-05-31 04:48:45 MetaFG_0] (main.py 265): INFO Train: [46/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2994 (0.3104)	loss 1.6212 (1.3550)	grad_norm 34.9924 (29.2685)	mem 4879MB
[2022-05-31 04:48:48 MetaFG_0] (main.py 265): INFO Train: [46/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2987 (0.3099)	loss 1.5651 (1.3625)	grad_norm 18.0573 (29.8456)	mem 4879MB
[2022-05-31 04:48:51 MetaFG_0] (main.py 265): INFO Train: [46/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.3043 (0.3096)	loss 1.1082 (1.3670)	grad_norm 52.5875 (30.4107)	mem 4879MB
[2022-05-31 04:48:54 MetaFG_0] (main.py 265): INFO Train: [46/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2929 (0.3093)	loss 1.4789 (1.3716)	grad_norm 18.0422 (30.5807)	mem 4879MB
[2022-05-31 04:48:57 MetaFG_0] (main.py 265): INFO Train: [46/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2929 (0.3090)	loss 1.1743 (1.3741)	grad_norm 26.8436 (30.6921)	mem 4879MB
[2022-05-31 04:49:00 MetaFG_0] (main.py 265): INFO Train: [46/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2923 (0.3088)	loss 1.5220 (1.3769)	grad_norm 21.5119 (30.4044)	mem 4879MB
[2022-05-31 04:49:03 MetaFG_0] (main.py 265): INFO Train: [46/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.3002 (0.3086)	loss 1.3665 (1.3746)	grad_norm 28.5094 (30.4649)	mem 4879MB
[2022-05-31 04:49:06 MetaFG_0] (main.py 265): INFO Train: [46/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2988 (0.3084)	loss 1.2171 (1.3729)	grad_norm 19.2407 (30.2547)	mem 4879MB
[2022-05-31 04:49:09 MetaFG_0] (main.py 265): INFO Train: [46/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2987 (0.3083)	loss 1.1690 (1.3688)	grad_norm 66.4872 (30.3850)	mem 4879MB
[2022-05-31 04:49:12 MetaFG_0] (main.py 265): INFO Train: [46/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2941 (0.3080)	loss 1.4450 (1.3698)	grad_norm 21.0676 (30.1458)	mem 4879MB
[2022-05-31 04:49:15 MetaFG_0] (main.py 265): INFO Train: [46/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2939 (0.3079)	loss 1.5752 (1.3742)	grad_norm 53.1544 (30.1671)	mem 4879MB
[2022-05-31 04:49:18 MetaFG_0] (main.py 265): INFO Train: [46/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2939 (0.3076)	loss 1.3643 (1.3785)	grad_norm 25.3469 (30.2013)	mem 4879MB
[2022-05-31 04:49:21 MetaFG_0] (main.py 265): INFO Train: [46/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2925 (0.3075)	loss 1.5155 (1.3806)	grad_norm 33.2536 (30.3932)	mem 4879MB
[2022-05-31 04:49:24 MetaFG_0] (main.py 265): INFO Train: [46/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2920 (0.3074)	loss 1.5355 (1.3836)	grad_norm 15.9815 (30.2819)	mem 4879MB
[2022-05-31 04:49:27 MetaFG_0] (main.py 265): INFO Train: [46/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2996 (0.3073)	loss 1.3720 (1.3816)	grad_norm 24.5419 (30.3234)	mem 4879MB
[2022-05-31 04:49:30 MetaFG_0] (main.py 265): INFO Train: [46/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2923 (0.3072)	loss 1.3926 (1.3812)	grad_norm 21.5790 (30.3993)	mem 4879MB
[2022-05-31 04:49:34 MetaFG_0] (main.py 265): INFO Train: [46/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2949 (0.3072)	loss 1.6315 (1.3795)	grad_norm 26.8773 (30.4116)	mem 4879MB
[2022-05-31 04:49:37 MetaFG_0] (main.py 265): INFO Train: [46/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2992 (0.3071)	loss 1.0895 (1.3798)	grad_norm 29.9032 (30.3451)	mem 4879MB
[2022-05-31 04:49:40 MetaFG_0] (main.py 265): INFO Train: [46/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2998 (0.3070)	loss 1.4328 (1.3787)	grad_norm 21.1877 (30.1634)	mem 4879MB
[2022-05-31 04:49:43 MetaFG_0] (main.py 265): INFO Train: [46/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2931 (0.3069)	loss 1.4142 (1.3781)	grad_norm 22.0959 (30.0167)	mem 4879MB
[2022-05-31 04:49:46 MetaFG_0] (main.py 265): INFO Train: [46/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2988 (0.3069)	loss 1.5969 (1.3791)	grad_norm 30.6566 (30.1246)	mem 4879MB
[2022-05-31 04:49:49 MetaFG_0] (main.py 265): INFO Train: [46/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2936 (0.3068)	loss 1.2614 (1.3764)	grad_norm 23.4513 (30.2644)	mem 4879MB
[2022-05-31 04:49:52 MetaFG_0] (main.py 265): INFO Train: [46/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2929 (0.3067)	loss 1.4548 (1.3781)	grad_norm 29.9306 (30.1956)	mem 4879MB
[2022-05-31 04:49:55 MetaFG_0] (main.py 265): INFO Train: [46/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2942 (0.3067)	loss 1.6282 (1.3814)	grad_norm 26.8909 (30.1706)	mem 4879MB
[2022-05-31 04:49:58 MetaFG_0] (main.py 265): INFO Train: [46/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2935 (0.3066)	loss 1.3684 (1.3820)	grad_norm 18.0488 (30.0822)	mem 4879MB
[2022-05-31 04:50:01 MetaFG_0] (main.py 265): INFO Train: [46/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2929 (0.3066)	loss 1.4816 (1.3851)	grad_norm 26.6590 (30.1761)	mem 4879MB
[2022-05-31 04:50:04 MetaFG_0] (main.py 265): INFO Train: [46/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2953 (0.3066)	loss 1.0547 (1.3842)	grad_norm 14.5139 (30.0836)	mem 4879MB
[2022-05-31 04:50:07 MetaFG_0] (main.py 265): INFO Train: [46/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2980 (0.3066)	loss 1.6086 (1.3832)	grad_norm 31.9055 (30.0036)	mem 4879MB
[2022-05-31 04:50:10 MetaFG_0] (main.py 265): INFO Train: [46/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2936 (0.3066)	loss 1.5082 (1.3834)	grad_norm 19.0780 (30.0249)	mem 4879MB
[2022-05-31 04:50:13 MetaFG_0] (main.py 265): INFO Train: [46/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2953 (0.3066)	loss 1.2623 (1.3834)	grad_norm 28.4671 (29.9104)	mem 4879MB
[2022-05-31 04:50:17 MetaFG_0] (main.py 265): INFO Train: [46/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.3357 (0.3072)	loss 1.5223 (1.3853)	grad_norm 41.6686 (29.9381)	mem 4879MB
[2022-05-31 04:50:20 MetaFG_0] (main.py 265): INFO Train: [46/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2928 (0.3072)	loss 1.5866 (1.3857)	grad_norm 22.5294 (29.8985)	mem 4879MB
[2022-05-31 04:50:23 MetaFG_0] (main.py 265): INFO Train: [46/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.3005 (0.3072)	loss 1.6179 (1.3866)	grad_norm 28.5651 (29.9249)	mem 4879MB
[2022-05-31 04:50:26 MetaFG_0] (main.py 265): INFO Train: [46/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2980 (0.3072)	loss 1.5194 (1.3874)	grad_norm 19.3675 (29.8585)	mem 4879MB
[2022-05-31 04:50:29 MetaFG_0] (main.py 265): INFO Train: [46/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2930 (0.3071)	loss 1.5912 (1.3855)	grad_norm 26.0406 (29.7981)	mem 4879MB
[2022-05-31 04:50:32 MetaFG_0] (main.py 265): INFO Train: [46/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.3085 (0.3071)	loss 1.4737 (1.3842)	grad_norm 28.6562 (29.7693)	mem 4879MB
[2022-05-31 04:50:35 MetaFG_0] (main.py 265): INFO Train: [46/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2987 (0.3071)	loss 1.2569 (1.3848)	grad_norm 29.9431 (29.7688)	mem 4879MB
[2022-05-31 04:50:38 MetaFG_0] (main.py 265): INFO Train: [46/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2927 (0.3070)	loss 1.3034 (1.3845)	grad_norm 42.1040 (29.8100)	mem 4879MB
[2022-05-31 04:50:41 MetaFG_0] (main.py 265): INFO Train: [46/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.3021 (0.3070)	loss 1.0075 (1.3840)	grad_norm 18.5125 (29.8313)	mem 4879MB
[2022-05-31 04:50:44 MetaFG_0] (main.py 265): INFO Train: [46/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2939 (0.3070)	loss 1.8523 (1.3829)	grad_norm 103.1092 (29.9509)	mem 4879MB
[2022-05-31 04:50:47 MetaFG_0] (main.py 265): INFO Train: [46/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.3001 (0.3070)	loss 1.4233 (1.3819)	grad_norm 30.8721 (29.9900)	mem 4879MB
[2022-05-31 04:50:50 MetaFG_0] (main.py 265): INFO Train: [46/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2929 (0.3070)	loss 1.3511 (1.3841)	grad_norm 19.6452 (29.9758)	mem 4879MB
[2022-05-31 04:50:53 MetaFG_0] (main.py 265): INFO Train: [46/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2921 (0.3069)	loss 1.2225 (1.3815)	grad_norm 68.6245 (30.0129)	mem 4879MB
[2022-05-31 04:50:56 MetaFG_0] (main.py 265): INFO Train: [46/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.3003 (0.3069)	loss 1.4179 (1.3828)	grad_norm 13.1019 (30.0367)	mem 4879MB
[2022-05-31 04:50:59 MetaFG_0] (main.py 265): INFO Train: [46/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2930 (0.3069)	loss 0.8211 (1.3804)	grad_norm 39.1295 (30.0467)	mem 4879MB
[2022-05-31 04:51:02 MetaFG_0] (main.py 265): INFO Train: [46/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2998 (0.3068)	loss 1.6233 (1.3796)	grad_norm 27.0044 (29.9885)	mem 4879MB
[2022-05-31 04:51:05 MetaFG_0] (main.py 265): INFO Train: [46/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2928 (0.3068)	loss 1.2664 (1.3794)	grad_norm 35.8530 (30.0602)	mem 4879MB
[2022-05-31 04:51:09 MetaFG_0] (main.py 265): INFO Train: [46/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2922 (0.3067)	loss 1.5166 (1.3798)	grad_norm 18.8926 (29.9993)	mem 4879MB
[2022-05-31 04:51:12 MetaFG_0] (main.py 265): INFO Train: [46/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2924 (0.3067)	loss 1.1077 (1.3795)	grad_norm 25.7136 (29.9286)	mem 4879MB
[2022-05-31 04:51:15 MetaFG_0] (main.py 265): INFO Train: [46/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2940 (0.3067)	loss 1.7257 (1.3788)	grad_norm 20.1830 (29.8607)	mem 4879MB
[2022-05-31 04:51:18 MetaFG_0] (main.py 265): INFO Train: [46/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3002 (0.3066)	loss 1.5140 (1.3773)	grad_norm 21.0688 (29.7866)	mem 4879MB
[2022-05-31 04:51:21 MetaFG_0] (main.py 265): INFO Train: [46/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2996 (0.3066)	loss 1.5317 (1.3767)	grad_norm 19.1461 (29.7060)	mem 4879MB
[2022-05-31 04:51:24 MetaFG_0] (main.py 265): INFO Train: [46/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2936 (0.3065)	loss 1.0300 (1.3767)	grad_norm 19.1079 (29.7306)	mem 4879MB
[2022-05-31 04:51:27 MetaFG_0] (main.py 265): INFO Train: [46/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2991 (0.3065)	loss 1.0460 (1.3751)	grad_norm 32.5838 (29.6586)	mem 4879MB
[2022-05-31 04:51:30 MetaFG_0] (main.py 265): INFO Train: [46/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2926 (0.3065)	loss 1.1013 (1.3736)	grad_norm 33.5896 (29.6786)	mem 4879MB
[2022-05-31 04:51:33 MetaFG_0] (main.py 265): INFO Train: [46/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2930 (0.3064)	loss 1.0827 (1.3713)	grad_norm 15.9322 (29.6414)	mem 4879MB
[2022-05-31 04:51:36 MetaFG_0] (main.py 265): INFO Train: [46/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2960 (0.3064)	loss 1.3070 (1.3723)	grad_norm 34.5940 (29.6342)	mem 4879MB
[2022-05-31 04:51:39 MetaFG_0] (main.py 265): INFO Train: [46/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2923 (0.3064)	loss 1.5449 (1.3731)	grad_norm 37.9388 (29.6707)	mem 4879MB
[2022-05-31 04:51:42 MetaFG_0] (main.py 265): INFO Train: [46/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2926 (0.3063)	loss 1.1610 (1.3727)	grad_norm 46.9611 (29.7232)	mem 4879MB
[2022-05-31 04:51:45 MetaFG_0] (main.py 265): INFO Train: [46/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2928 (0.3063)	loss 1.4480 (1.3702)	grad_norm 201.3408 (29.9623)	mem 4879MB
[2022-05-31 04:51:48 MetaFG_0] (main.py 265): INFO Train: [46/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2941 (0.3063)	loss 1.6489 (1.3709)	grad_norm 18.7357 (29.9744)	mem 4879MB
[2022-05-31 04:51:51 MetaFG_0] (main.py 265): INFO Train: [46/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2944 (0.3063)	loss 1.4632 (1.3714)	grad_norm 28.8468 (29.9836)	mem 4879MB
[2022-05-31 04:51:54 MetaFG_0] (main.py 265): INFO Train: [46/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2931 (0.3062)	loss 1.7792 (1.3714)	grad_norm 18.8410 (30.0419)	mem 4879MB
[2022-05-31 04:51:57 MetaFG_0] (main.py 265): INFO Train: [46/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.3040 (0.3062)	loss 1.4139 (1.3706)	grad_norm 25.2548 (30.0637)	mem 4879MB
[2022-05-31 04:52:00 MetaFG_0] (main.py 265): INFO Train: [46/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3055 (0.3062)	loss 1.0283 (1.3699)	grad_norm 34.0134 (30.0669)	mem 4879MB
[2022-05-31 04:52:03 MetaFG_0] (main.py 265): INFO Train: [46/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2953 (0.3062)	loss 1.6143 (1.3707)	grad_norm 32.4898 (30.0740)	mem 4879MB
[2022-05-31 04:52:06 MetaFG_0] (main.py 265): INFO Train: [46/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2927 (0.3062)	loss 1.1549 (1.3714)	grad_norm 29.6253 (30.0596)	mem 4879MB
[2022-05-31 04:52:09 MetaFG_0] (main.py 265): INFO Train: [46/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2939 (0.3062)	loss 1.1590 (1.3721)	grad_norm 39.2963 (30.0283)	mem 4879MB
[2022-05-31 04:52:13 MetaFG_0] (main.py 265): INFO Train: [46/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.3006 (0.3062)	loss 1.3802 (1.3725)	grad_norm 24.6104 (30.0372)	mem 4879MB
[2022-05-31 04:52:16 MetaFG_0] (main.py 265): INFO Train: [46/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2991 (0.3062)	loss 1.4615 (1.3717)	grad_norm 23.8628 (30.0643)	mem 4879MB
[2022-05-31 04:52:19 MetaFG_0] (main.py 265): INFO Train: [46/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2997 (0.3062)	loss 1.5782 (1.3720)	grad_norm 36.7245 (30.1188)	mem 4879MB
[2022-05-31 04:52:22 MetaFG_0] (main.py 265): INFO Train: [46/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2933 (0.3061)	loss 1.3581 (1.3707)	grad_norm 23.2083 (30.1143)	mem 4879MB
[2022-05-31 04:52:25 MetaFG_0] (main.py 265): INFO Train: [46/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2964 (0.3061)	loss 1.2503 (1.3721)	grad_norm 12.9586 (30.0624)	mem 4879MB
[2022-05-31 04:52:28 MetaFG_0] (main.py 265): INFO Train: [46/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.3001 (0.3061)	loss 1.3929 (1.3726)	grad_norm 73.8564 (30.1175)	mem 4879MB
[2022-05-31 04:52:31 MetaFG_0] (main.py 265): INFO Train: [46/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3031 (0.3061)	loss 1.2272 (1.3722)	grad_norm 17.6371 (30.0985)	mem 4879MB
[2022-05-31 04:52:34 MetaFG_0] (main.py 265): INFO Train: [46/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2997 (0.3061)	loss 0.9970 (1.3723)	grad_norm 26.3206 (30.1084)	mem 4879MB
[2022-05-31 04:52:37 MetaFG_0] (main.py 265): INFO Train: [46/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2993 (0.3061)	loss 1.5981 (1.3738)	grad_norm 30.6774 (30.1682)	mem 4879MB
[2022-05-31 04:52:40 MetaFG_0] (main.py 265): INFO Train: [46/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2938 (0.3060)	loss 1.4109 (1.3729)	grad_norm 29.7844 (30.1510)	mem 4879MB
[2022-05-31 04:52:43 MetaFG_0] (main.py 265): INFO Train: [46/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2943 (0.3060)	loss 1.3645 (1.3723)	grad_norm 24.9928 (30.1466)	mem 4879MB
[2022-05-31 04:52:46 MetaFG_0] (main.py 265): INFO Train: [46/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2993 (0.3060)	loss 1.3168 (1.3725)	grad_norm 24.5051 (30.1668)	mem 4879MB
[2022-05-31 04:52:49 MetaFG_0] (main.py 265): INFO Train: [46/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3043 (0.3060)	loss 1.7685 (1.3719)	grad_norm 26.6578 (30.1786)	mem 4879MB
[2022-05-31 04:52:52 MetaFG_0] (main.py 265): INFO Train: [46/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2989 (0.3061)	loss 1.1479 (1.3715)	grad_norm 23.4629 (30.1893)	mem 4879MB
[2022-05-31 04:52:55 MetaFG_0] (main.py 265): INFO Train: [46/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2927 (0.3060)	loss 1.3029 (1.3721)	grad_norm 30.1573 (30.2173)	mem 4879MB
[2022-05-31 04:52:58 MetaFG_0] (main.py 265): INFO Train: [46/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2920 (0.3060)	loss 1.1374 (1.3709)	grad_norm 19.3084 (30.2743)	mem 4879MB
[2022-05-31 04:53:01 MetaFG_0] (main.py 265): INFO Train: [46/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2976 (0.3060)	loss 1.0336 (1.3718)	grad_norm 33.3912 (30.2904)	mem 4879MB
[2022-05-31 04:53:04 MetaFG_0] (main.py 265): INFO Train: [46/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2924 (0.3060)	loss 1.8453 (1.3720)	grad_norm 113.2595 (30.3271)	mem 4879MB
[2022-05-31 04:53:07 MetaFG_0] (main.py 265): INFO Train: [46/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2988 (0.3060)	loss 1.6233 (1.3734)	grad_norm 17.7869 (30.3216)	mem 4879MB
[2022-05-31 04:53:10 MetaFG_0] (main.py 265): INFO Train: [46/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2967 (0.3059)	loss 1.3118 (1.3736)	grad_norm 34.4175 (30.3299)	mem 4879MB
[2022-05-31 04:53:13 MetaFG_0] (main.py 265): INFO Train: [46/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3068 (0.3059)	loss 0.9438 (1.3740)	grad_norm 18.4341 (30.3075)	mem 4879MB
[2022-05-31 04:53:17 MetaFG_0] (main.py 265): INFO Train: [46/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3048 (0.3059)	loss 1.0154 (1.3737)	grad_norm 26.5131 (30.3107)	mem 4879MB
[2022-05-31 04:53:20 MetaFG_0] (main.py 265): INFO Train: [46/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3022 (0.3059)	loss 1.5270 (1.3746)	grad_norm 39.7392 (30.3170)	mem 4879MB
[2022-05-31 04:53:23 MetaFG_0] (main.py 265): INFO Train: [46/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2943 (0.3059)	loss 1.7366 (1.3760)	grad_norm 85.2411 (30.3960)	mem 4879MB
[2022-05-31 04:53:26 MetaFG_0] (main.py 265): INFO Train: [46/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3001 (0.3059)	loss 1.5073 (1.3756)	grad_norm 29.5527 (30.4621)	mem 4879MB
[2022-05-31 04:53:29 MetaFG_0] (main.py 265): INFO Train: [46/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2958 (0.3059)	loss 1.2908 (1.3770)	grad_norm 17.5475 (30.4771)	mem 4879MB
[2022-05-31 04:53:32 MetaFG_0] (main.py 265): INFO Train: [46/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2941 (0.3059)	loss 1.4690 (1.3766)	grad_norm 27.8178 (30.5295)	mem 4879MB
[2022-05-31 04:53:35 MetaFG_0] (main.py 265): INFO Train: [46/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2931 (0.3059)	loss 1.5324 (1.3767)	grad_norm 22.2450 (30.5178)	mem 4879MB
[2022-05-31 04:53:38 MetaFG_0] (main.py 265): INFO Train: [46/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2931 (0.3059)	loss 1.7233 (1.3770)	grad_norm 36.0788 (30.5418)	mem 4879MB
[2022-05-31 04:53:41 MetaFG_0] (main.py 265): INFO Train: [46/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2955 (0.3058)	loss 1.5594 (1.3763)	grad_norm 33.8613 (30.5732)	mem 4879MB
[2022-05-31 04:53:44 MetaFG_0] (main.py 265): INFO Train: [46/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2936 (0.3058)	loss 1.2071 (1.3767)	grad_norm 27.2676 (30.5726)	mem 4879MB
[2022-05-31 04:53:47 MetaFG_0] (main.py 265): INFO Train: [46/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2937 (0.3058)	loss 1.7292 (1.3769)	grad_norm 35.1970 (30.5703)	mem 4879MB
[2022-05-31 04:53:50 MetaFG_0] (main.py 265): INFO Train: [46/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2936 (0.3058)	loss 1.4526 (1.3775)	grad_norm 52.9681 (30.6830)	mem 4879MB
[2022-05-31 04:53:53 MetaFG_0] (main.py 265): INFO Train: [46/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2928 (0.3058)	loss 1.7124 (1.3786)	grad_norm 48.6098 (30.7090)	mem 4879MB
[2022-05-31 04:53:56 MetaFG_0] (main.py 265): INFO Train: [46/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2947 (0.3058)	loss 1.4151 (1.3792)	grad_norm 25.1815 (30.7039)	mem 4879MB
[2022-05-31 04:53:59 MetaFG_0] (main.py 265): INFO Train: [46/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2941 (0.3058)	loss 1.3331 (1.3790)	grad_norm 43.5573 (30.7170)	mem 4879MB
[2022-05-31 04:54:02 MetaFG_0] (main.py 265): INFO Train: [46/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2927 (0.3058)	loss 1.0466 (1.3788)	grad_norm 26.4503 (30.7005)	mem 4879MB
[2022-05-31 04:54:05 MetaFG_0] (main.py 265): INFO Train: [46/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2930 (0.3058)	loss 1.8455 (1.3795)	grad_norm 35.8426 (30.6848)	mem 4879MB
[2022-05-31 04:54:08 MetaFG_0] (main.py 265): INFO Train: [46/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2989 (0.3058)	loss 0.8244 (1.3796)	grad_norm 45.0028 (30.7106)	mem 4879MB
[2022-05-31 04:54:11 MetaFG_0] (main.py 265): INFO Train: [46/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2935 (0.3058)	loss 1.4864 (1.3795)	grad_norm 61.4064 (30.7116)	mem 4879MB
[2022-05-31 04:54:14 MetaFG_0] (main.py 265): INFO Train: [46/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3019 (0.3058)	loss 1.3926 (1.3789)	grad_norm 50.3824 (30.7436)	mem 4879MB
[2022-05-31 04:54:17 MetaFG_0] (main.py 265): INFO Train: [46/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2929 (0.3058)	loss 1.1878 (1.3796)	grad_norm 28.4451 (30.7357)	mem 4879MB
[2022-05-31 04:54:21 MetaFG_0] (main.py 265): INFO Train: [46/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2924 (0.3058)	loss 1.5298 (1.3799)	grad_norm 42.2586 (30.7101)	mem 4879MB
[2022-05-31 04:54:24 MetaFG_0] (main.py 265): INFO Train: [46/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2998 (0.3058)	loss 1.4137 (1.3801)	grad_norm 25.0450 (30.7563)	mem 4879MB
[2022-05-31 04:54:27 MetaFG_0] (main.py 265): INFO Train: [46/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2991 (0.3058)	loss 1.1805 (1.3802)	grad_norm 20.9024 (30.7740)	mem 4879MB
[2022-05-31 04:54:30 MetaFG_0] (main.py 265): INFO Train: [46/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2995 (0.3058)	loss 1.5489 (1.3808)	grad_norm 30.5279 (30.7858)	mem 4879MB
[2022-05-31 04:54:33 MetaFG_0] (main.py 265): INFO Train: [46/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2924 (0.3057)	loss 1.3047 (1.3810)	grad_norm 22.9482 (30.7748)	mem 4879MB
[2022-05-31 04:54:36 MetaFG_0] (main.py 265): INFO Train: [46/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.3002 (0.3057)	loss 1.4445 (1.3821)	grad_norm 32.8022 (30.8061)	mem 4879MB
[2022-05-31 04:54:39 MetaFG_0] (main.py 265): INFO Train: [46/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2927 (0.3057)	loss 1.2464 (1.3820)	grad_norm 15.8416 (30.7967)	mem 4879MB
[2022-05-31 04:54:42 MetaFG_0] (main.py 265): INFO Train: [46/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2991 (0.3057)	loss 1.5824 (1.3817)	grad_norm 32.9884 (30.7939)	mem 4879MB
[2022-05-31 04:54:45 MetaFG_0] (main.py 265): INFO Train: [46/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2925 (0.3057)	loss 1.4585 (1.3815)	grad_norm 26.2439 (30.7878)	mem 4879MB
[2022-05-31 04:54:48 MetaFG_0] (main.py 265): INFO Train: [46/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2923 (0.3057)	loss 1.6731 (1.3820)	grad_norm 38.9620 (30.7940)	mem 4879MB
[2022-05-31 04:54:51 MetaFG_0] (main.py 265): INFO Train: [46/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2998 (0.3057)	loss 1.4872 (1.3819)	grad_norm 25.8838 (30.7809)	mem 4879MB
[2022-05-31 04:54:54 MetaFG_0] (main.py 265): INFO Train: [46/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2995 (0.3057)	loss 1.4574 (1.3818)	grad_norm 18.7164 (30.7648)	mem 4879MB
[2022-05-31 04:54:57 MetaFG_0] (main.py 265): INFO Train: [46/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2984 (0.3057)	loss 1.6325 (1.3825)	grad_norm 53.4668 (30.7966)	mem 4879MB
[2022-05-31 04:55:00 MetaFG_0] (main.py 265): INFO Train: [46/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2942 (0.3057)	loss 1.6230 (1.3824)	grad_norm 24.0796 (30.7847)	mem 4879MB
[2022-05-31 04:55:03 MetaFG_0] (main.py 265): INFO Train: [46/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2938 (0.3057)	loss 1.5469 (1.3829)	grad_norm 37.2737 (30.7492)	mem 4879MB
[2022-05-31 04:55:06 MetaFG_0] (main.py 265): INFO Train: [46/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2943 (0.3057)	loss 1.0089 (1.3826)	grad_norm 49.0750 (30.7392)	mem 4879MB
[2022-05-31 04:55:10 MetaFG_0] (main.py 265): INFO Train: [46/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3439 (0.3059)	loss 1.7064 (1.3823)	grad_norm 35.8529 (30.7412)	mem 4879MB
[2022-05-31 04:55:13 MetaFG_0] (main.py 265): INFO Train: [46/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2926 (0.3059)	loss 1.7934 (1.3830)	grad_norm 36.2201 (30.7986)	mem 4879MB
[2022-05-31 04:55:16 MetaFG_0] (main.py 265): INFO Train: [46/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2953 (0.3058)	loss 1.4865 (1.3829)	grad_norm 38.1634 (30.7821)	mem 4879MB
[2022-05-31 04:55:19 MetaFG_0] (main.py 265): INFO Train: [46/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2930 (0.3058)	loss 1.5258 (1.3836)	grad_norm 24.6761 (30.7735)	mem 4879MB
[2022-05-31 04:55:22 MetaFG_0] (main.py 265): INFO Train: [46/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2988 (0.3058)	loss 1.4025 (1.3831)	grad_norm 28.9107 (30.7792)	mem 4879MB
[2022-05-31 04:55:25 MetaFG_0] (main.py 265): INFO Train: [46/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2927 (0.3058)	loss 1.7546 (1.3833)	grad_norm 25.5197 (30.8194)	mem 4879MB
[2022-05-31 04:55:28 MetaFG_0] (main.py 265): INFO Train: [46/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2982 (0.3058)	loss 1.6170 (1.3832)	grad_norm 30.9806 (30.8211)	mem 4879MB
[2022-05-31 04:55:31 MetaFG_0] (main.py 265): INFO Train: [46/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2925 (0.3058)	loss 1.3612 (1.3831)	grad_norm 47.3727 (30.8163)	mem 4879MB
[2022-05-31 04:55:34 MetaFG_0] (main.py 265): INFO Train: [46/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2929 (0.3058)	loss 0.8542 (1.3826)	grad_norm 28.3052 (30.8105)	mem 4879MB
[2022-05-31 04:55:37 MetaFG_0] (main.py 265): INFO Train: [46/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2929 (0.3058)	loss 1.2281 (1.3829)	grad_norm 52.2455 (30.8169)	mem 4879MB
[2022-05-31 04:55:40 MetaFG_0] (main.py 265): INFO Train: [46/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2982 (0.3058)	loss 1.4891 (1.3824)	grad_norm 39.4961 (30.8114)	mem 4879MB
[2022-05-31 04:55:43 MetaFG_0] (main.py 265): INFO Train: [46/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2985 (0.3058)	loss 1.5931 (1.3827)	grad_norm 23.6307 (30.7944)	mem 4879MB
[2022-05-31 04:55:46 MetaFG_0] (main.py 265): INFO Train: [46/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2930 (0.3058)	loss 1.4846 (1.3827)	grad_norm 23.1456 (30.8178)	mem 4879MB
[2022-05-31 04:55:49 MetaFG_0] (main.py 265): INFO Train: [46/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2944 (0.3058)	loss 1.7022 (1.3829)	grad_norm 38.8466 (30.8369)	mem 4879MB
[2022-05-31 04:55:52 MetaFG_0] (main.py 265): INFO Train: [46/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2936 (0.3058)	loss 1.5786 (1.3826)	grad_norm 23.4887 (30.8158)	mem 4879MB
[2022-05-31 04:55:55 MetaFG_0] (main.py 265): INFO Train: [46/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2926 (0.3058)	loss 1.4296 (1.3825)	grad_norm 26.2385 (30.8172)	mem 4879MB
[2022-05-31 04:55:58 MetaFG_0] (main.py 265): INFO Train: [46/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2923 (0.3058)	loss 1.0056 (1.3819)	grad_norm 39.6740 (30.8540)	mem 4879MB
[2022-05-31 04:56:01 MetaFG_0] (main.py 265): INFO Train: [46/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2922 (0.3057)	loss 1.7152 (1.3824)	grad_norm 27.6816 (30.8720)	mem 4879MB
[2022-05-31 04:56:02 MetaFG_0] (main.py 272): INFO EPOCH 46 training takes 0:07:57
[2022-05-31 04:56:02 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_46.pth saving......
[2022-05-31 04:56:03 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_46.pth saved !!!
[2022-05-31 04:56:03 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 04:56:04 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 04:56:04 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 04:56:05 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.675 (0.675)	Loss 0.5095 (0.5095)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 04:56:06 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.099 (0.150)	Loss 0.7823 (0.6207)	Acc@1 84.375 (87.216)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 04:56:07 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.088 (0.124)	Loss 0.4937 (0.6239)	Acc@1 93.750 (86.905)	Acc@5 96.875 (98.214)	Mem 4879MB
[2022-05-31 04:56:08 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.115)	Loss 0.7408 (0.6462)	Acc@1 84.375 (85.887)	Acc@5 96.875 (98.185)	Mem 4879MB
[2022-05-31 04:56:09 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.097 (0.110)	Loss 0.6797 (0.6433)	Acc@1 87.500 (85.823)	Acc@5 96.875 (98.095)	Mem 4879MB
[2022-05-31 04:56:10 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.103 (0.107)	Loss 0.5627 (0.6508)	Acc@1 87.500 (85.662)	Acc@5 100.000 (98.223)	Mem 4879MB
[2022-05-31 04:56:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.093 (0.105)	Loss 0.4810 (0.6444)	Acc@1 90.625 (85.502)	Acc@5 100.000 (98.463)	Mem 4879MB
[2022-05-31 04:56:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.5614 (0.6420)	Acc@1 87.500 (85.607)	Acc@5 100.000 (98.460)	Mem 4879MB
[2022-05-31 04:56:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.093 (0.102)	Loss 0.3525 (0.6373)	Acc@1 96.875 (85.725)	Acc@5 100.000 (98.495)	Mem 4879MB
[2022-05-31 04:56:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.8959 (0.6366)	Acc@1 78.125 (85.817)	Acc@5 93.750 (98.489)	Mem 4879MB
[2022-05-31 04:56:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.100 (0.101)	Loss 0.7690 (0.6341)	Acc@1 87.500 (86.015)	Acc@5 96.875 (98.453)	Mem 4879MB
[2022-05-31 04:56:15 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.100)	Loss 0.7302 (0.6403)	Acc@1 78.125 (86.036)	Acc@5 100.000 (98.452)	Mem 4879MB
[2022-05-31 04:56:16 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.099)	Loss 0.6240 (0.6410)	Acc@1 87.500 (85.925)	Acc@5 100.000 (98.476)	Mem 4879MB
[2022-05-31 04:56:17 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.7750 (0.6468)	Acc@1 87.500 (85.878)	Acc@5 100.000 (98.473)	Mem 4879MB
[2022-05-31 04:56:18 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.091 (0.099)	Loss 0.6165 (0.6455)	Acc@1 84.375 (85.926)	Acc@5 100.000 (98.471)	Mem 4879MB
[2022-05-31 04:56:19 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.4908 (0.6500)	Acc@1 90.625 (85.824)	Acc@5 100.000 (98.448)	Mem 4879MB
[2022-05-31 04:56:20 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.098)	Loss 0.4920 (0.6515)	Acc@1 90.625 (85.714)	Acc@5 100.000 (98.486)	Mem 4879MB
[2022-05-31 04:56:21 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.5927 (0.6532)	Acc@1 87.500 (85.599)	Acc@5 96.875 (98.538)	Mem 4879MB
[2022-05-31 04:56:22 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.8194 (0.6511)	Acc@1 78.125 (85.704)	Acc@5 96.875 (98.584)	Mem 4879MB
[2022-05-31 04:56:23 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.098 (0.098)	Loss 0.3753 (0.6484)	Acc@1 96.875 (85.733)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 04:56:24 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.098)	Loss 0.8797 (0.6507)	Acc@1 78.125 (85.665)	Acc@5 93.750 (98.616)	Mem 4879MB
[2022-05-31 04:56:25 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.098)	Loss 0.6940 (0.6528)	Acc@1 90.625 (85.664)	Acc@5 96.875 (98.637)	Mem 4879MB
[2022-05-31 04:56:26 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.097)	Loss 0.5529 (0.6520)	Acc@1 81.250 (85.662)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 04:56:27 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.7455 (0.6519)	Acc@1 87.500 (85.741)	Acc@5 96.875 (98.647)	Mem 4879MB
[2022-05-31 04:56:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.095 (0.097)	Loss 0.5286 (0.6542)	Acc@1 87.500 (85.659)	Acc@5 100.000 (98.613)	Mem 4879MB
[2022-05-31 04:56:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.7003 (0.6548)	Acc@1 81.250 (85.558)	Acc@5 100.000 (98.630)	Mem 4879MB
[2022-05-31 04:56:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.097 (0.097)	Loss 0.6324 (0.6514)	Acc@1 78.125 (85.596)	Acc@5 100.000 (98.683)	Mem 4879MB
[2022-05-31 04:56:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.097)	Loss 0.5936 (0.6537)	Acc@1 87.500 (85.586)	Acc@5 100.000 (98.616)	Mem 4879MB
[2022-05-31 04:56:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 0.6244 (0.6547)	Acc@1 84.375 (85.565)	Acc@5 100.000 (98.588)	Mem 4879MB
[2022-05-31 04:56:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.097 (0.097)	Loss 0.8258 (0.6553)	Acc@1 81.250 (85.621)	Acc@5 96.875 (98.561)	Mem 4879MB
[2022-05-31 04:56:33 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.097)	Loss 0.8905 (0.6580)	Acc@1 78.125 (85.579)	Acc@5 100.000 (98.536)	Mem 4879MB
[2022-05-31 04:56:34 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6742 (0.6609)	Acc@1 87.500 (85.551)	Acc@5 96.875 (98.503)	Mem 4879MB
[2022-05-31 04:56:34 MetaFG_0] (main.py 330): INFO  * Acc@1 85.550 Acc@5 98.510
[2022-05-31 04:56:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.5%
[2022-05-31 04:56:34 MetaFG_0] (main.py 171): INFO Max accuracy: 85.55%
[2022-05-31 04:56:35 MetaFG_0] (main.py 265): INFO Train: [47/300][0/1562]	eta 0:27:02 lr 0.000006	time 1.0390 (1.0390)	loss 1.5778 (1.5778)	grad_norm 41.9813 (41.9813)	mem 4879MB
[2022-05-31 04:56:38 MetaFG_0] (main.py 265): INFO Train: [47/300][10/1562]	eta 0:09:46 lr 0.000006	time 0.2952 (0.3777)	loss 1.3921 (1.3214)	grad_norm 29.5179 (27.1081)	mem 4879MB
[2022-05-31 04:56:42 MetaFG_0] (main.py 265): INFO Train: [47/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.2940 (0.3425)	loss 1.6713 (1.3638)	grad_norm 32.1346 (28.5035)	mem 4879MB
[2022-05-31 04:56:45 MetaFG_0] (main.py 265): INFO Train: [47/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.2924 (0.3306)	loss 1.5251 (1.3661)	grad_norm 39.5348 (29.9730)	mem 4879MB
[2022-05-31 04:56:48 MetaFG_0] (main.py 265): INFO Train: [47/300][40/1562]	eta 0:08:14 lr 0.000006	time 0.3004 (0.3248)	loss 1.5292 (1.3558)	grad_norm 25.3615 (30.5040)	mem 4879MB
[2022-05-31 04:56:51 MetaFG_0] (main.py 265): INFO Train: [47/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2994 (0.3207)	loss 1.1347 (1.3605)	grad_norm 24.3699 (30.4389)	mem 4879MB
[2022-05-31 04:56:54 MetaFG_0] (main.py 265): INFO Train: [47/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2946 (0.3180)	loss 1.4421 (1.3549)	grad_norm 19.2708 (30.5906)	mem 4879MB
[2022-05-31 04:56:57 MetaFG_0] (main.py 265): INFO Train: [47/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.3008 (0.3161)	loss 1.2661 (1.3662)	grad_norm 25.1011 (30.9039)	mem 4879MB
[2022-05-31 04:57:00 MetaFG_0] (main.py 265): INFO Train: [47/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2994 (0.3148)	loss 1.6192 (1.3636)	grad_norm 31.8752 (30.5091)	mem 4879MB
[2022-05-31 04:57:03 MetaFG_0] (main.py 265): INFO Train: [47/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2917 (0.3135)	loss 1.3515 (1.3632)	grad_norm 30.5933 (30.2348)	mem 4879MB
[2022-05-31 04:57:06 MetaFG_0] (main.py 265): INFO Train: [47/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.3005 (0.3126)	loss 1.4110 (1.3618)	grad_norm 18.0558 (29.9296)	mem 4879MB
[2022-05-31 04:57:09 MetaFG_0] (main.py 265): INFO Train: [47/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2930 (0.3120)	loss 1.5217 (1.3659)	grad_norm 51.7013 (29.8874)	mem 4879MB
[2022-05-31 04:57:12 MetaFG_0] (main.py 265): INFO Train: [47/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2938 (0.3115)	loss 1.4989 (1.3725)	grad_norm 28.7287 (30.0048)	mem 4879MB
[2022-05-31 04:57:15 MetaFG_0] (main.py 265): INFO Train: [47/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2937 (0.3110)	loss 1.2007 (1.3655)	grad_norm 34.6041 (29.8944)	mem 4879MB
[2022-05-31 04:57:18 MetaFG_0] (main.py 265): INFO Train: [47/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2954 (0.3106)	loss 1.5843 (1.3636)	grad_norm 17.5305 (29.8442)	mem 4879MB
[2022-05-31 04:57:21 MetaFG_0] (main.py 265): INFO Train: [47/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2982 (0.3102)	loss 1.2689 (1.3577)	grad_norm 25.8515 (29.5612)	mem 4879MB
[2022-05-31 04:57:24 MetaFG_0] (main.py 265): INFO Train: [47/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2932 (0.3099)	loss 1.7366 (1.3623)	grad_norm 21.4932 (29.4393)	mem 4879MB
[2022-05-31 04:57:27 MetaFG_0] (main.py 265): INFO Train: [47/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2928 (0.3096)	loss 1.1463 (1.3609)	grad_norm 21.5083 (29.6003)	mem 4879MB
[2022-05-31 04:57:30 MetaFG_0] (main.py 265): INFO Train: [47/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2929 (0.3094)	loss 1.6776 (1.3599)	grad_norm 31.1111 (29.7509)	mem 4879MB
[2022-05-31 04:57:33 MetaFG_0] (main.py 265): INFO Train: [47/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2987 (0.3092)	loss 1.6730 (1.3586)	grad_norm 21.6831 (29.8389)	mem 4879MB
[2022-05-31 04:57:36 MetaFG_0] (main.py 265): INFO Train: [47/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2925 (0.3089)	loss 1.3118 (1.3592)	grad_norm 16.0888 (29.6381)	mem 4879MB
[2022-05-31 04:57:39 MetaFG_0] (main.py 265): INFO Train: [47/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2934 (0.3086)	loss 1.2326 (1.3620)	grad_norm 21.6158 (29.5153)	mem 4879MB
[2022-05-31 04:57:42 MetaFG_0] (main.py 265): INFO Train: [47/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2971 (0.3083)	loss 1.6210 (1.3628)	grad_norm 45.2590 (29.4877)	mem 4879MB
[2022-05-31 04:57:46 MetaFG_0] (main.py 265): INFO Train: [47/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2928 (0.3082)	loss 0.8638 (1.3610)	grad_norm 18.8924 (29.3653)	mem 4879MB
[2022-05-31 04:57:49 MetaFG_0] (main.py 265): INFO Train: [47/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.3008 (0.3081)	loss 1.1910 (1.3610)	grad_norm 21.9412 (29.2523)	mem 4879MB
[2022-05-31 04:57:52 MetaFG_0] (main.py 265): INFO Train: [47/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.3026 (0.3081)	loss 1.8884 (1.3585)	grad_norm 23.0969 (29.4020)	mem 4879MB
[2022-05-31 04:57:55 MetaFG_0] (main.py 265): INFO Train: [47/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2941 (0.3080)	loss 1.5381 (1.3607)	grad_norm 33.2132 (29.2935)	mem 4879MB
[2022-05-31 04:57:58 MetaFG_0] (main.py 265): INFO Train: [47/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2925 (0.3079)	loss 1.1910 (1.3585)	grad_norm 19.2749 (29.2195)	mem 4879MB
[2022-05-31 04:58:01 MetaFG_0] (main.py 265): INFO Train: [47/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.3000 (0.3078)	loss 1.2722 (1.3572)	grad_norm 16.6755 (29.2531)	mem 4879MB
[2022-05-31 04:58:04 MetaFG_0] (main.py 265): INFO Train: [47/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2928 (0.3077)	loss 1.3832 (1.3558)	grad_norm 28.2764 (29.3376)	mem 4879MB
[2022-05-31 04:58:07 MetaFG_0] (main.py 265): INFO Train: [47/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2990 (0.3076)	loss 0.8371 (1.3511)	grad_norm 17.0170 (29.3858)	mem 4879MB
[2022-05-31 04:58:10 MetaFG_0] (main.py 265): INFO Train: [47/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2924 (0.3075)	loss 1.5238 (1.3515)	grad_norm 36.0199 (29.5751)	mem 4879MB
[2022-05-31 04:58:13 MetaFG_0] (main.py 265): INFO Train: [47/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2928 (0.3074)	loss 1.5742 (1.3538)	grad_norm 23.9173 (29.7363)	mem 4879MB
[2022-05-31 04:58:16 MetaFG_0] (main.py 265): INFO Train: [47/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2994 (0.3072)	loss 1.7305 (1.3540)	grad_norm 26.7156 (29.6837)	mem 4879MB
[2022-05-31 04:58:19 MetaFG_0] (main.py 265): INFO Train: [47/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2929 (0.3071)	loss 1.6959 (1.3549)	grad_norm 45.6002 (29.7638)	mem 4879MB
[2022-05-31 04:58:22 MetaFG_0] (main.py 265): INFO Train: [47/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2937 (0.3070)	loss 1.2698 (1.3569)	grad_norm 19.8684 (29.8524)	mem 4879MB
[2022-05-31 04:58:25 MetaFG_0] (main.py 265): INFO Train: [47/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2978 (0.3069)	loss 1.1229 (1.3571)	grad_norm 22.7000 (29.9020)	mem 4879MB
[2022-05-31 04:58:28 MetaFG_0] (main.py 265): INFO Train: [47/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2935 (0.3069)	loss 0.8669 (1.3564)	grad_norm 34.9991 (29.8655)	mem 4879MB
[2022-05-31 04:58:31 MetaFG_0] (main.py 265): INFO Train: [47/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2921 (0.3068)	loss 1.5687 (1.3576)	grad_norm 52.9426 (29.9493)	mem 4879MB
[2022-05-31 04:58:34 MetaFG_0] (main.py 265): INFO Train: [47/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2984 (0.3068)	loss 1.4684 (1.3605)	grad_norm 33.4887 (30.0998)	mem 4879MB
[2022-05-31 04:58:37 MetaFG_0] (main.py 265): INFO Train: [47/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2999 (0.3068)	loss 1.5287 (1.3580)	grad_norm 31.7087 (30.2404)	mem 4879MB
[2022-05-31 04:58:40 MetaFG_0] (main.py 265): INFO Train: [47/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2926 (0.3066)	loss 0.9255 (1.3596)	grad_norm 49.6214 (nan)	mem 4879MB
[2022-05-31 04:58:43 MetaFG_0] (main.py 265): INFO Train: [47/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2934 (0.3066)	loss 1.5828 (1.3587)	grad_norm 32.2905 (nan)	mem 4879MB
[2022-05-31 04:58:46 MetaFG_0] (main.py 265): INFO Train: [47/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2945 (0.3065)	loss 1.4679 (1.3622)	grad_norm 17.8876 (nan)	mem 4879MB
[2022-05-31 04:58:49 MetaFG_0] (main.py 265): INFO Train: [47/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2940 (0.3064)	loss 1.1024 (1.3605)	grad_norm 36.6146 (nan)	mem 4879MB
[2022-05-31 04:58:53 MetaFG_0] (main.py 265): INFO Train: [47/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.3002 (0.3064)	loss 1.2302 (1.3628)	grad_norm 23.4087 (nan)	mem 4879MB
[2022-05-31 04:58:56 MetaFG_0] (main.py 265): INFO Train: [47/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2969 (0.3063)	loss 1.1415 (1.3615)	grad_norm 20.9720 (nan)	mem 4879MB
[2022-05-31 04:58:59 MetaFG_0] (main.py 265): INFO Train: [47/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2950 (0.3063)	loss 1.4486 (1.3613)	grad_norm 34.9009 (nan)	mem 4879MB
[2022-05-31 04:59:02 MetaFG_0] (main.py 265): INFO Train: [47/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2938 (0.3062)	loss 1.3902 (1.3617)	grad_norm 34.2146 (nan)	mem 4879MB
[2022-05-31 04:59:05 MetaFG_0] (main.py 265): INFO Train: [47/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2936 (0.3062)	loss 1.3787 (1.3618)	grad_norm 26.7578 (nan)	mem 4879MB
[2022-05-31 04:59:08 MetaFG_0] (main.py 265): INFO Train: [47/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2993 (0.3061)	loss 1.4656 (1.3641)	grad_norm 20.4022 (nan)	mem 4879MB
[2022-05-31 04:59:11 MetaFG_0] (main.py 265): INFO Train: [47/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2998 (0.3061)	loss 1.5855 (1.3646)	grad_norm 42.9575 (nan)	mem 4879MB
[2022-05-31 04:59:14 MetaFG_0] (main.py 265): INFO Train: [47/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.3007 (0.3061)	loss 1.0842 (1.3619)	grad_norm 27.0550 (nan)	mem 4879MB
[2022-05-31 04:59:17 MetaFG_0] (main.py 265): INFO Train: [47/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2951 (0.3060)	loss 1.1714 (1.3613)	grad_norm 21.9218 (nan)	mem 4879MB
[2022-05-31 04:59:20 MetaFG_0] (main.py 265): INFO Train: [47/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.3099 (0.3060)	loss 1.1636 (1.3599)	grad_norm 22.6224 (nan)	mem 4879MB
[2022-05-31 04:59:23 MetaFG_0] (main.py 265): INFO Train: [47/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2991 (0.3060)	loss 1.2117 (1.3610)	grad_norm 24.6236 (nan)	mem 4879MB
[2022-05-31 04:59:26 MetaFG_0] (main.py 265): INFO Train: [47/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.3052 (0.3060)	loss 1.3430 (1.3611)	grad_norm 31.3285 (nan)	mem 4879MB
[2022-05-31 04:59:29 MetaFG_0] (main.py 265): INFO Train: [47/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.3049 (0.3060)	loss 1.2626 (1.3604)	grad_norm 32.2529 (nan)	mem 4879MB
[2022-05-31 04:59:32 MetaFG_0] (main.py 265): INFO Train: [47/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2930 (0.3060)	loss 1.2251 (1.3625)	grad_norm 36.1939 (nan)	mem 4879MB
[2022-05-31 04:59:35 MetaFG_0] (main.py 265): INFO Train: [47/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.3086 (0.3060)	loss 1.1583 (1.3631)	grad_norm 35.6738 (nan)	mem 4879MB
[2022-05-31 04:59:38 MetaFG_0] (main.py 265): INFO Train: [47/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2936 (0.3059)	loss 1.4770 (1.3630)	grad_norm 22.7751 (nan)	mem 4879MB
[2022-05-31 04:59:41 MetaFG_0] (main.py 265): INFO Train: [47/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2952 (0.3059)	loss 1.4966 (1.3647)	grad_norm 21.5657 (nan)	mem 4879MB
[2022-05-31 04:59:44 MetaFG_0] (main.py 265): INFO Train: [47/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2996 (0.3059)	loss 1.7674 (1.3663)	grad_norm 33.4022 (nan)	mem 4879MB
[2022-05-31 04:59:47 MetaFG_0] (main.py 265): INFO Train: [47/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3000 (0.3059)	loss 1.1698 (1.3653)	grad_norm 32.5318 (nan)	mem 4879MB
[2022-05-31 04:59:50 MetaFG_0] (main.py 265): INFO Train: [47/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2988 (0.3058)	loss 1.5296 (1.3664)	grad_norm 14.5082 (nan)	mem 4879MB
[2022-05-31 04:59:53 MetaFG_0] (main.py 265): INFO Train: [47/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2928 (0.3058)	loss 1.1514 (1.3662)	grad_norm 34.3121 (nan)	mem 4879MB
[2022-05-31 04:59:56 MetaFG_0] (main.py 265): INFO Train: [47/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2990 (0.3058)	loss 1.4119 (1.3663)	grad_norm 23.6626 (nan)	mem 4879MB
[2022-05-31 04:59:59 MetaFG_0] (main.py 265): INFO Train: [47/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2938 (0.3058)	loss 1.4649 (1.3671)	grad_norm 53.1523 (nan)	mem 4879MB
[2022-05-31 05:00:03 MetaFG_0] (main.py 265): INFO Train: [47/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2928 (0.3057)	loss 1.3483 (1.3673)	grad_norm 29.0608 (nan)	mem 4879MB
[2022-05-31 05:00:06 MetaFG_0] (main.py 265): INFO Train: [47/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2927 (0.3057)	loss 1.2592 (1.3671)	grad_norm 33.1627 (nan)	mem 4879MB
[2022-05-31 05:00:09 MetaFG_0] (main.py 265): INFO Train: [47/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2933 (0.3057)	loss 1.5356 (1.3693)	grad_norm 19.3265 (nan)	mem 4879MB
[2022-05-31 05:00:12 MetaFG_0] (main.py 265): INFO Train: [47/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.3006 (0.3057)	loss 1.3345 (1.3709)	grad_norm 20.3185 (nan)	mem 4879MB
[2022-05-31 05:00:15 MetaFG_0] (main.py 265): INFO Train: [47/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2948 (0.3057)	loss 1.3192 (1.3710)	grad_norm 25.7278 (nan)	mem 4879MB
[2022-05-31 05:00:18 MetaFG_0] (main.py 265): INFO Train: [47/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3004 (0.3057)	loss 1.3323 (1.3703)	grad_norm 47.5166 (nan)	mem 4879MB
[2022-05-31 05:00:21 MetaFG_0] (main.py 265): INFO Train: [47/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2987 (0.3057)	loss 1.5027 (1.3707)	grad_norm 32.2617 (nan)	mem 4879MB
[2022-05-31 05:00:24 MetaFG_0] (main.py 265): INFO Train: [47/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2929 (0.3056)	loss 1.6849 (1.3722)	grad_norm 25.4729 (nan)	mem 4879MB
[2022-05-31 05:00:27 MetaFG_0] (main.py 265): INFO Train: [47/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2983 (0.3056)	loss 0.8782 (1.3708)	grad_norm 29.2764 (nan)	mem 4879MB
[2022-05-31 05:00:30 MetaFG_0] (main.py 265): INFO Train: [47/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2987 (0.3056)	loss 1.3797 (1.3722)	grad_norm 34.8693 (nan)	mem 4879MB
[2022-05-31 05:00:33 MetaFG_0] (main.py 265): INFO Train: [47/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2927 (0.3056)	loss 1.7136 (1.3722)	grad_norm 36.6246 (nan)	mem 4879MB
[2022-05-31 05:00:36 MetaFG_0] (main.py 265): INFO Train: [47/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2978 (0.3056)	loss 1.0995 (1.3715)	grad_norm 27.2567 (nan)	mem 4879MB
[2022-05-31 05:00:39 MetaFG_0] (main.py 265): INFO Train: [47/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2992 (0.3056)	loss 1.5430 (1.3697)	grad_norm 21.4038 (nan)	mem 4879MB
[2022-05-31 05:00:42 MetaFG_0] (main.py 265): INFO Train: [47/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2941 (0.3055)	loss 1.6020 (1.3687)	grad_norm 32.3358 (nan)	mem 4879MB
[2022-05-31 05:00:45 MetaFG_0] (main.py 265): INFO Train: [47/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2927 (0.3056)	loss 1.4792 (1.3687)	grad_norm 25.4301 (nan)	mem 4879MB
[2022-05-31 05:00:48 MetaFG_0] (main.py 265): INFO Train: [47/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2991 (0.3056)	loss 1.0165 (1.3683)	grad_norm 23.2200 (nan)	mem 4879MB
[2022-05-31 05:00:51 MetaFG_0] (main.py 265): INFO Train: [47/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2949 (0.3056)	loss 1.2026 (1.3688)	grad_norm 49.4983 (nan)	mem 4879MB
[2022-05-31 05:00:54 MetaFG_0] (main.py 265): INFO Train: [47/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2933 (0.3055)	loss 1.5631 (1.3689)	grad_norm 33.5668 (nan)	mem 4879MB
[2022-05-31 05:00:57 MetaFG_0] (main.py 265): INFO Train: [47/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2928 (0.3055)	loss 1.4461 (1.3697)	grad_norm 25.9315 (nan)	mem 4879MB
[2022-05-31 05:01:00 MetaFG_0] (main.py 265): INFO Train: [47/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2982 (0.3055)	loss 1.0484 (1.3683)	grad_norm 30.7988 (nan)	mem 4879MB
[2022-05-31 05:01:03 MetaFG_0] (main.py 265): INFO Train: [47/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2994 (0.3055)	loss 1.2836 (1.3695)	grad_norm 31.9004 (nan)	mem 4879MB
[2022-05-31 05:01:06 MetaFG_0] (main.py 265): INFO Train: [47/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2947 (0.3055)	loss 1.4997 (1.3695)	grad_norm 54.0247 (nan)	mem 4879MB
[2022-05-31 05:01:10 MetaFG_0] (main.py 265): INFO Train: [47/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2925 (0.3055)	loss 1.1552 (1.3701)	grad_norm 30.1374 (nan)	mem 4879MB
[2022-05-31 05:01:13 MetaFG_0] (main.py 265): INFO Train: [47/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3000 (0.3054)	loss 1.4218 (1.3719)	grad_norm 13.5151 (nan)	mem 4879MB
[2022-05-31 05:01:16 MetaFG_0] (main.py 265): INFO Train: [47/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2958 (0.3054)	loss 1.4853 (1.3723)	grad_norm 33.0634 (nan)	mem 4879MB
[2022-05-31 05:01:19 MetaFG_0] (main.py 265): INFO Train: [47/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2938 (0.3054)	loss 1.0421 (1.3718)	grad_norm 44.7216 (nan)	mem 4879MB
[2022-05-31 05:01:22 MetaFG_0] (main.py 265): INFO Train: [47/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2934 (0.3054)	loss 1.3347 (1.3727)	grad_norm 24.4350 (nan)	mem 4879MB
[2022-05-31 05:01:25 MetaFG_0] (main.py 265): INFO Train: [47/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2929 (0.3054)	loss 1.4316 (1.3734)	grad_norm 27.5554 (nan)	mem 4879MB
[2022-05-31 05:01:28 MetaFG_0] (main.py 265): INFO Train: [47/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2920 (0.3054)	loss 1.6461 (1.3728)	grad_norm 37.3249 (nan)	mem 4879MB
[2022-05-31 05:01:31 MetaFG_0] (main.py 265): INFO Train: [47/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2992 (0.3054)	loss 1.3622 (1.3736)	grad_norm 24.0176 (nan)	mem 4879MB
[2022-05-31 05:01:34 MetaFG_0] (main.py 265): INFO Train: [47/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2997 (0.3054)	loss 1.6318 (1.3741)	grad_norm 33.9585 (nan)	mem 4879MB
[2022-05-31 05:01:37 MetaFG_0] (main.py 265): INFO Train: [47/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2999 (0.3054)	loss 1.4990 (1.3739)	grad_norm 31.2391 (nan)	mem 4879MB
[2022-05-31 05:01:40 MetaFG_0] (main.py 265): INFO Train: [47/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2991 (0.3054)	loss 1.2579 (1.3733)	grad_norm 45.8368 (nan)	mem 4879MB
[2022-05-31 05:01:43 MetaFG_0] (main.py 265): INFO Train: [47/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2928 (0.3054)	loss 1.4194 (1.3733)	grad_norm 20.7165 (nan)	mem 4879MB
[2022-05-31 05:01:46 MetaFG_0] (main.py 265): INFO Train: [47/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.3018 (0.3053)	loss 1.0744 (1.3734)	grad_norm 33.3158 (nan)	mem 4879MB
[2022-05-31 05:01:49 MetaFG_0] (main.py 265): INFO Train: [47/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2932 (0.3054)	loss 0.9102 (1.3725)	grad_norm 37.1297 (nan)	mem 4879MB
[2022-05-31 05:01:52 MetaFG_0] (main.py 265): INFO Train: [47/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2947 (0.3054)	loss 1.5909 (1.3725)	grad_norm 27.4296 (nan)	mem 4879MB
[2022-05-31 05:01:55 MetaFG_0] (main.py 265): INFO Train: [47/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2935 (0.3054)	loss 1.5866 (1.3733)	grad_norm 25.4170 (nan)	mem 4879MB
[2022-05-31 05:01:58 MetaFG_0] (main.py 265): INFO Train: [47/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2994 (0.3053)	loss 1.4885 (1.3737)	grad_norm 21.3518 (nan)	mem 4879MB
[2022-05-31 05:02:01 MetaFG_0] (main.py 265): INFO Train: [47/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2929 (0.3053)	loss 1.1887 (1.3735)	grad_norm 21.3456 (nan)	mem 4879MB
[2022-05-31 05:02:04 MetaFG_0] (main.py 265): INFO Train: [47/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3055 (0.3054)	loss 1.3798 (1.3733)	grad_norm 25.4899 (nan)	mem 4879MB
[2022-05-31 05:02:07 MetaFG_0] (main.py 265): INFO Train: [47/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.3016 (0.3054)	loss 1.5047 (1.3729)	grad_norm 24.9825 (nan)	mem 4879MB
[2022-05-31 05:02:11 MetaFG_0] (main.py 265): INFO Train: [47/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2926 (0.3054)	loss 1.2451 (1.3734)	grad_norm 17.2173 (nan)	mem 4879MB
[2022-05-31 05:02:14 MetaFG_0] (main.py 265): INFO Train: [47/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2939 (0.3054)	loss 1.4378 (1.3735)	grad_norm 32.9709 (nan)	mem 4879MB
[2022-05-31 05:02:17 MetaFG_0] (main.py 265): INFO Train: [47/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2985 (0.3054)	loss 1.3696 (1.3730)	grad_norm 16.4534 (nan)	mem 4879MB
[2022-05-31 05:02:20 MetaFG_0] (main.py 265): INFO Train: [47/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2931 (0.3054)	loss 1.7421 (1.3729)	grad_norm 37.4992 (nan)	mem 4879MB
[2022-05-31 05:02:23 MetaFG_0] (main.py 265): INFO Train: [47/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2980 (0.3053)	loss 1.3069 (1.3722)	grad_norm 21.5867 (nan)	mem 4879MB
[2022-05-31 05:02:26 MetaFG_0] (main.py 265): INFO Train: [47/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.3018 (0.3054)	loss 1.4487 (1.3722)	grad_norm 26.9049 (nan)	mem 4879MB
[2022-05-31 05:02:29 MetaFG_0] (main.py 265): INFO Train: [47/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2943 (0.3053)	loss 1.3169 (1.3729)	grad_norm 28.7145 (nan)	mem 4879MB
[2022-05-31 05:02:32 MetaFG_0] (main.py 265): INFO Train: [47/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2960 (0.3053)	loss 1.4859 (1.3726)	grad_norm 22.4053 (nan)	mem 4879MB
[2022-05-31 05:02:35 MetaFG_0] (main.py 265): INFO Train: [47/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2933 (0.3053)	loss 1.3898 (1.3729)	grad_norm 34.8655 (nan)	mem 4879MB
[2022-05-31 05:02:38 MetaFG_0] (main.py 265): INFO Train: [47/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2976 (0.3053)	loss 1.6810 (1.3742)	grad_norm 33.4095 (nan)	mem 4879MB
[2022-05-31 05:02:41 MetaFG_0] (main.py 265): INFO Train: [47/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2969 (0.3053)	loss 1.1461 (1.3738)	grad_norm 25.0855 (nan)	mem 4879MB
[2022-05-31 05:02:44 MetaFG_0] (main.py 265): INFO Train: [47/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2939 (0.3053)	loss 1.5787 (1.3729)	grad_norm 18.9134 (nan)	mem 4879MB
[2022-05-31 05:02:47 MetaFG_0] (main.py 265): INFO Train: [47/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2990 (0.3053)	loss 1.5497 (1.3731)	grad_norm 31.2843 (nan)	mem 4879MB
[2022-05-31 05:02:50 MetaFG_0] (main.py 265): INFO Train: [47/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3023 (0.3053)	loss 1.3707 (1.3741)	grad_norm 25.8335 (nan)	mem 4879MB
[2022-05-31 05:02:53 MetaFG_0] (main.py 265): INFO Train: [47/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2969 (0.3053)	loss 1.3812 (1.3746)	grad_norm 23.7135 (nan)	mem 4879MB
[2022-05-31 05:02:56 MetaFG_0] (main.py 265): INFO Train: [47/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2964 (0.3053)	loss 1.7645 (1.3755)	grad_norm 38.3746 (nan)	mem 4879MB
[2022-05-31 05:02:59 MetaFG_0] (main.py 265): INFO Train: [47/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2917 (0.3053)	loss 1.3105 (1.3752)	grad_norm 36.9194 (nan)	mem 4879MB
[2022-05-31 05:03:02 MetaFG_0] (main.py 265): INFO Train: [47/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2951 (0.3053)	loss 1.5741 (1.3753)	grad_norm 30.5541 (nan)	mem 4879MB
[2022-05-31 05:03:05 MetaFG_0] (main.py 265): INFO Train: [47/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2992 (0.3053)	loss 1.6390 (1.3762)	grad_norm 32.3167 (nan)	mem 4879MB
[2022-05-31 05:03:08 MetaFG_0] (main.py 265): INFO Train: [47/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2985 (0.3053)	loss 1.3368 (1.3755)	grad_norm 25.6329 (nan)	mem 4879MB
[2022-05-31 05:03:12 MetaFG_0] (main.py 265): INFO Train: [47/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.3010 (0.3053)	loss 1.4745 (1.3754)	grad_norm 33.6271 (nan)	mem 4879MB
[2022-05-31 05:03:15 MetaFG_0] (main.py 265): INFO Train: [47/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2935 (0.3053)	loss 1.4543 (1.3763)	grad_norm 25.7763 (nan)	mem 4879MB
[2022-05-31 05:03:18 MetaFG_0] (main.py 265): INFO Train: [47/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2948 (0.3053)	loss 1.4927 (1.3771)	grad_norm 34.2018 (nan)	mem 4879MB
[2022-05-31 05:03:21 MetaFG_0] (main.py 265): INFO Train: [47/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.3020 (0.3053)	loss 1.2178 (1.3773)	grad_norm 39.6266 (nan)	mem 4879MB
[2022-05-31 05:03:24 MetaFG_0] (main.py 265): INFO Train: [47/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.3022 (0.3053)	loss 1.6007 (1.3770)	grad_norm 17.2302 (nan)	mem 4879MB
[2022-05-31 05:03:27 MetaFG_0] (main.py 265): INFO Train: [47/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2942 (0.3053)	loss 1.3542 (1.3770)	grad_norm 18.3887 (nan)	mem 4879MB
[2022-05-31 05:03:30 MetaFG_0] (main.py 265): INFO Train: [47/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3080 (0.3053)	loss 1.3625 (1.3764)	grad_norm 26.3549 (nan)	mem 4879MB
[2022-05-31 05:03:33 MetaFG_0] (main.py 265): INFO Train: [47/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2932 (0.3053)	loss 1.3041 (1.3763)	grad_norm 26.6971 (nan)	mem 4879MB
[2022-05-31 05:03:36 MetaFG_0] (main.py 265): INFO Train: [47/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2995 (0.3053)	loss 1.3239 (1.3767)	grad_norm 36.3188 (nan)	mem 4879MB
[2022-05-31 05:03:39 MetaFG_0] (main.py 265): INFO Train: [47/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3058 (0.3053)	loss 0.8794 (1.3770)	grad_norm 22.2962 (nan)	mem 4879MB
[2022-05-31 05:03:42 MetaFG_0] (main.py 265): INFO Train: [47/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2982 (0.3053)	loss 1.2404 (1.3776)	grad_norm 31.9152 (nan)	mem 4879MB
[2022-05-31 05:03:45 MetaFG_0] (main.py 265): INFO Train: [47/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2920 (0.3053)	loss 1.6813 (1.3774)	grad_norm 46.7865 (nan)	mem 4879MB
[2022-05-31 05:03:48 MetaFG_0] (main.py 265): INFO Train: [47/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2985 (0.3053)	loss 1.4569 (1.3771)	grad_norm 29.7593 (nan)	mem 4879MB
[2022-05-31 05:03:51 MetaFG_0] (main.py 265): INFO Train: [47/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2978 (0.3053)	loss 1.3292 (1.3773)	grad_norm 41.8939 (nan)	mem 4879MB
[2022-05-31 05:03:54 MetaFG_0] (main.py 265): INFO Train: [47/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2980 (0.3053)	loss 1.5318 (1.3774)	grad_norm 42.4433 (nan)	mem 4879MB
[2022-05-31 05:03:57 MetaFG_0] (main.py 265): INFO Train: [47/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2931 (0.3053)	loss 1.5856 (1.3776)	grad_norm 35.7119 (nan)	mem 4879MB
[2022-05-31 05:04:00 MetaFG_0] (main.py 265): INFO Train: [47/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2946 (0.3053)	loss 1.6123 (1.3778)	grad_norm 34.1250 (nan)	mem 4879MB
[2022-05-31 05:04:03 MetaFG_0] (main.py 265): INFO Train: [47/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2935 (0.3053)	loss 1.4786 (1.3774)	grad_norm 23.9232 (nan)	mem 4879MB
[2022-05-31 05:04:06 MetaFG_0] (main.py 265): INFO Train: [47/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2942 (0.3053)	loss 1.5479 (1.3783)	grad_norm 21.2968 (nan)	mem 4879MB
[2022-05-31 05:04:09 MetaFG_0] (main.py 265): INFO Train: [47/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2926 (0.3053)	loss 1.4364 (1.3782)	grad_norm 29.0987 (nan)	mem 4879MB
[2022-05-31 05:04:13 MetaFG_0] (main.py 265): INFO Train: [47/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2922 (0.3053)	loss 1.0712 (1.3780)	grad_norm 15.5718 (nan)	mem 4879MB
[2022-05-31 05:04:16 MetaFG_0] (main.py 265): INFO Train: [47/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2925 (0.3053)	loss 1.2979 (1.3780)	grad_norm 33.1860 (nan)	mem 4879MB
[2022-05-31 05:04:19 MetaFG_0] (main.py 265): INFO Train: [47/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2982 (0.3052)	loss 1.5420 (1.3782)	grad_norm 23.3819 (nan)	mem 4879MB
[2022-05-31 05:04:22 MetaFG_0] (main.py 265): INFO Train: [47/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2937 (0.3052)	loss 1.4963 (1.3782)	grad_norm 33.7773 (nan)	mem 4879MB
[2022-05-31 05:04:25 MetaFG_0] (main.py 265): INFO Train: [47/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2997 (0.3052)	loss 1.4341 (1.3785)	grad_norm 30.3155 (nan)	mem 4879MB
[2022-05-31 05:04:28 MetaFG_0] (main.py 265): INFO Train: [47/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3008 (0.3052)	loss 1.0684 (1.3780)	grad_norm 21.8067 (nan)	mem 4879MB
[2022-05-31 05:04:31 MetaFG_0] (main.py 265): INFO Train: [47/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2937 (0.3052)	loss 1.5343 (1.3776)	grad_norm 18.4951 (nan)	mem 4879MB
[2022-05-31 05:04:31 MetaFG_0] (main.py 272): INFO EPOCH 47 training takes 0:07:56
[2022-05-31 05:04:31 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_47.pth saving......
[2022-05-31 05:04:32 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_47.pth saved !!!
[2022-05-31 05:04:32 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:04:33 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:04:33 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:04:34 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.789 (0.789)	Loss 0.8231 (0.8231)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 05:04:35 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.104 (0.160)	Loss 0.6198 (0.6074)	Acc@1 81.250 (87.784)	Acc@5 100.000 (98.011)	Mem 4879MB
[2022-05-31 05:04:36 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.097 (0.129)	Loss 0.5781 (0.6159)	Acc@1 84.375 (86.607)	Acc@5 96.875 (98.363)	Mem 4879MB
[2022-05-31 05:04:37 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.092 (0.118)	Loss 0.6650 (0.6210)	Acc@1 81.250 (86.492)	Acc@5 100.000 (98.387)	Mem 4879MB
[2022-05-31 05:04:38 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.113)	Loss 0.8331 (0.6406)	Acc@1 81.250 (85.671)	Acc@5 100.000 (98.323)	Mem 4879MB
[2022-05-31 05:04:39 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.103 (0.109)	Loss 0.4845 (0.6257)	Acc@1 90.625 (86.213)	Acc@5 100.000 (98.407)	Mem 4879MB
[2022-05-31 05:04:40 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.107)	Loss 0.7013 (0.6310)	Acc@1 84.375 (86.270)	Acc@5 96.875 (98.514)	Mem 4879MB
[2022-05-31 05:04:41 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.105)	Loss 0.5047 (0.6385)	Acc@1 87.500 (86.092)	Acc@5 100.000 (98.504)	Mem 4879MB
[2022-05-31 05:04:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.104)	Loss 0.6579 (0.6459)	Acc@1 90.625 (85.880)	Acc@5 100.000 (98.457)	Mem 4879MB
[2022-05-31 05:04:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.103)	Loss 0.5840 (0.6449)	Acc@1 87.500 (85.817)	Acc@5 100.000 (98.455)	Mem 4879MB
[2022-05-31 05:04:44 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.088 (0.102)	Loss 0.9408 (0.6482)	Acc@1 65.625 (85.675)	Acc@5 96.875 (98.422)	Mem 4879MB
[2022-05-31 05:04:45 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.4407 (0.6488)	Acc@1 93.750 (85.698)	Acc@5 100.000 (98.452)	Mem 4879MB
[2022-05-31 05:04:46 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 0.6784 (0.6536)	Acc@1 84.375 (85.460)	Acc@5 100.000 (98.450)	Mem 4879MB
[2022-05-31 05:04:47 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.089 (0.100)	Loss 0.7049 (0.6547)	Acc@1 87.500 (85.496)	Acc@5 96.875 (98.402)	Mem 4879MB
[2022-05-31 05:04:48 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.088 (0.099)	Loss 0.6116 (0.6490)	Acc@1 87.500 (85.660)	Acc@5 96.875 (98.471)	Mem 4879MB
[2022-05-31 05:04:48 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.099)	Loss 0.8029 (0.6471)	Acc@1 75.000 (85.741)	Acc@5 96.875 (98.489)	Mem 4879MB
[2022-05-31 05:04:49 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.099)	Loss 0.6001 (0.6424)	Acc@1 87.500 (85.908)	Acc@5 100.000 (98.544)	Mem 4879MB
[2022-05-31 05:04:50 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.088 (0.098)	Loss 0.7916 (0.6450)	Acc@1 78.125 (85.654)	Acc@5 100.000 (98.556)	Mem 4879MB
[2022-05-31 05:04:51 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.098 (0.098)	Loss 0.6783 (0.6442)	Acc@1 84.375 (85.601)	Acc@5 100.000 (98.584)	Mem 4879MB
[2022-05-31 05:04:52 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.5804 (0.6440)	Acc@1 84.375 (85.471)	Acc@5 100.000 (98.609)	Mem 4879MB
[2022-05-31 05:04:53 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.090 (0.098)	Loss 0.9986 (0.6482)	Acc@1 71.875 (85.199)	Acc@5 93.750 (98.585)	Mem 4879MB
[2022-05-31 05:04:54 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.098)	Loss 0.9308 (0.6520)	Acc@1 71.875 (85.145)	Acc@5 96.875 (98.534)	Mem 4879MB
[2022-05-31 05:04:55 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.3918 (0.6507)	Acc@1 93.750 (85.153)	Acc@5 100.000 (98.544)	Mem 4879MB
[2022-05-31 05:04:56 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.097)	Loss 0.6179 (0.6496)	Acc@1 87.500 (85.187)	Acc@5 96.875 (98.566)	Mem 4879MB
[2022-05-31 05:04:57 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.6363 (0.6455)	Acc@1 93.750 (85.438)	Acc@5 96.875 (98.561)	Mem 4879MB
[2022-05-31 05:04:58 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.097)	Loss 0.7618 (0.6463)	Acc@1 75.000 (85.396)	Acc@5 96.875 (98.531)	Mem 4879MB
[2022-05-31 05:04:59 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 0.4777 (0.6454)	Acc@1 93.750 (85.489)	Acc@5 100.000 (98.539)	Mem 4879MB
[2022-05-31 05:05:00 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.093 (0.097)	Loss 0.5821 (0.6477)	Acc@1 84.375 (85.344)	Acc@5 100.000 (98.547)	Mem 4879MB
[2022-05-31 05:05:01 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.6714 (0.6480)	Acc@1 84.375 (85.331)	Acc@5 96.875 (98.521)	Mem 4879MB
[2022-05-31 05:05:02 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 0.4161 (0.6478)	Acc@1 90.625 (85.352)	Acc@5 100.000 (98.529)	Mem 4879MB
[2022-05-31 05:05:03 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.098 (0.096)	Loss 0.7392 (0.6483)	Acc@1 81.250 (85.341)	Acc@5 96.875 (98.505)	Mem 4879MB
[2022-05-31 05:05:03 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6043 (0.6471)	Acc@1 87.500 (85.430)	Acc@5 96.875 (98.513)	Mem 4879MB
[2022-05-31 05:05:04 MetaFG_0] (main.py 330): INFO  * Acc@1 85.440 Acc@5 98.510
[2022-05-31 05:05:04 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.4%
[2022-05-31 05:05:04 MetaFG_0] (main.py 171): INFO Max accuracy: 85.55%
[2022-05-31 05:05:05 MetaFG_0] (main.py 265): INFO Train: [48/300][0/1562]	eta 0:24:28 lr 0.000006	time 0.9402 (0.9402)	loss 1.4068 (1.4068)	grad_norm 40.3791 (40.3791)	mem 4879MB
[2022-05-31 05:05:08 MetaFG_0] (main.py 265): INFO Train: [48/300][10/1562]	eta 0:09:35 lr 0.000006	time 0.3002 (0.3708)	loss 1.0540 (1.3682)	grad_norm 29.5546 (25.7562)	mem 4879MB
[2022-05-31 05:05:11 MetaFG_0] (main.py 265): INFO Train: [48/300][20/1562]	eta 0:08:42 lr 0.000006	time 0.2928 (0.3388)	loss 1.6817 (1.4127)	grad_norm 14.8123 (25.5338)	mem 4879MB
[2022-05-31 05:05:14 MetaFG_0] (main.py 265): INFO Train: [48/300][30/1562]	eta 0:08:21 lr 0.000006	time 0.2928 (0.3273)	loss 1.0850 (1.3638)	grad_norm 44.5650 (27.6493)	mem 4879MB
[2022-05-31 05:05:17 MetaFG_0] (main.py 265): INFO Train: [48/300][40/1562]	eta 0:08:09 lr 0.000006	time 0.2931 (0.3218)	loss 1.4774 (1.3681)	grad_norm 33.1584 (29.8827)	mem 4879MB
[2022-05-31 05:05:20 MetaFG_0] (main.py 265): INFO Train: [48/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2935 (0.3183)	loss 1.2520 (1.3695)	grad_norm 27.6567 (29.7146)	mem 4879MB
[2022-05-31 05:05:23 MetaFG_0] (main.py 265): INFO Train: [48/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.2979 (0.3161)	loss 1.5500 (1.3904)	grad_norm 37.0270 (29.6272)	mem 4879MB
[2022-05-31 05:05:26 MetaFG_0] (main.py 265): INFO Train: [48/300][70/1562]	eta 0:07:48 lr 0.000006	time 0.2936 (0.3143)	loss 1.2976 (1.3861)	grad_norm 24.2971 (29.5362)	mem 4879MB
[2022-05-31 05:05:29 MetaFG_0] (main.py 265): INFO Train: [48/300][80/1562]	eta 0:07:43 lr 0.000006	time 0.2982 (0.3131)	loss 1.2846 (1.3793)	grad_norm 30.4182 (29.5774)	mem 4879MB
[2022-05-31 05:05:32 MetaFG_0] (main.py 265): INFO Train: [48/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.3001 (0.3123)	loss 1.5021 (1.3727)	grad_norm 41.3208 (29.5319)	mem 4879MB
[2022-05-31 05:05:35 MetaFG_0] (main.py 265): INFO Train: [48/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2937 (0.3114)	loss 1.2146 (1.3612)	grad_norm 61.7041 (30.2793)	mem 4879MB
[2022-05-31 05:05:38 MetaFG_0] (main.py 265): INFO Train: [48/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2921 (0.3107)	loss 1.3446 (1.3716)	grad_norm 27.4684 (30.1883)	mem 4879MB
[2022-05-31 05:05:41 MetaFG_0] (main.py 265): INFO Train: [48/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2935 (0.3106)	loss 1.5468 (1.3760)	grad_norm 27.8506 (30.6196)	mem 4879MB
[2022-05-31 05:05:44 MetaFG_0] (main.py 265): INFO Train: [48/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2986 (0.3101)	loss 1.4171 (1.3780)	grad_norm 20.8330 (30.6678)	mem 4879MB
[2022-05-31 05:05:47 MetaFG_0] (main.py 265): INFO Train: [48/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2921 (0.3096)	loss 1.3562 (1.3739)	grad_norm 22.7807 (30.7653)	mem 4879MB
[2022-05-31 05:05:50 MetaFG_0] (main.py 265): INFO Train: [48/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2984 (0.3094)	loss 1.1452 (1.3783)	grad_norm 18.0078 (31.1764)	mem 4879MB
[2022-05-31 05:05:53 MetaFG_0] (main.py 265): INFO Train: [48/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3013 (0.3092)	loss 1.5238 (1.3785)	grad_norm 48.6117 (31.3003)	mem 4879MB
[2022-05-31 05:05:57 MetaFG_0] (main.py 265): INFO Train: [48/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.3001 (0.3090)	loss 1.6202 (1.3744)	grad_norm 32.8736 (31.4828)	mem 4879MB
[2022-05-31 05:06:00 MetaFG_0] (main.py 265): INFO Train: [48/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.3007 (0.3089)	loss 1.1962 (1.3746)	grad_norm 24.4139 (31.3005)	mem 4879MB
[2022-05-31 05:06:03 MetaFG_0] (main.py 265): INFO Train: [48/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2932 (0.3086)	loss 1.3522 (1.3746)	grad_norm 30.3290 (31.2268)	mem 4879MB
[2022-05-31 05:06:06 MetaFG_0] (main.py 265): INFO Train: [48/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.3011 (0.3085)	loss 1.4081 (1.3781)	grad_norm 28.8080 (31.2191)	mem 4879MB
[2022-05-31 05:06:09 MetaFG_0] (main.py 265): INFO Train: [48/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.3010 (0.3083)	loss 1.5991 (1.3760)	grad_norm 27.9594 (31.1155)	mem 4879MB
[2022-05-31 05:06:12 MetaFG_0] (main.py 265): INFO Train: [48/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2941 (0.3081)	loss 1.6221 (1.3791)	grad_norm 44.4926 (31.3367)	mem 4879MB
[2022-05-31 05:06:15 MetaFG_0] (main.py 265): INFO Train: [48/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2924 (0.3079)	loss 0.9317 (1.3786)	grad_norm 77.9655 (31.3077)	mem 4879MB
[2022-05-31 05:06:18 MetaFG_0] (main.py 265): INFO Train: [48/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.3005 (0.3079)	loss 1.6341 (1.3787)	grad_norm 29.0850 (31.0239)	mem 4879MB
[2022-05-31 05:06:21 MetaFG_0] (main.py 265): INFO Train: [48/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2980 (0.3078)	loss 1.4663 (1.3802)	grad_norm 27.6042 (30.9427)	mem 4879MB
[2022-05-31 05:06:24 MetaFG_0] (main.py 265): INFO Train: [48/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2933 (0.3077)	loss 1.0047 (1.3829)	grad_norm 25.7248 (30.7632)	mem 4879MB
[2022-05-31 05:06:27 MetaFG_0] (main.py 265): INFO Train: [48/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2992 (0.3075)	loss 1.1368 (1.3792)	grad_norm 21.8172 (30.7448)	mem 4879MB
[2022-05-31 05:06:30 MetaFG_0] (main.py 265): INFO Train: [48/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.3006 (0.3074)	loss 1.6455 (1.3820)	grad_norm 55.2859 (30.8809)	mem 4879MB
[2022-05-31 05:06:33 MetaFG_0] (main.py 265): INFO Train: [48/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2991 (0.3073)	loss 1.3991 (1.3787)	grad_norm 24.1902 (31.1438)	mem 4879MB
[2022-05-31 05:06:36 MetaFG_0] (main.py 265): INFO Train: [48/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2940 (0.3072)	loss 1.3977 (1.3796)	grad_norm 23.1212 (30.8680)	mem 4879MB
[2022-05-31 05:06:39 MetaFG_0] (main.py 265): INFO Train: [48/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2935 (0.3072)	loss 1.5379 (1.3795)	grad_norm 33.2767 (30.7434)	mem 4879MB
[2022-05-31 05:06:42 MetaFG_0] (main.py 265): INFO Train: [48/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.3005 (0.3072)	loss 1.0082 (1.3765)	grad_norm 20.9900 (30.6738)	mem 4879MB
[2022-05-31 05:06:45 MetaFG_0] (main.py 265): INFO Train: [48/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2981 (0.3071)	loss 1.4721 (1.3786)	grad_norm 25.1258 (30.6197)	mem 4879MB
[2022-05-31 05:06:48 MetaFG_0] (main.py 265): INFO Train: [48/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2989 (0.3070)	loss 1.5602 (1.3791)	grad_norm 36.9479 (30.5677)	mem 4879MB
[2022-05-31 05:06:51 MetaFG_0] (main.py 265): INFO Train: [48/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2931 (0.3069)	loss 1.3888 (1.3796)	grad_norm 25.3575 (30.5384)	mem 4879MB
[2022-05-31 05:06:54 MetaFG_0] (main.py 265): INFO Train: [48/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2928 (0.3068)	loss 1.5943 (1.3820)	grad_norm 25.2491 (30.3918)	mem 4879MB
[2022-05-31 05:06:57 MetaFG_0] (main.py 265): INFO Train: [48/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2996 (0.3067)	loss 1.3306 (1.3800)	grad_norm 38.7295 (30.4464)	mem 4879MB
[2022-05-31 05:07:01 MetaFG_0] (main.py 265): INFO Train: [48/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2932 (0.3067)	loss 1.3543 (1.3771)	grad_norm 22.3170 (30.5132)	mem 4879MB
[2022-05-31 05:07:04 MetaFG_0] (main.py 265): INFO Train: [48/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2989 (0.3067)	loss 1.0065 (1.3768)	grad_norm 89.3460 (30.6694)	mem 4879MB
[2022-05-31 05:07:07 MetaFG_0] (main.py 265): INFO Train: [48/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2983 (0.3066)	loss 1.3957 (1.3770)	grad_norm 34.4654 (30.7565)	mem 4879MB
[2022-05-31 05:07:10 MetaFG_0] (main.py 265): INFO Train: [48/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2927 (0.3066)	loss 0.9038 (1.3745)	grad_norm 29.2014 (30.7260)	mem 4879MB
[2022-05-31 05:07:13 MetaFG_0] (main.py 265): INFO Train: [48/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.3008 (0.3066)	loss 1.2494 (1.3756)	grad_norm 25.7649 (30.8179)	mem 4879MB
[2022-05-31 05:07:16 MetaFG_0] (main.py 265): INFO Train: [48/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2926 (0.3065)	loss 1.4370 (1.3775)	grad_norm 30.9694 (30.7294)	mem 4879MB
[2022-05-31 05:07:19 MetaFG_0] (main.py 265): INFO Train: [48/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2962 (0.3064)	loss 1.5997 (1.3786)	grad_norm 17.0812 (30.7188)	mem 4879MB
[2022-05-31 05:07:22 MetaFG_0] (main.py 265): INFO Train: [48/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2938 (0.3064)	loss 1.4806 (1.3810)	grad_norm 38.4140 (30.7966)	mem 4879MB
[2022-05-31 05:07:25 MetaFG_0] (main.py 265): INFO Train: [48/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2980 (0.3063)	loss 1.5126 (1.3813)	grad_norm 31.7327 (30.8221)	mem 4879MB
[2022-05-31 05:07:28 MetaFG_0] (main.py 265): INFO Train: [48/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2991 (0.3063)	loss 1.5254 (1.3813)	grad_norm 30.4963 (30.8783)	mem 4879MB
[2022-05-31 05:07:31 MetaFG_0] (main.py 265): INFO Train: [48/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2995 (0.3063)	loss 1.5379 (1.3827)	grad_norm 19.0413 (30.8458)	mem 4879MB
[2022-05-31 05:07:34 MetaFG_0] (main.py 265): INFO Train: [48/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2998 (0.3063)	loss 1.5530 (1.3836)	grad_norm 16.8670 (30.7883)	mem 4879MB
[2022-05-31 05:07:37 MetaFG_0] (main.py 265): INFO Train: [48/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2925 (0.3062)	loss 1.5180 (1.3823)	grad_norm 23.3520 (30.7717)	mem 4879MB
[2022-05-31 05:07:40 MetaFG_0] (main.py 265): INFO Train: [48/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2930 (0.3062)	loss 1.4941 (1.3822)	grad_norm 20.2183 (30.7323)	mem 4879MB
[2022-05-31 05:07:43 MetaFG_0] (main.py 265): INFO Train: [48/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2962 (0.3061)	loss 1.4224 (1.3814)	grad_norm 24.2078 (30.7387)	mem 4879MB
[2022-05-31 05:07:46 MetaFG_0] (main.py 265): INFO Train: [48/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2927 (0.3061)	loss 1.4690 (1.3807)	grad_norm 25.7238 (30.8064)	mem 4879MB
[2022-05-31 05:07:49 MetaFG_0] (main.py 265): INFO Train: [48/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2983 (0.3061)	loss 1.1110 (1.3826)	grad_norm 32.5877 (30.9067)	mem 4879MB
[2022-05-31 05:07:52 MetaFG_0] (main.py 265): INFO Train: [48/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2934 (0.3060)	loss 1.5372 (1.3847)	grad_norm 23.2578 (30.9342)	mem 4879MB
[2022-05-31 05:07:55 MetaFG_0] (main.py 265): INFO Train: [48/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2995 (0.3060)	loss 1.8308 (1.3846)	grad_norm 25.9339 (30.9067)	mem 4879MB
[2022-05-31 05:07:58 MetaFG_0] (main.py 265): INFO Train: [48/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2930 (0.3060)	loss 0.9614 (1.3832)	grad_norm 40.1922 (30.8912)	mem 4879MB
[2022-05-31 05:08:01 MetaFG_0] (main.py 265): INFO Train: [48/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.3032 (0.3060)	loss 1.5332 (1.3823)	grad_norm 20.5155 (30.8167)	mem 4879MB
[2022-05-31 05:08:05 MetaFG_0] (main.py 265): INFO Train: [48/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.3160 (0.3060)	loss 1.3526 (1.3826)	grad_norm 23.0791 (30.8286)	mem 4879MB
[2022-05-31 05:08:08 MetaFG_0] (main.py 265): INFO Train: [48/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.3002 (0.3064)	loss 1.4987 (1.3825)	grad_norm 26.8904 (30.8590)	mem 4879MB
[2022-05-31 05:08:11 MetaFG_0] (main.py 265): INFO Train: [48/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2920 (0.3064)	loss 1.3140 (1.3832)	grad_norm 39.7591 (30.8876)	mem 4879MB
[2022-05-31 05:08:14 MetaFG_0] (main.py 265): INFO Train: [48/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2984 (0.3064)	loss 1.4129 (1.3855)	grad_norm 29.0322 (30.9113)	mem 4879MB
[2022-05-31 05:08:17 MetaFG_0] (main.py 265): INFO Train: [48/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2924 (0.3063)	loss 0.9950 (1.3847)	grad_norm 23.2874 (30.8384)	mem 4879MB
[2022-05-31 05:08:20 MetaFG_0] (main.py 265): INFO Train: [48/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2936 (0.3063)	loss 1.3884 (1.3848)	grad_norm 24.8950 (30.7790)	mem 4879MB
[2022-05-31 05:08:23 MetaFG_0] (main.py 265): INFO Train: [48/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2965 (0.3063)	loss 1.4674 (1.3838)	grad_norm 25.5596 (30.6643)	mem 4879MB
[2022-05-31 05:08:26 MetaFG_0] (main.py 265): INFO Train: [48/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2926 (0.3062)	loss 1.3789 (1.3845)	grad_norm 59.5950 (30.7203)	mem 4879MB
[2022-05-31 05:08:29 MetaFG_0] (main.py 265): INFO Train: [48/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2940 (0.3062)	loss 1.3415 (1.3851)	grad_norm 28.5514 (30.6540)	mem 4879MB
[2022-05-31 05:08:32 MetaFG_0] (main.py 265): INFO Train: [48/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2927 (0.3062)	loss 1.6562 (1.3849)	grad_norm 22.5368 (30.5831)	mem 4879MB
[2022-05-31 05:08:35 MetaFG_0] (main.py 265): INFO Train: [48/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2945 (0.3061)	loss 1.2211 (1.3846)	grad_norm 21.3693 (30.5886)	mem 4879MB
[2022-05-31 05:08:38 MetaFG_0] (main.py 265): INFO Train: [48/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2934 (0.3061)	loss 1.5301 (1.3858)	grad_norm 25.0306 (30.6070)	mem 4879MB
[2022-05-31 05:08:41 MetaFG_0] (main.py 265): INFO Train: [48/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2970 (0.3061)	loss 0.9363 (1.3849)	grad_norm 51.4532 (30.6552)	mem 4879MB
[2022-05-31 05:08:44 MetaFG_0] (main.py 265): INFO Train: [48/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2920 (0.3061)	loss 1.4363 (1.3848)	grad_norm 22.9811 (30.6930)	mem 4879MB
[2022-05-31 05:08:47 MetaFG_0] (main.py 265): INFO Train: [48/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2979 (0.3061)	loss 1.0731 (1.3850)	grad_norm 16.2279 (30.6256)	mem 4879MB
[2022-05-31 05:08:51 MetaFG_0] (main.py 265): INFO Train: [48/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2928 (0.3061)	loss 1.2600 (1.3850)	grad_norm 28.7993 (30.6120)	mem 4879MB
[2022-05-31 05:08:54 MetaFG_0] (main.py 265): INFO Train: [48/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2935 (0.3061)	loss 1.2157 (1.3843)	grad_norm 25.1378 (30.5344)	mem 4879MB
[2022-05-31 05:08:57 MetaFG_0] (main.py 265): INFO Train: [48/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2942 (0.3061)	loss 1.4751 (1.3850)	grad_norm 34.7258 (30.4561)	mem 4879MB
[2022-05-31 05:09:00 MetaFG_0] (main.py 265): INFO Train: [48/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3005 (0.3060)	loss 1.6182 (1.3860)	grad_norm 34.9593 (30.5409)	mem 4879MB
[2022-05-31 05:09:03 MetaFG_0] (main.py 265): INFO Train: [48/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2940 (0.3060)	loss 1.5114 (1.3853)	grad_norm 19.6215 (30.6092)	mem 4879MB
[2022-05-31 05:09:06 MetaFG_0] (main.py 265): INFO Train: [48/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2971 (0.3060)	loss 1.6280 (1.3870)	grad_norm 53.3800 (30.6548)	mem 4879MB
[2022-05-31 05:09:09 MetaFG_0] (main.py 265): INFO Train: [48/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2995 (0.3060)	loss 0.9653 (1.3863)	grad_norm 27.6263 (30.6489)	mem 4879MB
[2022-05-31 05:09:12 MetaFG_0] (main.py 265): INFO Train: [48/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2978 (0.3060)	loss 1.1968 (1.3869)	grad_norm 20.1754 (30.6721)	mem 4879MB
[2022-05-31 05:09:15 MetaFG_0] (main.py 265): INFO Train: [48/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2926 (0.3060)	loss 1.5329 (1.3866)	grad_norm 25.3067 (30.6309)	mem 4879MB
[2022-05-31 05:09:18 MetaFG_0] (main.py 265): INFO Train: [48/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2927 (0.3059)	loss 1.0679 (1.3857)	grad_norm 29.3183 (30.5461)	mem 4879MB
[2022-05-31 05:09:21 MetaFG_0] (main.py 265): INFO Train: [48/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2926 (0.3059)	loss 1.1014 (1.3845)	grad_norm 40.3712 (30.5082)	mem 4879MB
[2022-05-31 05:09:24 MetaFG_0] (main.py 265): INFO Train: [48/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2926 (0.3059)	loss 1.5239 (1.3841)	grad_norm 18.4941 (30.4736)	mem 4879MB
[2022-05-31 05:09:27 MetaFG_0] (main.py 265): INFO Train: [48/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2934 (0.3059)	loss 1.7324 (1.3841)	grad_norm 22.5231 (30.4473)	mem 4879MB
[2022-05-31 05:09:30 MetaFG_0] (main.py 265): INFO Train: [48/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2929 (0.3059)	loss 1.3452 (1.3838)	grad_norm 31.2701 (30.4868)	mem 4879MB
[2022-05-31 05:09:33 MetaFG_0] (main.py 265): INFO Train: [48/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3003 (0.3059)	loss 1.3511 (1.3844)	grad_norm 17.3874 (30.5147)	mem 4879MB
[2022-05-31 05:09:36 MetaFG_0] (main.py 265): INFO Train: [48/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2918 (0.3059)	loss 1.4879 (1.3843)	grad_norm 26.0283 (30.5122)	mem 4879MB
[2022-05-31 05:09:39 MetaFG_0] (main.py 265): INFO Train: [48/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2929 (0.3058)	loss 1.4023 (1.3845)	grad_norm 25.0926 (30.4491)	mem 4879MB
[2022-05-31 05:09:42 MetaFG_0] (main.py 265): INFO Train: [48/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2932 (0.3058)	loss 1.0737 (1.3838)	grad_norm 61.0826 (30.5054)	mem 4879MB
[2022-05-31 05:09:45 MetaFG_0] (main.py 265): INFO Train: [48/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2951 (0.3058)	loss 1.3215 (1.3836)	grad_norm 31.5295 (30.5286)	mem 4879MB
[2022-05-31 05:09:48 MetaFG_0] (main.py 265): INFO Train: [48/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3001 (0.3058)	loss 1.4749 (1.3828)	grad_norm 28.2678 (30.5075)	mem 4879MB
[2022-05-31 05:09:51 MetaFG_0] (main.py 265): INFO Train: [48/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3002 (0.3058)	loss 1.5343 (1.3825)	grad_norm 17.8478 (30.5397)	mem 4879MB
[2022-05-31 05:09:55 MetaFG_0] (main.py 265): INFO Train: [48/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2993 (0.3058)	loss 0.7867 (1.3815)	grad_norm 68.7990 (30.5221)	mem 4879MB
[2022-05-31 05:09:58 MetaFG_0] (main.py 265): INFO Train: [48/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2927 (0.3058)	loss 1.3977 (1.3818)	grad_norm 19.1129 (30.5174)	mem 4879MB
[2022-05-31 05:10:01 MetaFG_0] (main.py 265): INFO Train: [48/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2936 (0.3058)	loss 1.2809 (1.3820)	grad_norm 40.6347 (30.5438)	mem 4879MB
[2022-05-31 05:10:04 MetaFG_0] (main.py 265): INFO Train: [48/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2934 (0.3058)	loss 1.3265 (1.3819)	grad_norm 39.6566 (30.6089)	mem 4879MB
[2022-05-31 05:10:07 MetaFG_0] (main.py 265): INFO Train: [48/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2952 (0.3058)	loss 1.0076 (1.3808)	grad_norm 54.3539 (30.6777)	mem 4879MB
[2022-05-31 05:10:10 MetaFG_0] (main.py 265): INFO Train: [48/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2926 (0.3057)	loss 1.2262 (1.3808)	grad_norm 22.1495 (30.6453)	mem 4879MB
[2022-05-31 05:10:13 MetaFG_0] (main.py 265): INFO Train: [48/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2995 (0.3058)	loss 1.2491 (1.3805)	grad_norm 26.4315 (30.6481)	mem 4879MB
[2022-05-31 05:10:16 MetaFG_0] (main.py 265): INFO Train: [48/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2936 (0.3057)	loss 1.2429 (1.3803)	grad_norm 30.8323 (30.6018)	mem 4879MB
[2022-05-31 05:10:19 MetaFG_0] (main.py 265): INFO Train: [48/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2925 (0.3057)	loss 1.0950 (1.3797)	grad_norm 47.3473 (30.6316)	mem 4879MB
[2022-05-31 05:10:22 MetaFG_0] (main.py 265): INFO Train: [48/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2924 (0.3057)	loss 0.9304 (1.3790)	grad_norm 15.6331 (30.6145)	mem 4879MB
[2022-05-31 05:10:25 MetaFG_0] (main.py 265): INFO Train: [48/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2950 (0.3057)	loss 1.4480 (1.3791)	grad_norm 23.4681 (30.6081)	mem 4879MB
[2022-05-31 05:10:28 MetaFG_0] (main.py 265): INFO Train: [48/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2946 (0.3057)	loss 1.6989 (1.3795)	grad_norm 43.9808 (30.6500)	mem 4879MB
[2022-05-31 05:10:31 MetaFG_0] (main.py 265): INFO Train: [48/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2993 (0.3057)	loss 1.2906 (1.3789)	grad_norm 36.1004 (30.6988)	mem 4879MB
[2022-05-31 05:10:34 MetaFG_0] (main.py 265): INFO Train: [48/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2935 (0.3057)	loss 1.1129 (1.3783)	grad_norm 25.2064 (30.6859)	mem 4879MB
[2022-05-31 05:10:37 MetaFG_0] (main.py 265): INFO Train: [48/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2981 (0.3057)	loss 1.3339 (1.3776)	grad_norm 29.8272 (30.6927)	mem 4879MB
[2022-05-31 05:10:40 MetaFG_0] (main.py 265): INFO Train: [48/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2938 (0.3057)	loss 1.5751 (1.3787)	grad_norm 31.2200 (inf)	mem 4879MB
[2022-05-31 05:10:43 MetaFG_0] (main.py 265): INFO Train: [48/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2942 (0.3057)	loss 1.3522 (1.3791)	grad_norm 18.9577 (inf)	mem 4879MB
[2022-05-31 05:10:46 MetaFG_0] (main.py 265): INFO Train: [48/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2982 (0.3056)	loss 1.3632 (1.3785)	grad_norm 22.8000 (inf)	mem 4879MB
[2022-05-31 05:10:49 MetaFG_0] (main.py 265): INFO Train: [48/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2937 (0.3056)	loss 1.5311 (1.3781)	grad_norm 25.8744 (inf)	mem 4879MB
[2022-05-31 05:10:52 MetaFG_0] (main.py 265): INFO Train: [48/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2930 (0.3056)	loss 1.7727 (1.3791)	grad_norm 40.7020 (inf)	mem 4879MB
[2022-05-31 05:10:55 MetaFG_0] (main.py 265): INFO Train: [48/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2987 (0.3056)	loss 1.5161 (1.3800)	grad_norm 16.3557 (inf)	mem 4879MB
[2022-05-31 05:10:59 MetaFG_0] (main.py 265): INFO Train: [48/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2987 (0.3056)	loss 1.2132 (1.3806)	grad_norm 42.1684 (inf)	mem 4879MB
[2022-05-31 05:11:02 MetaFG_0] (main.py 265): INFO Train: [48/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2955 (0.3056)	loss 1.0769 (1.3804)	grad_norm 19.9652 (inf)	mem 4879MB
[2022-05-31 05:11:05 MetaFG_0] (main.py 265): INFO Train: [48/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2939 (0.3056)	loss 0.7443 (1.3800)	grad_norm 35.5399 (inf)	mem 4879MB
[2022-05-31 05:11:08 MetaFG_0] (main.py 265): INFO Train: [48/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2953 (0.3056)	loss 1.4737 (1.3797)	grad_norm 36.9424 (inf)	mem 4879MB
[2022-05-31 05:11:11 MetaFG_0] (main.py 265): INFO Train: [48/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.3013 (0.3056)	loss 1.2874 (1.3793)	grad_norm 20.0397 (inf)	mem 4879MB
[2022-05-31 05:11:14 MetaFG_0] (main.py 265): INFO Train: [48/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2937 (0.3056)	loss 1.5323 (1.3801)	grad_norm 24.5765 (inf)	mem 4879MB
[2022-05-31 05:11:17 MetaFG_0] (main.py 265): INFO Train: [48/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2924 (0.3056)	loss 1.4403 (1.3801)	grad_norm 16.2301 (inf)	mem 4879MB
[2022-05-31 05:11:20 MetaFG_0] (main.py 265): INFO Train: [48/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2924 (0.3056)	loss 1.1455 (1.3799)	grad_norm 23.3909 (inf)	mem 4879MB
[2022-05-31 05:11:23 MetaFG_0] (main.py 265): INFO Train: [48/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2925 (0.3056)	loss 1.5401 (1.3792)	grad_norm 25.2073 (inf)	mem 4879MB
[2022-05-31 05:11:26 MetaFG_0] (main.py 265): INFO Train: [48/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2923 (0.3056)	loss 1.3641 (1.3795)	grad_norm 34.5139 (inf)	mem 4879MB
[2022-05-31 05:11:29 MetaFG_0] (main.py 265): INFO Train: [48/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2935 (0.3056)	loss 1.2219 (1.3796)	grad_norm 27.4795 (inf)	mem 4879MB
[2022-05-31 05:11:32 MetaFG_0] (main.py 265): INFO Train: [48/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2991 (0.3055)	loss 1.1085 (1.3783)	grad_norm 32.1061 (inf)	mem 4879MB
[2022-05-31 05:11:35 MetaFG_0] (main.py 265): INFO Train: [48/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2927 (0.3055)	loss 1.6799 (1.3790)	grad_norm 47.2922 (inf)	mem 4879MB
[2022-05-31 05:11:38 MetaFG_0] (main.py 265): INFO Train: [48/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2994 (0.3055)	loss 1.2318 (1.3785)	grad_norm 26.6746 (inf)	mem 4879MB
[2022-05-31 05:11:41 MetaFG_0] (main.py 265): INFO Train: [48/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2935 (0.3055)	loss 1.1445 (1.3786)	grad_norm 23.4819 (inf)	mem 4879MB
[2022-05-31 05:11:44 MetaFG_0] (main.py 265): INFO Train: [48/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2946 (0.3055)	loss 1.3278 (1.3786)	grad_norm 19.7559 (inf)	mem 4879MB
[2022-05-31 05:11:47 MetaFG_0] (main.py 265): INFO Train: [48/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2953 (0.3055)	loss 1.4098 (1.3780)	grad_norm 13.8652 (inf)	mem 4879MB
[2022-05-31 05:11:50 MetaFG_0] (main.py 265): INFO Train: [48/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2932 (0.3055)	loss 1.2122 (1.3778)	grad_norm 48.0994 (inf)	mem 4879MB
[2022-05-31 05:11:53 MetaFG_0] (main.py 265): INFO Train: [48/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2999 (0.3055)	loss 1.0085 (1.3776)	grad_norm 20.2334 (inf)	mem 4879MB
[2022-05-31 05:11:56 MetaFG_0] (main.py 265): INFO Train: [48/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2930 (0.3055)	loss 0.9705 (1.3772)	grad_norm 20.9566 (inf)	mem 4879MB
[2022-05-31 05:11:59 MetaFG_0] (main.py 265): INFO Train: [48/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2932 (0.3055)	loss 1.0904 (1.3769)	grad_norm 28.5601 (inf)	mem 4879MB
[2022-05-31 05:12:03 MetaFG_0] (main.py 265): INFO Train: [48/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2993 (0.3055)	loss 1.5133 (1.3768)	grad_norm 28.0343 (inf)	mem 4879MB
[2022-05-31 05:12:06 MetaFG_0] (main.py 265): INFO Train: [48/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3004 (0.3055)	loss 1.5995 (1.3771)	grad_norm 14.2486 (inf)	mem 4879MB
[2022-05-31 05:12:09 MetaFG_0] (main.py 265): INFO Train: [48/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2995 (0.3055)	loss 1.2171 (1.3757)	grad_norm 28.5700 (inf)	mem 4879MB
[2022-05-31 05:12:12 MetaFG_0] (main.py 265): INFO Train: [48/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2948 (0.3055)	loss 1.3137 (1.3757)	grad_norm 22.1694 (inf)	mem 4879MB
[2022-05-31 05:12:15 MetaFG_0] (main.py 265): INFO Train: [48/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2980 (0.3055)	loss 1.1513 (1.3754)	grad_norm 24.0282 (inf)	mem 4879MB
[2022-05-31 05:12:18 MetaFG_0] (main.py 265): INFO Train: [48/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3001 (0.3055)	loss 1.1838 (1.3752)	grad_norm 22.4619 (inf)	mem 4879MB
[2022-05-31 05:12:21 MetaFG_0] (main.py 265): INFO Train: [48/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2969 (0.3055)	loss 1.6884 (1.3759)	grad_norm 38.4571 (inf)	mem 4879MB
[2022-05-31 05:12:24 MetaFG_0] (main.py 265): INFO Train: [48/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2933 (0.3054)	loss 1.4237 (1.3764)	grad_norm 44.1499 (inf)	mem 4879MB
[2022-05-31 05:12:27 MetaFG_0] (main.py 265): INFO Train: [48/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.3012 (0.3055)	loss 1.8307 (1.3764)	grad_norm 32.7521 (inf)	mem 4879MB
[2022-05-31 05:12:30 MetaFG_0] (main.py 265): INFO Train: [48/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2936 (0.3054)	loss 1.2231 (1.3767)	grad_norm 45.9348 (inf)	mem 4879MB
[2022-05-31 05:12:33 MetaFG_0] (main.py 265): INFO Train: [48/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2997 (0.3054)	loss 1.7510 (1.3770)	grad_norm 39.5120 (inf)	mem 4879MB
[2022-05-31 05:12:36 MetaFG_0] (main.py 265): INFO Train: [48/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.3026 (0.3055)	loss 0.9303 (1.3766)	grad_norm 28.1824 (inf)	mem 4879MB
[2022-05-31 05:12:39 MetaFG_0] (main.py 265): INFO Train: [48/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2928 (0.3054)	loss 1.3084 (1.3765)	grad_norm 29.3517 (inf)	mem 4879MB
[2022-05-31 05:12:42 MetaFG_0] (main.py 265): INFO Train: [48/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2934 (0.3054)	loss 0.9931 (1.3760)	grad_norm 24.8795 (inf)	mem 4879MB
[2022-05-31 05:12:45 MetaFG_0] (main.py 265): INFO Train: [48/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2928 (0.3054)	loss 1.6036 (1.3762)	grad_norm 30.5167 (inf)	mem 4879MB
[2022-05-31 05:12:48 MetaFG_0] (main.py 265): INFO Train: [48/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2934 (0.3054)	loss 1.4547 (1.3760)	grad_norm 29.7438 (inf)	mem 4879MB
[2022-05-31 05:12:51 MetaFG_0] (main.py 265): INFO Train: [48/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2920 (0.3054)	loss 1.5124 (1.3757)	grad_norm 26.4611 (inf)	mem 4879MB
[2022-05-31 05:12:54 MetaFG_0] (main.py 265): INFO Train: [48/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2922 (0.3055)	loss 1.4659 (1.3758)	grad_norm 25.7014 (inf)	mem 4879MB
[2022-05-31 05:12:57 MetaFG_0] (main.py 265): INFO Train: [48/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3000 (0.3055)	loss 1.6223 (1.3752)	grad_norm 22.8725 (inf)	mem 4879MB
[2022-05-31 05:13:01 MetaFG_0] (main.py 265): INFO Train: [48/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2917 (0.3054)	loss 1.3313 (1.3758)	grad_norm 25.9207 (inf)	mem 4879MB
[2022-05-31 05:13:01 MetaFG_0] (main.py 272): INFO EPOCH 48 training takes 0:07:57
[2022-05-31 05:13:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_48.pth saving......
[2022-05-31 05:13:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_48.pth saved !!!
[2022-05-31 05:13:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:13:03 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:13:03 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:13:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.730 (0.730)	Loss 0.7179 (0.7179)	Acc@1 84.375 (84.375)	Acc@5 93.750 (93.750)	Mem 4879MB
[2022-05-31 05:13:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.157)	Loss 0.6083 (0.6884)	Acc@1 87.500 (87.500)	Acc@5 100.000 (98.580)	Mem 4879MB
[2022-05-31 05:13:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.126)	Loss 0.5757 (0.6605)	Acc@1 90.625 (88.244)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 05:13:07 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.104 (0.115)	Loss 0.6556 (0.6823)	Acc@1 84.375 (86.895)	Acc@5 100.000 (98.891)	Mem 4879MB
[2022-05-31 05:13:08 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.110)	Loss 0.7957 (0.6887)	Acc@1 84.375 (86.966)	Acc@5 96.875 (98.476)	Mem 4879MB
[2022-05-31 05:13:09 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.107)	Loss 0.8733 (0.7021)	Acc@1 78.125 (86.336)	Acc@5 100.000 (98.468)	Mem 4879MB
[2022-05-31 05:13:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 0.6364 (0.6931)	Acc@1 90.625 (86.168)	Acc@5 96.875 (98.566)	Mem 4879MB
[2022-05-31 05:13:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.098 (0.104)	Loss 0.6682 (0.6925)	Acc@1 90.625 (86.048)	Acc@5 93.750 (98.460)	Mem 4879MB
[2022-05-31 05:13:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.103)	Loss 0.6769 (0.6997)	Acc@1 84.375 (85.610)	Acc@5 100.000 (98.457)	Mem 4879MB
[2022-05-31 05:13:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.102)	Loss 0.5539 (0.6981)	Acc@1 87.500 (85.611)	Acc@5 100.000 (98.489)	Mem 4879MB
[2022-05-31 05:13:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.101)	Loss 0.5945 (0.6958)	Acc@1 87.500 (85.736)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 05:13:14 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.100 (0.100)	Loss 0.6999 (0.6912)	Acc@1 81.250 (85.811)	Acc@5 100.000 (98.564)	Mem 4879MB
[2022-05-31 05:13:15 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 0.7849 (0.6933)	Acc@1 81.250 (85.873)	Acc@5 93.750 (98.502)	Mem 4879MB
[2022-05-31 05:13:16 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.092 (0.099)	Loss 0.5530 (0.6904)	Acc@1 87.500 (85.854)	Acc@5 100.000 (98.545)	Mem 4879MB
[2022-05-31 05:13:17 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.5399 (0.6917)	Acc@1 93.750 (85.727)	Acc@5 96.875 (98.515)	Mem 4879MB
[2022-05-31 05:13:18 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.6830 (0.6903)	Acc@1 87.500 (85.658)	Acc@5 96.875 (98.531)	Mem 4879MB
[2022-05-31 05:13:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 0.6157 (0.6852)	Acc@1 81.250 (85.792)	Acc@5 100.000 (98.564)	Mem 4879MB
[2022-05-31 05:13:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.098)	Loss 0.7042 (0.6880)	Acc@1 87.500 (85.654)	Acc@5 96.875 (98.593)	Mem 4879MB
[2022-05-31 05:13:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.097)	Loss 0.4603 (0.6882)	Acc@1 93.750 (85.670)	Acc@5 100.000 (98.584)	Mem 4879MB
[2022-05-31 05:13:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.5362 (0.6897)	Acc@1 93.750 (85.635)	Acc@5 100.000 (98.544)	Mem 4879MB
[2022-05-31 05:13:23 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.089 (0.097)	Loss 0.4920 (0.6895)	Acc@1 93.750 (85.634)	Acc@5 100.000 (98.539)	Mem 4879MB
[2022-05-31 05:13:24 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.8440 (0.6887)	Acc@1 75.000 (85.545)	Acc@5 96.875 (98.578)	Mem 4879MB
[2022-05-31 05:13:25 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.5905 (0.6898)	Acc@1 90.625 (85.464)	Acc@5 100.000 (98.558)	Mem 4879MB
[2022-05-31 05:13:26 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.9606 (0.6903)	Acc@1 75.000 (85.403)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 05:13:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.6160 (0.6889)	Acc@1 81.250 (85.425)	Acc@5 100.000 (98.561)	Mem 4879MB
[2022-05-31 05:13:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.097 (0.096)	Loss 0.5853 (0.6891)	Acc@1 90.625 (85.471)	Acc@5 100.000 (98.506)	Mem 4879MB
[2022-05-31 05:13:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.096)	Loss 0.3840 (0.6863)	Acc@1 96.875 (85.584)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 05:13:29 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.9488 (0.6864)	Acc@1 87.500 (85.643)	Acc@5 96.875 (98.536)	Mem 4879MB
[2022-05-31 05:13:30 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.098 (0.096)	Loss 0.7698 (0.6870)	Acc@1 81.250 (85.665)	Acc@5 96.875 (98.543)	Mem 4879MB
[2022-05-31 05:13:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.134 (0.097)	Loss 0.5686 (0.6896)	Acc@1 90.625 (85.685)	Acc@5 100.000 (98.507)	Mem 4879MB
[2022-05-31 05:13:33 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.106 (0.098)	Loss 0.4571 (0.6899)	Acc@1 93.750 (85.662)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 05:13:34 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.098)	Loss 0.6690 (0.6907)	Acc@1 84.375 (85.621)	Acc@5 100.000 (98.533)	Mem 4879MB
[2022-05-31 05:13:34 MetaFG_0] (main.py 330): INFO  * Acc@1 85.600 Acc@5 98.530
[2022-05-31 05:13:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.6%
[2022-05-31 05:13:34 MetaFG_0] (main.py 171): INFO Max accuracy: 85.60%
[2022-05-31 05:13:35 MetaFG_0] (main.py 265): INFO Train: [49/300][0/1562]	eta 0:24:29 lr 0.000006	time 0.9410 (0.9410)	loss 1.4452 (1.4452)	grad_norm 30.9593 (30.9593)	mem 4879MB
[2022-05-31 05:13:38 MetaFG_0] (main.py 265): INFO Train: [49/300][10/1562]	eta 0:09:34 lr 0.000006	time 0.2923 (0.3702)	loss 1.3522 (1.4052)	grad_norm 30.1899 (30.0369)	mem 4879MB
[2022-05-31 05:13:41 MetaFG_0] (main.py 265): INFO Train: [49/300][20/1562]	eta 0:08:43 lr 0.000006	time 0.2923 (0.3393)	loss 1.5764 (1.3913)	grad_norm 29.0007 (28.2855)	mem 4879MB
[2022-05-31 05:13:44 MetaFG_0] (main.py 265): INFO Train: [49/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2930 (0.3277)	loss 0.8956 (1.3666)	grad_norm 28.5353 (29.1689)	mem 4879MB
[2022-05-31 05:13:47 MetaFG_0] (main.py 265): INFO Train: [49/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2923 (0.3223)	loss 1.6876 (1.3833)	grad_norm 42.9798 (29.2980)	mem 4879MB
[2022-05-31 05:13:50 MetaFG_0] (main.py 265): INFO Train: [49/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2920 (0.3188)	loss 1.5494 (1.3863)	grad_norm 39.1872 (29.1164)	mem 4879MB
[2022-05-31 05:13:53 MetaFG_0] (main.py 265): INFO Train: [49/300][60/1562]	eta 0:07:55 lr 0.000006	time 0.2941 (0.3168)	loss 1.5544 (1.4044)	grad_norm 24.4564 (30.8729)	mem 4879MB
[2022-05-31 05:13:56 MetaFG_0] (main.py 265): INFO Train: [49/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2933 (0.3151)	loss 1.6225 (1.3969)	grad_norm 26.2443 (30.9868)	mem 4879MB
[2022-05-31 05:13:59 MetaFG_0] (main.py 265): INFO Train: [49/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2997 (0.3140)	loss 1.3404 (1.3998)	grad_norm 21.8573 (31.2137)	mem 4879MB
[2022-05-31 05:14:02 MetaFG_0] (main.py 265): INFO Train: [49/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.2964 (0.3130)	loss 1.5155 (1.3935)	grad_norm 25.1573 (31.0113)	mem 4879MB
[2022-05-31 05:14:06 MetaFG_0] (main.py 265): INFO Train: [49/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2989 (0.3121)	loss 1.1527 (1.4023)	grad_norm 24.1898 (30.9419)	mem 4879MB
[2022-05-31 05:14:09 MetaFG_0] (main.py 265): INFO Train: [49/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2981 (0.3116)	loss 1.3697 (1.3881)	grad_norm 18.8249 (30.6664)	mem 4879MB
[2022-05-31 05:14:12 MetaFG_0] (main.py 265): INFO Train: [49/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2988 (0.3110)	loss 1.2489 (1.3832)	grad_norm 22.5288 (30.4335)	mem 4879MB
[2022-05-31 05:14:15 MetaFG_0] (main.py 265): INFO Train: [49/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2996 (0.3106)	loss 1.1189 (1.3878)	grad_norm 19.0338 (30.4875)	mem 4879MB
[2022-05-31 05:14:18 MetaFG_0] (main.py 265): INFO Train: [49/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2938 (0.3102)	loss 1.5575 (1.3903)	grad_norm 19.1945 (30.1752)	mem 4879MB
[2022-05-31 05:14:21 MetaFG_0] (main.py 265): INFO Train: [49/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2942 (0.3097)	loss 1.4795 (1.3946)	grad_norm 37.2880 (30.2585)	mem 4879MB
[2022-05-31 05:14:24 MetaFG_0] (main.py 265): INFO Train: [49/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2974 (0.3094)	loss 1.4459 (1.3962)	grad_norm 14.8144 (30.0961)	mem 4879MB
[2022-05-31 05:14:27 MetaFG_0] (main.py 265): INFO Train: [49/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2924 (0.3092)	loss 1.0894 (1.3958)	grad_norm 19.5750 (30.1073)	mem 4879MB
[2022-05-31 05:14:30 MetaFG_0] (main.py 265): INFO Train: [49/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.3038 (0.3089)	loss 1.6333 (1.3930)	grad_norm 21.0752 (29.8122)	mem 4879MB
[2022-05-31 05:14:33 MetaFG_0] (main.py 265): INFO Train: [49/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2923 (0.3086)	loss 1.2898 (1.3934)	grad_norm 28.6700 (29.7031)	mem 4879MB
[2022-05-31 05:14:36 MetaFG_0] (main.py 265): INFO Train: [49/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2928 (0.3083)	loss 1.0375 (1.3898)	grad_norm 30.1229 (29.8519)	mem 4879MB
[2022-05-31 05:14:39 MetaFG_0] (main.py 265): INFO Train: [49/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2925 (0.3081)	loss 1.6089 (1.3882)	grad_norm 26.2976 (29.6050)	mem 4879MB
[2022-05-31 05:14:42 MetaFG_0] (main.py 265): INFO Train: [49/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2982 (0.3080)	loss 1.0064 (1.3878)	grad_norm 29.8976 (29.5742)	mem 4879MB
[2022-05-31 05:14:45 MetaFG_0] (main.py 265): INFO Train: [49/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2991 (0.3078)	loss 1.4022 (1.3871)	grad_norm 37.2626 (29.6036)	mem 4879MB
[2022-05-31 05:14:48 MetaFG_0] (main.py 265): INFO Train: [49/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2932 (0.3077)	loss 1.0127 (1.3851)	grad_norm 33.3707 (29.5155)	mem 4879MB
[2022-05-31 05:14:51 MetaFG_0] (main.py 265): INFO Train: [49/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2933 (0.3076)	loss 1.3480 (1.3873)	grad_norm 35.0700 (29.6083)	mem 4879MB
[2022-05-31 05:14:54 MetaFG_0] (main.py 265): INFO Train: [49/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2935 (0.3076)	loss 1.3873 (1.3880)	grad_norm 21.2709 (29.4476)	mem 4879MB
[2022-05-31 05:14:57 MetaFG_0] (main.py 265): INFO Train: [49/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2989 (0.3075)	loss 1.6791 (1.3849)	grad_norm 24.5841 (29.6218)	mem 4879MB
[2022-05-31 05:15:00 MetaFG_0] (main.py 265): INFO Train: [49/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2920 (0.3074)	loss 1.0361 (1.3859)	grad_norm 48.2425 (29.6054)	mem 4879MB
[2022-05-31 05:15:03 MetaFG_0] (main.py 265): INFO Train: [49/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2995 (0.3074)	loss 1.3653 (1.3902)	grad_norm 18.5143 (29.4506)	mem 4879MB
[2022-05-31 05:15:06 MetaFG_0] (main.py 265): INFO Train: [49/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2987 (0.3073)	loss 1.1967 (1.3902)	grad_norm 23.2196 (29.4224)	mem 4879MB
[2022-05-31 05:15:10 MetaFG_0] (main.py 265): INFO Train: [49/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2935 (0.3073)	loss 1.3758 (1.3926)	grad_norm 15.4463 (29.3423)	mem 4879MB
[2022-05-31 05:15:13 MetaFG_0] (main.py 265): INFO Train: [49/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2987 (0.3072)	loss 1.3500 (1.3910)	grad_norm 27.1187 (29.2972)	mem 4879MB
[2022-05-31 05:15:16 MetaFG_0] (main.py 265): INFO Train: [49/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2981 (0.3072)	loss 1.8164 (1.3938)	grad_norm 28.4066 (29.3806)	mem 4879MB
[2022-05-31 05:15:19 MetaFG_0] (main.py 265): INFO Train: [49/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2936 (0.3071)	loss 1.5893 (1.3955)	grad_norm 17.8964 (29.2065)	mem 4879MB
[2022-05-31 05:15:22 MetaFG_0] (main.py 265): INFO Train: [49/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2931 (0.3070)	loss 1.5034 (1.3939)	grad_norm 35.8769 (29.1459)	mem 4879MB
[2022-05-31 05:15:25 MetaFG_0] (main.py 265): INFO Train: [49/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2976 (0.3069)	loss 1.3274 (1.3928)	grad_norm 28.8169 (29.2118)	mem 4879MB
[2022-05-31 05:15:28 MetaFG_0] (main.py 265): INFO Train: [49/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2921 (0.3068)	loss 1.3442 (1.3922)	grad_norm 38.2345 (29.2711)	mem 4879MB
[2022-05-31 05:15:31 MetaFG_0] (main.py 265): INFO Train: [49/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2990 (0.3068)	loss 1.4440 (1.3915)	grad_norm 33.9904 (29.3524)	mem 4879MB
[2022-05-31 05:15:34 MetaFG_0] (main.py 265): INFO Train: [49/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.3000 (0.3068)	loss 1.1011 (1.3922)	grad_norm 59.3120 (29.3663)	mem 4879MB
[2022-05-31 05:15:37 MetaFG_0] (main.py 265): INFO Train: [49/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3031 (0.3067)	loss 1.0546 (1.3914)	grad_norm 28.1479 (29.3035)	mem 4879MB
[2022-05-31 05:15:40 MetaFG_0] (main.py 265): INFO Train: [49/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2998 (0.3067)	loss 1.3194 (1.3913)	grad_norm 25.5234 (29.2787)	mem 4879MB
[2022-05-31 05:15:43 MetaFG_0] (main.py 265): INFO Train: [49/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2984 (0.3066)	loss 1.0019 (1.3909)	grad_norm 46.8085 (29.4448)	mem 4879MB
[2022-05-31 05:15:46 MetaFG_0] (main.py 265): INFO Train: [49/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2925 (0.3066)	loss 1.2959 (1.3902)	grad_norm 26.9138 (29.4385)	mem 4879MB
[2022-05-31 05:15:49 MetaFG_0] (main.py 265): INFO Train: [49/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2947 (0.3065)	loss 0.9679 (1.3875)	grad_norm 21.2070 (29.4744)	mem 4879MB
[2022-05-31 05:15:52 MetaFG_0] (main.py 265): INFO Train: [49/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2918 (0.3065)	loss 1.0821 (1.3857)	grad_norm 28.7432 (29.4361)	mem 4879MB
[2022-05-31 05:15:55 MetaFG_0] (main.py 265): INFO Train: [49/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2922 (0.3065)	loss 1.3007 (1.3851)	grad_norm 23.7790 (29.4028)	mem 4879MB
[2022-05-31 05:15:58 MetaFG_0] (main.py 265): INFO Train: [49/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2922 (0.3064)	loss 1.4952 (1.3874)	grad_norm 27.0333 (29.4371)	mem 4879MB
[2022-05-31 05:16:01 MetaFG_0] (main.py 265): INFO Train: [49/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.3025 (0.3064)	loss 0.9907 (1.3854)	grad_norm 25.2144 (29.5285)	mem 4879MB
[2022-05-31 05:16:04 MetaFG_0] (main.py 265): INFO Train: [49/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2937 (0.3064)	loss 1.4627 (1.3855)	grad_norm 30.4547 (29.7530)	mem 4879MB
[2022-05-31 05:16:07 MetaFG_0] (main.py 265): INFO Train: [49/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2963 (0.3063)	loss 1.6124 (1.3845)	grad_norm 26.0190 (29.7304)	mem 4879MB
[2022-05-31 05:16:10 MetaFG_0] (main.py 265): INFO Train: [49/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2923 (0.3062)	loss 1.6773 (1.3845)	grad_norm 25.4533 (29.7110)	mem 4879MB
[2022-05-31 05:16:14 MetaFG_0] (main.py 265): INFO Train: [49/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.3004 (0.3062)	loss 1.3340 (1.3818)	grad_norm 40.1968 (29.7525)	mem 4879MB
[2022-05-31 05:16:17 MetaFG_0] (main.py 265): INFO Train: [49/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2930 (0.3062)	loss 1.5624 (1.3834)	grad_norm 26.7882 (29.7507)	mem 4879MB
[2022-05-31 05:16:20 MetaFG_0] (main.py 265): INFO Train: [49/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2933 (0.3062)	loss 1.5840 (1.3831)	grad_norm 30.9213 (29.7085)	mem 4879MB
[2022-05-31 05:16:23 MetaFG_0] (main.py 265): INFO Train: [49/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2928 (0.3061)	loss 1.2117 (1.3843)	grad_norm 35.3523 (29.7173)	mem 4879MB
[2022-05-31 05:16:26 MetaFG_0] (main.py 265): INFO Train: [49/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2938 (0.3061)	loss 1.6346 (1.3846)	grad_norm 30.2623 (29.6989)	mem 4879MB
[2022-05-31 05:16:29 MetaFG_0] (main.py 265): INFO Train: [49/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2934 (0.3060)	loss 1.4858 (1.3843)	grad_norm 22.9037 (29.7078)	mem 4879MB
[2022-05-31 05:16:32 MetaFG_0] (main.py 265): INFO Train: [49/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2997 (0.3060)	loss 1.4697 (1.3844)	grad_norm 38.2646 (29.7277)	mem 4879MB
[2022-05-31 05:16:35 MetaFG_0] (main.py 265): INFO Train: [49/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2989 (0.3060)	loss 1.3290 (1.3838)	grad_norm 23.1979 (29.6383)	mem 4879MB
[2022-05-31 05:16:38 MetaFG_0] (main.py 265): INFO Train: [49/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2933 (0.3060)	loss 1.5937 (1.3823)	grad_norm 29.8661 (29.7530)	mem 4879MB
[2022-05-31 05:16:41 MetaFG_0] (main.py 265): INFO Train: [49/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2978 (0.3060)	loss 1.3077 (1.3844)	grad_norm 29.1317 (29.9523)	mem 4879MB
[2022-05-31 05:16:44 MetaFG_0] (main.py 265): INFO Train: [49/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2931 (0.3060)	loss 1.1612 (1.3842)	grad_norm 24.9507 (29.8714)	mem 4879MB
[2022-05-31 05:16:47 MetaFG_0] (main.py 265): INFO Train: [49/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2927 (0.3060)	loss 1.4862 (1.3853)	grad_norm 35.6267 (29.8090)	mem 4879MB
[2022-05-31 05:16:50 MetaFG_0] (main.py 265): INFO Train: [49/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2932 (0.3059)	loss 1.5359 (1.3868)	grad_norm 32.4192 (29.8099)	mem 4879MB
[2022-05-31 05:16:53 MetaFG_0] (main.py 265): INFO Train: [49/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2995 (0.3059)	loss 1.4097 (1.3871)	grad_norm 27.8947 (29.7573)	mem 4879MB
[2022-05-31 05:16:56 MetaFG_0] (main.py 265): INFO Train: [49/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2920 (0.3059)	loss 1.3869 (1.3868)	grad_norm 29.9211 (29.7380)	mem 4879MB
[2022-05-31 05:16:59 MetaFG_0] (main.py 265): INFO Train: [49/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2932 (0.3058)	loss 1.1403 (1.3880)	grad_norm 33.8049 (29.8411)	mem 4879MB
[2022-05-31 05:17:02 MetaFG_0] (main.py 265): INFO Train: [49/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.3000 (0.3058)	loss 0.9828 (1.3886)	grad_norm 34.6486 (29.8643)	mem 4879MB
[2022-05-31 05:17:05 MetaFG_0] (main.py 265): INFO Train: [49/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2984 (0.3058)	loss 1.3670 (1.3877)	grad_norm 19.8989 (29.8415)	mem 4879MB
[2022-05-31 05:17:08 MetaFG_0] (main.py 265): INFO Train: [49/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.3005 (0.3058)	loss 1.7798 (1.3875)	grad_norm 26.3879 (29.8257)	mem 4879MB
[2022-05-31 05:17:11 MetaFG_0] (main.py 265): INFO Train: [49/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2929 (0.3057)	loss 1.5330 (1.3888)	grad_norm 38.4657 (29.8559)	mem 4879MB
[2022-05-31 05:17:14 MetaFG_0] (main.py 265): INFO Train: [49/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2988 (0.3058)	loss 1.0577 (1.3872)	grad_norm 38.8098 (29.8786)	mem 4879MB
[2022-05-31 05:17:18 MetaFG_0] (main.py 265): INFO Train: [49/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2922 (0.3058)	loss 1.3880 (1.3879)	grad_norm 35.4870 (29.8942)	mem 4879MB
[2022-05-31 05:17:21 MetaFG_0] (main.py 265): INFO Train: [49/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2998 (0.3057)	loss 1.4966 (1.3880)	grad_norm 29.7152 (29.8954)	mem 4879MB
[2022-05-31 05:17:24 MetaFG_0] (main.py 265): INFO Train: [49/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2923 (0.3057)	loss 1.0012 (1.3863)	grad_norm 25.4827 (29.8559)	mem 4879MB
[2022-05-31 05:17:27 MetaFG_0] (main.py 265): INFO Train: [49/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2925 (0.3057)	loss 1.3988 (1.3868)	grad_norm 21.7560 (29.8591)	mem 4879MB
[2022-05-31 05:17:30 MetaFG_0] (main.py 265): INFO Train: [49/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2999 (0.3057)	loss 1.4140 (1.3849)	grad_norm 33.8968 (29.8920)	mem 4879MB
[2022-05-31 05:17:33 MetaFG_0] (main.py 265): INFO Train: [49/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2986 (0.3057)	loss 1.4350 (1.3854)	grad_norm 19.5465 (29.9208)	mem 4879MB
[2022-05-31 05:17:36 MetaFG_0] (main.py 265): INFO Train: [49/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2997 (0.3057)	loss 1.0614 (1.3857)	grad_norm 18.7207 (29.9489)	mem 4879MB
[2022-05-31 05:17:39 MetaFG_0] (main.py 265): INFO Train: [49/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2936 (0.3057)	loss 1.4406 (1.3860)	grad_norm 52.0240 (30.0466)	mem 4879MB
[2022-05-31 05:17:42 MetaFG_0] (main.py 265): INFO Train: [49/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2935 (0.3056)	loss 1.6458 (1.3867)	grad_norm 35.9597 (30.0158)	mem 4879MB
[2022-05-31 05:17:45 MetaFG_0] (main.py 265): INFO Train: [49/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2946 (0.3056)	loss 1.3800 (1.3877)	grad_norm 22.8192 (30.0254)	mem 4879MB
[2022-05-31 05:17:48 MetaFG_0] (main.py 265): INFO Train: [49/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2985 (0.3056)	loss 1.2612 (1.3860)	grad_norm 24.2839 (30.0016)	mem 4879MB
[2022-05-31 05:17:51 MetaFG_0] (main.py 265): INFO Train: [49/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2930 (0.3056)	loss 1.3976 (1.3864)	grad_norm 48.5723 (inf)	mem 4879MB
[2022-05-31 05:17:54 MetaFG_0] (main.py 265): INFO Train: [49/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2927 (0.3056)	loss 1.2270 (1.3871)	grad_norm 21.0902 (inf)	mem 4879MB
[2022-05-31 05:17:57 MetaFG_0] (main.py 265): INFO Train: [49/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2980 (0.3056)	loss 1.4938 (1.3880)	grad_norm 19.0160 (inf)	mem 4879MB
[2022-05-31 05:18:00 MetaFG_0] (main.py 265): INFO Train: [49/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2922 (0.3056)	loss 1.5487 (1.3886)	grad_norm 32.2974 (inf)	mem 4879MB
[2022-05-31 05:18:03 MetaFG_0] (main.py 265): INFO Train: [49/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2989 (0.3056)	loss 1.5804 (1.3885)	grad_norm 19.6444 (inf)	mem 4879MB
[2022-05-31 05:18:06 MetaFG_0] (main.py 265): INFO Train: [49/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2925 (0.3055)	loss 1.0360 (1.3886)	grad_norm 34.8090 (inf)	mem 4879MB
[2022-05-31 05:18:09 MetaFG_0] (main.py 265): INFO Train: [49/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2923 (0.3055)	loss 0.9804 (1.3871)	grad_norm 30.8636 (inf)	mem 4879MB
[2022-05-31 05:18:12 MetaFG_0] (main.py 265): INFO Train: [49/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2930 (0.3055)	loss 1.5823 (1.3863)	grad_norm 22.5221 (inf)	mem 4879MB
[2022-05-31 05:18:15 MetaFG_0] (main.py 265): INFO Train: [49/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2922 (0.3055)	loss 1.1660 (1.3855)	grad_norm 45.1659 (inf)	mem 4879MB
[2022-05-31 05:18:18 MetaFG_0] (main.py 265): INFO Train: [49/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2933 (0.3055)	loss 1.3836 (1.3855)	grad_norm 20.9485 (inf)	mem 4879MB
[2022-05-31 05:18:21 MetaFG_0] (main.py 265): INFO Train: [49/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2936 (0.3055)	loss 1.5855 (1.3862)	grad_norm 28.6518 (inf)	mem 4879MB
[2022-05-31 05:18:25 MetaFG_0] (main.py 265): INFO Train: [49/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2929 (0.3055)	loss 1.3924 (1.3860)	grad_norm 27.4822 (inf)	mem 4879MB
[2022-05-31 05:18:28 MetaFG_0] (main.py 265): INFO Train: [49/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2947 (0.3055)	loss 1.7842 (1.3856)	grad_norm 43.9437 (inf)	mem 4879MB
[2022-05-31 05:18:31 MetaFG_0] (main.py 265): INFO Train: [49/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.3109 (0.3055)	loss 1.5889 (1.3858)	grad_norm 12.2562 (inf)	mem 4879MB
[2022-05-31 05:18:34 MetaFG_0] (main.py 265): INFO Train: [49/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3057 (0.3055)	loss 1.0444 (1.3845)	grad_norm 29.8778 (inf)	mem 4879MB
[2022-05-31 05:18:37 MetaFG_0] (main.py 265): INFO Train: [49/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2921 (0.3058)	loss 1.2689 (1.3840)	grad_norm 25.2115 (inf)	mem 4879MB
[2022-05-31 05:18:40 MetaFG_0] (main.py 265): INFO Train: [49/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2946 (0.3057)	loss 1.3677 (1.3824)	grad_norm 33.9510 (inf)	mem 4879MB
[2022-05-31 05:18:43 MetaFG_0] (main.py 265): INFO Train: [49/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2918 (0.3057)	loss 1.4230 (1.3824)	grad_norm 37.2756 (inf)	mem 4879MB
[2022-05-31 05:18:46 MetaFG_0] (main.py 265): INFO Train: [49/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2950 (0.3057)	loss 1.7341 (1.3821)	grad_norm 40.5116 (inf)	mem 4879MB
[2022-05-31 05:18:49 MetaFG_0] (main.py 265): INFO Train: [49/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3001 (0.3057)	loss 1.4856 (1.3828)	grad_norm 27.8756 (inf)	mem 4879MB
[2022-05-31 05:18:52 MetaFG_0] (main.py 265): INFO Train: [49/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2924 (0.3057)	loss 1.4584 (1.3822)	grad_norm 26.2748 (inf)	mem 4879MB
[2022-05-31 05:18:55 MetaFG_0] (main.py 265): INFO Train: [49/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.3006 (0.3057)	loss 1.5184 (1.3825)	grad_norm 42.8495 (inf)	mem 4879MB
[2022-05-31 05:18:58 MetaFG_0] (main.py 265): INFO Train: [49/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2951 (0.3057)	loss 0.8072 (1.3812)	grad_norm 35.1701 (inf)	mem 4879MB
[2022-05-31 05:19:01 MetaFG_0] (main.py 265): INFO Train: [49/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2922 (0.3056)	loss 1.6274 (1.3822)	grad_norm 22.4477 (inf)	mem 4879MB
[2022-05-31 05:19:04 MetaFG_0] (main.py 265): INFO Train: [49/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2983 (0.3056)	loss 1.0317 (1.3817)	grad_norm 51.5842 (inf)	mem 4879MB
[2022-05-31 05:19:07 MetaFG_0] (main.py 265): INFO Train: [49/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2932 (0.3056)	loss 1.4092 (1.3816)	grad_norm 21.7783 (inf)	mem 4879MB
[2022-05-31 05:19:10 MetaFG_0] (main.py 265): INFO Train: [49/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2938 (0.3056)	loss 1.4283 (1.3815)	grad_norm 26.1965 (inf)	mem 4879MB
[2022-05-31 05:19:14 MetaFG_0] (main.py 265): INFO Train: [49/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2928 (0.3056)	loss 0.8910 (1.3803)	grad_norm 39.7826 (inf)	mem 4879MB
[2022-05-31 05:19:17 MetaFG_0] (main.py 265): INFO Train: [49/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2948 (0.3056)	loss 1.4483 (1.3797)	grad_norm 26.4527 (inf)	mem 4879MB
[2022-05-31 05:19:20 MetaFG_0] (main.py 265): INFO Train: [49/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2930 (0.3056)	loss 1.2073 (1.3793)	grad_norm 44.8827 (inf)	mem 4879MB
[2022-05-31 05:19:23 MetaFG_0] (main.py 265): INFO Train: [49/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.3054 (0.3056)	loss 1.4150 (1.3799)	grad_norm 13.5785 (inf)	mem 4879MB
[2022-05-31 05:19:26 MetaFG_0] (main.py 265): INFO Train: [49/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.3014 (0.3056)	loss 1.0873 (1.3794)	grad_norm 35.1772 (inf)	mem 4879MB
[2022-05-31 05:19:29 MetaFG_0] (main.py 265): INFO Train: [49/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2982 (0.3056)	loss 1.1080 (1.3777)	grad_norm 28.1462 (inf)	mem 4879MB
[2022-05-31 05:19:32 MetaFG_0] (main.py 265): INFO Train: [49/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.3007 (0.3056)	loss 1.1864 (1.3775)	grad_norm 32.2700 (inf)	mem 4879MB
[2022-05-31 05:19:35 MetaFG_0] (main.py 265): INFO Train: [49/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2923 (0.3056)	loss 1.5016 (1.3774)	grad_norm 19.3983 (inf)	mem 4879MB
[2022-05-31 05:19:38 MetaFG_0] (main.py 265): INFO Train: [49/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2982 (0.3056)	loss 1.3112 (1.3768)	grad_norm 26.0564 (inf)	mem 4879MB
[2022-05-31 05:19:41 MetaFG_0] (main.py 265): INFO Train: [49/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2999 (0.3056)	loss 1.4881 (1.3760)	grad_norm 24.6185 (inf)	mem 4879MB
[2022-05-31 05:19:44 MetaFG_0] (main.py 265): INFO Train: [49/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2981 (0.3056)	loss 0.9376 (1.3757)	grad_norm 24.4053 (inf)	mem 4879MB
[2022-05-31 05:19:47 MetaFG_0] (main.py 265): INFO Train: [49/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2998 (0.3056)	loss 1.5943 (1.3757)	grad_norm 30.2321 (inf)	mem 4879MB
[2022-05-31 05:19:50 MetaFG_0] (main.py 265): INFO Train: [49/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2995 (0.3056)	loss 1.0534 (1.3766)	grad_norm 24.7959 (inf)	mem 4879MB
[2022-05-31 05:19:53 MetaFG_0] (main.py 265): INFO Train: [49/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2976 (0.3056)	loss 0.9752 (1.3776)	grad_norm 38.7744 (inf)	mem 4879MB
[2022-05-31 05:19:56 MetaFG_0] (main.py 265): INFO Train: [49/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2925 (0.3055)	loss 1.5530 (1.3768)	grad_norm 32.6091 (inf)	mem 4879MB
[2022-05-31 05:19:59 MetaFG_0] (main.py 265): INFO Train: [49/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2941 (0.3055)	loss 1.5067 (1.3777)	grad_norm 27.1641 (inf)	mem 4879MB
[2022-05-31 05:20:02 MetaFG_0] (main.py 265): INFO Train: [49/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2923 (0.3055)	loss 1.2854 (1.3779)	grad_norm 42.2968 (inf)	mem 4879MB
[2022-05-31 05:20:05 MetaFG_0] (main.py 265): INFO Train: [49/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2922 (0.3055)	loss 1.0138 (1.3772)	grad_norm 19.6699 (inf)	mem 4879MB
[2022-05-31 05:20:08 MetaFG_0] (main.py 265): INFO Train: [49/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2936 (0.3055)	loss 1.6519 (1.3766)	grad_norm 33.8851 (inf)	mem 4879MB
[2022-05-31 05:20:11 MetaFG_0] (main.py 265): INFO Train: [49/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2965 (0.3055)	loss 0.8568 (1.3762)	grad_norm 25.0834 (inf)	mem 4879MB
[2022-05-31 05:20:15 MetaFG_0] (main.py 265): INFO Train: [49/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2933 (0.3055)	loss 1.6010 (1.3760)	grad_norm 25.8150 (inf)	mem 4879MB
[2022-05-31 05:20:18 MetaFG_0] (main.py 265): INFO Train: [49/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2941 (0.3055)	loss 1.4304 (1.3767)	grad_norm 31.8600 (inf)	mem 4879MB
[2022-05-31 05:20:21 MetaFG_0] (main.py 265): INFO Train: [49/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2955 (0.3055)	loss 1.5687 (1.3772)	grad_norm 31.1882 (inf)	mem 4879MB
[2022-05-31 05:20:24 MetaFG_0] (main.py 265): INFO Train: [49/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2938 (0.3055)	loss 1.1624 (1.3768)	grad_norm 55.4606 (inf)	mem 4879MB
[2022-05-31 05:20:27 MetaFG_0] (main.py 265): INFO Train: [49/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2993 (0.3055)	loss 1.3304 (1.3763)	grad_norm 20.9621 (inf)	mem 4879MB
[2022-05-31 05:20:30 MetaFG_0] (main.py 265): INFO Train: [49/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3019 (0.3055)	loss 0.9748 (1.3758)	grad_norm 16.3385 (inf)	mem 4879MB
[2022-05-31 05:20:33 MetaFG_0] (main.py 265): INFO Train: [49/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2937 (0.3055)	loss 0.9952 (1.3755)	grad_norm 22.9016 (inf)	mem 4879MB
[2022-05-31 05:20:36 MetaFG_0] (main.py 265): INFO Train: [49/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2935 (0.3055)	loss 1.5189 (1.3761)	grad_norm 33.6501 (inf)	mem 4879MB
[2022-05-31 05:20:39 MetaFG_0] (main.py 265): INFO Train: [49/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2929 (0.3055)	loss 1.5284 (1.3769)	grad_norm 27.6432 (inf)	mem 4879MB
[2022-05-31 05:20:42 MetaFG_0] (main.py 265): INFO Train: [49/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2991 (0.3055)	loss 1.5351 (1.3760)	grad_norm 39.4246 (inf)	mem 4879MB
[2022-05-31 05:20:45 MetaFG_0] (main.py 265): INFO Train: [49/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2939 (0.3054)	loss 1.0121 (1.3766)	grad_norm 60.3027 (inf)	mem 4879MB
[2022-05-31 05:20:48 MetaFG_0] (main.py 265): INFO Train: [49/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2916 (0.3054)	loss 1.5285 (1.3769)	grad_norm 24.6097 (inf)	mem 4879MB
[2022-05-31 05:20:51 MetaFG_0] (main.py 265): INFO Train: [49/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2989 (0.3054)	loss 1.3802 (1.3761)	grad_norm 26.3400 (inf)	mem 4879MB
[2022-05-31 05:20:54 MetaFG_0] (main.py 265): INFO Train: [49/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2925 (0.3054)	loss 1.5058 (1.3768)	grad_norm 27.6369 (inf)	mem 4879MB
[2022-05-31 05:20:57 MetaFG_0] (main.py 265): INFO Train: [49/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2935 (0.3054)	loss 1.3116 (1.3771)	grad_norm 50.5026 (inf)	mem 4879MB
[2022-05-31 05:21:00 MetaFG_0] (main.py 265): INFO Train: [49/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2980 (0.3054)	loss 1.4519 (1.3766)	grad_norm 16.4235 (inf)	mem 4879MB
[2022-05-31 05:21:03 MetaFG_0] (main.py 265): INFO Train: [49/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2951 (0.3054)	loss 1.2516 (1.3756)	grad_norm 35.7829 (inf)	mem 4879MB
[2022-05-31 05:21:06 MetaFG_0] (main.py 265): INFO Train: [49/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2928 (0.3054)	loss 1.3737 (1.3755)	grad_norm 24.9737 (inf)	mem 4879MB
[2022-05-31 05:21:09 MetaFG_0] (main.py 265): INFO Train: [49/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2924 (0.3054)	loss 1.1380 (1.3748)	grad_norm 48.8991 (inf)	mem 4879MB
[2022-05-31 05:21:12 MetaFG_0] (main.py 265): INFO Train: [49/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2927 (0.3054)	loss 1.2316 (1.3755)	grad_norm 47.6164 (inf)	mem 4879MB
[2022-05-31 05:21:15 MetaFG_0] (main.py 265): INFO Train: [49/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2943 (0.3054)	loss 1.5456 (1.3750)	grad_norm 42.6964 (inf)	mem 4879MB
[2022-05-31 05:21:19 MetaFG_0] (main.py 265): INFO Train: [49/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2922 (0.3054)	loss 1.5801 (1.3755)	grad_norm 36.2180 (inf)	mem 4879MB
[2022-05-31 05:21:22 MetaFG_0] (main.py 265): INFO Train: [49/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2926 (0.3054)	loss 1.4610 (1.3758)	grad_norm 31.1382 (inf)	mem 4879MB
[2022-05-31 05:21:25 MetaFG_0] (main.py 265): INFO Train: [49/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2926 (0.3054)	loss 1.4943 (1.3750)	grad_norm 20.0371 (inf)	mem 4879MB
[2022-05-31 05:21:28 MetaFG_0] (main.py 265): INFO Train: [49/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2985 (0.3054)	loss 1.2678 (1.3750)	grad_norm 34.5701 (inf)	mem 4879MB
[2022-05-31 05:21:31 MetaFG_0] (main.py 265): INFO Train: [49/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2921 (0.3054)	loss 1.6132 (1.3755)	grad_norm 26.0177 (inf)	mem 4879MB
[2022-05-31 05:21:31 MetaFG_0] (main.py 272): INFO EPOCH 49 training takes 0:07:57
[2022-05-31 05:21:31 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_49.pth saving......
[2022-05-31 05:21:32 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_49.pth saved !!!
[2022-05-31 05:21:32 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:21:33 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:21:33 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:21:34 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.704 (0.704)	Loss 0.7166 (0.7166)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 05:21:35 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.153)	Loss 0.8745 (0.6582)	Acc@1 75.000 (88.352)	Acc@5 93.750 (98.011)	Mem 4879MB
[2022-05-31 05:21:36 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.125)	Loss 0.7181 (0.6903)	Acc@1 84.375 (86.310)	Acc@5 96.875 (98.512)	Mem 4879MB
[2022-05-31 05:21:37 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.115)	Loss 0.8939 (0.6829)	Acc@1 78.125 (86.290)	Acc@5 96.875 (98.488)	Mem 4879MB
[2022-05-31 05:21:38 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.097 (0.110)	Loss 0.7468 (0.6724)	Acc@1 87.500 (86.890)	Acc@5 100.000 (98.476)	Mem 4879MB
[2022-05-31 05:21:39 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.106)	Loss 0.7737 (0.6652)	Acc@1 87.500 (87.132)	Acc@5 93.750 (98.346)	Mem 4879MB
[2022-05-31 05:21:40 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.104)	Loss 0.8694 (0.6654)	Acc@1 84.375 (87.090)	Acc@5 96.875 (98.463)	Mem 4879MB
[2022-05-31 05:21:41 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.103)	Loss 0.6146 (0.6725)	Acc@1 87.500 (86.752)	Acc@5 100.000 (98.415)	Mem 4879MB
[2022-05-31 05:21:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 0.7157 (0.6740)	Acc@1 78.125 (86.574)	Acc@5 100.000 (98.418)	Mem 4879MB
[2022-05-31 05:21:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.091 (0.101)	Loss 0.6405 (0.6762)	Acc@1 84.375 (86.264)	Acc@5 100.000 (98.386)	Mem 4879MB
[2022-05-31 05:21:43 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.099 (0.100)	Loss 0.7659 (0.6800)	Acc@1 87.500 (86.077)	Acc@5 96.875 (98.298)	Mem 4879MB
[2022-05-31 05:21:44 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 0.8683 (0.6858)	Acc@1 78.125 (85.867)	Acc@5 96.875 (98.255)	Mem 4879MB
[2022-05-31 05:21:45 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.8071 (0.6897)	Acc@1 81.250 (85.666)	Acc@5 96.875 (98.244)	Mem 4879MB
[2022-05-31 05:21:46 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.099)	Loss 0.8073 (0.6906)	Acc@1 78.125 (85.544)	Acc@5 96.875 (98.282)	Mem 4879MB
[2022-05-31 05:21:47 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.7637 (0.6978)	Acc@1 90.625 (85.217)	Acc@5 96.875 (98.316)	Mem 4879MB
[2022-05-31 05:21:48 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.4411 (0.6991)	Acc@1 96.875 (85.286)	Acc@5 100.000 (98.303)	Mem 4879MB
[2022-05-31 05:21:49 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.098)	Loss 0.6769 (0.6978)	Acc@1 81.250 (85.365)	Acc@5 100.000 (98.331)	Mem 4879MB
[2022-05-31 05:21:50 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 0.5963 (0.6999)	Acc@1 93.750 (85.252)	Acc@5 100.000 (98.355)	Mem 4879MB
[2022-05-31 05:21:51 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.097)	Loss 0.7359 (0.6949)	Acc@1 84.375 (85.480)	Acc@5 96.875 (98.394)	Mem 4879MB
[2022-05-31 05:21:52 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.092 (0.097)	Loss 0.9505 (0.6953)	Acc@1 78.125 (85.373)	Acc@5 93.750 (98.413)	Mem 4879MB
[2022-05-31 05:21:53 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 0.7711 (0.6970)	Acc@1 84.375 (85.261)	Acc@5 100.000 (98.383)	Mem 4879MB
[2022-05-31 05:21:54 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.4967 (0.6945)	Acc@1 96.875 (85.367)	Acc@5 100.000 (98.371)	Mem 4879MB
[2022-05-31 05:21:55 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.6047 (0.6919)	Acc@1 84.375 (85.450)	Acc@5 100.000 (98.430)	Mem 4879MB
[2022-05-31 05:21:56 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.088 (0.097)	Loss 0.7112 (0.6911)	Acc@1 84.375 (85.471)	Acc@5 100.000 (98.498)	Mem 4879MB
[2022-05-31 05:21:57 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.097 (0.096)	Loss 0.5864 (0.6901)	Acc@1 81.250 (85.464)	Acc@5 100.000 (98.535)	Mem 4879MB
[2022-05-31 05:21:58 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.096)	Loss 0.6270 (0.6905)	Acc@1 93.750 (85.458)	Acc@5 100.000 (98.531)	Mem 4879MB
[2022-05-31 05:21:58 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.100 (0.096)	Loss 0.7023 (0.6898)	Acc@1 84.375 (85.489)	Acc@5 100.000 (98.527)	Mem 4879MB
[2022-05-31 05:21:59 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.096)	Loss 0.5239 (0.6900)	Acc@1 90.625 (85.551)	Acc@5 100.000 (98.489)	Mem 4879MB
[2022-05-31 05:22:00 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.096)	Loss 0.6343 (0.6912)	Acc@1 87.500 (85.443)	Acc@5 96.875 (98.499)	Mem 4879MB
[2022-05-31 05:22:01 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 1.0289 (0.6909)	Acc@1 75.000 (85.492)	Acc@5 93.750 (98.497)	Mem 4879MB
[2022-05-31 05:22:02 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.097 (0.096)	Loss 0.8406 (0.6890)	Acc@1 78.125 (85.590)	Acc@5 93.750 (98.505)	Mem 4879MB
[2022-05-31 05:22:03 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.6216 (0.6897)	Acc@1 90.625 (85.541)	Acc@5 100.000 (98.523)	Mem 4879MB
[2022-05-31 05:22:03 MetaFG_0] (main.py 330): INFO  * Acc@1 85.510 Acc@5 98.520
[2022-05-31 05:22:03 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.5%
[2022-05-31 05:22:03 MetaFG_0] (main.py 171): INFO Max accuracy: 85.60%
[2022-05-31 05:22:04 MetaFG_0] (main.py 265): INFO Train: [50/300][0/1562]	eta 0:26:56 lr 0.000006	time 1.0351 (1.0351)	loss 1.4206 (1.4206)	grad_norm 23.9699 (23.9699)	mem 4879MB
[2022-05-31 05:22:07 MetaFG_0] (main.py 265): INFO Train: [50/300][10/1562]	eta 0:09:43 lr 0.000006	time 0.2945 (0.3758)	loss 1.2264 (1.3383)	grad_norm 29.2450 (26.8085)	mem 4879MB
[2022-05-31 05:22:11 MetaFG_0] (main.py 265): INFO Train: [50/300][20/1562]	eta 0:08:47 lr 0.000006	time 0.2980 (0.3423)	loss 1.4773 (1.3664)	grad_norm 25.7640 (29.7182)	mem 4879MB
[2022-05-31 05:22:14 MetaFG_0] (main.py 265): INFO Train: [50/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.3021 (0.3307)	loss 1.4715 (1.3911)	grad_norm 34.7720 (28.8415)	mem 4879MB
[2022-05-31 05:22:17 MetaFG_0] (main.py 265): INFO Train: [50/300][40/1562]	eta 0:08:14 lr 0.000006	time 0.3018 (0.3246)	loss 1.6563 (1.3930)	grad_norm 30.7841 (29.1501)	mem 4879MB
[2022-05-31 05:22:20 MetaFG_0] (main.py 265): INFO Train: [50/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2981 (0.3208)	loss 1.5079 (1.3862)	grad_norm 17.9447 (28.6716)	mem 4879MB
[2022-05-31 05:22:23 MetaFG_0] (main.py 265): INFO Train: [50/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2930 (0.3181)	loss 1.4683 (1.4056)	grad_norm 27.0227 (28.2655)	mem 4879MB
[2022-05-31 05:22:26 MetaFG_0] (main.py 265): INFO Train: [50/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2998 (0.3162)	loss 1.5752 (1.4134)	grad_norm 25.1607 (28.5009)	mem 4879MB
[2022-05-31 05:22:29 MetaFG_0] (main.py 265): INFO Train: [50/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2976 (0.3146)	loss 1.4415 (1.4114)	grad_norm 20.7561 (28.5239)	mem 4879MB
[2022-05-31 05:22:32 MetaFG_0] (main.py 265): INFO Train: [50/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2927 (0.3134)	loss 1.4946 (1.4067)	grad_norm 20.4670 (27.8904)	mem 4879MB
[2022-05-31 05:22:35 MetaFG_0] (main.py 265): INFO Train: [50/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2980 (0.3126)	loss 1.0936 (1.4046)	grad_norm 42.9804 (28.1404)	mem 4879MB
[2022-05-31 05:22:38 MetaFG_0] (main.py 265): INFO Train: [50/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2917 (0.3119)	loss 1.3849 (1.3919)	grad_norm 15.3784 (27.8147)	mem 4879MB
[2022-05-31 05:22:41 MetaFG_0] (main.py 265): INFO Train: [50/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2995 (0.3112)	loss 1.3799 (1.3971)	grad_norm 16.8921 (28.4698)	mem 4879MB
[2022-05-31 05:22:44 MetaFG_0] (main.py 265): INFO Train: [50/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2990 (0.3109)	loss 1.0921 (1.3959)	grad_norm 42.3106 (28.8509)	mem 4879MB
[2022-05-31 05:22:47 MetaFG_0] (main.py 265): INFO Train: [50/300][140/1562]	eta 0:07:21 lr 0.000006	time 0.2981 (0.3104)	loss 1.8947 (1.3923)	grad_norm 47.6103 (28.9148)	mem 4879MB
[2022-05-31 05:22:50 MetaFG_0] (main.py 265): INFO Train: [50/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2997 (0.3100)	loss 1.4085 (1.3812)	grad_norm 39.3880 (29.0764)	mem 4879MB
[2022-05-31 05:22:53 MetaFG_0] (main.py 265): INFO Train: [50/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.3000 (0.3097)	loss 1.0410 (1.3833)	grad_norm 18.6356 (29.0484)	mem 4879MB
[2022-05-31 05:22:56 MetaFG_0] (main.py 265): INFO Train: [50/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2934 (0.3094)	loss 1.4455 (1.3819)	grad_norm 31.1721 (29.0295)	mem 4879MB
[2022-05-31 05:22:59 MetaFG_0] (main.py 265): INFO Train: [50/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2926 (0.3091)	loss 1.3463 (1.3772)	grad_norm 28.5149 (29.2573)	mem 4879MB
[2022-05-31 05:23:02 MetaFG_0] (main.py 265): INFO Train: [50/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2940 (0.3090)	loss 1.0616 (1.3763)	grad_norm 17.1858 (29.0997)	mem 4879MB
[2022-05-31 05:23:05 MetaFG_0] (main.py 265): INFO Train: [50/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2924 (0.3088)	loss 1.3948 (1.3807)	grad_norm 32.0397 (29.3687)	mem 4879MB
[2022-05-31 05:23:08 MetaFG_0] (main.py 265): INFO Train: [50/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2929 (0.3086)	loss 1.2323 (1.3754)	grad_norm 25.2953 (29.4892)	mem 4879MB
[2022-05-31 05:23:12 MetaFG_0] (main.py 265): INFO Train: [50/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2942 (0.3084)	loss 1.3039 (1.3800)	grad_norm 28.1990 (29.4990)	mem 4879MB
[2022-05-31 05:23:15 MetaFG_0] (main.py 265): INFO Train: [50/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.3001 (0.3082)	loss 1.4337 (1.3811)	grad_norm 30.8743 (29.3676)	mem 4879MB
[2022-05-31 05:23:18 MetaFG_0] (main.py 265): INFO Train: [50/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2928 (0.3080)	loss 1.2298 (1.3782)	grad_norm 18.3123 (29.4570)	mem 4879MB
[2022-05-31 05:23:21 MetaFG_0] (main.py 265): INFO Train: [50/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2958 (0.3079)	loss 1.6071 (1.3769)	grad_norm 37.9457 (29.4597)	mem 4879MB
[2022-05-31 05:23:24 MetaFG_0] (main.py 265): INFO Train: [50/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2937 (0.3079)	loss 1.6115 (1.3761)	grad_norm 29.5623 (29.5157)	mem 4879MB
[2022-05-31 05:23:27 MetaFG_0] (main.py 265): INFO Train: [50/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2932 (0.3077)	loss 1.2699 (1.3772)	grad_norm 42.3674 (29.5451)	mem 4879MB
[2022-05-31 05:23:30 MetaFG_0] (main.py 265): INFO Train: [50/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2978 (0.3077)	loss 1.3958 (1.3808)	grad_norm 37.7900 (29.6896)	mem 4879MB
[2022-05-31 05:23:33 MetaFG_0] (main.py 265): INFO Train: [50/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2941 (0.3075)	loss 1.4891 (1.3813)	grad_norm 22.4071 (29.9131)	mem 4879MB
[2022-05-31 05:23:36 MetaFG_0] (main.py 265): INFO Train: [50/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.3016 (0.3085)	loss 1.2343 (1.3769)	grad_norm 17.6665 (29.8116)	mem 4879MB
[2022-05-31 05:23:39 MetaFG_0] (main.py 265): INFO Train: [50/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.2933 (0.3084)	loss 1.3528 (1.3781)	grad_norm 41.3889 (29.9409)	mem 4879MB
[2022-05-31 05:23:42 MetaFG_0] (main.py 265): INFO Train: [50/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2937 (0.3082)	loss 1.3517 (1.3787)	grad_norm 15.8968 (29.9774)	mem 4879MB
[2022-05-31 05:23:45 MetaFG_0] (main.py 265): INFO Train: [50/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2950 (0.3082)	loss 1.0071 (1.3745)	grad_norm 47.8987 (30.0557)	mem 4879MB
[2022-05-31 05:23:48 MetaFG_0] (main.py 265): INFO Train: [50/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2977 (0.3081)	loss 1.5641 (1.3726)	grad_norm 22.1024 (30.1546)	mem 4879MB
[2022-05-31 05:23:51 MetaFG_0] (main.py 265): INFO Train: [50/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2935 (0.3080)	loss 1.1720 (1.3742)	grad_norm 40.8189 (30.0029)	mem 4879MB
[2022-05-31 05:23:55 MetaFG_0] (main.py 265): INFO Train: [50/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.3009 (0.3079)	loss 1.5490 (1.3735)	grad_norm 36.2136 (30.0922)	mem 4879MB
[2022-05-31 05:23:58 MetaFG_0] (main.py 265): INFO Train: [50/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2996 (0.3079)	loss 1.5858 (1.3740)	grad_norm 46.2110 (30.0824)	mem 4879MB
[2022-05-31 05:24:01 MetaFG_0] (main.py 265): INFO Train: [50/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2991 (0.3077)	loss 1.5134 (1.3714)	grad_norm 24.9713 (29.9914)	mem 4879MB
[2022-05-31 05:24:04 MetaFG_0] (main.py 265): INFO Train: [50/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.3021 (0.3077)	loss 1.1496 (1.3715)	grad_norm 39.6942 (29.9599)	mem 4879MB
[2022-05-31 05:24:07 MetaFG_0] (main.py 265): INFO Train: [50/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2987 (0.3076)	loss 1.4004 (1.3700)	grad_norm 35.3116 (30.0659)	mem 4879MB
[2022-05-31 05:24:10 MetaFG_0] (main.py 265): INFO Train: [50/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2939 (0.3075)	loss 0.9713 (1.3711)	grad_norm 41.4738 (30.2075)	mem 4879MB
[2022-05-31 05:24:13 MetaFG_0] (main.py 265): INFO Train: [50/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.3004 (0.3074)	loss 1.3505 (1.3706)	grad_norm 39.8346 (30.1436)	mem 4879MB
[2022-05-31 05:24:16 MetaFG_0] (main.py 265): INFO Train: [50/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2917 (0.3074)	loss 1.3232 (1.3726)	grad_norm 27.5352 (30.1693)	mem 4879MB
[2022-05-31 05:24:19 MetaFG_0] (main.py 265): INFO Train: [50/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2929 (0.3073)	loss 1.1985 (1.3721)	grad_norm 42.2441 (30.2853)	mem 4879MB
[2022-05-31 05:24:22 MetaFG_0] (main.py 265): INFO Train: [50/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2937 (0.3073)	loss 1.5597 (1.3723)	grad_norm 16.6305 (30.2146)	mem 4879MB
[2022-05-31 05:24:25 MetaFG_0] (main.py 265): INFO Train: [50/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2992 (0.3072)	loss 1.6290 (1.3725)	grad_norm 31.5477 (30.1791)	mem 4879MB
[2022-05-31 05:24:28 MetaFG_0] (main.py 265): INFO Train: [50/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2925 (0.3071)	loss 1.2247 (1.3701)	grad_norm 27.2353 (30.1353)	mem 4879MB
[2022-05-31 05:24:31 MetaFG_0] (main.py 265): INFO Train: [50/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2924 (0.3070)	loss 1.6267 (1.3729)	grad_norm 22.3824 (30.0966)	mem 4879MB
[2022-05-31 05:24:34 MetaFG_0] (main.py 265): INFO Train: [50/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2925 (0.3070)	loss 1.4937 (1.3750)	grad_norm 34.8147 (30.1063)	mem 4879MB
[2022-05-31 05:24:37 MetaFG_0] (main.py 265): INFO Train: [50/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2929 (0.3069)	loss 1.0775 (1.3747)	grad_norm 20.1122 (30.0920)	mem 4879MB
[2022-05-31 05:24:40 MetaFG_0] (main.py 265): INFO Train: [50/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2927 (0.3069)	loss 1.5203 (1.3732)	grad_norm 21.3432 (30.0530)	mem 4879MB
[2022-05-31 05:24:43 MetaFG_0] (main.py 265): INFO Train: [50/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2924 (0.3069)	loss 1.4059 (1.3740)	grad_norm 33.4343 (30.0312)	mem 4879MB
[2022-05-31 05:24:46 MetaFG_0] (main.py 265): INFO Train: [50/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2931 (0.3069)	loss 1.0363 (1.3734)	grad_norm 22.3022 (30.0826)	mem 4879MB
[2022-05-31 05:24:49 MetaFG_0] (main.py 265): INFO Train: [50/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2929 (0.3069)	loss 1.3524 (1.3743)	grad_norm 40.9808 (30.0710)	mem 4879MB
[2022-05-31 05:24:52 MetaFG_0] (main.py 265): INFO Train: [50/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2925 (0.3068)	loss 1.3760 (1.3744)	grad_norm 22.4965 (30.0587)	mem 4879MB
[2022-05-31 05:24:55 MetaFG_0] (main.py 265): INFO Train: [50/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2947 (0.3068)	loss 1.3876 (1.3751)	grad_norm 42.9098 (30.0642)	mem 4879MB
[2022-05-31 05:24:59 MetaFG_0] (main.py 265): INFO Train: [50/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2984 (0.3067)	loss 1.1920 (1.3760)	grad_norm 33.7990 (30.1040)	mem 4879MB
[2022-05-31 05:25:02 MetaFG_0] (main.py 265): INFO Train: [50/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2948 (0.3067)	loss 1.4000 (1.3768)	grad_norm 28.2975 (30.1311)	mem 4879MB
[2022-05-31 05:25:05 MetaFG_0] (main.py 265): INFO Train: [50/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.3022 (0.3067)	loss 1.4775 (1.3773)	grad_norm 26.8839 (30.0649)	mem 4879MB
[2022-05-31 05:25:08 MetaFG_0] (main.py 265): INFO Train: [50/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2942 (0.3066)	loss 1.4350 (1.3780)	grad_norm 34.1241 (30.1102)	mem 4879MB
[2022-05-31 05:25:11 MetaFG_0] (main.py 265): INFO Train: [50/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2989 (0.3066)	loss 1.6050 (1.3776)	grad_norm 30.2129 (30.0713)	mem 4879MB
[2022-05-31 05:25:14 MetaFG_0] (main.py 265): INFO Train: [50/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2987 (0.3066)	loss 1.6238 (1.3784)	grad_norm 22.7762 (30.0295)	mem 4879MB
[2022-05-31 05:25:17 MetaFG_0] (main.py 265): INFO Train: [50/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2980 (0.3066)	loss 1.6413 (1.3785)	grad_norm 23.5526 (30.1015)	mem 4879MB
[2022-05-31 05:25:20 MetaFG_0] (main.py 265): INFO Train: [50/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3014 (0.3065)	loss 1.3139 (1.3790)	grad_norm 36.0741 (30.0732)	mem 4879MB
[2022-05-31 05:25:23 MetaFG_0] (main.py 265): INFO Train: [50/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2993 (0.3065)	loss 1.3049 (1.3799)	grad_norm 32.5718 (30.0225)	mem 4879MB
[2022-05-31 05:25:26 MetaFG_0] (main.py 265): INFO Train: [50/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2939 (0.3064)	loss 0.9559 (1.3784)	grad_norm 87.6663 (30.0614)	mem 4879MB
[2022-05-31 05:25:29 MetaFG_0] (main.py 265): INFO Train: [50/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2995 (0.3064)	loss 1.4647 (1.3777)	grad_norm 25.4433 (30.0493)	mem 4879MB
[2022-05-31 05:25:32 MetaFG_0] (main.py 265): INFO Train: [50/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2931 (0.3064)	loss 1.4618 (1.3770)	grad_norm 24.7301 (30.0218)	mem 4879MB
[2022-05-31 05:25:35 MetaFG_0] (main.py 265): INFO Train: [50/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2928 (0.3064)	loss 1.5541 (1.3755)	grad_norm 22.8640 (30.0900)	mem 4879MB
[2022-05-31 05:25:38 MetaFG_0] (main.py 265): INFO Train: [50/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2925 (0.3063)	loss 1.2371 (1.3737)	grad_norm 38.7637 (30.0340)	mem 4879MB
[2022-05-31 05:25:41 MetaFG_0] (main.py 265): INFO Train: [50/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2939 (0.3063)	loss 1.4979 (1.3732)	grad_norm 37.0123 (30.0180)	mem 4879MB
[2022-05-31 05:25:44 MetaFG_0] (main.py 265): INFO Train: [50/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2943 (0.3062)	loss 1.2427 (1.3732)	grad_norm 25.0278 (29.9674)	mem 4879MB
[2022-05-31 05:25:47 MetaFG_0] (main.py 265): INFO Train: [50/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2931 (0.3062)	loss 1.3060 (1.3713)	grad_norm 28.2933 (30.0137)	mem 4879MB
[2022-05-31 05:25:50 MetaFG_0] (main.py 265): INFO Train: [50/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2922 (0.3062)	loss 1.2235 (1.3717)	grad_norm 30.1994 (29.9581)	mem 4879MB
[2022-05-31 05:25:53 MetaFG_0] (main.py 265): INFO Train: [50/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2994 (0.3061)	loss 1.5414 (1.3728)	grad_norm 27.4652 (29.9445)	mem 4879MB
[2022-05-31 05:25:56 MetaFG_0] (main.py 265): INFO Train: [50/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2982 (0.3061)	loss 1.4963 (1.3727)	grad_norm 27.0123 (30.2035)	mem 4879MB
[2022-05-31 05:25:59 MetaFG_0] (main.py 265): INFO Train: [50/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2992 (0.3061)	loss 1.4068 (1.3736)	grad_norm 23.5729 (30.1440)	mem 4879MB
[2022-05-31 05:26:02 MetaFG_0] (main.py 265): INFO Train: [50/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2932 (0.3061)	loss 1.3074 (1.3737)	grad_norm 50.8255 (30.1484)	mem 4879MB
[2022-05-31 05:26:06 MetaFG_0] (main.py 265): INFO Train: [50/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2979 (0.3061)	loss 1.3314 (1.3733)	grad_norm 16.5393 (30.1634)	mem 4879MB
[2022-05-31 05:26:09 MetaFG_0] (main.py 265): INFO Train: [50/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2987 (0.3061)	loss 1.3792 (1.3721)	grad_norm 27.6250 (30.1834)	mem 4879MB
[2022-05-31 05:26:12 MetaFG_0] (main.py 265): INFO Train: [50/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2984 (0.3061)	loss 1.6204 (1.3733)	grad_norm 37.4663 (30.1642)	mem 4879MB
[2022-05-31 05:26:15 MetaFG_0] (main.py 265): INFO Train: [50/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2923 (0.3061)	loss 1.4662 (1.3738)	grad_norm 24.2110 (30.1611)	mem 4879MB
[2022-05-31 05:26:18 MetaFG_0] (main.py 265): INFO Train: [50/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2996 (0.3061)	loss 1.4931 (1.3740)	grad_norm 48.9988 (30.1613)	mem 4879MB
[2022-05-31 05:26:21 MetaFG_0] (main.py 265): INFO Train: [50/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2920 (0.3060)	loss 0.9782 (1.3730)	grad_norm 29.5583 (30.1433)	mem 4879MB
[2022-05-31 05:26:24 MetaFG_0] (main.py 265): INFO Train: [50/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2925 (0.3060)	loss 1.3421 (1.3731)	grad_norm 22.2015 (30.1005)	mem 4879MB
[2022-05-31 05:26:27 MetaFG_0] (main.py 265): INFO Train: [50/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2925 (0.3060)	loss 1.0632 (1.3727)	grad_norm 16.8341 (30.0431)	mem 4879MB
[2022-05-31 05:26:30 MetaFG_0] (main.py 265): INFO Train: [50/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2993 (0.3060)	loss 0.9612 (1.3702)	grad_norm 36.7127 (30.1203)	mem 4879MB
[2022-05-31 05:26:33 MetaFG_0] (main.py 265): INFO Train: [50/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2921 (0.3060)	loss 1.4873 (1.3702)	grad_norm 27.9186 (30.1149)	mem 4879MB
[2022-05-31 05:26:36 MetaFG_0] (main.py 265): INFO Train: [50/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.3005 (0.3060)	loss 1.2805 (1.3695)	grad_norm 25.9188 (30.1334)	mem 4879MB
[2022-05-31 05:26:39 MetaFG_0] (main.py 265): INFO Train: [50/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2996 (0.3060)	loss 1.5833 (1.3703)	grad_norm 46.2231 (30.2359)	mem 4879MB
[2022-05-31 05:26:42 MetaFG_0] (main.py 265): INFO Train: [50/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2928 (0.3060)	loss 0.9636 (1.3690)	grad_norm 24.5489 (30.1843)	mem 4879MB
[2022-05-31 05:26:45 MetaFG_0] (main.py 265): INFO Train: [50/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2988 (0.3060)	loss 1.5476 (1.3698)	grad_norm 20.5675 (30.1477)	mem 4879MB
[2022-05-31 05:26:48 MetaFG_0] (main.py 265): INFO Train: [50/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2918 (0.3060)	loss 1.3842 (1.3700)	grad_norm 20.9146 (30.1180)	mem 4879MB
[2022-05-31 05:26:51 MetaFG_0] (main.py 265): INFO Train: [50/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2996 (0.3060)	loss 1.5644 (1.3703)	grad_norm 33.6441 (30.1886)	mem 4879MB
[2022-05-31 05:26:54 MetaFG_0] (main.py 265): INFO Train: [50/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3013 (0.3059)	loss 1.2999 (1.3701)	grad_norm 30.5995 (30.1474)	mem 4879MB
[2022-05-31 05:26:57 MetaFG_0] (main.py 265): INFO Train: [50/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2934 (0.3059)	loss 1.4055 (1.3709)	grad_norm 59.5648 (30.2039)	mem 4879MB
[2022-05-31 05:27:00 MetaFG_0] (main.py 265): INFO Train: [50/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2982 (0.3059)	loss 1.4270 (1.3711)	grad_norm 22.7068 (30.1960)	mem 4879MB
[2022-05-31 05:27:03 MetaFG_0] (main.py 265): INFO Train: [50/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.3022 (0.3059)	loss 1.3565 (1.3710)	grad_norm 25.8233 (30.1891)	mem 4879MB
[2022-05-31 05:27:07 MetaFG_0] (main.py 265): INFO Train: [50/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.3012 (0.3059)	loss 1.1776 (1.3703)	grad_norm 30.8379 (30.2055)	mem 4879MB
[2022-05-31 05:27:10 MetaFG_0] (main.py 265): INFO Train: [50/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2945 (0.3059)	loss 1.3109 (1.3697)	grad_norm 21.7860 (30.1933)	mem 4879MB
[2022-05-31 05:27:13 MetaFG_0] (main.py 265): INFO Train: [50/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2918 (0.3059)	loss 1.4598 (1.3692)	grad_norm 35.0318 (30.2153)	mem 4879MB
[2022-05-31 05:27:16 MetaFG_0] (main.py 265): INFO Train: [50/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2985 (0.3059)	loss 1.4641 (1.3680)	grad_norm 23.9740 (30.1929)	mem 4879MB
[2022-05-31 05:27:19 MetaFG_0] (main.py 265): INFO Train: [50/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2942 (0.3058)	loss 1.4616 (1.3682)	grad_norm 29.8796 (30.1511)	mem 4879MB
[2022-05-31 05:27:22 MetaFG_0] (main.py 265): INFO Train: [50/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3001 (0.3058)	loss 1.2351 (1.3687)	grad_norm 19.7234 (30.1275)	mem 4879MB
[2022-05-31 05:27:25 MetaFG_0] (main.py 265): INFO Train: [50/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2949 (0.3058)	loss 1.4506 (1.3691)	grad_norm 23.6407 (30.1319)	mem 4879MB
[2022-05-31 05:27:28 MetaFG_0] (main.py 265): INFO Train: [50/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2997 (0.3058)	loss 1.5267 (1.3690)	grad_norm 27.7061 (30.0938)	mem 4879MB
[2022-05-31 05:27:31 MetaFG_0] (main.py 265): INFO Train: [50/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2925 (0.3058)	loss 1.1349 (1.3696)	grad_norm 21.4451 (30.0795)	mem 4879MB
[2022-05-31 05:27:34 MetaFG_0] (main.py 265): INFO Train: [50/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2934 (0.3058)	loss 1.1545 (1.3698)	grad_norm 38.9180 (30.1275)	mem 4879MB
[2022-05-31 05:27:37 MetaFG_0] (main.py 265): INFO Train: [50/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2927 (0.3058)	loss 1.5115 (1.3701)	grad_norm 17.0396 (30.0642)	mem 4879MB
[2022-05-31 05:27:40 MetaFG_0] (main.py 265): INFO Train: [50/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2922 (0.3058)	loss 1.3657 (1.3706)	grad_norm 15.5784 (30.0512)	mem 4879MB
[2022-05-31 05:27:43 MetaFG_0] (main.py 265): INFO Train: [50/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2925 (0.3058)	loss 1.5383 (1.3707)	grad_norm 27.3992 (30.0209)	mem 4879MB
[2022-05-31 05:27:46 MetaFG_0] (main.py 265): INFO Train: [50/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.3009 (0.3058)	loss 1.6072 (1.3719)	grad_norm 32.5800 (30.0500)	mem 4879MB
[2022-05-31 05:27:49 MetaFG_0] (main.py 265): INFO Train: [50/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2965 (0.3058)	loss 1.4411 (1.3723)	grad_norm 25.4703 (30.0005)	mem 4879MB
[2022-05-31 05:27:52 MetaFG_0] (main.py 265): INFO Train: [50/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2932 (0.3057)	loss 1.6734 (1.3736)	grad_norm 25.8835 (29.9905)	mem 4879MB
[2022-05-31 05:27:55 MetaFG_0] (main.py 265): INFO Train: [50/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2930 (0.3057)	loss 1.2786 (1.3743)	grad_norm 17.0245 (30.0663)	mem 4879MB
[2022-05-31 05:27:58 MetaFG_0] (main.py 265): INFO Train: [50/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2926 (0.3057)	loss 0.9978 (1.3739)	grad_norm 35.9478 (30.0412)	mem 4879MB
[2022-05-31 05:28:01 MetaFG_0] (main.py 265): INFO Train: [50/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2987 (0.3057)	loss 1.5901 (1.3742)	grad_norm 24.4630 (30.0308)	mem 4879MB
[2022-05-31 05:28:04 MetaFG_0] (main.py 265): INFO Train: [50/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3002 (0.3057)	loss 1.2155 (1.3741)	grad_norm 23.9198 (29.9970)	mem 4879MB
[2022-05-31 05:28:07 MetaFG_0] (main.py 265): INFO Train: [50/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2916 (0.3057)	loss 0.9641 (1.3747)	grad_norm 30.6780 (29.9948)	mem 4879MB
[2022-05-31 05:28:10 MetaFG_0] (main.py 265): INFO Train: [50/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2988 (0.3056)	loss 1.5061 (1.3742)	grad_norm 29.7243 (30.0093)	mem 4879MB
[2022-05-31 05:28:13 MetaFG_0] (main.py 265): INFO Train: [50/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2944 (0.3056)	loss 1.3626 (1.3751)	grad_norm 27.6624 (nan)	mem 4879MB
[2022-05-31 05:28:17 MetaFG_0] (main.py 265): INFO Train: [50/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2946 (0.3056)	loss 1.1179 (1.3753)	grad_norm 26.0428 (nan)	mem 4879MB
[2022-05-31 05:28:20 MetaFG_0] (main.py 265): INFO Train: [50/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2999 (0.3056)	loss 1.3080 (1.3757)	grad_norm 28.7336 (nan)	mem 4879MB
[2022-05-31 05:28:23 MetaFG_0] (main.py 265): INFO Train: [50/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2983 (0.3056)	loss 1.3786 (1.3756)	grad_norm 21.8426 (nan)	mem 4879MB
[2022-05-31 05:28:26 MetaFG_0] (main.py 265): INFO Train: [50/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2921 (0.3056)	loss 1.5770 (1.3753)	grad_norm 18.5607 (nan)	mem 4879MB
[2022-05-31 05:28:29 MetaFG_0] (main.py 265): INFO Train: [50/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2989 (0.3056)	loss 1.5535 (1.3757)	grad_norm 38.6740 (nan)	mem 4879MB
[2022-05-31 05:28:32 MetaFG_0] (main.py 265): INFO Train: [50/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.3066 (0.3056)	loss 1.6144 (1.3764)	grad_norm 29.0205 (nan)	mem 4879MB
[2022-05-31 05:28:35 MetaFG_0] (main.py 265): INFO Train: [50/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2929 (0.3058)	loss 1.0127 (1.3758)	grad_norm 20.9874 (nan)	mem 4879MB
[2022-05-31 05:28:38 MetaFG_0] (main.py 265): INFO Train: [50/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2957 (0.3058)	loss 1.6369 (1.3755)	grad_norm 34.3402 (nan)	mem 4879MB
[2022-05-31 05:28:41 MetaFG_0] (main.py 265): INFO Train: [50/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2946 (0.3058)	loss 1.4513 (1.3752)	grad_norm 50.9015 (nan)	mem 4879MB
[2022-05-31 05:28:44 MetaFG_0] (main.py 265): INFO Train: [50/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2987 (0.3058)	loss 1.3079 (1.3743)	grad_norm 48.6742 (nan)	mem 4879MB
[2022-05-31 05:28:47 MetaFG_0] (main.py 265): INFO Train: [50/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2981 (0.3058)	loss 1.5387 (1.3744)	grad_norm 17.5787 (nan)	mem 4879MB
[2022-05-31 05:28:50 MetaFG_0] (main.py 265): INFO Train: [50/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2930 (0.3058)	loss 1.4625 (1.3754)	grad_norm 35.5970 (nan)	mem 4879MB
[2022-05-31 05:28:53 MetaFG_0] (main.py 265): INFO Train: [50/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2986 (0.3058)	loss 1.7100 (1.3750)	grad_norm 23.5787 (nan)	mem 4879MB
[2022-05-31 05:28:56 MetaFG_0] (main.py 265): INFO Train: [50/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2933 (0.3058)	loss 1.3487 (1.3758)	grad_norm 15.9867 (nan)	mem 4879MB
[2022-05-31 05:28:59 MetaFG_0] (main.py 265): INFO Train: [50/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2938 (0.3058)	loss 1.5487 (1.3762)	grad_norm 22.5950 (nan)	mem 4879MB
[2022-05-31 05:29:03 MetaFG_0] (main.py 265): INFO Train: [50/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2937 (0.3057)	loss 1.2814 (1.3760)	grad_norm 25.2406 (nan)	mem 4879MB
[2022-05-31 05:29:06 MetaFG_0] (main.py 265): INFO Train: [50/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2932 (0.3057)	loss 1.2172 (1.3759)	grad_norm 16.0592 (nan)	mem 4879MB
[2022-05-31 05:29:09 MetaFG_0] (main.py 265): INFO Train: [50/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.3034 (0.3057)	loss 0.9620 (1.3758)	grad_norm 20.7055 (nan)	mem 4879MB
[2022-05-31 05:29:12 MetaFG_0] (main.py 265): INFO Train: [50/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2981 (0.3057)	loss 1.3887 (1.3754)	grad_norm 30.5839 (nan)	mem 4879MB
[2022-05-31 05:29:15 MetaFG_0] (main.py 265): INFO Train: [50/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2925 (0.3057)	loss 1.7708 (1.3754)	grad_norm 33.4166 (nan)	mem 4879MB
[2022-05-31 05:29:18 MetaFG_0] (main.py 265): INFO Train: [50/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2999 (0.3057)	loss 1.5381 (1.3761)	grad_norm 24.2529 (nan)	mem 4879MB
[2022-05-31 05:29:21 MetaFG_0] (main.py 265): INFO Train: [50/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2985 (0.3057)	loss 1.1129 (1.3756)	grad_norm 25.4665 (nan)	mem 4879MB
[2022-05-31 05:29:24 MetaFG_0] (main.py 265): INFO Train: [50/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2936 (0.3057)	loss 1.3732 (1.3750)	grad_norm 21.3937 (nan)	mem 4879MB
[2022-05-31 05:29:27 MetaFG_0] (main.py 265): INFO Train: [50/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2929 (0.3057)	loss 1.2514 (1.3749)	grad_norm 24.3690 (nan)	mem 4879MB
[2022-05-31 05:29:30 MetaFG_0] (main.py 265): INFO Train: [50/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2962 (0.3057)	loss 1.6648 (1.3745)	grad_norm 35.1845 (nan)	mem 4879MB
[2022-05-31 05:29:33 MetaFG_0] (main.py 265): INFO Train: [50/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2986 (0.3057)	loss 1.5337 (1.3749)	grad_norm 30.6910 (nan)	mem 4879MB
[2022-05-31 05:29:36 MetaFG_0] (main.py 265): INFO Train: [50/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2980 (0.3056)	loss 1.4099 (1.3748)	grad_norm 26.7430 (nan)	mem 4879MB
[2022-05-31 05:29:39 MetaFG_0] (main.py 265): INFO Train: [50/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2937 (0.3056)	loss 1.1759 (1.3744)	grad_norm 31.1099 (nan)	mem 4879MB
[2022-05-31 05:29:42 MetaFG_0] (main.py 265): INFO Train: [50/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2983 (0.3056)	loss 1.5633 (1.3741)	grad_norm 32.5293 (nan)	mem 4879MB
[2022-05-31 05:29:45 MetaFG_0] (main.py 265): INFO Train: [50/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2990 (0.3056)	loss 1.6868 (1.3737)	grad_norm 49.7775 (nan)	mem 4879MB
[2022-05-31 05:29:48 MetaFG_0] (main.py 265): INFO Train: [50/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2916 (0.3056)	loss 1.3908 (1.3738)	grad_norm 29.0026 (nan)	mem 4879MB
[2022-05-31 05:29:51 MetaFG_0] (main.py 265): INFO Train: [50/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2925 (0.3056)	loss 1.1078 (1.3737)	grad_norm 17.7644 (nan)	mem 4879MB
[2022-05-31 05:29:54 MetaFG_0] (main.py 265): INFO Train: [50/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2928 (0.3056)	loss 1.0045 (1.3732)	grad_norm 18.3178 (nan)	mem 4879MB
[2022-05-31 05:29:57 MetaFG_0] (main.py 265): INFO Train: [50/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3008 (0.3056)	loss 1.4661 (1.3734)	grad_norm 18.3828 (nan)	mem 4879MB
[2022-05-31 05:30:00 MetaFG_0] (main.py 265): INFO Train: [50/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2923 (0.3056)	loss 1.4264 (1.3735)	grad_norm 25.0180 (nan)	mem 4879MB
[2022-05-31 05:30:01 MetaFG_0] (main.py 272): INFO EPOCH 50 training takes 0:07:57
[2022-05-31 05:30:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_50.pth saving......
[2022-05-31 05:30:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_50.pth saved !!!
[2022-05-31 05:30:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:30:03 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:30:03 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:30:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.614 (0.614)	Loss 0.8441 (0.8441)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 05:30:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.155)	Loss 0.3872 (0.6796)	Acc@1 93.750 (83.523)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 05:30:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.095 (0.126)	Loss 0.7050 (0.6318)	Acc@1 81.250 (84.970)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 05:30:07 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.092 (0.115)	Loss 0.5047 (0.6175)	Acc@1 90.625 (85.585)	Acc@5 100.000 (98.690)	Mem 4879MB
[2022-05-31 05:30:08 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.107 (0.111)	Loss 0.4648 (0.5999)	Acc@1 90.625 (86.509)	Acc@5 100.000 (98.780)	Mem 4879MB
[2022-05-31 05:30:09 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.108)	Loss 0.4859 (0.5933)	Acc@1 90.625 (86.397)	Acc@5 100.000 (98.836)	Mem 4879MB
[2022-05-31 05:30:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.105)	Loss 0.6666 (0.6031)	Acc@1 87.500 (85.758)	Acc@5 96.875 (98.770)	Mem 4879MB
[2022-05-31 05:30:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.103)	Loss 0.5383 (0.6022)	Acc@1 90.625 (86.004)	Acc@5 96.875 (98.768)	Mem 4879MB
[2022-05-31 05:30:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.102)	Loss 0.5309 (0.6058)	Acc@1 90.625 (86.073)	Acc@5 96.875 (98.650)	Mem 4879MB
[2022-05-31 05:30:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.4746 (0.6079)	Acc@1 90.625 (85.817)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 05:30:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.101)	Loss 0.4790 (0.6099)	Acc@1 90.625 (85.767)	Acc@5 100.000 (98.577)	Mem 4879MB
[2022-05-31 05:30:14 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.101 (0.100)	Loss 0.7378 (0.6091)	Acc@1 78.125 (85.839)	Acc@5 100.000 (98.620)	Mem 4879MB
[2022-05-31 05:30:15 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.100)	Loss 0.6086 (0.6103)	Acc@1 87.500 (85.873)	Acc@5 100.000 (98.631)	Mem 4879MB
[2022-05-31 05:30:16 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.099 (0.099)	Loss 0.6540 (0.6074)	Acc@1 87.500 (86.140)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 05:30:17 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 0.5861 (0.6137)	Acc@1 84.375 (85.816)	Acc@5 100.000 (98.692)	Mem 4879MB
[2022-05-31 05:30:18 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.099)	Loss 0.7398 (0.6140)	Acc@1 81.250 (85.844)	Acc@5 93.750 (98.696)	Mem 4879MB
[2022-05-31 05:30:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.100 (0.098)	Loss 0.5423 (0.6120)	Acc@1 93.750 (86.005)	Acc@5 96.875 (98.641)	Mem 4879MB
[2022-05-31 05:30:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.6061 (0.6113)	Acc@1 90.625 (85.910)	Acc@5 100.000 (98.648)	Mem 4879MB
[2022-05-31 05:30:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.5679 (0.6089)	Acc@1 87.500 (85.981)	Acc@5 100.000 (98.619)	Mem 4879MB
[2022-05-31 05:30:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.098)	Loss 0.5490 (0.6065)	Acc@1 87.500 (86.126)	Acc@5 100.000 (98.642)	Mem 4879MB
[2022-05-31 05:30:23 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.107 (0.098)	Loss 0.6789 (0.6041)	Acc@1 78.125 (86.241)	Acc@5 100.000 (98.632)	Mem 4879MB
[2022-05-31 05:30:24 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.5085 (0.6070)	Acc@1 87.500 (86.152)	Acc@5 100.000 (98.593)	Mem 4879MB
[2022-05-31 05:30:25 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.7009 (0.6113)	Acc@1 81.250 (86.058)	Acc@5 100.000 (98.586)	Mem 4879MB
[2022-05-31 05:30:26 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.099 (0.097)	Loss 0.5803 (0.6138)	Acc@1 84.375 (85.931)	Acc@5 100.000 (98.620)	Mem 4879MB
[2022-05-31 05:30:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.097 (0.097)	Loss 0.7486 (0.6159)	Acc@1 81.250 (85.788)	Acc@5 96.875 (98.651)	Mem 4879MB
[2022-05-31 05:30:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.6558 (0.6180)	Acc@1 87.500 (85.657)	Acc@5 100.000 (98.630)	Mem 4879MB
[2022-05-31 05:30:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 0.3778 (0.6152)	Acc@1 93.750 (85.716)	Acc@5 100.000 (98.623)	Mem 4879MB
[2022-05-31 05:30:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.097)	Loss 0.7228 (0.6187)	Acc@1 84.375 (85.643)	Acc@5 93.750 (98.582)	Mem 4879MB
[2022-05-31 05:30:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.5296 (0.6140)	Acc@1 90.625 (85.798)	Acc@5 100.000 (98.610)	Mem 4879MB
[2022-05-31 05:30:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.092 (0.097)	Loss 0.4764 (0.6152)	Acc@1 93.750 (85.825)	Acc@5 100.000 (98.604)	Mem 4879MB
[2022-05-31 05:30:32 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.093 (0.097)	Loss 0.8590 (0.6198)	Acc@1 78.125 (85.631)	Acc@5 96.875 (98.588)	Mem 4879MB
[2022-05-31 05:30:33 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5094 (0.6231)	Acc@1 93.750 (85.611)	Acc@5 96.875 (98.533)	Mem 4879MB
[2022-05-31 05:30:34 MetaFG_0] (main.py 330): INFO  * Acc@1 85.620 Acc@5 98.540
[2022-05-31 05:30:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 85.6%
[2022-05-31 05:30:34 MetaFG_0] (main.py 171): INFO Max accuracy: 85.62%
[2022-05-31 05:30:34 MetaFG_0] (main.py 265): INFO Train: [51/300][0/1562]	eta 0:22:18 lr 0.000006	time 0.8569 (0.8569)	loss 1.4170 (1.4170)	grad_norm 35.1693 (35.1693)	mem 4879MB
[2022-05-31 05:30:38 MetaFG_0] (main.py 265): INFO Train: [51/300][10/1562]	eta 0:09:38 lr 0.000006	time 0.2982 (0.3727)	loss 1.7002 (1.2933)	grad_norm 52.0451 (32.6897)	mem 4879MB
[2022-05-31 05:30:41 MetaFG_0] (main.py 265): INFO Train: [51/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.3003 (0.3405)	loss 1.4934 (1.3078)	grad_norm 51.8831 (34.2220)	mem 4879MB
[2022-05-31 05:30:44 MetaFG_0] (main.py 265): INFO Train: [51/300][30/1562]	eta 0:08:23 lr 0.000006	time 0.2991 (0.3288)	loss 1.3673 (1.3367)	grad_norm 20.7527 (33.5768)	mem 4879MB
[2022-05-31 05:30:47 MetaFG_0] (main.py 265): INFO Train: [51/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2938 (0.3237)	loss 1.2471 (1.3099)	grad_norm 34.7115 (33.3929)	mem 4879MB
[2022-05-31 05:30:50 MetaFG_0] (main.py 265): INFO Train: [51/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2935 (0.3197)	loss 1.5953 (1.3346)	grad_norm 35.7981 (32.8378)	mem 4879MB
[2022-05-31 05:30:53 MetaFG_0] (main.py 265): INFO Train: [51/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2958 (0.3174)	loss 1.2092 (1.3379)	grad_norm 31.6823 (32.8902)	mem 4879MB
[2022-05-31 05:30:56 MetaFG_0] (main.py 265): INFO Train: [51/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2932 (0.3155)	loss 1.4421 (1.3285)	grad_norm 40.7636 (33.0325)	mem 4879MB
[2022-05-31 05:30:59 MetaFG_0] (main.py 265): INFO Train: [51/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2925 (0.3138)	loss 1.4167 (1.3409)	grad_norm 40.3402 (32.6688)	mem 4879MB
[2022-05-31 05:31:02 MetaFG_0] (main.py 265): INFO Train: [51/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2928 (0.3124)	loss 1.5307 (1.3434)	grad_norm 30.4976 (32.6303)	mem 4879MB
[2022-05-31 05:31:05 MetaFG_0] (main.py 265): INFO Train: [51/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2997 (0.3117)	loss 1.3551 (1.3490)	grad_norm 19.4508 (32.7460)	mem 4879MB
[2022-05-31 05:31:08 MetaFG_0] (main.py 265): INFO Train: [51/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2926 (0.3112)	loss 1.6634 (1.3418)	grad_norm 24.5737 (32.4589)	mem 4879MB
[2022-05-31 05:31:11 MetaFG_0] (main.py 265): INFO Train: [51/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2989 (0.3106)	loss 1.3806 (1.3448)	grad_norm 21.3939 (31.9357)	mem 4879MB
[2022-05-31 05:31:14 MetaFG_0] (main.py 265): INFO Train: [51/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.3003 (0.3102)	loss 1.0615 (1.3355)	grad_norm 21.2676 (31.2369)	mem 4879MB
[2022-05-31 05:31:17 MetaFG_0] (main.py 265): INFO Train: [51/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2953 (0.3098)	loss 1.5818 (1.3415)	grad_norm 20.7797 (31.1813)	mem 4879MB
[2022-05-31 05:31:20 MetaFG_0] (main.py 265): INFO Train: [51/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.3009 (0.3094)	loss 1.4692 (1.3576)	grad_norm 20.4358 (30.9239)	mem 4879MB
[2022-05-31 05:31:23 MetaFG_0] (main.py 265): INFO Train: [51/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3090 (0.3091)	loss 1.5749 (1.3569)	grad_norm 18.5122 (30.8298)	mem 4879MB
[2022-05-31 05:31:26 MetaFG_0] (main.py 265): INFO Train: [51/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2989 (0.3088)	loss 1.4190 (1.3590)	grad_norm 23.9929 (31.0247)	mem 4879MB
[2022-05-31 05:31:29 MetaFG_0] (main.py 265): INFO Train: [51/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2987 (0.3085)	loss 1.2691 (1.3559)	grad_norm 14.9140 (30.9195)	mem 4879MB
[2022-05-31 05:31:32 MetaFG_0] (main.py 265): INFO Train: [51/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2965 (0.3083)	loss 1.8126 (1.3588)	grad_norm 21.3375 (30.6992)	mem 4879MB
[2022-05-31 05:31:36 MetaFG_0] (main.py 265): INFO Train: [51/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2927 (0.3080)	loss 1.6370 (1.3581)	grad_norm 26.9018 (30.6570)	mem 4879MB
[2022-05-31 05:31:39 MetaFG_0] (main.py 265): INFO Train: [51/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2981 (0.3080)	loss 1.4045 (1.3545)	grad_norm 22.2820 (30.6547)	mem 4879MB
[2022-05-31 05:31:42 MetaFG_0] (main.py 265): INFO Train: [51/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2993 (0.3078)	loss 1.5459 (1.3543)	grad_norm 19.9546 (30.5371)	mem 4879MB
[2022-05-31 05:31:45 MetaFG_0] (main.py 265): INFO Train: [51/300][230/1562]	eta 0:06:49 lr 0.000006	time 0.2929 (0.3077)	loss 1.3222 (1.3530)	grad_norm 33.2953 (32.3778)	mem 4879MB
[2022-05-31 05:31:48 MetaFG_0] (main.py 265): INFO Train: [51/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2916 (0.3075)	loss 1.5139 (1.3540)	grad_norm 16.4437 (32.0458)	mem 4879MB
[2022-05-31 05:31:51 MetaFG_0] (main.py 265): INFO Train: [51/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2931 (0.3075)	loss 0.8998 (1.3505)	grad_norm 20.9596 (32.0889)	mem 4879MB
[2022-05-31 05:31:54 MetaFG_0] (main.py 265): INFO Train: [51/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2985 (0.3073)	loss 0.9544 (1.3515)	grad_norm 55.6665 (32.2443)	mem 4879MB
[2022-05-31 05:31:57 MetaFG_0] (main.py 265): INFO Train: [51/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.3015 (0.3072)	loss 1.6032 (1.3543)	grad_norm 17.2672 (32.0269)	mem 4879MB
[2022-05-31 05:32:00 MetaFG_0] (main.py 265): INFO Train: [51/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2981 (0.3072)	loss 1.3554 (1.3537)	grad_norm 30.7101 (31.8211)	mem 4879MB
[2022-05-31 05:32:03 MetaFG_0] (main.py 265): INFO Train: [51/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2973 (0.3071)	loss 1.3679 (1.3548)	grad_norm 23.9639 (31.7367)	mem 4879MB
[2022-05-31 05:32:06 MetaFG_0] (main.py 265): INFO Train: [51/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2981 (0.3070)	loss 1.3243 (1.3548)	grad_norm 36.1631 (31.4578)	mem 4879MB
[2022-05-31 05:32:09 MetaFG_0] (main.py 265): INFO Train: [51/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2935 (0.3069)	loss 1.2662 (1.3593)	grad_norm 57.5322 (31.4536)	mem 4879MB
[2022-05-31 05:32:12 MetaFG_0] (main.py 265): INFO Train: [51/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2927 (0.3068)	loss 1.5866 (1.3612)	grad_norm 28.5440 (31.3648)	mem 4879MB
[2022-05-31 05:32:15 MetaFG_0] (main.py 265): INFO Train: [51/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2935 (0.3067)	loss 1.5653 (1.3598)	grad_norm 47.3961 (31.3907)	mem 4879MB
[2022-05-31 05:32:18 MetaFG_0] (main.py 265): INFO Train: [51/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2916 (0.3066)	loss 1.4303 (1.3607)	grad_norm 23.3530 (31.2501)	mem 4879MB
[2022-05-31 05:32:21 MetaFG_0] (main.py 265): INFO Train: [51/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2986 (0.3067)	loss 1.6573 (1.3611)	grad_norm 30.6151 (31.1737)	mem 4879MB
[2022-05-31 05:32:24 MetaFG_0] (main.py 265): INFO Train: [51/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2935 (0.3066)	loss 1.4145 (1.3632)	grad_norm 19.3844 (31.5615)	mem 4879MB
[2022-05-31 05:32:27 MetaFG_0] (main.py 265): INFO Train: [51/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2979 (0.3066)	loss 1.3104 (1.3609)	grad_norm 49.1932 (31.5412)	mem 4879MB
[2022-05-31 05:32:30 MetaFG_0] (main.py 265): INFO Train: [51/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2926 (0.3065)	loss 1.6389 (1.3598)	grad_norm 52.2166 (31.5336)	mem 4879MB
[2022-05-31 05:32:33 MetaFG_0] (main.py 265): INFO Train: [51/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2934 (0.3064)	loss 1.7395 (1.3610)	grad_norm 40.6427 (31.4957)	mem 4879MB
[2022-05-31 05:32:36 MetaFG_0] (main.py 265): INFO Train: [51/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2944 (0.3064)	loss 1.5567 (1.3585)	grad_norm 16.5609 (31.4679)	mem 4879MB
[2022-05-31 05:32:39 MetaFG_0] (main.py 265): INFO Train: [51/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2955 (0.3063)	loss 1.7624 (1.3592)	grad_norm 97.9272 (31.6444)	mem 4879MB
[2022-05-31 05:32:43 MetaFG_0] (main.py 265): INFO Train: [51/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2920 (0.3062)	loss 1.2957 (1.3611)	grad_norm 26.1060 (31.5838)	mem 4879MB
[2022-05-31 05:32:46 MetaFG_0] (main.py 265): INFO Train: [51/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.3001 (0.3062)	loss 1.1853 (1.3641)	grad_norm 36.4401 (31.5537)	mem 4879MB
[2022-05-31 05:32:49 MetaFG_0] (main.py 265): INFO Train: [51/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2932 (0.3061)	loss 1.5618 (1.3601)	grad_norm 30.6815 (31.5236)	mem 4879MB
[2022-05-31 05:32:52 MetaFG_0] (main.py 265): INFO Train: [51/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2924 (0.3061)	loss 1.3038 (1.3628)	grad_norm 33.2788 (31.4960)	mem 4879MB
[2022-05-31 05:32:55 MetaFG_0] (main.py 265): INFO Train: [51/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2936 (0.3061)	loss 1.4388 (1.3639)	grad_norm 54.5004 (31.5153)	mem 4879MB
[2022-05-31 05:32:58 MetaFG_0] (main.py 265): INFO Train: [51/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2987 (0.3062)	loss 1.4322 (1.3643)	grad_norm 30.4963 (31.4361)	mem 4879MB
[2022-05-31 05:33:01 MetaFG_0] (main.py 265): INFO Train: [51/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2974 (0.3061)	loss 1.4979 (1.3633)	grad_norm 56.3080 (31.3985)	mem 4879MB
[2022-05-31 05:33:04 MetaFG_0] (main.py 265): INFO Train: [51/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2999 (0.3061)	loss 0.9419 (1.3608)	grad_norm 36.3601 (31.3209)	mem 4879MB
[2022-05-31 05:33:07 MetaFG_0] (main.py 265): INFO Train: [51/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2983 (0.3061)	loss 1.2623 (1.3631)	grad_norm 31.0104 (31.3253)	mem 4879MB
[2022-05-31 05:33:10 MetaFG_0] (main.py 265): INFO Train: [51/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2936 (0.3061)	loss 1.5154 (1.3616)	grad_norm 31.4877 (31.3417)	mem 4879MB
[2022-05-31 05:33:13 MetaFG_0] (main.py 265): INFO Train: [51/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2924 (0.3061)	loss 1.2469 (1.3586)	grad_norm 44.1077 (31.4267)	mem 4879MB
[2022-05-31 05:33:16 MetaFG_0] (main.py 265): INFO Train: [51/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2976 (0.3061)	loss 1.5674 (1.3579)	grad_norm 27.1687 (31.4393)	mem 4879MB
[2022-05-31 05:33:19 MetaFG_0] (main.py 265): INFO Train: [51/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2924 (0.3061)	loss 1.5894 (1.3577)	grad_norm 24.3002 (31.3187)	mem 4879MB
[2022-05-31 05:33:22 MetaFG_0] (main.py 265): INFO Train: [51/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2936 (0.3061)	loss 1.1042 (1.3606)	grad_norm 26.9864 (32.2007)	mem 4879MB
[2022-05-31 05:33:25 MetaFG_0] (main.py 265): INFO Train: [51/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2932 (0.3061)	loss 1.4619 (1.3610)	grad_norm 22.6125 (32.2411)	mem 4879MB
[2022-05-31 05:33:28 MetaFG_0] (main.py 265): INFO Train: [51/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2986 (0.3060)	loss 1.5604 (1.3602)	grad_norm 28.5877 (32.1864)	mem 4879MB
[2022-05-31 05:33:31 MetaFG_0] (main.py 265): INFO Train: [51/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.3211 (0.3061)	loss 1.5372 (1.3602)	grad_norm 18.5902 (32.0762)	mem 4879MB
[2022-05-31 05:33:35 MetaFG_0] (main.py 265): INFO Train: [51/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2981 (0.3064)	loss 1.3908 (1.3605)	grad_norm 22.5493 (32.1512)	mem 4879MB
[2022-05-31 05:33:38 MetaFG_0] (main.py 265): INFO Train: [51/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2986 (0.3064)	loss 1.4429 (1.3623)	grad_norm 41.3880 (32.1470)	mem 4879MB
[2022-05-31 05:33:41 MetaFG_0] (main.py 265): INFO Train: [51/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2925 (0.3063)	loss 1.2986 (1.3613)	grad_norm 40.4950 (32.1054)	mem 4879MB
[2022-05-31 05:33:44 MetaFG_0] (main.py 265): INFO Train: [51/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2923 (0.3063)	loss 1.4686 (1.3602)	grad_norm 29.7776 (32.0810)	mem 4879MB
[2022-05-31 05:33:47 MetaFG_0] (main.py 265): INFO Train: [51/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2936 (0.3063)	loss 1.6284 (1.3615)	grad_norm 47.8242 (32.0757)	mem 4879MB
[2022-05-31 05:33:50 MetaFG_0] (main.py 265): INFO Train: [51/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2988 (0.3063)	loss 1.4839 (1.3631)	grad_norm 25.1039 (32.0277)	mem 4879MB
[2022-05-31 05:33:53 MetaFG_0] (main.py 265): INFO Train: [51/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2918 (0.3062)	loss 0.9475 (1.3624)	grad_norm 31.9571 (31.9922)	mem 4879MB
[2022-05-31 05:33:56 MetaFG_0] (main.py 265): INFO Train: [51/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2997 (0.3062)	loss 1.2106 (1.3623)	grad_norm 27.4106 (32.0396)	mem 4879MB
[2022-05-31 05:33:59 MetaFG_0] (main.py 265): INFO Train: [51/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2995 (0.3062)	loss 1.5881 (1.3644)	grad_norm 33.1903 (32.0231)	mem 4879MB
[2022-05-31 05:34:02 MetaFG_0] (main.py 265): INFO Train: [51/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2988 (0.3062)	loss 1.3742 (1.3649)	grad_norm 25.5093 (32.0007)	mem 4879MB
[2022-05-31 05:34:05 MetaFG_0] (main.py 265): INFO Train: [51/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2927 (0.3061)	loss 1.5472 (1.3646)	grad_norm 23.0165 (31.9318)	mem 4879MB
[2022-05-31 05:34:08 MetaFG_0] (main.py 265): INFO Train: [51/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2991 (0.3061)	loss 1.6553 (1.3631)	grad_norm 22.6536 (31.9734)	mem 4879MB
[2022-05-31 05:34:11 MetaFG_0] (main.py 265): INFO Train: [51/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2989 (0.3061)	loss 1.4074 (1.3624)	grad_norm 30.8456 (31.9823)	mem 4879MB
[2022-05-31 05:34:14 MetaFG_0] (main.py 265): INFO Train: [51/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2942 (0.3061)	loss 1.3969 (1.3621)	grad_norm 32.9130 (31.9036)	mem 4879MB
[2022-05-31 05:34:17 MetaFG_0] (main.py 265): INFO Train: [51/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2926 (0.3061)	loss 1.2652 (1.3632)	grad_norm 56.1701 (31.9578)	mem 4879MB
[2022-05-31 05:34:20 MetaFG_0] (main.py 265): INFO Train: [51/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2985 (0.3061)	loss 1.1220 (1.3629)	grad_norm 22.3197 (31.9485)	mem 4879MB
[2022-05-31 05:34:23 MetaFG_0] (main.py 265): INFO Train: [51/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2973 (0.3060)	loss 1.2641 (1.3646)	grad_norm 30.7702 (31.8911)	mem 4879MB
[2022-05-31 05:34:26 MetaFG_0] (main.py 265): INFO Train: [51/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2966 (0.3060)	loss 1.2128 (1.3641)	grad_norm 24.3684 (31.8155)	mem 4879MB
[2022-05-31 05:34:30 MetaFG_0] (main.py 265): INFO Train: [51/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2992 (0.3060)	loss 1.4469 (1.3640)	grad_norm 25.0765 (31.7372)	mem 4879MB
[2022-05-31 05:34:33 MetaFG_0] (main.py 265): INFO Train: [51/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2929 (0.3060)	loss 1.4669 (1.3647)	grad_norm 41.2142 (31.8017)	mem 4879MB
[2022-05-31 05:34:36 MetaFG_0] (main.py 265): INFO Train: [51/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2924 (0.3060)	loss 1.1191 (1.3637)	grad_norm 40.8348 (31.8212)	mem 4879MB
[2022-05-31 05:34:39 MetaFG_0] (main.py 265): INFO Train: [51/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2927 (0.3060)	loss 0.9410 (1.3636)	grad_norm 32.7220 (31.7740)	mem 4879MB
[2022-05-31 05:34:42 MetaFG_0] (main.py 265): INFO Train: [51/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2945 (0.3060)	loss 1.8493 (1.3651)	grad_norm 45.0692 (31.7430)	mem 4879MB
[2022-05-31 05:34:45 MetaFG_0] (main.py 265): INFO Train: [51/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2927 (0.3059)	loss 1.6046 (1.3630)	grad_norm 15.9294 (31.7734)	mem 4879MB
[2022-05-31 05:34:48 MetaFG_0] (main.py 265): INFO Train: [51/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2993 (0.3059)	loss 1.3670 (1.3626)	grad_norm 53.2634 (31.8664)	mem 4879MB
[2022-05-31 05:34:51 MetaFG_0] (main.py 265): INFO Train: [51/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2986 (0.3059)	loss 1.1707 (1.3623)	grad_norm 53.0052 (31.9043)	mem 4879MB
[2022-05-31 05:34:54 MetaFG_0] (main.py 265): INFO Train: [51/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2934 (0.3059)	loss 1.5127 (1.3638)	grad_norm 23.9714 (31.9063)	mem 4879MB
[2022-05-31 05:34:57 MetaFG_0] (main.py 265): INFO Train: [51/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2937 (0.3059)	loss 1.2605 (1.3635)	grad_norm 26.8824 (31.8959)	mem 4879MB
[2022-05-31 05:35:00 MetaFG_0] (main.py 265): INFO Train: [51/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2996 (0.3059)	loss 1.3104 (1.3632)	grad_norm 32.1038 (31.8854)	mem 4879MB
[2022-05-31 05:35:03 MetaFG_0] (main.py 265): INFO Train: [51/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2939 (0.3058)	loss 1.4615 (1.3632)	grad_norm 19.1413 (31.8645)	mem 4879MB
[2022-05-31 05:35:06 MetaFG_0] (main.py 265): INFO Train: [51/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2940 (0.3058)	loss 1.5439 (1.3625)	grad_norm 30.6006 (31.8984)	mem 4879MB
[2022-05-31 05:35:09 MetaFG_0] (main.py 265): INFO Train: [51/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2927 (0.3058)	loss 1.1395 (1.3626)	grad_norm 27.4337 (31.8584)	mem 4879MB
[2022-05-31 05:35:12 MetaFG_0] (main.py 265): INFO Train: [51/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2919 (0.3058)	loss 1.3965 (1.3620)	grad_norm 39.0132 (31.9012)	mem 4879MB
[2022-05-31 05:35:15 MetaFG_0] (main.py 265): INFO Train: [51/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2936 (0.3057)	loss 1.2913 (1.3618)	grad_norm 38.4224 (31.8832)	mem 4879MB
[2022-05-31 05:35:18 MetaFG_0] (main.py 265): INFO Train: [51/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2952 (0.3057)	loss 1.3314 (1.3619)	grad_norm 22.0253 (31.8877)	mem 4879MB
[2022-05-31 05:35:21 MetaFG_0] (main.py 265): INFO Train: [51/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2982 (0.3057)	loss 1.6201 (1.3629)	grad_norm 17.4699 (31.8854)	mem 4879MB
[2022-05-31 05:35:24 MetaFG_0] (main.py 265): INFO Train: [51/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2941 (0.3057)	loss 1.8769 (1.3631)	grad_norm 38.0164 (31.8617)	mem 4879MB
[2022-05-31 05:35:27 MetaFG_0] (main.py 265): INFO Train: [51/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.3006 (0.3057)	loss 1.3354 (1.3630)	grad_norm 23.9968 (31.9004)	mem 4879MB
[2022-05-31 05:35:30 MetaFG_0] (main.py 265): INFO Train: [51/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2952 (0.3057)	loss 1.2404 (1.3625)	grad_norm 35.9284 (31.8733)	mem 4879MB
[2022-05-31 05:35:33 MetaFG_0] (main.py 265): INFO Train: [51/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2970 (0.3056)	loss 1.1925 (1.3628)	grad_norm 44.5429 (31.8345)	mem 4879MB
[2022-05-31 05:35:36 MetaFG_0] (main.py 265): INFO Train: [51/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.3033 (0.3056)	loss 1.6103 (1.3642)	grad_norm 21.6145 (31.7826)	mem 4879MB
[2022-05-31 05:35:40 MetaFG_0] (main.py 265): INFO Train: [51/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2949 (0.3056)	loss 1.5528 (1.3640)	grad_norm 48.7149 (31.7785)	mem 4879MB
[2022-05-31 05:35:43 MetaFG_0] (main.py 265): INFO Train: [51/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2934 (0.3056)	loss 1.4677 (1.3635)	grad_norm 26.8972 (31.8161)	mem 4879MB
[2022-05-31 05:35:46 MetaFG_0] (main.py 265): INFO Train: [51/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2998 (0.3056)	loss 1.1848 (1.3636)	grad_norm 26.7640 (31.8034)	mem 4879MB
[2022-05-31 05:35:49 MetaFG_0] (main.py 265): INFO Train: [51/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2985 (0.3056)	loss 1.3831 (1.3634)	grad_norm 20.8310 (31.7592)	mem 4879MB
[2022-05-31 05:35:52 MetaFG_0] (main.py 265): INFO Train: [51/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3001 (0.3055)	loss 1.5285 (1.3633)	grad_norm 39.7680 (31.8020)	mem 4879MB
[2022-05-31 05:35:55 MetaFG_0] (main.py 265): INFO Train: [51/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2947 (0.3055)	loss 1.3835 (1.3619)	grad_norm 22.3923 (31.7180)	mem 4879MB
[2022-05-31 05:35:58 MetaFG_0] (main.py 265): INFO Train: [51/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2936 (0.3055)	loss 1.0001 (1.3613)	grad_norm 33.1724 (31.7078)	mem 4879MB
[2022-05-31 05:36:01 MetaFG_0] (main.py 265): INFO Train: [51/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2982 (0.3055)	loss 1.5992 (1.3622)	grad_norm 29.4787 (31.7069)	mem 4879MB
[2022-05-31 05:36:04 MetaFG_0] (main.py 265): INFO Train: [51/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3017 (0.3055)	loss 1.4897 (1.3624)	grad_norm 22.2853 (31.7022)	mem 4879MB
[2022-05-31 05:36:07 MetaFG_0] (main.py 265): INFO Train: [51/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2997 (0.3055)	loss 1.1668 (1.3624)	grad_norm 49.1071 (31.6585)	mem 4879MB
[2022-05-31 05:36:10 MetaFG_0] (main.py 265): INFO Train: [51/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2932 (0.3055)	loss 1.5186 (1.3618)	grad_norm 24.2500 (31.6334)	mem 4879MB
[2022-05-31 05:36:13 MetaFG_0] (main.py 265): INFO Train: [51/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2927 (0.3055)	loss 0.9373 (1.3610)	grad_norm 37.8056 (31.6341)	mem 4879MB
[2022-05-31 05:36:16 MetaFG_0] (main.py 265): INFO Train: [51/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2941 (0.3055)	loss 1.2094 (1.3605)	grad_norm 23.2708 (31.6124)	mem 4879MB
[2022-05-31 05:36:19 MetaFG_0] (main.py 265): INFO Train: [51/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2924 (0.3054)	loss 1.3433 (1.3599)	grad_norm 30.3842 (31.5809)	mem 4879MB
[2022-05-31 05:36:22 MetaFG_0] (main.py 265): INFO Train: [51/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2930 (0.3055)	loss 1.6365 (1.3603)	grad_norm 24.1876 (31.5669)	mem 4879MB
[2022-05-31 05:36:25 MetaFG_0] (main.py 265): INFO Train: [51/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2994 (0.3054)	loss 1.3476 (1.3609)	grad_norm 30.4297 (31.5512)	mem 4879MB
[2022-05-31 05:36:28 MetaFG_0] (main.py 265): INFO Train: [51/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.3065 (0.3054)	loss 1.4442 (1.3605)	grad_norm 20.9044 (31.5621)	mem 4879MB
[2022-05-31 05:36:31 MetaFG_0] (main.py 265): INFO Train: [51/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2928 (0.3054)	loss 1.6633 (1.3610)	grad_norm 19.8720 (31.6005)	mem 4879MB
[2022-05-31 05:36:34 MetaFG_0] (main.py 265): INFO Train: [51/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2927 (0.3054)	loss 1.3976 (1.3607)	grad_norm 23.2987 (31.5904)	mem 4879MB
[2022-05-31 05:36:37 MetaFG_0] (main.py 265): INFO Train: [51/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2995 (0.3054)	loss 1.7013 (1.3609)	grad_norm 40.9992 (31.5635)	mem 4879MB
[2022-05-31 05:36:40 MetaFG_0] (main.py 265): INFO Train: [51/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2936 (0.3054)	loss 1.2251 (1.3614)	grad_norm 32.5061 (31.5567)	mem 4879MB
[2022-05-31 05:36:43 MetaFG_0] (main.py 265): INFO Train: [51/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2937 (0.3054)	loss 1.0900 (1.3613)	grad_norm 32.7715 (31.5663)	mem 4879MB
[2022-05-31 05:36:46 MetaFG_0] (main.py 265): INFO Train: [51/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2992 (0.3054)	loss 1.4341 (1.3621)	grad_norm 24.1090 (31.5684)	mem 4879MB
[2022-05-31 05:36:50 MetaFG_0] (main.py 265): INFO Train: [51/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2920 (0.3054)	loss 1.1717 (1.3626)	grad_norm 29.7946 (31.5495)	mem 4879MB
[2022-05-31 05:36:53 MetaFG_0] (main.py 265): INFO Train: [51/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2934 (0.3054)	loss 1.5715 (1.3632)	grad_norm 27.4090 (31.5482)	mem 4879MB
[2022-05-31 05:36:56 MetaFG_0] (main.py 265): INFO Train: [51/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2929 (0.3054)	loss 1.2550 (1.3627)	grad_norm 28.8422 (31.5151)	mem 4879MB
[2022-05-31 05:36:59 MetaFG_0] (main.py 265): INFO Train: [51/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2945 (0.3054)	loss 1.4572 (1.3636)	grad_norm 20.8992 (31.4902)	mem 4879MB
[2022-05-31 05:37:02 MetaFG_0] (main.py 265): INFO Train: [51/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2988 (0.3054)	loss 1.3397 (1.3641)	grad_norm 44.0064 (31.4722)	mem 4879MB
[2022-05-31 05:37:05 MetaFG_0] (main.py 265): INFO Train: [51/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2983 (0.3054)	loss 1.1140 (1.3644)	grad_norm 29.0696 (31.4726)	mem 4879MB
[2022-05-31 05:37:08 MetaFG_0] (main.py 265): INFO Train: [51/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2996 (0.3054)	loss 1.4218 (1.3649)	grad_norm 25.9389 (31.4629)	mem 4879MB
[2022-05-31 05:37:11 MetaFG_0] (main.py 265): INFO Train: [51/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.3009 (0.3054)	loss 1.3820 (1.3648)	grad_norm 45.2928 (31.4642)	mem 4879MB
[2022-05-31 05:37:14 MetaFG_0] (main.py 265): INFO Train: [51/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2936 (0.3054)	loss 1.5331 (1.3657)	grad_norm 20.3039 (31.4312)	mem 4879MB
[2022-05-31 05:37:17 MetaFG_0] (main.py 265): INFO Train: [51/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3010 (0.3054)	loss 1.0670 (1.3663)	grad_norm 46.5939 (31.4170)	mem 4879MB
[2022-05-31 05:37:20 MetaFG_0] (main.py 265): INFO Train: [51/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2982 (0.3053)	loss 1.5980 (1.3667)	grad_norm 29.3520 (31.4151)	mem 4879MB
[2022-05-31 05:37:23 MetaFG_0] (main.py 265): INFO Train: [51/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2928 (0.3053)	loss 1.3171 (1.3673)	grad_norm 35.6548 (31.3649)	mem 4879MB
[2022-05-31 05:37:26 MetaFG_0] (main.py 265): INFO Train: [51/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2926 (0.3053)	loss 1.4940 (1.3683)	grad_norm 23.0078 (31.3746)	mem 4879MB
[2022-05-31 05:37:29 MetaFG_0] (main.py 265): INFO Train: [51/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.3013 (0.3053)	loss 1.1467 (1.3681)	grad_norm 43.6413 (31.3522)	mem 4879MB
[2022-05-31 05:37:32 MetaFG_0] (main.py 265): INFO Train: [51/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2987 (0.3053)	loss 1.7796 (1.3674)	grad_norm 27.5647 (31.3386)	mem 4879MB
[2022-05-31 05:37:35 MetaFG_0] (main.py 265): INFO Train: [51/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2928 (0.3053)	loss 1.2960 (1.3675)	grad_norm 31.2543 (31.3329)	mem 4879MB
[2022-05-31 05:37:38 MetaFG_0] (main.py 265): INFO Train: [51/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2927 (0.3053)	loss 1.5541 (1.3675)	grad_norm 45.9700 (31.3546)	mem 4879MB
[2022-05-31 05:37:41 MetaFG_0] (main.py 265): INFO Train: [51/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2993 (0.3053)	loss 1.2834 (1.3675)	grad_norm 26.9795 (31.3309)	mem 4879MB
[2022-05-31 05:37:44 MetaFG_0] (main.py 265): INFO Train: [51/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2983 (0.3053)	loss 1.3320 (1.3674)	grad_norm 28.7914 (31.2961)	mem 4879MB
[2022-05-31 05:37:47 MetaFG_0] (main.py 265): INFO Train: [51/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3002 (0.3053)	loss 0.8721 (1.3673)	grad_norm 22.9811 (31.2655)	mem 4879MB
[2022-05-31 05:37:51 MetaFG_0] (main.py 265): INFO Train: [51/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2929 (0.3053)	loss 1.4675 (1.3670)	grad_norm 32.2627 (31.2622)	mem 4879MB
[2022-05-31 05:37:54 MetaFG_0] (main.py 265): INFO Train: [51/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2993 (0.3053)	loss 1.2445 (1.3668)	grad_norm 32.3358 (31.3210)	mem 4879MB
[2022-05-31 05:37:57 MetaFG_0] (main.py 265): INFO Train: [51/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2938 (0.3053)	loss 1.4433 (1.3668)	grad_norm 21.6842 (31.3391)	mem 4879MB
[2022-05-31 05:38:00 MetaFG_0] (main.py 265): INFO Train: [51/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2934 (0.3053)	loss 1.4927 (1.3663)	grad_norm 34.0754 (31.3598)	mem 4879MB
[2022-05-31 05:38:03 MetaFG_0] (main.py 265): INFO Train: [51/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2937 (0.3053)	loss 0.9837 (1.3664)	grad_norm 35.0996 (31.3398)	mem 4879MB
[2022-05-31 05:38:06 MetaFG_0] (main.py 265): INFO Train: [51/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2980 (0.3053)	loss 1.4589 (1.3665)	grad_norm 47.2288 (31.3578)	mem 4879MB
[2022-05-31 05:38:09 MetaFG_0] (main.py 265): INFO Train: [51/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2920 (0.3053)	loss 1.4171 (1.3669)	grad_norm 33.0008 (31.3502)	mem 4879MB
[2022-05-31 05:38:12 MetaFG_0] (main.py 265): INFO Train: [51/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2999 (0.3053)	loss 1.5722 (1.3670)	grad_norm 25.3419 (31.3532)	mem 4879MB
[2022-05-31 05:38:15 MetaFG_0] (main.py 265): INFO Train: [51/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2921 (0.3053)	loss 1.5857 (1.3665)	grad_norm 28.1092 (31.3175)	mem 4879MB
[2022-05-31 05:38:18 MetaFG_0] (main.py 265): INFO Train: [51/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2925 (0.3053)	loss 1.0857 (1.3669)	grad_norm 18.3223 (31.2942)	mem 4879MB
[2022-05-31 05:38:21 MetaFG_0] (main.py 265): INFO Train: [51/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2990 (0.3052)	loss 1.0499 (1.3666)	grad_norm 41.7384 (31.2659)	mem 4879MB
[2022-05-31 05:38:24 MetaFG_0] (main.py 265): INFO Train: [51/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2935 (0.3052)	loss 1.0428 (1.3662)	grad_norm 54.0695 (31.2789)	mem 4879MB
[2022-05-31 05:38:27 MetaFG_0] (main.py 265): INFO Train: [51/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3005 (0.3052)	loss 1.5779 (1.3663)	grad_norm 28.3010 (31.2694)	mem 4879MB
[2022-05-31 05:38:30 MetaFG_0] (main.py 265): INFO Train: [51/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2920 (0.3052)	loss 1.0447 (1.3656)	grad_norm 27.7854 (31.3012)	mem 4879MB
[2022-05-31 05:38:30 MetaFG_0] (main.py 272): INFO EPOCH 51 training takes 0:07:56
[2022-05-31 05:38:30 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_51.pth saving......
[2022-05-31 05:38:31 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_51.pth saved !!!
[2022-05-31 05:38:31 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:38:33 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:38:33 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:38:33 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.639 (0.639)	Loss 0.5695 (0.5695)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 05:38:34 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.095 (0.149)	Loss 0.8064 (0.5992)	Acc@1 78.125 (86.080)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 05:38:35 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.103 (0.125)	Loss 0.3600 (0.6199)	Acc@1 90.625 (85.417)	Acc@5 100.000 (98.214)	Mem 4879MB
[2022-05-31 05:38:36 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.193 (0.119)	Loss 0.6419 (0.6039)	Acc@1 78.125 (86.593)	Acc@5 96.875 (98.085)	Mem 4879MB
[2022-05-31 05:38:38 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.122 (0.119)	Loss 0.5577 (0.5989)	Acc@1 90.625 (86.738)	Acc@5 100.000 (98.247)	Mem 4879MB
[2022-05-31 05:38:39 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.095 (0.118)	Loss 0.4626 (0.5965)	Acc@1 87.500 (86.458)	Acc@5 100.000 (98.284)	Mem 4879MB
[2022-05-31 05:38:40 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.114)	Loss 0.7647 (0.5993)	Acc@1 78.125 (86.270)	Acc@5 96.875 (98.309)	Mem 4879MB
[2022-05-31 05:38:41 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.098 (0.111)	Loss 0.7835 (0.6027)	Acc@1 78.125 (85.960)	Acc@5 100.000 (98.415)	Mem 4879MB
[2022-05-31 05:38:42 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.088 (0.109)	Loss 0.5418 (0.5999)	Acc@1 90.625 (85.802)	Acc@5 100.000 (98.611)	Mem 4879MB
[2022-05-31 05:38:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.091 (0.108)	Loss 0.2863 (0.6010)	Acc@1 96.875 (85.817)	Acc@5 100.000 (98.695)	Mem 4879MB
[2022-05-31 05:38:43 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.106)	Loss 0.4843 (0.5959)	Acc@1 90.625 (86.200)	Acc@5 100.000 (98.700)	Mem 4879MB
[2022-05-31 05:38:44 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.105)	Loss 0.4626 (0.5912)	Acc@1 87.500 (86.289)	Acc@5 100.000 (98.761)	Mem 4879MB
[2022-05-31 05:38:45 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.105)	Loss 0.7122 (0.5824)	Acc@1 81.250 (86.674)	Acc@5 96.875 (98.838)	Mem 4879MB
[2022-05-31 05:38:46 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.104)	Loss 0.5854 (0.5800)	Acc@1 87.500 (86.665)	Acc@5 100.000 (98.855)	Mem 4879MB
[2022-05-31 05:38:47 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.103)	Loss 0.5910 (0.5805)	Acc@1 84.375 (86.702)	Acc@5 96.875 (98.825)	Mem 4879MB
[2022-05-31 05:38:48 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.102)	Loss 0.5734 (0.5853)	Acc@1 87.500 (86.486)	Acc@5 100.000 (98.758)	Mem 4879MB
[2022-05-31 05:38:49 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.102)	Loss 0.5240 (0.5833)	Acc@1 90.625 (86.471)	Acc@5 96.875 (98.758)	Mem 4879MB
[2022-05-31 05:38:50 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.102)	Loss 0.5627 (0.5847)	Acc@1 75.000 (86.294)	Acc@5 100.000 (98.721)	Mem 4879MB
[2022-05-31 05:38:51 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.101)	Loss 0.6315 (0.5859)	Acc@1 90.625 (86.360)	Acc@5 96.875 (98.705)	Mem 4879MB
[2022-05-31 05:38:52 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.101)	Loss 0.5802 (0.5870)	Acc@1 84.375 (86.224)	Acc@5 100.000 (98.658)	Mem 4879MB
[2022-05-31 05:38:53 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.089 (0.101)	Loss 0.4029 (0.5855)	Acc@1 93.750 (86.272)	Acc@5 100.000 (98.663)	Mem 4879MB
[2022-05-31 05:38:54 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.100)	Loss 0.8472 (0.5865)	Acc@1 81.250 (86.182)	Acc@5 96.875 (98.652)	Mem 4879MB
[2022-05-31 05:38:55 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.100)	Loss 0.5612 (0.5875)	Acc@1 84.375 (86.143)	Acc@5 100.000 (98.657)	Mem 4879MB
[2022-05-31 05:38:56 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.100)	Loss 0.5733 (0.5862)	Acc@1 84.375 (86.066)	Acc@5 100.000 (98.674)	Mem 4879MB
[2022-05-31 05:38:57 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.099)	Loss 0.4741 (0.5874)	Acc@1 90.625 (85.983)	Acc@5 96.875 (98.703)	Mem 4879MB
[2022-05-31 05:38:58 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.099)	Loss 0.3824 (0.5905)	Acc@1 93.750 (85.844)	Acc@5 100.000 (98.655)	Mem 4879MB
[2022-05-31 05:38:59 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.099)	Loss 0.2864 (0.5880)	Acc@1 93.750 (85.943)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 05:39:00 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.099)	Loss 0.4654 (0.5890)	Acc@1 90.625 (85.966)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 05:39:01 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.099)	Loss 0.3919 (0.5885)	Acc@1 93.750 (85.965)	Acc@5 100.000 (98.621)	Mem 4879MB
[2022-05-31 05:39:01 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.099)	Loss 0.6416 (0.5894)	Acc@1 87.500 (85.911)	Acc@5 100.000 (98.593)	Mem 4879MB
[2022-05-31 05:39:02 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.103 (0.099)	Loss 0.6438 (0.5885)	Acc@1 84.375 (85.932)	Acc@5 100.000 (98.588)	Mem 4879MB
[2022-05-31 05:39:03 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.098)	Loss 0.4456 (0.5877)	Acc@1 93.750 (85.993)	Acc@5 100.000 (98.593)	Mem 4879MB
[2022-05-31 05:39:04 MetaFG_0] (main.py 330): INFO  * Acc@1 86.000 Acc@5 98.600
[2022-05-31 05:39:04 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.0%
[2022-05-31 05:39:04 MetaFG_0] (main.py 171): INFO Max accuracy: 86.00%
[2022-05-31 05:39:05 MetaFG_0] (main.py 265): INFO Train: [52/300][0/1562]	eta 0:28:18 lr 0.000006	time 1.0872 (1.0872)	loss 1.2696 (1.2696)	grad_norm 25.1026 (25.1026)	mem 4879MB
[2022-05-31 05:39:08 MetaFG_0] (main.py 265): INFO Train: [52/300][10/1562]	eta 0:09:44 lr 0.000006	time 0.2937 (0.3768)	loss 1.5984 (1.3610)	grad_norm 34.4610 (28.2783)	mem 4879MB
[2022-05-31 05:39:11 MetaFG_0] (main.py 265): INFO Train: [52/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.3014 (0.3430)	loss 0.9761 (1.3246)	grad_norm 50.4453 (28.4528)	mem 4879MB
[2022-05-31 05:39:14 MetaFG_0] (main.py 265): INFO Train: [52/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.2926 (0.3309)	loss 1.5040 (1.3338)	grad_norm 16.7996 (30.1503)	mem 4879MB
[2022-05-31 05:39:17 MetaFG_0] (main.py 265): INFO Train: [52/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2928 (0.3239)	loss 1.4631 (1.3484)	grad_norm 38.6501 (29.3287)	mem 4879MB
[2022-05-31 05:39:20 MetaFG_0] (main.py 265): INFO Train: [52/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.3024 (0.3198)	loss 1.1181 (1.3225)	grad_norm 71.8546 (29.4262)	mem 4879MB
[2022-05-31 05:39:23 MetaFG_0] (main.py 265): INFO Train: [52/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2979 (0.3173)	loss 0.9961 (1.3191)	grad_norm 35.8846 (30.0377)	mem 4879MB
[2022-05-31 05:39:26 MetaFG_0] (main.py 265): INFO Train: [52/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2980 (0.3154)	loss 1.4274 (1.3290)	grad_norm 26.3689 (29.6672)	mem 4879MB
[2022-05-31 05:39:29 MetaFG_0] (main.py 265): INFO Train: [52/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2938 (0.3141)	loss 1.0458 (1.3201)	grad_norm 52.6173 (29.9331)	mem 4879MB
[2022-05-31 05:39:32 MetaFG_0] (main.py 265): INFO Train: [52/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2927 (0.3132)	loss 1.3528 (1.3243)	grad_norm 31.1738 (30.0975)	mem 4879MB
[2022-05-31 05:39:35 MetaFG_0] (main.py 265): INFO Train: [52/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2925 (0.3124)	loss 1.6373 (1.3217)	grad_norm 30.8208 (30.5088)	mem 4879MB
[2022-05-31 05:39:38 MetaFG_0] (main.py 265): INFO Train: [52/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2930 (0.3116)	loss 1.5642 (1.3252)	grad_norm 35.8970 (30.4809)	mem 4879MB
[2022-05-31 05:39:41 MetaFG_0] (main.py 265): INFO Train: [52/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2926 (0.3111)	loss 1.7246 (1.3277)	grad_norm 40.7167 (30.4934)	mem 4879MB
[2022-05-31 05:39:44 MetaFG_0] (main.py 265): INFO Train: [52/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2928 (0.3106)	loss 1.4170 (1.3244)	grad_norm 17.6516 (30.2259)	mem 4879MB
[2022-05-31 05:39:47 MetaFG_0] (main.py 265): INFO Train: [52/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2942 (0.3101)	loss 1.5321 (1.3326)	grad_norm 16.9941 (30.1488)	mem 4879MB
[2022-05-31 05:39:50 MetaFG_0] (main.py 265): INFO Train: [52/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2936 (0.3097)	loss 1.2010 (1.3328)	grad_norm 37.3083 (30.0388)	mem 4879MB
[2022-05-31 05:39:53 MetaFG_0] (main.py 265): INFO Train: [52/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2936 (0.3095)	loss 1.5121 (1.3365)	grad_norm 22.4221 (30.1588)	mem 4879MB
[2022-05-31 05:39:56 MetaFG_0] (main.py 265): INFO Train: [52/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2992 (0.3091)	loss 1.2242 (1.3343)	grad_norm 44.2337 (30.2345)	mem 4879MB
[2022-05-31 05:39:59 MetaFG_0] (main.py 265): INFO Train: [52/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2945 (0.3088)	loss 1.6459 (1.3340)	grad_norm 23.6719 (29.9464)	mem 4879MB
[2022-05-31 05:40:03 MetaFG_0] (main.py 265): INFO Train: [52/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2933 (0.3087)	loss 1.5729 (1.3298)	grad_norm 20.0470 (29.7115)	mem 4879MB
[2022-05-31 05:40:06 MetaFG_0] (main.py 265): INFO Train: [52/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2930 (0.3084)	loss 1.5459 (1.3361)	grad_norm 49.3761 (29.9609)	mem 4879MB
[2022-05-31 05:40:09 MetaFG_0] (main.py 265): INFO Train: [52/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2981 (0.3082)	loss 1.1692 (1.3356)	grad_norm 33.1689 (30.0380)	mem 4879MB
[2022-05-31 05:40:12 MetaFG_0] (main.py 265): INFO Train: [52/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2927 (0.3080)	loss 1.4713 (1.3357)	grad_norm 23.5939 (30.2091)	mem 4879MB
[2022-05-31 05:40:15 MetaFG_0] (main.py 265): INFO Train: [52/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2939 (0.3078)	loss 1.6307 (1.3433)	grad_norm 26.4841 (30.0456)	mem 4879MB
[2022-05-31 05:40:18 MetaFG_0] (main.py 265): INFO Train: [52/300][240/1562]	eta 0:06:46 lr 0.000006	time 0.2920 (0.3076)	loss 1.5082 (1.3459)	grad_norm 22.6215 (29.9339)	mem 4879MB
[2022-05-31 05:40:21 MetaFG_0] (main.py 265): INFO Train: [52/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2928 (0.3074)	loss 1.2102 (1.3453)	grad_norm 18.2983 (29.7102)	mem 4879MB
[2022-05-31 05:40:24 MetaFG_0] (main.py 265): INFO Train: [52/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2926 (0.3072)	loss 1.6557 (1.3480)	grad_norm 68.7292 (29.7840)	mem 4879MB
[2022-05-31 05:40:27 MetaFG_0] (main.py 265): INFO Train: [52/300][270/1562]	eta 0:06:36 lr 0.000006	time 0.2916 (0.3072)	loss 0.9562 (1.3427)	grad_norm 21.7331 (29.7698)	mem 4879MB
[2022-05-31 05:40:30 MetaFG_0] (main.py 265): INFO Train: [52/300][280/1562]	eta 0:06:33 lr 0.000006	time 0.2927 (0.3070)	loss 1.1027 (1.3395)	grad_norm 30.2673 (29.9022)	mem 4879MB
[2022-05-31 05:40:33 MetaFG_0] (main.py 265): INFO Train: [52/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2926 (0.3070)	loss 1.6027 (1.3417)	grad_norm 38.4299 (29.9768)	mem 4879MB
[2022-05-31 05:40:36 MetaFG_0] (main.py 265): INFO Train: [52/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2997 (0.3069)	loss 1.4002 (1.3431)	grad_norm 19.7190 (29.9634)	mem 4879MB
[2022-05-31 05:40:39 MetaFG_0] (main.py 265): INFO Train: [52/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2937 (0.3068)	loss 1.3718 (1.3428)	grad_norm 25.9995 (30.0556)	mem 4879MB
[2022-05-31 05:40:42 MetaFG_0] (main.py 265): INFO Train: [52/300][320/1562]	eta 0:06:20 lr 0.000006	time 0.2986 (0.3068)	loss 1.1126 (1.3407)	grad_norm 24.8154 (30.1959)	mem 4879MB
[2022-05-31 05:40:45 MetaFG_0] (main.py 265): INFO Train: [52/300][330/1562]	eta 0:06:17 lr 0.000006	time 0.2988 (0.3067)	loss 1.5356 (1.3404)	grad_norm 37.5355 (30.2673)	mem 4879MB
[2022-05-31 05:40:48 MetaFG_0] (main.py 265): INFO Train: [52/300][340/1562]	eta 0:06:14 lr 0.000006	time 0.2924 (0.3066)	loss 1.2134 (1.3393)	grad_norm 26.9329 (30.3112)	mem 4879MB
[2022-05-31 05:40:51 MetaFG_0] (main.py 265): INFO Train: [52/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2992 (0.3066)	loss 1.4478 (1.3434)	grad_norm 37.8003 (30.4469)	mem 4879MB
[2022-05-31 05:40:54 MetaFG_0] (main.py 265): INFO Train: [52/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2937 (0.3066)	loss 0.9929 (1.3432)	grad_norm 29.0747 (30.5345)	mem 4879MB
[2022-05-31 05:40:57 MetaFG_0] (main.py 265): INFO Train: [52/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2938 (0.3065)	loss 1.2060 (1.3432)	grad_norm 31.7523 (30.5072)	mem 4879MB
[2022-05-31 05:41:00 MetaFG_0] (main.py 265): INFO Train: [52/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2955 (0.3064)	loss 1.0893 (1.3453)	grad_norm 31.3430 (30.4283)	mem 4879MB
[2022-05-31 05:41:03 MetaFG_0] (main.py 265): INFO Train: [52/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2984 (0.3064)	loss 1.4773 (1.3448)	grad_norm 28.1817 (30.4892)	mem 4879MB
[2022-05-31 05:41:06 MetaFG_0] (main.py 265): INFO Train: [52/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.3012 (0.3064)	loss 1.5495 (1.3457)	grad_norm 42.2525 (30.4759)	mem 4879MB
[2022-05-31 05:41:09 MetaFG_0] (main.py 265): INFO Train: [52/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2976 (0.3063)	loss 1.5557 (1.3487)	grad_norm 32.0415 (30.5434)	mem 4879MB
[2022-05-31 05:41:12 MetaFG_0] (main.py 265): INFO Train: [52/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.3008 (0.3063)	loss 1.5089 (1.3479)	grad_norm 23.3721 (30.4974)	mem 4879MB
[2022-05-31 05:41:16 MetaFG_0] (main.py 265): INFO Train: [52/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2948 (0.3062)	loss 1.2496 (1.3467)	grad_norm 21.2563 (30.4966)	mem 4879MB
[2022-05-31 05:41:19 MetaFG_0] (main.py 265): INFO Train: [52/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2947 (0.3062)	loss 1.5628 (1.3506)	grad_norm 26.4890 (30.3727)	mem 4879MB
[2022-05-31 05:41:22 MetaFG_0] (main.py 265): INFO Train: [52/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2956 (0.3061)	loss 1.6761 (1.3526)	grad_norm 40.3680 (30.4243)	mem 4879MB
[2022-05-31 05:41:25 MetaFG_0] (main.py 265): INFO Train: [52/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2936 (0.3061)	loss 1.2723 (1.3525)	grad_norm 41.5331 (30.3572)	mem 4879MB
[2022-05-31 05:41:28 MetaFG_0] (main.py 265): INFO Train: [52/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2931 (0.3061)	loss 1.2616 (1.3520)	grad_norm 45.6510 (30.3517)	mem 4879MB
[2022-05-31 05:41:31 MetaFG_0] (main.py 265): INFO Train: [52/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2978 (0.3060)	loss 1.4235 (1.3524)	grad_norm 28.7038 (30.2619)	mem 4879MB
[2022-05-31 05:41:34 MetaFG_0] (main.py 265): INFO Train: [52/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.3038 (0.3061)	loss 1.2506 (1.3535)	grad_norm 31.6488 (30.2575)	mem 4879MB
[2022-05-31 05:41:37 MetaFG_0] (main.py 265): INFO Train: [52/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2938 (0.3060)	loss 1.3417 (1.3534)	grad_norm 33.8549 (30.3104)	mem 4879MB
[2022-05-31 05:41:40 MetaFG_0] (main.py 265): INFO Train: [52/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2937 (0.3060)	loss 1.4643 (1.3553)	grad_norm 24.7630 (30.4500)	mem 4879MB
[2022-05-31 05:41:43 MetaFG_0] (main.py 265): INFO Train: [52/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2989 (0.3060)	loss 1.4472 (1.3542)	grad_norm 43.2138 (30.3553)	mem 4879MB
[2022-05-31 05:41:46 MetaFG_0] (main.py 265): INFO Train: [52/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2981 (0.3060)	loss 1.4560 (1.3542)	grad_norm 25.6250 (30.2826)	mem 4879MB
[2022-05-31 05:41:49 MetaFG_0] (main.py 265): INFO Train: [52/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.3003 (0.3060)	loss 1.6773 (1.3540)	grad_norm 29.0575 (30.3178)	mem 4879MB
[2022-05-31 05:41:52 MetaFG_0] (main.py 265): INFO Train: [52/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2981 (0.3060)	loss 1.5891 (1.3547)	grad_norm 26.7280 (30.2679)	mem 4879MB
[2022-05-31 05:41:55 MetaFG_0] (main.py 265): INFO Train: [52/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.3004 (0.3060)	loss 1.0797 (1.3552)	grad_norm 17.3075 (30.2725)	mem 4879MB
[2022-05-31 05:41:58 MetaFG_0] (main.py 265): INFO Train: [52/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2931 (0.3059)	loss 1.4329 (1.3555)	grad_norm 24.4414 (30.3636)	mem 4879MB
[2022-05-31 05:42:01 MetaFG_0] (main.py 265): INFO Train: [52/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2920 (0.3059)	loss 1.5224 (1.3559)	grad_norm 29.0450 (30.3282)	mem 4879MB
[2022-05-31 05:42:04 MetaFG_0] (main.py 265): INFO Train: [52/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2966 (0.3059)	loss 1.6280 (1.3586)	grad_norm 29.1682 (30.3349)	mem 4879MB
[2022-05-31 05:42:07 MetaFG_0] (main.py 265): INFO Train: [52/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2946 (0.3059)	loss 1.6389 (1.3595)	grad_norm 50.0462 (30.3158)	mem 4879MB
[2022-05-31 05:42:10 MetaFG_0] (main.py 265): INFO Train: [52/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2954 (0.3059)	loss 1.3453 (1.3593)	grad_norm 33.0041 (30.3506)	mem 4879MB
[2022-05-31 05:42:13 MetaFG_0] (main.py 265): INFO Train: [52/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3000 (0.3058)	loss 1.5677 (1.3604)	grad_norm 33.6253 (30.3455)	mem 4879MB
[2022-05-31 05:42:17 MetaFG_0] (main.py 265): INFO Train: [52/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2999 (0.3058)	loss 1.7773 (1.3622)	grad_norm 24.3534 (30.3429)	mem 4879MB
[2022-05-31 05:42:20 MetaFG_0] (main.py 265): INFO Train: [52/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2933 (0.3058)	loss 1.1724 (1.3616)	grad_norm 20.4770 (30.3410)	mem 4879MB
[2022-05-31 05:42:23 MetaFG_0] (main.py 265): INFO Train: [52/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2947 (0.3058)	loss 1.3768 (1.3616)	grad_norm 30.0427 (30.3826)	mem 4879MB
[2022-05-31 05:42:26 MetaFG_0] (main.py 265): INFO Train: [52/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2937 (0.3058)	loss 1.1208 (1.3600)	grad_norm 23.3956 (30.3726)	mem 4879MB
[2022-05-31 05:42:29 MetaFG_0] (main.py 265): INFO Train: [52/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2923 (0.3057)	loss 1.3827 (1.3604)	grad_norm 20.7661 (30.4140)	mem 4879MB
[2022-05-31 05:42:32 MetaFG_0] (main.py 265): INFO Train: [52/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2983 (0.3057)	loss 1.5826 (1.3623)	grad_norm 23.5744 (30.3686)	mem 4879MB
[2022-05-31 05:42:35 MetaFG_0] (main.py 265): INFO Train: [52/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2934 (0.3057)	loss 1.6201 (1.3618)	grad_norm 37.3449 (30.4487)	mem 4879MB
[2022-05-31 05:42:38 MetaFG_0] (main.py 265): INFO Train: [52/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2939 (0.3056)	loss 1.4046 (1.3607)	grad_norm 22.0053 (30.5005)	mem 4879MB
[2022-05-31 05:42:41 MetaFG_0] (main.py 265): INFO Train: [52/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2942 (0.3057)	loss 1.7062 (1.3607)	grad_norm 31.0881 (30.4906)	mem 4879MB
[2022-05-31 05:42:44 MetaFG_0] (main.py 265): INFO Train: [52/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2995 (0.3056)	loss 0.9073 (1.3597)	grad_norm 35.8633 (30.4739)	mem 4879MB
[2022-05-31 05:42:47 MetaFG_0] (main.py 265): INFO Train: [52/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2930 (0.3056)	loss 1.3208 (1.3597)	grad_norm 15.7453 (30.5529)	mem 4879MB
[2022-05-31 05:42:50 MetaFG_0] (main.py 265): INFO Train: [52/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2937 (0.3056)	loss 1.3963 (1.3604)	grad_norm 42.9024 (30.5818)	mem 4879MB
[2022-05-31 05:42:53 MetaFG_0] (main.py 265): INFO Train: [52/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2947 (0.3056)	loss 1.6119 (1.3610)	grad_norm 30.3907 (30.5586)	mem 4879MB
[2022-05-31 05:42:56 MetaFG_0] (main.py 265): INFO Train: [52/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2934 (0.3056)	loss 0.9200 (1.3612)	grad_norm 23.6623 (30.5419)	mem 4879MB
[2022-05-31 05:42:59 MetaFG_0] (main.py 265): INFO Train: [52/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2948 (0.3055)	loss 1.4486 (1.3628)	grad_norm 19.3199 (30.5293)	mem 4879MB
[2022-05-31 05:43:02 MetaFG_0] (main.py 265): INFO Train: [52/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2921 (0.3055)	loss 1.5939 (1.3633)	grad_norm 44.8280 (30.5354)	mem 4879MB
[2022-05-31 05:43:05 MetaFG_0] (main.py 265): INFO Train: [52/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2998 (0.3055)	loss 0.9628 (1.3633)	grad_norm 51.7057 (30.5479)	mem 4879MB
[2022-05-31 05:43:08 MetaFG_0] (main.py 265): INFO Train: [52/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2964 (0.3055)	loss 1.3550 (1.3641)	grad_norm 34.4452 (30.5504)	mem 4879MB
[2022-05-31 05:43:11 MetaFG_0] (main.py 265): INFO Train: [52/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2987 (0.3055)	loss 1.2083 (1.3642)	grad_norm 20.3576 (30.5427)	mem 4879MB
[2022-05-31 05:43:14 MetaFG_0] (main.py 265): INFO Train: [52/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2931 (0.3055)	loss 1.4679 (1.3647)	grad_norm 22.4390 (30.5626)	mem 4879MB
[2022-05-31 05:43:17 MetaFG_0] (main.py 265): INFO Train: [52/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2998 (0.3055)	loss 1.4578 (1.3658)	grad_norm 13.9084 (30.5627)	mem 4879MB
[2022-05-31 05:43:21 MetaFG_0] (main.py 265): INFO Train: [52/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2924 (0.3055)	loss 1.5342 (1.3664)	grad_norm 24.5637 (30.5656)	mem 4879MB
[2022-05-31 05:43:24 MetaFG_0] (main.py 265): INFO Train: [52/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2928 (0.3056)	loss 1.5125 (1.3674)	grad_norm 64.7033 (30.6168)	mem 4879MB
[2022-05-31 05:43:27 MetaFG_0] (main.py 265): INFO Train: [52/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2991 (0.3055)	loss 1.4078 (1.3680)	grad_norm 20.5789 (30.5857)	mem 4879MB
[2022-05-31 05:43:30 MetaFG_0] (main.py 265): INFO Train: [52/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2929 (0.3055)	loss 1.3600 (1.3682)	grad_norm 44.5397 (30.6352)	mem 4879MB
[2022-05-31 05:43:33 MetaFG_0] (main.py 265): INFO Train: [52/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2930 (0.3055)	loss 1.5930 (1.3691)	grad_norm 23.7964 (30.6153)	mem 4879MB
[2022-05-31 05:43:36 MetaFG_0] (main.py 265): INFO Train: [52/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2927 (0.3055)	loss 1.4893 (1.3685)	grad_norm 22.8911 (30.6790)	mem 4879MB
[2022-05-31 05:43:39 MetaFG_0] (main.py 265): INFO Train: [52/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2999 (0.3055)	loss 1.3834 (1.3690)	grad_norm 14.3491 (30.6734)	mem 4879MB
[2022-05-31 05:43:42 MetaFG_0] (main.py 265): INFO Train: [52/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2977 (0.3055)	loss 0.8087 (1.3682)	grad_norm 21.5807 (30.6649)	mem 4879MB
[2022-05-31 05:43:45 MetaFG_0] (main.py 265): INFO Train: [52/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2926 (0.3058)	loss 1.5728 (1.3686)	grad_norm 23.2802 (30.6756)	mem 4879MB
[2022-05-31 05:43:48 MetaFG_0] (main.py 265): INFO Train: [52/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2965 (0.3057)	loss 1.4301 (1.3682)	grad_norm 22.5684 (30.6802)	mem 4879MB
[2022-05-31 05:43:51 MetaFG_0] (main.py 265): INFO Train: [52/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3011 (0.3057)	loss 1.3706 (1.3677)	grad_norm 15.4040 (30.6751)	mem 4879MB
[2022-05-31 05:43:54 MetaFG_0] (main.py 265): INFO Train: [52/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2939 (0.3057)	loss 1.4130 (1.3689)	grad_norm 31.3729 (30.6682)	mem 4879MB
[2022-05-31 05:43:57 MetaFG_0] (main.py 265): INFO Train: [52/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.3022 (0.3057)	loss 1.4309 (1.3685)	grad_norm 43.7284 (30.7175)	mem 4879MB
[2022-05-31 05:44:00 MetaFG_0] (main.py 265): INFO Train: [52/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2929 (0.3057)	loss 1.1225 (1.3681)	grad_norm 28.0002 (30.6737)	mem 4879MB
[2022-05-31 05:44:03 MetaFG_0] (main.py 265): INFO Train: [52/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.2924 (0.3057)	loss 1.2357 (1.3688)	grad_norm 39.7669 (30.7182)	mem 4879MB
[2022-05-31 05:44:06 MetaFG_0] (main.py 265): INFO Train: [52/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2949 (0.3057)	loss 1.4480 (1.3684)	grad_norm 50.3990 (30.7031)	mem 4879MB
[2022-05-31 05:44:10 MetaFG_0] (main.py 265): INFO Train: [52/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2938 (0.3057)	loss 1.5029 (1.3679)	grad_norm 30.7800 (30.7113)	mem 4879MB
[2022-05-31 05:44:13 MetaFG_0] (main.py 265): INFO Train: [52/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2934 (0.3056)	loss 1.5391 (1.3691)	grad_norm 42.8939 (30.6917)	mem 4879MB
[2022-05-31 05:44:16 MetaFG_0] (main.py 265): INFO Train: [52/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2990 (0.3057)	loss 1.6714 (1.3682)	grad_norm 40.7062 (30.7064)	mem 4879MB
[2022-05-31 05:44:19 MetaFG_0] (main.py 265): INFO Train: [52/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3000 (0.3056)	loss 1.4058 (1.3678)	grad_norm 29.0198 (30.7107)	mem 4879MB
[2022-05-31 05:44:22 MetaFG_0] (main.py 265): INFO Train: [52/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2987 (0.3056)	loss 1.4855 (1.3676)	grad_norm 17.2852 (30.6556)	mem 4879MB
[2022-05-31 05:44:25 MetaFG_0] (main.py 265): INFO Train: [52/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2954 (0.3056)	loss 1.4004 (1.3674)	grad_norm 23.4836 (30.6496)	mem 4879MB
[2022-05-31 05:44:28 MetaFG_0] (main.py 265): INFO Train: [52/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2988 (0.3056)	loss 1.3518 (1.3674)	grad_norm 19.7959 (30.6570)	mem 4879MB
[2022-05-31 05:44:31 MetaFG_0] (main.py 265): INFO Train: [52/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2946 (0.3056)	loss 1.5145 (1.3674)	grad_norm 29.6671 (30.6980)	mem 4879MB
[2022-05-31 05:44:34 MetaFG_0] (main.py 265): INFO Train: [52/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2928 (0.3056)	loss 1.2885 (1.3677)	grad_norm 17.7232 (30.6981)	mem 4879MB
[2022-05-31 05:44:37 MetaFG_0] (main.py 265): INFO Train: [52/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2951 (0.3055)	loss 1.7737 (1.3679)	grad_norm 36.0161 (30.7333)	mem 4879MB
[2022-05-31 05:44:40 MetaFG_0] (main.py 265): INFO Train: [52/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2981 (0.3055)	loss 1.5702 (1.3668)	grad_norm 24.9632 (30.6637)	mem 4879MB
[2022-05-31 05:44:43 MetaFG_0] (main.py 265): INFO Train: [52/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2985 (0.3055)	loss 1.1136 (1.3659)	grad_norm 35.5024 (30.6530)	mem 4879MB
[2022-05-31 05:44:46 MetaFG_0] (main.py 265): INFO Train: [52/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2930 (0.3055)	loss 1.2195 (1.3662)	grad_norm 29.2662 (30.6910)	mem 4879MB
[2022-05-31 05:44:49 MetaFG_0] (main.py 265): INFO Train: [52/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2942 (0.3055)	loss 0.8103 (1.3659)	grad_norm 46.0208 (30.7564)	mem 4879MB
[2022-05-31 05:44:52 MetaFG_0] (main.py 265): INFO Train: [52/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2957 (0.3055)	loss 1.4201 (1.3654)	grad_norm 29.0246 (30.7534)	mem 4879MB
[2022-05-31 05:44:55 MetaFG_0] (main.py 265): INFO Train: [52/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.3032 (0.3055)	loss 1.5209 (1.3659)	grad_norm 34.4714 (30.7723)	mem 4879MB
[2022-05-31 05:44:58 MetaFG_0] (main.py 265): INFO Train: [52/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.3007 (0.3055)	loss 1.5814 (1.3657)	grad_norm 30.5456 (30.7590)	mem 4879MB
[2022-05-31 05:45:01 MetaFG_0] (main.py 265): INFO Train: [52/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2973 (0.3055)	loss 0.9464 (1.3654)	grad_norm 46.6589 (30.8341)	mem 4879MB
[2022-05-31 05:45:04 MetaFG_0] (main.py 265): INFO Train: [52/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2924 (0.3055)	loss 1.1788 (1.3646)	grad_norm 32.6654 (30.8241)	mem 4879MB
[2022-05-31 05:45:07 MetaFG_0] (main.py 265): INFO Train: [52/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2994 (0.3055)	loss 1.1068 (1.3652)	grad_norm 33.5622 (30.7974)	mem 4879MB
[2022-05-31 05:45:10 MetaFG_0] (main.py 265): INFO Train: [52/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2971 (0.3055)	loss 1.4861 (1.3654)	grad_norm 13.7882 (30.7780)	mem 4879MB
[2022-05-31 05:45:14 MetaFG_0] (main.py 265): INFO Train: [52/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2995 (0.3055)	loss 1.1957 (1.3657)	grad_norm 48.2336 (30.8249)	mem 4879MB
[2022-05-31 05:45:17 MetaFG_0] (main.py 265): INFO Train: [52/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3025 (0.3055)	loss 1.4497 (1.3652)	grad_norm 31.7620 (30.7999)	mem 4879MB
[2022-05-31 05:45:20 MetaFG_0] (main.py 265): INFO Train: [52/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2926 (0.3055)	loss 1.3338 (1.3661)	grad_norm 24.8095 (30.8199)	mem 4879MB
[2022-05-31 05:45:23 MetaFG_0] (main.py 265): INFO Train: [52/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2941 (0.3055)	loss 1.5429 (1.3660)	grad_norm 24.6291 (30.7997)	mem 4879MB
[2022-05-31 05:45:26 MetaFG_0] (main.py 265): INFO Train: [52/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2932 (0.3055)	loss 1.6237 (1.3659)	grad_norm 54.3181 (30.8501)	mem 4879MB
[2022-05-31 05:45:29 MetaFG_0] (main.py 265): INFO Train: [52/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2927 (0.3055)	loss 1.3327 (1.3657)	grad_norm 36.3956 (30.8361)	mem 4879MB
[2022-05-31 05:45:32 MetaFG_0] (main.py 265): INFO Train: [52/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2926 (0.3055)	loss 1.1123 (1.3664)	grad_norm 32.6506 (30.7937)	mem 4879MB
[2022-05-31 05:45:35 MetaFG_0] (main.py 265): INFO Train: [52/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2940 (0.3055)	loss 1.1253 (1.3665)	grad_norm 24.4566 (30.7728)	mem 4879MB
[2022-05-31 05:45:38 MetaFG_0] (main.py 265): INFO Train: [52/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2942 (0.3055)	loss 1.6005 (1.3673)	grad_norm 28.8169 (30.7949)	mem 4879MB
[2022-05-31 05:45:41 MetaFG_0] (main.py 265): INFO Train: [52/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2982 (0.3054)	loss 1.0059 (1.3666)	grad_norm 30.0270 (30.8317)	mem 4879MB
[2022-05-31 05:45:44 MetaFG_0] (main.py 265): INFO Train: [52/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2963 (0.3054)	loss 1.4959 (1.3671)	grad_norm 16.8805 (30.8559)	mem 4879MB
[2022-05-31 05:45:47 MetaFG_0] (main.py 265): INFO Train: [52/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2918 (0.3054)	loss 1.5760 (1.3676)	grad_norm 39.8448 (30.8410)	mem 4879MB
[2022-05-31 05:45:50 MetaFG_0] (main.py 265): INFO Train: [52/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2983 (0.3054)	loss 0.9271 (1.3679)	grad_norm 30.8386 (30.8202)	mem 4879MB
[2022-05-31 05:45:53 MetaFG_0] (main.py 265): INFO Train: [52/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2934 (0.3054)	loss 1.4245 (1.3676)	grad_norm 54.4088 (30.8070)	mem 4879MB
[2022-05-31 05:45:56 MetaFG_0] (main.py 265): INFO Train: [52/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2979 (0.3054)	loss 1.4420 (1.3682)	grad_norm 27.2067 (30.8293)	mem 4879MB
[2022-05-31 05:45:59 MetaFG_0] (main.py 265): INFO Train: [52/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2932 (0.3054)	loss 1.4931 (1.3680)	grad_norm 15.5083 (30.8177)	mem 4879MB
[2022-05-31 05:46:02 MetaFG_0] (main.py 265): INFO Train: [52/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2941 (0.3054)	loss 1.2103 (1.3674)	grad_norm 32.1393 (30.8128)	mem 4879MB
[2022-05-31 05:46:05 MetaFG_0] (main.py 265): INFO Train: [52/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3003 (0.3054)	loss 1.5130 (1.3677)	grad_norm 26.1520 (30.8479)	mem 4879MB
[2022-05-31 05:46:08 MetaFG_0] (main.py 265): INFO Train: [52/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2986 (0.3054)	loss 1.7162 (1.3687)	grad_norm 57.5524 (30.8703)	mem 4879MB
[2022-05-31 05:46:11 MetaFG_0] (main.py 265): INFO Train: [52/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.3003 (0.3054)	loss 1.3657 (1.3688)	grad_norm 23.6911 (30.8634)	mem 4879MB
[2022-05-31 05:46:14 MetaFG_0] (main.py 265): INFO Train: [52/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2974 (0.3054)	loss 1.0636 (1.3687)	grad_norm 23.1014 (30.8681)	mem 4879MB
[2022-05-31 05:46:18 MetaFG_0] (main.py 265): INFO Train: [52/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2988 (0.3054)	loss 1.6137 (1.3686)	grad_norm 31.1371 (30.8596)	mem 4879MB
[2022-05-31 05:46:21 MetaFG_0] (main.py 265): INFO Train: [52/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2987 (0.3054)	loss 1.1300 (1.3689)	grad_norm 18.7493 (30.8224)	mem 4879MB
[2022-05-31 05:46:24 MetaFG_0] (main.py 265): INFO Train: [52/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2969 (0.3054)	loss 1.5858 (1.3680)	grad_norm 37.9671 (30.8337)	mem 4879MB
[2022-05-31 05:46:27 MetaFG_0] (main.py 265): INFO Train: [52/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2927 (0.3054)	loss 1.5559 (1.3684)	grad_norm 23.7125 (30.8600)	mem 4879MB
[2022-05-31 05:46:30 MetaFG_0] (main.py 265): INFO Train: [52/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2983 (0.3054)	loss 1.0632 (1.3685)	grad_norm 35.6834 (30.8168)	mem 4879MB
[2022-05-31 05:46:33 MetaFG_0] (main.py 265): INFO Train: [52/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2921 (0.3054)	loss 1.4848 (1.3686)	grad_norm 25.1685 (30.7871)	mem 4879MB
[2022-05-31 05:46:36 MetaFG_0] (main.py 265): INFO Train: [52/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2929 (0.3054)	loss 0.9429 (1.3684)	grad_norm 17.8283 (30.7666)	mem 4879MB
[2022-05-31 05:46:39 MetaFG_0] (main.py 265): INFO Train: [52/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.3004 (0.3054)	loss 1.2831 (1.3685)	grad_norm 18.6590 (30.7387)	mem 4879MB
[2022-05-31 05:46:42 MetaFG_0] (main.py 265): INFO Train: [52/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2978 (0.3054)	loss 1.5224 (1.3690)	grad_norm 21.6829 (30.6953)	mem 4879MB
[2022-05-31 05:46:45 MetaFG_0] (main.py 265): INFO Train: [52/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2929 (0.3054)	loss 1.6525 (1.3687)	grad_norm 16.0240 (30.6824)	mem 4879MB
[2022-05-31 05:46:48 MetaFG_0] (main.py 265): INFO Train: [52/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2933 (0.3054)	loss 1.3444 (1.3684)	grad_norm 36.5578 (30.7433)	mem 4879MB
[2022-05-31 05:46:51 MetaFG_0] (main.py 265): INFO Train: [52/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2924 (0.3054)	loss 1.5157 (1.3684)	grad_norm 25.5540 (30.7168)	mem 4879MB
[2022-05-31 05:46:54 MetaFG_0] (main.py 265): INFO Train: [52/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3008 (0.3054)	loss 1.5001 (1.3696)	grad_norm 36.2749 (30.6856)	mem 4879MB
[2022-05-31 05:46:57 MetaFG_0] (main.py 265): INFO Train: [52/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2941 (0.3054)	loss 1.6121 (1.3692)	grad_norm 23.1392 (30.6726)	mem 4879MB
[2022-05-31 05:47:00 MetaFG_0] (main.py 265): INFO Train: [52/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2917 (0.3054)	loss 1.4312 (1.3692)	grad_norm 38.3348 (30.6684)	mem 4879MB
[2022-05-31 05:47:01 MetaFG_0] (main.py 272): INFO EPOCH 52 training takes 0:07:57
[2022-05-31 05:47:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_52.pth saving......
[2022-05-31 05:47:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_52.pth saved !!!
[2022-05-31 05:47:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:47:03 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:47:03 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:47:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.749 (0.749)	Loss 0.5733 (0.5733)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 05:47:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.103 (0.158)	Loss 0.6560 (0.6228)	Acc@1 84.375 (86.932)	Acc@5 96.875 (98.295)	Mem 4879MB
[2022-05-31 05:47:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.092 (0.126)	Loss 0.6453 (0.6255)	Acc@1 87.500 (87.351)	Acc@5 93.750 (98.065)	Mem 4879MB
[2022-05-31 05:47:06 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.092 (0.115)	Loss 0.6016 (0.6301)	Acc@1 84.375 (86.895)	Acc@5 100.000 (98.085)	Mem 4879MB
[2022-05-31 05:47:07 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.110)	Loss 0.5200 (0.6319)	Acc@1 93.750 (86.814)	Acc@5 100.000 (98.476)	Mem 4879MB
[2022-05-31 05:47:08 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.107)	Loss 0.5319 (0.6374)	Acc@1 93.750 (86.765)	Acc@5 100.000 (98.591)	Mem 4879MB
[2022-05-31 05:47:09 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.105)	Loss 0.6831 (0.6245)	Acc@1 84.375 (87.039)	Acc@5 96.875 (98.719)	Mem 4879MB
[2022-05-31 05:47:10 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.103)	Loss 0.5439 (0.6265)	Acc@1 87.500 (86.752)	Acc@5 100.000 (98.812)	Mem 4879MB
[2022-05-31 05:47:11 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 0.6716 (0.6221)	Acc@1 90.625 (87.114)	Acc@5 93.750 (98.727)	Mem 4879MB
[2022-05-31 05:47:12 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.4511 (0.6292)	Acc@1 90.625 (86.951)	Acc@5 100.000 (98.695)	Mem 4879MB
[2022-05-31 05:47:13 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.092 (0.101)	Loss 0.7686 (0.6287)	Acc@1 81.250 (87.067)	Acc@5 100.000 (98.700)	Mem 4879MB
[2022-05-31 05:47:14 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 0.7675 (0.6302)	Acc@1 84.375 (87.021)	Acc@5 96.875 (98.677)	Mem 4879MB
[2022-05-31 05:47:15 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 1.2246 (0.6330)	Acc@1 68.750 (87.035)	Acc@5 93.750 (98.631)	Mem 4879MB
[2022-05-31 05:47:16 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.4688 (0.6331)	Acc@1 96.875 (86.856)	Acc@5 100.000 (98.712)	Mem 4879MB
[2022-05-31 05:47:17 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.099)	Loss 0.7810 (0.6366)	Acc@1 84.375 (86.769)	Acc@5 96.875 (98.715)	Mem 4879MB
[2022-05-31 05:47:18 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.090 (0.098)	Loss 0.3741 (0.6333)	Acc@1 93.750 (86.776)	Acc@5 100.000 (98.717)	Mem 4879MB
[2022-05-31 05:47:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.091 (0.098)	Loss 0.5321 (0.6345)	Acc@1 93.750 (86.821)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 05:47:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.9721 (0.6378)	Acc@1 75.000 (86.568)	Acc@5 93.750 (98.648)	Mem 4879MB
[2022-05-31 05:47:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.103 (0.098)	Loss 0.5128 (0.6373)	Acc@1 90.625 (86.568)	Acc@5 100.000 (98.636)	Mem 4879MB
[2022-05-31 05:47:21 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.090 (0.097)	Loss 0.4965 (0.6389)	Acc@1 90.625 (86.469)	Acc@5 100.000 (98.675)	Mem 4879MB
[2022-05-31 05:47:22 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 0.7601 (0.6417)	Acc@1 84.375 (86.350)	Acc@5 96.875 (98.632)	Mem 4879MB
[2022-05-31 05:47:23 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.098 (0.097)	Loss 0.6548 (0.6433)	Acc@1 87.500 (86.256)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 05:47:24 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.5502 (0.6396)	Acc@1 90.625 (86.411)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 05:47:25 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 0.6483 (0.6394)	Acc@1 87.500 (86.391)	Acc@5 100.000 (98.674)	Mem 4879MB
[2022-05-31 05:47:26 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.104 (0.097)	Loss 0.6701 (0.6401)	Acc@1 81.250 (86.346)	Acc@5 100.000 (98.703)	Mem 4879MB
[2022-05-31 05:47:27 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.097 (0.097)	Loss 0.5850 (0.6425)	Acc@1 81.250 (86.230)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 05:47:28 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 0.7744 (0.6442)	Acc@1 84.375 (86.303)	Acc@5 100.000 (98.695)	Mem 4879MB
[2022-05-31 05:47:29 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 0.5871 (0.6457)	Acc@1 93.750 (86.278)	Acc@5 100.000 (98.697)	Mem 4879MB
[2022-05-31 05:47:30 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.088 (0.096)	Loss 0.5845 (0.6447)	Acc@1 93.750 (86.321)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 05:47:31 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.108 (0.096)	Loss 0.4699 (0.6420)	Acc@1 90.625 (86.383)	Acc@5 100.000 (98.701)	Mem 4879MB
[2022-05-31 05:47:32 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.098 (0.096)	Loss 0.8019 (0.6446)	Acc@1 78.125 (86.233)	Acc@5 96.875 (98.681)	Mem 4879MB
[2022-05-31 05:47:33 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5925 (0.6477)	Acc@1 84.375 (86.174)	Acc@5 100.000 (98.654)	Mem 4879MB
[2022-05-31 05:47:33 MetaFG_0] (main.py 330): INFO  * Acc@1 86.190 Acc@5 98.650
[2022-05-31 05:47:33 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.2%
[2022-05-31 05:47:33 MetaFG_0] (main.py 171): INFO Max accuracy: 86.19%
[2022-05-31 05:47:34 MetaFG_0] (main.py 265): INFO Train: [53/300][0/1562]	eta 0:20:20 lr 0.000006	time 0.7814 (0.7814)	loss 0.9768 (0.9768)	grad_norm 22.5809 (22.5809)	mem 4879MB
[2022-05-31 05:47:37 MetaFG_0] (main.py 265): INFO Train: [53/300][10/1562]	eta 0:09:30 lr 0.000006	time 0.3001 (0.3677)	loss 1.0302 (1.3702)	grad_norm 65.8428 (32.1239)	mem 4879MB
[2022-05-31 05:47:40 MetaFG_0] (main.py 265): INFO Train: [53/300][20/1562]	eta 0:08:41 lr 0.000006	time 0.2927 (0.3382)	loss 1.4732 (1.3628)	grad_norm 25.2802 (31.0050)	mem 4879MB
[2022-05-31 05:47:43 MetaFG_0] (main.py 265): INFO Train: [53/300][30/1562]	eta 0:08:21 lr 0.000006	time 0.2984 (0.3273)	loss 1.2282 (1.3043)	grad_norm 39.9227 (30.4818)	mem 4879MB
[2022-05-31 05:47:46 MetaFG_0] (main.py 265): INFO Train: [53/300][40/1562]	eta 0:08:09 lr 0.000006	time 0.2927 (0.3217)	loss 1.2629 (1.3283)	grad_norm 23.2037 (29.9027)	mem 4879MB
[2022-05-31 05:47:49 MetaFG_0] (main.py 265): INFO Train: [53/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2927 (0.3185)	loss 1.3128 (1.3404)	grad_norm 25.4874 (29.8888)	mem 4879MB
[2022-05-31 05:47:52 MetaFG_0] (main.py 265): INFO Train: [53/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.2940 (0.3161)	loss 1.4474 (1.3458)	grad_norm 20.9243 (29.6310)	mem 4879MB
[2022-05-31 05:47:55 MetaFG_0] (main.py 265): INFO Train: [53/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.3030 (0.3147)	loss 1.2322 (1.3609)	grad_norm 25.3602 (29.5464)	mem 4879MB
[2022-05-31 05:47:58 MetaFG_0] (main.py 265): INFO Train: [53/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.2941 (0.3136)	loss 1.3775 (1.3701)	grad_norm 28.0304 (29.1902)	mem 4879MB
[2022-05-31 05:48:01 MetaFG_0] (main.py 265): INFO Train: [53/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.2948 (0.3124)	loss 1.3788 (1.3672)	grad_norm 44.5724 (29.3179)	mem 4879MB
[2022-05-31 05:48:04 MetaFG_0] (main.py 265): INFO Train: [53/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.3003 (0.3117)	loss 1.2332 (1.3662)	grad_norm 38.5865 (29.4361)	mem 4879MB
[2022-05-31 05:48:08 MetaFG_0] (main.py 265): INFO Train: [53/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.3015 (0.3110)	loss 1.5561 (1.3691)	grad_norm 44.5683 (29.5487)	mem 4879MB
[2022-05-31 05:48:11 MetaFG_0] (main.py 265): INFO Train: [53/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2928 (0.3103)	loss 1.6463 (1.3748)	grad_norm 32.1136 (29.6522)	mem 4879MB
[2022-05-31 05:48:14 MetaFG_0] (main.py 265): INFO Train: [53/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.2936 (0.3097)	loss 1.0737 (1.3649)	grad_norm 24.9967 (29.5068)	mem 4879MB
[2022-05-31 05:48:17 MetaFG_0] (main.py 265): INFO Train: [53/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.3009 (0.3093)	loss 1.6590 (1.3725)	grad_norm 22.0494 (29.6220)	mem 4879MB
[2022-05-31 05:48:20 MetaFG_0] (main.py 265): INFO Train: [53/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.2940 (0.3091)	loss 1.3701 (1.3775)	grad_norm 21.8200 (29.6707)	mem 4879MB
[2022-05-31 05:48:23 MetaFG_0] (main.py 265): INFO Train: [53/300][160/1562]	eta 0:07:12 lr 0.000006	time 0.2920 (0.3087)	loss 1.6434 (1.3797)	grad_norm 21.0385 (29.5398)	mem 4879MB
[2022-05-31 05:48:26 MetaFG_0] (main.py 265): INFO Train: [53/300][170/1562]	eta 0:07:09 lr 0.000006	time 0.2931 (0.3085)	loss 1.6042 (1.3764)	grad_norm 27.4349 (29.6908)	mem 4879MB
[2022-05-31 05:48:29 MetaFG_0] (main.py 265): INFO Train: [53/300][180/1562]	eta 0:07:06 lr 0.000006	time 0.2985 (0.3083)	loss 1.7499 (1.3781)	grad_norm 20.0337 (29.3776)	mem 4879MB
[2022-05-31 05:48:32 MetaFG_0] (main.py 265): INFO Train: [53/300][190/1562]	eta 0:07:02 lr 0.000006	time 0.2986 (0.3081)	loss 1.0699 (1.3725)	grad_norm 31.4344 (29.6591)	mem 4879MB
[2022-05-31 05:48:35 MetaFG_0] (main.py 265): INFO Train: [53/300][200/1562]	eta 0:06:59 lr 0.000006	time 0.2945 (0.3079)	loss 1.4135 (1.3745)	grad_norm 24.9142 (29.5718)	mem 4879MB
[2022-05-31 05:48:38 MetaFG_0] (main.py 265): INFO Train: [53/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2933 (0.3079)	loss 1.3665 (1.3744)	grad_norm 31.2428 (29.6795)	mem 4879MB
[2022-05-31 05:48:41 MetaFG_0] (main.py 265): INFO Train: [53/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.3148 (0.3084)	loss 0.7814 (1.3742)	grad_norm 44.9181 (29.7277)	mem 4879MB
[2022-05-31 05:48:44 MetaFG_0] (main.py 265): INFO Train: [53/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2995 (0.3089)	loss 1.1036 (1.3716)	grad_norm 33.1313 (29.8161)	mem 4879MB
[2022-05-31 05:48:47 MetaFG_0] (main.py 265): INFO Train: [53/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.3017 (0.3088)	loss 1.4956 (1.3715)	grad_norm 19.1292 (29.8357)	mem 4879MB
[2022-05-31 05:48:50 MetaFG_0] (main.py 265): INFO Train: [53/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2987 (0.3087)	loss 0.9426 (1.3690)	grad_norm 26.8359 (29.8587)	mem 4879MB
[2022-05-31 05:48:54 MetaFG_0] (main.py 265): INFO Train: [53/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2938 (0.3085)	loss 1.4224 (1.3689)	grad_norm 29.6808 (29.8260)	mem 4879MB
[2022-05-31 05:48:57 MetaFG_0] (main.py 265): INFO Train: [53/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2932 (0.3084)	loss 1.3592 (1.3645)	grad_norm 37.4295 (29.8743)	mem 4879MB
[2022-05-31 05:49:00 MetaFG_0] (main.py 265): INFO Train: [53/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.2952 (0.3084)	loss 1.5483 (1.3635)	grad_norm 32.8730 (29.7992)	mem 4879MB
[2022-05-31 05:49:03 MetaFG_0] (main.py 265): INFO Train: [53/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2924 (0.3083)	loss 1.2378 (1.3655)	grad_norm 23.1201 (29.8405)	mem 4879MB
[2022-05-31 05:49:06 MetaFG_0] (main.py 265): INFO Train: [53/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2951 (0.3081)	loss 1.1944 (1.3669)	grad_norm 44.3142 (29.9727)	mem 4879MB
[2022-05-31 05:49:09 MetaFG_0] (main.py 265): INFO Train: [53/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2985 (0.3081)	loss 1.4053 (1.3676)	grad_norm 31.1986 (29.9503)	mem 4879MB
[2022-05-31 05:49:12 MetaFG_0] (main.py 265): INFO Train: [53/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2949 (0.3080)	loss 0.7516 (1.3648)	grad_norm 33.8598 (29.7992)	mem 4879MB
[2022-05-31 05:49:15 MetaFG_0] (main.py 265): INFO Train: [53/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2951 (0.3079)	loss 1.4961 (1.3678)	grad_norm 24.4210 (29.9485)	mem 4879MB
[2022-05-31 05:49:18 MetaFG_0] (main.py 265): INFO Train: [53/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.3003 (0.3078)	loss 1.2362 (1.3657)	grad_norm 39.3828 (29.9863)	mem 4879MB
[2022-05-31 05:49:21 MetaFG_0] (main.py 265): INFO Train: [53/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2952 (0.3078)	loss 1.2134 (1.3686)	grad_norm 21.4175 (29.8331)	mem 4879MB
[2022-05-31 05:49:24 MetaFG_0] (main.py 265): INFO Train: [53/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.3003 (0.3078)	loss 1.3492 (1.3692)	grad_norm 42.8746 (29.9006)	mem 4879MB
[2022-05-31 05:49:27 MetaFG_0] (main.py 265): INFO Train: [53/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2926 (0.3077)	loss 0.9648 (1.3687)	grad_norm 21.4392 (29.7467)	mem 4879MB
[2022-05-31 05:49:30 MetaFG_0] (main.py 265): INFO Train: [53/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2934 (0.3077)	loss 1.3278 (1.3698)	grad_norm 12.7581 (29.6930)	mem 4879MB
[2022-05-31 05:49:33 MetaFG_0] (main.py 265): INFO Train: [53/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2990 (0.3076)	loss 0.9489 (1.3698)	grad_norm 24.6558 (29.5553)	mem 4879MB
[2022-05-31 05:49:36 MetaFG_0] (main.py 265): INFO Train: [53/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2992 (0.3076)	loss 1.3700 (1.3678)	grad_norm 23.0671 (29.5951)	mem 4879MB
[2022-05-31 05:49:39 MetaFG_0] (main.py 265): INFO Train: [53/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2934 (0.3075)	loss 1.1014 (1.3687)	grad_norm 32.3955 (29.6169)	mem 4879MB
[2022-05-31 05:49:42 MetaFG_0] (main.py 265): INFO Train: [53/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2930 (0.3074)	loss 1.4391 (1.3702)	grad_norm 24.3492 (29.4368)	mem 4879MB
[2022-05-31 05:49:45 MetaFG_0] (main.py 265): INFO Train: [53/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2925 (0.3073)	loss 1.5193 (1.3709)	grad_norm 26.6761 (29.3760)	mem 4879MB
[2022-05-31 05:49:48 MetaFG_0] (main.py 265): INFO Train: [53/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2990 (0.3073)	loss 1.2388 (1.3718)	grad_norm 24.1321 (29.4416)	mem 4879MB
[2022-05-31 05:49:52 MetaFG_0] (main.py 265): INFO Train: [53/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2939 (0.3072)	loss 1.0049 (1.3724)	grad_norm 36.9163 (29.5320)	mem 4879MB
[2022-05-31 05:49:55 MetaFG_0] (main.py 265): INFO Train: [53/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2934 (0.3072)	loss 1.1919 (1.3716)	grad_norm 32.7083 (29.6149)	mem 4879MB
[2022-05-31 05:49:58 MetaFG_0] (main.py 265): INFO Train: [53/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2940 (0.3071)	loss 1.5364 (1.3707)	grad_norm 20.4725 (29.5878)	mem 4879MB
[2022-05-31 05:50:01 MetaFG_0] (main.py 265): INFO Train: [53/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2922 (0.3071)	loss 1.1748 (1.3724)	grad_norm 31.3696 (29.7817)	mem 4879MB
[2022-05-31 05:50:04 MetaFG_0] (main.py 265): INFO Train: [53/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2941 (0.3071)	loss 1.2325 (1.3730)	grad_norm 22.9143 (29.6896)	mem 4879MB
[2022-05-31 05:50:07 MetaFG_0] (main.py 265): INFO Train: [53/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2929 (0.3071)	loss 1.4402 (1.3749)	grad_norm 14.2229 (29.7717)	mem 4879MB
[2022-05-31 05:50:10 MetaFG_0] (main.py 265): INFO Train: [53/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2938 (0.3069)	loss 1.2615 (1.3737)	grad_norm 25.0906 (29.7516)	mem 4879MB
[2022-05-31 05:50:13 MetaFG_0] (main.py 265): INFO Train: [53/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2948 (0.3069)	loss 1.5361 (1.3728)	grad_norm 32.8669 (29.7552)	mem 4879MB
[2022-05-31 05:50:16 MetaFG_0] (main.py 265): INFO Train: [53/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2994 (0.3069)	loss 1.3244 (1.3734)	grad_norm 62.2478 (29.8000)	mem 4879MB
[2022-05-31 05:50:19 MetaFG_0] (main.py 265): INFO Train: [53/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2932 (0.3068)	loss 0.9145 (1.3723)	grad_norm 24.3134 (29.7999)	mem 4879MB
[2022-05-31 05:50:22 MetaFG_0] (main.py 265): INFO Train: [53/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.3000 (0.3068)	loss 1.6545 (1.3712)	grad_norm 29.5371 (29.8638)	mem 4879MB
[2022-05-31 05:50:25 MetaFG_0] (main.py 265): INFO Train: [53/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2997 (0.3067)	loss 1.5016 (1.3717)	grad_norm 21.9519 (29.7746)	mem 4879MB
[2022-05-31 05:50:28 MetaFG_0] (main.py 265): INFO Train: [53/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2921 (0.3067)	loss 1.3320 (1.3715)	grad_norm 25.4963 (29.8230)	mem 4879MB
[2022-05-31 05:50:31 MetaFG_0] (main.py 265): INFO Train: [53/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2988 (0.3067)	loss 1.2836 (1.3702)	grad_norm 20.6764 (29.7864)	mem 4879MB
[2022-05-31 05:50:34 MetaFG_0] (main.py 265): INFO Train: [53/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2980 (0.3066)	loss 1.3430 (1.3681)	grad_norm 24.9756 (29.7287)	mem 4879MB
[2022-05-31 05:50:37 MetaFG_0] (main.py 265): INFO Train: [53/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2930 (0.3066)	loss 1.4705 (1.3683)	grad_norm 18.8101 (29.6988)	mem 4879MB
[2022-05-31 05:50:40 MetaFG_0] (main.py 265): INFO Train: [53/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.3028 (0.3065)	loss 1.2603 (1.3685)	grad_norm 21.4707 (29.6418)	mem 4879MB
[2022-05-31 05:50:43 MetaFG_0] (main.py 265): INFO Train: [53/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3001 (0.3065)	loss 1.3481 (1.3680)	grad_norm 28.6586 (29.5818)	mem 4879MB
[2022-05-31 05:50:46 MetaFG_0] (main.py 265): INFO Train: [53/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.3006 (0.3064)	loss 1.6857 (1.3686)	grad_norm 18.6097 (29.5336)	mem 4879MB
[2022-05-31 05:50:49 MetaFG_0] (main.py 265): INFO Train: [53/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3004 (0.3064)	loss 1.3533 (1.3689)	grad_norm 31.8203 (29.5729)	mem 4879MB
[2022-05-31 05:50:52 MetaFG_0] (main.py 265): INFO Train: [53/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2933 (0.3064)	loss 0.8857 (1.3667)	grad_norm 26.4627 (29.5893)	mem 4879MB
[2022-05-31 05:50:56 MetaFG_0] (main.py 265): INFO Train: [53/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.3000 (0.3064)	loss 1.6017 (1.3653)	grad_norm 25.2286 (29.5608)	mem 4879MB
[2022-05-31 05:50:59 MetaFG_0] (main.py 265): INFO Train: [53/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2931 (0.3064)	loss 1.5269 (1.3657)	grad_norm 26.2407 (29.6239)	mem 4879MB
[2022-05-31 05:51:02 MetaFG_0] (main.py 265): INFO Train: [53/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2924 (0.3064)	loss 0.9586 (1.3667)	grad_norm 26.5713 (29.6502)	mem 4879MB
[2022-05-31 05:51:05 MetaFG_0] (main.py 265): INFO Train: [53/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2984 (0.3063)	loss 1.4857 (1.3681)	grad_norm 15.7701 (29.6499)	mem 4879MB
[2022-05-31 05:51:08 MetaFG_0] (main.py 265): INFO Train: [53/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2928 (0.3063)	loss 1.5132 (1.3688)	grad_norm 37.1392 (29.6991)	mem 4879MB
[2022-05-31 05:51:11 MetaFG_0] (main.py 265): INFO Train: [53/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2982 (0.3062)	loss 1.3886 (1.3673)	grad_norm 32.1085 (29.7067)	mem 4879MB
[2022-05-31 05:51:14 MetaFG_0] (main.py 265): INFO Train: [53/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2953 (0.3062)	loss 0.9983 (1.3680)	grad_norm 18.0943 (29.6933)	mem 4879MB
[2022-05-31 05:51:17 MetaFG_0] (main.py 265): INFO Train: [53/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2939 (0.3062)	loss 0.9411 (1.3670)	grad_norm 20.5075 (29.6980)	mem 4879MB
[2022-05-31 05:51:20 MetaFG_0] (main.py 265): INFO Train: [53/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2928 (0.3062)	loss 1.1429 (1.3661)	grad_norm 29.3547 (29.7060)	mem 4879MB
[2022-05-31 05:51:23 MetaFG_0] (main.py 265): INFO Train: [53/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2993 (0.3061)	loss 1.4008 (1.3668)	grad_norm 23.2783 (29.7058)	mem 4879MB
[2022-05-31 05:51:26 MetaFG_0] (main.py 265): INFO Train: [53/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2926 (0.3061)	loss 1.5871 (1.3665)	grad_norm 22.0583 (29.7052)	mem 4879MB
[2022-05-31 05:51:29 MetaFG_0] (main.py 265): INFO Train: [53/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2927 (0.3061)	loss 1.1236 (1.3654)	grad_norm 44.2558 (29.7584)	mem 4879MB
[2022-05-31 05:51:32 MetaFG_0] (main.py 265): INFO Train: [53/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2930 (0.3061)	loss 0.8917 (1.3670)	grad_norm 15.9163 (29.7214)	mem 4879MB
[2022-05-31 05:51:35 MetaFG_0] (main.py 265): INFO Train: [53/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2991 (0.3061)	loss 1.5318 (1.3674)	grad_norm 27.5953 (29.7710)	mem 4879MB
[2022-05-31 05:51:38 MetaFG_0] (main.py 265): INFO Train: [53/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2986 (0.3061)	loss 1.2519 (1.3675)	grad_norm 23.2418 (29.7858)	mem 4879MB
[2022-05-31 05:51:41 MetaFG_0] (main.py 265): INFO Train: [53/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2933 (0.3061)	loss 1.4431 (1.3693)	grad_norm 22.3406 (29.7547)	mem 4879MB
[2022-05-31 05:51:44 MetaFG_0] (main.py 265): INFO Train: [53/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2993 (0.3060)	loss 1.6757 (1.3693)	grad_norm 25.7580 (29.7691)	mem 4879MB
[2022-05-31 05:51:47 MetaFG_0] (main.py 265): INFO Train: [53/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2988 (0.3060)	loss 1.3855 (1.3701)	grad_norm 25.4738 (29.7735)	mem 4879MB
[2022-05-31 05:51:50 MetaFG_0] (main.py 265): INFO Train: [53/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2933 (0.3060)	loss 1.4154 (1.3710)	grad_norm 17.1593 (29.7182)	mem 4879MB
[2022-05-31 05:51:53 MetaFG_0] (main.py 265): INFO Train: [53/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2981 (0.3061)	loss 1.3533 (1.3708)	grad_norm 25.8943 (29.7096)	mem 4879MB
[2022-05-31 05:51:57 MetaFG_0] (main.py 265): INFO Train: [53/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2932 (0.3061)	loss 1.1773 (1.3704)	grad_norm 25.5031 (29.7854)	mem 4879MB
[2022-05-31 05:52:00 MetaFG_0] (main.py 265): INFO Train: [53/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2933 (0.3060)	loss 1.4236 (1.3705)	grad_norm 27.4209 (29.7657)	mem 4879MB
[2022-05-31 05:52:03 MetaFG_0] (main.py 265): INFO Train: [53/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2994 (0.3060)	loss 1.4237 (1.3716)	grad_norm 24.7788 (29.7474)	mem 4879MB
[2022-05-31 05:52:06 MetaFG_0] (main.py 265): INFO Train: [53/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2958 (0.3060)	loss 1.6315 (1.3726)	grad_norm 42.0359 (29.7207)	mem 4879MB
[2022-05-31 05:52:09 MetaFG_0] (main.py 265): INFO Train: [53/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2924 (0.3060)	loss 1.2903 (1.3723)	grad_norm 28.1357 (29.7060)	mem 4879MB
[2022-05-31 05:52:12 MetaFG_0] (main.py 265): INFO Train: [53/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2932 (0.3059)	loss 1.3864 (1.3721)	grad_norm 23.1430 (29.6468)	mem 4879MB
[2022-05-31 05:52:15 MetaFG_0] (main.py 265): INFO Train: [53/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2982 (0.3059)	loss 0.8934 (1.3704)	grad_norm 26.0587 (29.6174)	mem 4879MB
[2022-05-31 05:52:18 MetaFG_0] (main.py 265): INFO Train: [53/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2922 (0.3059)	loss 1.4159 (1.3701)	grad_norm 17.5528 (29.5871)	mem 4879MB
[2022-05-31 05:52:21 MetaFG_0] (main.py 265): INFO Train: [53/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2937 (0.3059)	loss 1.2587 (1.3696)	grad_norm 28.5729 (29.5623)	mem 4879MB
[2022-05-31 05:52:24 MetaFG_0] (main.py 265): INFO Train: [53/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2941 (0.3059)	loss 1.3972 (1.3690)	grad_norm 46.0833 (29.5457)	mem 4879MB
[2022-05-31 05:52:27 MetaFG_0] (main.py 265): INFO Train: [53/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2956 (0.3059)	loss 1.5766 (1.3701)	grad_norm 52.2161 (29.6139)	mem 4879MB
[2022-05-31 05:52:30 MetaFG_0] (main.py 265): INFO Train: [53/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2982 (0.3059)	loss 1.5995 (1.3705)	grad_norm 44.9461 (29.6377)	mem 4879MB
[2022-05-31 05:52:33 MetaFG_0] (main.py 265): INFO Train: [53/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2985 (0.3059)	loss 1.2191 (1.3701)	grad_norm 22.3526 (29.6443)	mem 4879MB
[2022-05-31 05:52:36 MetaFG_0] (main.py 265): INFO Train: [53/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2933 (0.3059)	loss 1.2976 (1.3705)	grad_norm 26.7330 (29.6416)	mem 4879MB
[2022-05-31 05:52:39 MetaFG_0] (main.py 265): INFO Train: [53/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2937 (0.3059)	loss 1.4069 (1.3705)	grad_norm 21.3592 (29.7160)	mem 4879MB
[2022-05-31 05:52:42 MetaFG_0] (main.py 265): INFO Train: [53/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2929 (0.3058)	loss 1.6109 (1.3698)	grad_norm 30.3128 (29.6560)	mem 4879MB
[2022-05-31 05:52:45 MetaFG_0] (main.py 265): INFO Train: [53/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2926 (0.3058)	loss 0.9997 (1.3702)	grad_norm 19.4960 (29.6439)	mem 4879MB
[2022-05-31 05:52:48 MetaFG_0] (main.py 265): INFO Train: [53/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2927 (0.3058)	loss 1.5975 (1.3716)	grad_norm 18.9463 (29.6638)	mem 4879MB
[2022-05-31 05:52:51 MetaFG_0] (main.py 265): INFO Train: [53/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2930 (0.3058)	loss 0.9257 (1.3715)	grad_norm 38.8370 (29.6612)	mem 4879MB
[2022-05-31 05:52:54 MetaFG_0] (main.py 265): INFO Train: [53/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2942 (0.3058)	loss 1.6539 (1.3705)	grad_norm 29.2179 (29.6952)	mem 4879MB
[2022-05-31 05:52:57 MetaFG_0] (main.py 265): INFO Train: [53/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2988 (0.3057)	loss 1.6985 (1.3715)	grad_norm 37.5841 (29.7332)	mem 4879MB
[2022-05-31 05:53:00 MetaFG_0] (main.py 265): INFO Train: [53/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2942 (0.3057)	loss 0.9296 (1.3709)	grad_norm 19.6426 (29.6902)	mem 4879MB
[2022-05-31 05:53:04 MetaFG_0] (main.py 265): INFO Train: [53/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2986 (0.3058)	loss 0.7791 (1.3704)	grad_norm 11.9548 (29.6742)	mem 4879MB
[2022-05-31 05:53:07 MetaFG_0] (main.py 265): INFO Train: [53/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2927 (0.3057)	loss 1.4759 (1.3714)	grad_norm 35.0325 (29.6390)	mem 4879MB
[2022-05-31 05:53:10 MetaFG_0] (main.py 265): INFO Train: [53/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2925 (0.3057)	loss 1.2586 (1.3716)	grad_norm 11.3458 (29.6092)	mem 4879MB
[2022-05-31 05:53:13 MetaFG_0] (main.py 265): INFO Train: [53/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2987 (0.3057)	loss 1.5141 (1.3721)	grad_norm 26.6075 (29.5984)	mem 4879MB
[2022-05-31 05:53:16 MetaFG_0] (main.py 265): INFO Train: [53/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2993 (0.3057)	loss 1.0324 (1.3712)	grad_norm 32.1390 (29.5993)	mem 4879MB
[2022-05-31 05:53:19 MetaFG_0] (main.py 265): INFO Train: [53/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2924 (0.3057)	loss 1.0268 (1.3705)	grad_norm 25.9453 (29.5767)	mem 4879MB
[2022-05-31 05:53:22 MetaFG_0] (main.py 265): INFO Train: [53/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2999 (0.3057)	loss 1.2288 (1.3700)	grad_norm 14.8388 (29.5793)	mem 4879MB
[2022-05-31 05:53:25 MetaFG_0] (main.py 265): INFO Train: [53/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2988 (0.3057)	loss 1.4763 (1.3702)	grad_norm 22.7778 (29.5573)	mem 4879MB
[2022-05-31 05:53:28 MetaFG_0] (main.py 265): INFO Train: [53/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2977 (0.3057)	loss 1.0135 (1.3700)	grad_norm 46.3959 (29.5880)	mem 4879MB
[2022-05-31 05:53:31 MetaFG_0] (main.py 265): INFO Train: [53/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2994 (0.3057)	loss 1.4160 (1.3690)	grad_norm 20.9276 (29.5584)	mem 4879MB
[2022-05-31 05:53:34 MetaFG_0] (main.py 265): INFO Train: [53/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3000 (0.3057)	loss 1.4612 (1.3689)	grad_norm 32.1485 (29.5432)	mem 4879MB
[2022-05-31 05:53:37 MetaFG_0] (main.py 265): INFO Train: [53/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2997 (0.3057)	loss 1.3993 (1.3676)	grad_norm 35.5276 (29.5305)	mem 4879MB
[2022-05-31 05:53:40 MetaFG_0] (main.py 265): INFO Train: [53/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.3022 (0.3057)	loss 1.2155 (1.3673)	grad_norm 16.2352 (29.4924)	mem 4879MB
[2022-05-31 05:53:43 MetaFG_0] (main.py 265): INFO Train: [53/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2982 (0.3057)	loss 1.3069 (1.3677)	grad_norm 21.8662 (29.4683)	mem 4879MB
[2022-05-31 05:53:46 MetaFG_0] (main.py 265): INFO Train: [53/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2983 (0.3057)	loss 1.3146 (1.3673)	grad_norm 23.1498 (29.5007)	mem 4879MB
[2022-05-31 05:53:49 MetaFG_0] (main.py 265): INFO Train: [53/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2972 (0.3056)	loss 1.4356 (1.3685)	grad_norm 24.8690 (29.5091)	mem 4879MB
[2022-05-31 05:53:52 MetaFG_0] (main.py 265): INFO Train: [53/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2928 (0.3056)	loss 0.8167 (1.3677)	grad_norm 18.7977 (29.5018)	mem 4879MB
[2022-05-31 05:53:55 MetaFG_0] (main.py 265): INFO Train: [53/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2934 (0.3056)	loss 1.0070 (1.3670)	grad_norm 33.0307 (29.4699)	mem 4879MB
[2022-05-31 05:53:58 MetaFG_0] (main.py 265): INFO Train: [53/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2949 (0.3056)	loss 1.4472 (1.3673)	grad_norm 17.7168 (29.4553)	mem 4879MB
[2022-05-31 05:54:01 MetaFG_0] (main.py 265): INFO Train: [53/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2923 (0.3056)	loss 1.6824 (1.3673)	grad_norm 55.0022 (29.4715)	mem 4879MB
[2022-05-31 05:54:04 MetaFG_0] (main.py 265): INFO Train: [53/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2929 (0.3056)	loss 1.4301 (1.3668)	grad_norm 30.4397 (29.4521)	mem 4879MB
[2022-05-31 05:54:08 MetaFG_0] (main.py 265): INFO Train: [53/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2938 (0.3056)	loss 0.9919 (1.3668)	grad_norm 36.0838 (29.5531)	mem 4879MB
[2022-05-31 05:54:11 MetaFG_0] (main.py 265): INFO Train: [53/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2996 (0.3056)	loss 1.3059 (1.3672)	grad_norm 19.1840 (29.5420)	mem 4879MB
[2022-05-31 05:54:14 MetaFG_0] (main.py 265): INFO Train: [53/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2985 (0.3056)	loss 1.4469 (1.3684)	grad_norm 27.6305 (29.5597)	mem 4879MB
[2022-05-31 05:54:17 MetaFG_0] (main.py 265): INFO Train: [53/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.3009 (0.3056)	loss 0.9306 (1.3686)	grad_norm 23.7920 (29.5836)	mem 4879MB
[2022-05-31 05:54:20 MetaFG_0] (main.py 265): INFO Train: [53/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2968 (0.3056)	loss 1.2609 (1.3682)	grad_norm 18.1671 (29.5767)	mem 4879MB
[2022-05-31 05:54:23 MetaFG_0] (main.py 265): INFO Train: [53/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2956 (0.3056)	loss 1.2131 (1.3682)	grad_norm 34.1553 (29.5693)	mem 4879MB
[2022-05-31 05:54:26 MetaFG_0] (main.py 265): INFO Train: [53/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2987 (0.3056)	loss 0.8089 (1.3684)	grad_norm 41.9935 (29.6376)	mem 4879MB
[2022-05-31 05:54:29 MetaFG_0] (main.py 265): INFO Train: [53/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2986 (0.3056)	loss 1.1825 (1.3686)	grad_norm 31.3681 (29.6913)	mem 4879MB
[2022-05-31 05:54:32 MetaFG_0] (main.py 265): INFO Train: [53/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2979 (0.3056)	loss 1.4349 (1.3685)	grad_norm 21.7681 (29.6952)	mem 4879MB
[2022-05-31 05:54:35 MetaFG_0] (main.py 265): INFO Train: [53/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2930 (0.3055)	loss 1.6955 (1.3693)	grad_norm 30.8950 (29.7390)	mem 4879MB
[2022-05-31 05:54:38 MetaFG_0] (main.py 265): INFO Train: [53/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2996 (0.3055)	loss 1.2954 (1.3692)	grad_norm 13.2332 (29.6998)	mem 4879MB
[2022-05-31 05:54:41 MetaFG_0] (main.py 265): INFO Train: [53/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2997 (0.3055)	loss 1.3543 (1.3689)	grad_norm 21.2318 (29.6817)	mem 4879MB
[2022-05-31 05:54:44 MetaFG_0] (main.py 265): INFO Train: [53/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2930 (0.3055)	loss 1.1656 (1.3685)	grad_norm 29.3066 (29.6725)	mem 4879MB
[2022-05-31 05:54:47 MetaFG_0] (main.py 265): INFO Train: [53/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2975 (0.3055)	loss 1.3538 (1.3686)	grad_norm 16.6879 (29.6838)	mem 4879MB
[2022-05-31 05:54:50 MetaFG_0] (main.py 265): INFO Train: [53/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2994 (0.3055)	loss 1.5154 (1.3683)	grad_norm 50.3656 (29.7077)	mem 4879MB
[2022-05-31 05:54:53 MetaFG_0] (main.py 265): INFO Train: [53/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.3009 (0.3056)	loss 1.4337 (1.3683)	grad_norm 27.0620 (29.6919)	mem 4879MB
[2022-05-31 05:54:56 MetaFG_0] (main.py 265): INFO Train: [53/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2938 (0.3056)	loss 1.3200 (1.3687)	grad_norm 25.1062 (29.6851)	mem 4879MB
[2022-05-31 05:54:59 MetaFG_0] (main.py 265): INFO Train: [53/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2938 (0.3056)	loss 1.5389 (1.3685)	grad_norm 33.0257 (29.6768)	mem 4879MB
[2022-05-31 05:55:02 MetaFG_0] (main.py 265): INFO Train: [53/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2924 (0.3056)	loss 1.3971 (1.3681)	grad_norm 31.7232 (29.6941)	mem 4879MB
[2022-05-31 05:55:06 MetaFG_0] (main.py 265): INFO Train: [53/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2984 (0.3056)	loss 1.4674 (1.3676)	grad_norm 26.0804 (29.6769)	mem 4879MB
[2022-05-31 05:55:09 MetaFG_0] (main.py 265): INFO Train: [53/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2937 (0.3055)	loss 1.4524 (1.3678)	grad_norm 19.1173 (29.6450)	mem 4879MB
[2022-05-31 05:55:12 MetaFG_0] (main.py 265): INFO Train: [53/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2941 (0.3055)	loss 1.5034 (1.3681)	grad_norm 27.7705 (29.5966)	mem 4879MB
[2022-05-31 05:55:15 MetaFG_0] (main.py 265): INFO Train: [53/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2927 (0.3055)	loss 1.5621 (1.3675)	grad_norm 24.7301 (29.6047)	mem 4879MB
[2022-05-31 05:55:18 MetaFG_0] (main.py 265): INFO Train: [53/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.3008 (0.3055)	loss 1.5878 (1.3677)	grad_norm 47.3349 (29.5907)	mem 4879MB
[2022-05-31 05:55:21 MetaFG_0] (main.py 265): INFO Train: [53/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2944 (0.3055)	loss 1.3754 (1.3672)	grad_norm 23.6305 (29.5620)	mem 4879MB
[2022-05-31 05:55:24 MetaFG_0] (main.py 265): INFO Train: [53/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2944 (0.3055)	loss 1.8108 (1.3679)	grad_norm 51.4849 (29.5785)	mem 4879MB
[2022-05-31 05:55:27 MetaFG_0] (main.py 265): INFO Train: [53/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2939 (0.3055)	loss 1.4932 (1.3685)	grad_norm 28.4558 (29.6180)	mem 4879MB
[2022-05-31 05:55:30 MetaFG_0] (main.py 265): INFO Train: [53/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2920 (0.3055)	loss 1.3308 (1.3683)	grad_norm 25.7438 (29.6354)	mem 4879MB
[2022-05-31 05:55:30 MetaFG_0] (main.py 272): INFO EPOCH 53 training takes 0:07:57
[2022-05-31 05:55:30 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_53.pth saving......
[2022-05-31 05:55:31 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_53.pth saved !!!
[2022-05-31 05:55:31 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 05:55:33 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 05:55:33 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 05:55:33 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.552 (0.552)	Loss 0.7031 (0.7031)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 05:55:34 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.145)	Loss 0.6120 (0.5997)	Acc@1 90.625 (87.500)	Acc@5 96.875 (98.864)	Mem 4879MB
[2022-05-31 05:55:35 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.122)	Loss 0.6893 (0.6315)	Acc@1 87.500 (87.500)	Acc@5 96.875 (98.512)	Mem 4879MB
[2022-05-31 05:55:36 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.113)	Loss 0.6756 (0.6522)	Acc@1 87.500 (87.198)	Acc@5 96.875 (98.286)	Mem 4879MB
[2022-05-31 05:55:37 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.092 (0.108)	Loss 0.5608 (0.6514)	Acc@1 90.625 (87.195)	Acc@5 93.750 (98.247)	Mem 4879MB
[2022-05-31 05:55:38 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.097 (0.106)	Loss 0.4455 (0.6363)	Acc@1 93.750 (87.316)	Acc@5 100.000 (98.407)	Mem 4879MB
[2022-05-31 05:55:39 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.094 (0.104)	Loss 0.4683 (0.6377)	Acc@1 90.625 (87.090)	Acc@5 100.000 (98.514)	Mem 4879MB
[2022-05-31 05:55:40 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.088 (0.103)	Loss 0.5763 (0.6372)	Acc@1 84.375 (86.884)	Acc@5 100.000 (98.636)	Mem 4879MB
[2022-05-31 05:55:41 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.101)	Loss 0.4820 (0.6466)	Acc@1 96.875 (86.535)	Acc@5 100.000 (98.611)	Mem 4879MB
[2022-05-31 05:55:42 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.092 (0.101)	Loss 0.4482 (0.6369)	Acc@1 96.875 (86.641)	Acc@5 100.000 (98.695)	Mem 4879MB
[2022-05-31 05:55:43 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.088 (0.100)	Loss 0.5391 (0.6285)	Acc@1 90.625 (86.943)	Acc@5 100.000 (98.639)	Mem 4879MB
[2022-05-31 05:55:44 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.099)	Loss 0.4701 (0.6246)	Acc@1 90.625 (87.106)	Acc@5 100.000 (98.620)	Mem 4879MB
[2022-05-31 05:55:45 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.097 (0.099)	Loss 0.8070 (0.6235)	Acc@1 78.125 (87.164)	Acc@5 93.750 (98.605)	Mem 4879MB
[2022-05-31 05:55:46 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.5560 (0.6287)	Acc@1 84.375 (86.832)	Acc@5 100.000 (98.640)	Mem 4879MB
[2022-05-31 05:55:47 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.100 (0.098)	Loss 0.5280 (0.6279)	Acc@1 87.500 (86.835)	Acc@5 100.000 (98.582)	Mem 4879MB
[2022-05-31 05:55:48 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.088 (0.098)	Loss 0.5281 (0.6332)	Acc@1 90.625 (86.548)	Acc@5 100.000 (98.613)	Mem 4879MB
[2022-05-31 05:55:48 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.099 (0.098)	Loss 0.5994 (0.6355)	Acc@1 81.250 (86.277)	Acc@5 100.000 (98.641)	Mem 4879MB
[2022-05-31 05:55:49 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.097)	Loss 0.2569 (0.6355)	Acc@1 100.000 (86.312)	Acc@5 100.000 (98.666)	Mem 4879MB
[2022-05-31 05:55:50 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.088 (0.097)	Loss 0.7183 (0.6339)	Acc@1 81.250 (86.326)	Acc@5 93.750 (98.688)	Mem 4879MB
[2022-05-31 05:55:51 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.103 (0.097)	Loss 1.1892 (0.6381)	Acc@1 68.750 (86.306)	Acc@5 93.750 (98.691)	Mem 4879MB
[2022-05-31 05:55:52 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.095 (0.097)	Loss 0.6710 (0.6362)	Acc@1 78.125 (86.396)	Acc@5 100.000 (98.678)	Mem 4879MB
[2022-05-31 05:55:53 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 0.4589 (0.6331)	Acc@1 90.625 (86.463)	Acc@5 100.000 (98.697)	Mem 4879MB
[2022-05-31 05:55:54 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 0.6961 (0.6335)	Acc@1 84.375 (86.397)	Acc@5 96.875 (98.685)	Mem 4879MB
[2022-05-31 05:55:55 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.088 (0.097)	Loss 0.6992 (0.6364)	Acc@1 81.250 (86.323)	Acc@5 100.000 (98.647)	Mem 4879MB
[2022-05-31 05:55:56 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.101 (0.096)	Loss 0.6745 (0.6337)	Acc@1 84.375 (86.463)	Acc@5 100.000 (98.664)	Mem 4879MB
[2022-05-31 05:55:57 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.096)	Loss 0.6044 (0.6347)	Acc@1 87.500 (86.467)	Acc@5 96.875 (98.643)	Mem 4879MB
[2022-05-31 05:55:58 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 0.5078 (0.6373)	Acc@1 84.375 (86.339)	Acc@5 100.000 (98.623)	Mem 4879MB
[2022-05-31 05:55:59 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.8333 (0.6364)	Acc@1 78.125 (86.301)	Acc@5 96.875 (98.639)	Mem 4879MB
[2022-05-31 05:56:00 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.4441 (0.6371)	Acc@1 90.625 (86.277)	Acc@5 100.000 (98.632)	Mem 4879MB
[2022-05-31 05:56:01 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.5564 (0.6370)	Acc@1 87.500 (86.287)	Acc@5 100.000 (98.636)	Mem 4879MB
[2022-05-31 05:56:02 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 0.6820 (0.6372)	Acc@1 78.125 (86.233)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 05:56:03 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.5219 (0.6357)	Acc@1 90.625 (86.264)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 05:56:03 MetaFG_0] (main.py 330): INFO  * Acc@1 86.240 Acc@5 98.660
[2022-05-31 05:56:03 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.2%
[2022-05-31 05:56:03 MetaFG_0] (main.py 171): INFO Max accuracy: 86.24%
[2022-05-31 05:56:04 MetaFG_0] (main.py 265): INFO Train: [54/300][0/1562]	eta 0:27:09 lr 0.000006	time 1.0432 (1.0432)	loss 1.3453 (1.3453)	grad_norm 21.7685 (21.7685)	mem 4879MB
[2022-05-31 05:56:07 MetaFG_0] (main.py 265): INFO Train: [54/300][10/1562]	eta 0:09:51 lr 0.000006	time 0.3029 (0.3812)	loss 1.5931 (1.3446)	grad_norm 26.3968 (32.1734)	mem 4879MB
[2022-05-31 05:56:10 MetaFG_0] (main.py 265): INFO Train: [54/300][20/1562]	eta 0:08:51 lr 0.000006	time 0.2936 (0.3444)	loss 1.1264 (1.3692)	grad_norm 48.0367 (30.8425)	mem 4879MB
[2022-05-31 05:56:13 MetaFG_0] (main.py 265): INFO Train: [54/300][30/1562]	eta 0:08:27 lr 0.000006	time 0.2938 (0.3315)	loss 1.4809 (1.3466)	grad_norm 44.8659 (31.6135)	mem 4879MB
[2022-05-31 05:56:16 MetaFG_0] (main.py 265): INFO Train: [54/300][40/1562]	eta 0:08:14 lr 0.000006	time 0.2937 (0.3251)	loss 0.8617 (1.3361)	grad_norm 35.1392 (32.1097)	mem 4879MB
[2022-05-31 05:56:19 MetaFG_0] (main.py 265): INFO Train: [54/300][50/1562]	eta 0:08:05 lr 0.000006	time 0.2955 (0.3213)	loss 1.5080 (1.3549)	grad_norm 28.7593 (32.0941)	mem 4879MB
[2022-05-31 05:56:22 MetaFG_0] (main.py 265): INFO Train: [54/300][60/1562]	eta 0:07:58 lr 0.000006	time 0.2987 (0.3188)	loss 1.6159 (1.3594)	grad_norm 51.0166 (32.8149)	mem 4879MB
[2022-05-31 05:56:25 MetaFG_0] (main.py 265): INFO Train: [54/300][70/1562]	eta 0:07:54 lr 0.000006	time 0.3376 (0.3183)	loss 1.4029 (1.3678)	grad_norm 27.7766 (32.7846)	mem 4879MB
[2022-05-31 05:56:29 MetaFG_0] (main.py 265): INFO Train: [54/300][80/1562]	eta 0:07:51 lr 0.000006	time 0.2982 (0.3184)	loss 1.4179 (1.3586)	grad_norm 30.2617 (34.1587)	mem 4879MB
[2022-05-31 05:56:32 MetaFG_0] (main.py 265): INFO Train: [54/300][90/1562]	eta 0:07:46 lr 0.000006	time 0.2997 (0.3169)	loss 1.6315 (1.3497)	grad_norm 28.9251 (34.2412)	mem 4879MB
[2022-05-31 05:56:35 MetaFG_0] (main.py 265): INFO Train: [54/300][100/1562]	eta 0:07:41 lr 0.000006	time 0.2998 (0.3158)	loss 1.5002 (1.3549)	grad_norm 32.1192 (34.1140)	mem 4879MB
[2022-05-31 05:56:38 MetaFG_0] (main.py 265): INFO Train: [54/300][110/1562]	eta 0:07:37 lr 0.000006	time 0.2923 (0.3148)	loss 1.4566 (1.3551)	grad_norm 36.3657 (33.6901)	mem 4879MB
[2022-05-31 05:56:41 MetaFG_0] (main.py 265): INFO Train: [54/300][120/1562]	eta 0:07:32 lr 0.000006	time 0.2995 (0.3141)	loss 1.5116 (1.3654)	grad_norm 23.3087 (33.0143)	mem 4879MB
[2022-05-31 05:56:44 MetaFG_0] (main.py 265): INFO Train: [54/300][130/1562]	eta 0:07:28 lr 0.000006	time 0.2972 (0.3132)	loss 1.2179 (1.3698)	grad_norm 41.0856 (32.6985)	mem 4879MB
[2022-05-31 05:56:47 MetaFG_0] (main.py 265): INFO Train: [54/300][140/1562]	eta 0:07:24 lr 0.000006	time 0.2970 (0.3126)	loss 1.5279 (1.3743)	grad_norm 26.9242 (32.3426)	mem 4879MB
[2022-05-31 05:56:50 MetaFG_0] (main.py 265): INFO Train: [54/300][150/1562]	eta 0:07:20 lr 0.000006	time 0.2997 (0.3123)	loss 1.3035 (1.3786)	grad_norm 33.8865 (31.9673)	mem 4879MB
[2022-05-31 05:56:53 MetaFG_0] (main.py 265): INFO Train: [54/300][160/1562]	eta 0:07:17 lr 0.000006	time 0.2946 (0.3118)	loss 1.2299 (1.3804)	grad_norm 29.5818 (32.0873)	mem 4879MB
[2022-05-31 05:56:56 MetaFG_0] (main.py 265): INFO Train: [54/300][170/1562]	eta 0:07:13 lr 0.000006	time 0.2979 (0.3114)	loss 1.5606 (1.3697)	grad_norm 24.0605 (32.2433)	mem 4879MB
[2022-05-31 05:56:59 MetaFG_0] (main.py 265): INFO Train: [54/300][180/1562]	eta 0:07:09 lr 0.000006	time 0.2931 (0.3109)	loss 1.5079 (1.3708)	grad_norm 22.8132 (32.1138)	mem 4879MB
[2022-05-31 05:57:02 MetaFG_0] (main.py 265): INFO Train: [54/300][190/1562]	eta 0:07:06 lr 0.000006	time 0.2937 (0.3107)	loss 1.6031 (1.3654)	grad_norm 40.4252 (31.9319)	mem 4879MB
[2022-05-31 05:57:05 MetaFG_0] (main.py 265): INFO Train: [54/300][200/1562]	eta 0:07:02 lr 0.000006	time 0.2925 (0.3104)	loss 1.3353 (1.3645)	grad_norm 25.2343 (31.6182)	mem 4879MB
[2022-05-31 05:57:08 MetaFG_0] (main.py 265): INFO Train: [54/300][210/1562]	eta 0:06:59 lr 0.000006	time 0.2936 (0.3102)	loss 0.9438 (1.3611)	grad_norm 25.3968 (31.7406)	mem 4879MB
[2022-05-31 05:57:11 MetaFG_0] (main.py 265): INFO Train: [54/300][220/1562]	eta 0:06:55 lr 0.000006	time 0.2925 (0.3099)	loss 1.6538 (1.3610)	grad_norm 21.9351 (31.4707)	mem 4879MB
[2022-05-31 05:57:14 MetaFG_0] (main.py 265): INFO Train: [54/300][230/1562]	eta 0:06:52 lr 0.000006	time 0.2948 (0.3097)	loss 1.7903 (1.3611)	grad_norm 30.9654 (31.2364)	mem 4879MB
[2022-05-31 05:57:18 MetaFG_0] (main.py 265): INFO Train: [54/300][240/1562]	eta 0:06:49 lr 0.000006	time 0.3005 (0.3096)	loss 1.5610 (1.3643)	grad_norm 20.5741 (31.2403)	mem 4879MB
[2022-05-31 05:57:21 MetaFG_0] (main.py 265): INFO Train: [54/300][250/1562]	eta 0:06:46 lr 0.000006	time 0.2992 (0.3095)	loss 1.6004 (1.3620)	grad_norm 36.0682 (31.3867)	mem 4879MB
[2022-05-31 05:57:24 MetaFG_0] (main.py 265): INFO Train: [54/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.2924 (0.3093)	loss 1.4979 (1.3648)	grad_norm 31.2011 (31.1836)	mem 4879MB
[2022-05-31 05:57:27 MetaFG_0] (main.py 265): INFO Train: [54/300][270/1562]	eta 0:06:39 lr 0.000006	time 0.2932 (0.3091)	loss 1.4654 (1.3626)	grad_norm 26.3709 (31.2582)	mem 4879MB
[2022-05-31 05:57:30 MetaFG_0] (main.py 265): INFO Train: [54/300][280/1562]	eta 0:06:36 lr 0.000006	time 0.2980 (0.3090)	loss 1.0460 (1.3617)	grad_norm 24.1802 (31.1836)	mem 4879MB
[2022-05-31 05:57:33 MetaFG_0] (main.py 265): INFO Train: [54/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2937 (0.3088)	loss 1.4889 (1.3671)	grad_norm 24.2239 (31.2154)	mem 4879MB
[2022-05-31 05:57:36 MetaFG_0] (main.py 265): INFO Train: [54/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.2994 (0.3088)	loss 0.8643 (1.3668)	grad_norm 53.8332 (31.2098)	mem 4879MB
[2022-05-31 05:57:39 MetaFG_0] (main.py 265): INFO Train: [54/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.3018 (0.3087)	loss 1.2865 (1.3663)	grad_norm 26.3876 (31.1058)	mem 4879MB
[2022-05-31 05:57:42 MetaFG_0] (main.py 265): INFO Train: [54/300][320/1562]	eta 0:06:23 lr 0.000006	time 0.2939 (0.3087)	loss 1.5097 (1.3687)	grad_norm 22.0380 (31.2716)	mem 4879MB
[2022-05-31 05:57:45 MetaFG_0] (main.py 265): INFO Train: [54/300][330/1562]	eta 0:06:20 lr 0.000006	time 0.2983 (0.3085)	loss 1.6116 (1.3749)	grad_norm 31.1785 (31.2277)	mem 4879MB
[2022-05-31 05:57:48 MetaFG_0] (main.py 265): INFO Train: [54/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2951 (0.3084)	loss 1.4811 (1.3775)	grad_norm 26.4820 (31.0951)	mem 4879MB
[2022-05-31 05:57:51 MetaFG_0] (main.py 265): INFO Train: [54/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2989 (0.3083)	loss 1.2388 (1.3736)	grad_norm 22.2048 (30.9626)	mem 4879MB
[2022-05-31 05:57:54 MetaFG_0] (main.py 265): INFO Train: [54/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.2961 (0.3081)	loss 1.6194 (1.3745)	grad_norm 23.2946 (30.9960)	mem 4879MB
[2022-05-31 05:57:57 MetaFG_0] (main.py 265): INFO Train: [54/300][370/1562]	eta 0:06:07 lr 0.000006	time 0.2927 (0.3081)	loss 1.6221 (1.3773)	grad_norm 29.4490 (31.1442)	mem 4879MB
[2022-05-31 05:58:00 MetaFG_0] (main.py 265): INFO Train: [54/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2936 (0.3080)	loss 1.1949 (1.3770)	grad_norm 20.9039 (31.2318)	mem 4879MB
[2022-05-31 05:58:03 MetaFG_0] (main.py 265): INFO Train: [54/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2931 (0.3079)	loss 1.6126 (1.3752)	grad_norm 47.4852 (31.4249)	mem 4879MB
[2022-05-31 05:58:06 MetaFG_0] (main.py 265): INFO Train: [54/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2930 (0.3078)	loss 1.1374 (1.3751)	grad_norm 14.9755 (31.3533)	mem 4879MB
[2022-05-31 05:58:09 MetaFG_0] (main.py 265): INFO Train: [54/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.3015 (0.3077)	loss 1.2336 (1.3759)	grad_norm 28.3644 (31.2747)	mem 4879MB
[2022-05-31 05:58:12 MetaFG_0] (main.py 265): INFO Train: [54/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2924 (0.3076)	loss 1.6579 (1.3738)	grad_norm 15.9761 (31.1371)	mem 4879MB
[2022-05-31 05:58:15 MetaFG_0] (main.py 265): INFO Train: [54/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2992 (0.3076)	loss 1.0532 (1.3709)	grad_norm 20.1896 (31.0678)	mem 4879MB
[2022-05-31 05:58:18 MetaFG_0] (main.py 265): INFO Train: [54/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2941 (0.3075)	loss 1.4591 (1.3707)	grad_norm 38.0429 (30.9316)	mem 4879MB
[2022-05-31 05:58:22 MetaFG_0] (main.py 265): INFO Train: [54/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.3003 (0.3075)	loss 1.3938 (1.3692)	grad_norm 29.3461 (30.9624)	mem 4879MB
[2022-05-31 05:58:25 MetaFG_0] (main.py 265): INFO Train: [54/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2941 (0.3074)	loss 1.4299 (1.3718)	grad_norm 18.8026 (30.9729)	mem 4879MB
[2022-05-31 05:58:28 MetaFG_0] (main.py 265): INFO Train: [54/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2928 (0.3074)	loss 1.5117 (1.3719)	grad_norm 54.9287 (30.9735)	mem 4879MB
[2022-05-31 05:58:31 MetaFG_0] (main.py 265): INFO Train: [54/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2927 (0.3073)	loss 1.3288 (1.3719)	grad_norm 16.6293 (30.8997)	mem 4879MB
[2022-05-31 05:58:34 MetaFG_0] (main.py 265): INFO Train: [54/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2943 (0.3073)	loss 1.5255 (1.3713)	grad_norm 43.9097 (30.9356)	mem 4879MB
[2022-05-31 05:58:37 MetaFG_0] (main.py 265): INFO Train: [54/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2923 (0.3072)	loss 0.9186 (1.3729)	grad_norm 17.8543 (30.8383)	mem 4879MB
[2022-05-31 05:58:40 MetaFG_0] (main.py 265): INFO Train: [54/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2983 (0.3072)	loss 1.5467 (1.3732)	grad_norm 26.5939 (30.8084)	mem 4879MB
[2022-05-31 05:58:43 MetaFG_0] (main.py 265): INFO Train: [54/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.3047 (0.3072)	loss 1.0826 (1.3709)	grad_norm 15.9580 (30.7442)	mem 4879MB
[2022-05-31 05:58:46 MetaFG_0] (main.py 265): INFO Train: [54/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.3002 (0.3072)	loss 1.5481 (1.3707)	grad_norm 40.5388 (30.8697)	mem 4879MB
[2022-05-31 05:58:49 MetaFG_0] (main.py 265): INFO Train: [54/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2922 (0.3071)	loss 1.5830 (1.3692)	grad_norm 22.1867 (31.0708)	mem 4879MB
[2022-05-31 05:58:52 MetaFG_0] (main.py 265): INFO Train: [54/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.3006 (0.3071)	loss 1.5673 (1.3695)	grad_norm 28.8963 (30.9916)	mem 4879MB
[2022-05-31 05:58:55 MetaFG_0] (main.py 265): INFO Train: [54/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2923 (0.3070)	loss 1.4669 (1.3695)	grad_norm 23.4900 (30.9600)	mem 4879MB
[2022-05-31 05:58:58 MetaFG_0] (main.py 265): INFO Train: [54/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.3012 (0.3070)	loss 1.4567 (1.3719)	grad_norm 49.0790 (30.9475)	mem 4879MB
[2022-05-31 05:59:01 MetaFG_0] (main.py 265): INFO Train: [54/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2930 (0.3070)	loss 1.4760 (1.3720)	grad_norm 21.0753 (30.8060)	mem 4879MB
[2022-05-31 05:59:04 MetaFG_0] (main.py 265): INFO Train: [54/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2923 (0.3070)	loss 1.5966 (1.3730)	grad_norm 29.0535 (30.7553)	mem 4879MB
[2022-05-31 05:59:07 MetaFG_0] (main.py 265): INFO Train: [54/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2930 (0.3069)	loss 1.5613 (1.3739)	grad_norm 40.2051 (30.7790)	mem 4879MB
[2022-05-31 05:59:10 MetaFG_0] (main.py 265): INFO Train: [54/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2943 (0.3069)	loss 1.7220 (1.3743)	grad_norm 38.1912 (30.7870)	mem 4879MB
[2022-05-31 05:59:13 MetaFG_0] (main.py 265): INFO Train: [54/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2927 (0.3069)	loss 1.3926 (1.3738)	grad_norm 19.0921 (30.7275)	mem 4879MB
[2022-05-31 05:59:16 MetaFG_0] (main.py 265): INFO Train: [54/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2937 (0.3068)	loss 1.1214 (1.3741)	grad_norm 20.9574 (30.7043)	mem 4879MB
[2022-05-31 05:59:20 MetaFG_0] (main.py 265): INFO Train: [54/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2940 (0.3068)	loss 1.3154 (1.3727)	grad_norm 37.4828 (30.6780)	mem 4879MB
[2022-05-31 05:59:23 MetaFG_0] (main.py 265): INFO Train: [54/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.3014 (0.3068)	loss 1.5449 (1.3729)	grad_norm 29.1972 (30.6519)	mem 4879MB
[2022-05-31 05:59:26 MetaFG_0] (main.py 265): INFO Train: [54/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.3008 (0.3068)	loss 1.2229 (1.3734)	grad_norm 15.1341 (30.6276)	mem 4879MB
[2022-05-31 05:59:29 MetaFG_0] (main.py 265): INFO Train: [54/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2928 (0.3068)	loss 1.4327 (1.3717)	grad_norm 24.2537 (30.6404)	mem 4879MB
[2022-05-31 05:59:32 MetaFG_0] (main.py 265): INFO Train: [54/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2997 (0.3067)	loss 1.4939 (1.3707)	grad_norm 31.0124 (30.6563)	mem 4879MB
[2022-05-31 05:59:35 MetaFG_0] (main.py 265): INFO Train: [54/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2939 (0.3067)	loss 1.5665 (1.3713)	grad_norm 30.5852 (30.5970)	mem 4879MB
[2022-05-31 05:59:38 MetaFG_0] (main.py 265): INFO Train: [54/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2932 (0.3066)	loss 1.4310 (1.3713)	grad_norm 20.7155 (30.5734)	mem 4879MB
[2022-05-31 05:59:41 MetaFG_0] (main.py 265): INFO Train: [54/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2928 (0.3066)	loss 1.4674 (1.3705)	grad_norm 48.2689 (30.6991)	mem 4879MB
[2022-05-31 05:59:44 MetaFG_0] (main.py 265): INFO Train: [54/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2929 (0.3066)	loss 1.7093 (1.3697)	grad_norm 23.7063 (30.6741)	mem 4879MB
[2022-05-31 05:59:47 MetaFG_0] (main.py 265): INFO Train: [54/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2928 (0.3066)	loss 1.4984 (1.3691)	grad_norm 18.9293 (30.7010)	mem 4879MB
[2022-05-31 05:59:50 MetaFG_0] (main.py 265): INFO Train: [54/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2984 (0.3066)	loss 1.3294 (1.3674)	grad_norm 17.8502 (30.6382)	mem 4879MB
[2022-05-31 05:59:53 MetaFG_0] (main.py 265): INFO Train: [54/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.3006 (0.3065)	loss 1.6755 (1.3681)	grad_norm 23.6101 (30.5551)	mem 4879MB
[2022-05-31 05:59:56 MetaFG_0] (main.py 265): INFO Train: [54/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2933 (0.3065)	loss 1.2645 (1.3694)	grad_norm 29.1051 (30.5287)	mem 4879MB
[2022-05-31 05:59:59 MetaFG_0] (main.py 265): INFO Train: [54/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2929 (0.3065)	loss 1.3869 (1.3684)	grad_norm 17.1075 (30.5316)	mem 4879MB
[2022-05-31 06:00:02 MetaFG_0] (main.py 265): INFO Train: [54/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2934 (0.3065)	loss 1.4932 (1.3677)	grad_norm 13.6478 (30.4483)	mem 4879MB
[2022-05-31 06:00:05 MetaFG_0] (main.py 265): INFO Train: [54/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2947 (0.3064)	loss 1.2508 (1.3686)	grad_norm 25.4806 (30.4255)	mem 4879MB
[2022-05-31 06:00:08 MetaFG_0] (main.py 265): INFO Train: [54/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2925 (0.3064)	loss 1.4373 (1.3694)	grad_norm 18.5893 (30.3656)	mem 4879MB
[2022-05-31 06:00:11 MetaFG_0] (main.py 265): INFO Train: [54/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.3038 (0.3064)	loss 1.5502 (1.3696)	grad_norm 16.8733 (30.3719)	mem 4879MB
[2022-05-31 06:00:14 MetaFG_0] (main.py 265): INFO Train: [54/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2990 (0.3064)	loss 0.8791 (1.3696)	grad_norm 28.5042 (30.3397)	mem 4879MB
[2022-05-31 06:00:17 MetaFG_0] (main.py 265): INFO Train: [54/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2952 (0.3064)	loss 1.4709 (1.3707)	grad_norm 23.8342 (30.3186)	mem 4879MB
[2022-05-31 06:00:21 MetaFG_0] (main.py 265): INFO Train: [54/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.3003 (0.3063)	loss 1.3805 (1.3712)	grad_norm 24.1261 (30.2932)	mem 4879MB
[2022-05-31 06:00:24 MetaFG_0] (main.py 265): INFO Train: [54/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2939 (0.3063)	loss 1.6311 (1.3704)	grad_norm 27.4281 (30.2619)	mem 4879MB
[2022-05-31 06:00:27 MetaFG_0] (main.py 265): INFO Train: [54/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.3004 (0.3063)	loss 1.6045 (1.3700)	grad_norm 18.6322 (30.2696)	mem 4879MB
[2022-05-31 06:00:30 MetaFG_0] (main.py 265): INFO Train: [54/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3000 (0.3063)	loss 1.1508 (1.3702)	grad_norm 20.8038 (30.2802)	mem 4879MB
[2022-05-31 06:00:33 MetaFG_0] (main.py 265): INFO Train: [54/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.3050 (0.3063)	loss 1.3143 (1.3710)	grad_norm 41.9929 (30.3311)	mem 4879MB
[2022-05-31 06:00:36 MetaFG_0] (main.py 265): INFO Train: [54/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2991 (0.3063)	loss 1.7048 (1.3717)	grad_norm 26.5903 (30.3475)	mem 4879MB
[2022-05-31 06:00:39 MetaFG_0] (main.py 265): INFO Train: [54/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2990 (0.3063)	loss 1.2169 (1.3708)	grad_norm 27.2549 (30.3341)	mem 4879MB
[2022-05-31 06:00:42 MetaFG_0] (main.py 265): INFO Train: [54/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2966 (0.3062)	loss 1.5443 (1.3700)	grad_norm 18.9277 (30.3471)	mem 4879MB
[2022-05-31 06:00:45 MetaFG_0] (main.py 265): INFO Train: [54/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2948 (0.3062)	loss 1.1164 (1.3687)	grad_norm 32.4300 (30.3612)	mem 4879MB
[2022-05-31 06:00:48 MetaFG_0] (main.py 265): INFO Train: [54/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2980 (0.3062)	loss 1.0434 (1.3681)	grad_norm 38.5894 (30.3603)	mem 4879MB
[2022-05-31 06:00:51 MetaFG_0] (main.py 265): INFO Train: [54/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2934 (0.3062)	loss 1.6805 (1.3677)	grad_norm 28.3454 (30.3564)	mem 4879MB
[2022-05-31 06:00:54 MetaFG_0] (main.py 265): INFO Train: [54/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3009 (0.3062)	loss 1.2259 (1.3677)	grad_norm 33.3283 (30.3312)	mem 4879MB
[2022-05-31 06:00:57 MetaFG_0] (main.py 265): INFO Train: [54/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2997 (0.3062)	loss 1.1031 (1.3661)	grad_norm 27.1809 (30.3386)	mem 4879MB
[2022-05-31 06:01:00 MetaFG_0] (main.py 265): INFO Train: [54/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2924 (0.3062)	loss 1.5242 (1.3668)	grad_norm 28.1996 (30.3425)	mem 4879MB
[2022-05-31 06:01:03 MetaFG_0] (main.py 265): INFO Train: [54/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2926 (0.3062)	loss 1.0688 (1.3667)	grad_norm 29.1903 (30.3564)	mem 4879MB
[2022-05-31 06:01:06 MetaFG_0] (main.py 265): INFO Train: [54/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2931 (0.3062)	loss 1.8265 (1.3668)	grad_norm 47.3826 (30.3638)	mem 4879MB
[2022-05-31 06:01:09 MetaFG_0] (main.py 265): INFO Train: [54/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2938 (0.3062)	loss 1.5664 (1.3662)	grad_norm 40.3162 (30.3612)	mem 4879MB
[2022-05-31 06:01:12 MetaFG_0] (main.py 265): INFO Train: [54/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2927 (0.3061)	loss 1.3195 (1.3667)	grad_norm 28.9100 (30.3254)	mem 4879MB
[2022-05-31 06:01:15 MetaFG_0] (main.py 265): INFO Train: [54/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2922 (0.3061)	loss 1.6712 (1.3669)	grad_norm 26.6254 (30.3397)	mem 4879MB
[2022-05-31 06:01:18 MetaFG_0] (main.py 265): INFO Train: [54/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.3006 (0.3061)	loss 1.5740 (1.3653)	grad_norm 37.6142 (30.3374)	mem 4879MB
[2022-05-31 06:01:22 MetaFG_0] (main.py 265): INFO Train: [54/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2945 (0.3061)	loss 1.6325 (1.3658)	grad_norm 23.3655 (30.3407)	mem 4879MB
[2022-05-31 06:01:25 MetaFG_0] (main.py 265): INFO Train: [54/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2918 (0.3061)	loss 1.7004 (1.3658)	grad_norm 26.2302 (30.3719)	mem 4879MB
[2022-05-31 06:01:28 MetaFG_0] (main.py 265): INFO Train: [54/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.3001 (0.3061)	loss 1.3563 (1.3663)	grad_norm 46.3215 (30.3559)	mem 4879MB
[2022-05-31 06:01:31 MetaFG_0] (main.py 265): INFO Train: [54/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2929 (0.3061)	loss 1.6549 (1.3655)	grad_norm 33.7354 (30.3750)	mem 4879MB
[2022-05-31 06:01:34 MetaFG_0] (main.py 265): INFO Train: [54/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2979 (0.3061)	loss 0.9738 (1.3652)	grad_norm 21.3961 (30.3600)	mem 4879MB
[2022-05-31 06:01:37 MetaFG_0] (main.py 265): INFO Train: [54/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2986 (0.3060)	loss 1.5864 (1.3664)	grad_norm 24.4810 (30.4510)	mem 4879MB
[2022-05-31 06:01:40 MetaFG_0] (main.py 265): INFO Train: [54/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2937 (0.3061)	loss 1.4965 (1.3662)	grad_norm 36.1585 (30.4501)	mem 4879MB
[2022-05-31 06:01:43 MetaFG_0] (main.py 265): INFO Train: [54/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.3043 (0.3060)	loss 1.3718 (1.3662)	grad_norm 21.9165 (30.4708)	mem 4879MB
[2022-05-31 06:01:46 MetaFG_0] (main.py 265): INFO Train: [54/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2935 (0.3061)	loss 1.5329 (1.3661)	grad_norm 16.2117 (30.4083)	mem 4879MB
[2022-05-31 06:01:49 MetaFG_0] (main.py 265): INFO Train: [54/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2996 (0.3060)	loss 1.5193 (1.3663)	grad_norm 28.8839 (30.3638)	mem 4879MB
[2022-05-31 06:01:52 MetaFG_0] (main.py 265): INFO Train: [54/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2925 (0.3060)	loss 1.4931 (1.3659)	grad_norm 33.0110 (30.3515)	mem 4879MB
[2022-05-31 06:01:55 MetaFG_0] (main.py 265): INFO Train: [54/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2919 (0.3060)	loss 1.5427 (1.3660)	grad_norm 28.8627 (30.3581)	mem 4879MB
[2022-05-31 06:01:58 MetaFG_0] (main.py 265): INFO Train: [54/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2919 (0.3060)	loss 1.6069 (1.3652)	grad_norm 27.7160 (30.3470)	mem 4879MB
[2022-05-31 06:02:01 MetaFG_0] (main.py 265): INFO Train: [54/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2919 (0.3060)	loss 1.5631 (1.3652)	grad_norm 38.8662 (30.3427)	mem 4879MB
[2022-05-31 06:02:04 MetaFG_0] (main.py 265): INFO Train: [54/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3002 (0.3060)	loss 1.4093 (1.3650)	grad_norm 25.7122 (30.3299)	mem 4879MB
[2022-05-31 06:02:07 MetaFG_0] (main.py 265): INFO Train: [54/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.3027 (0.3060)	loss 1.4303 (1.3644)	grad_norm 23.6583 (30.3044)	mem 4879MB
[2022-05-31 06:02:10 MetaFG_0] (main.py 265): INFO Train: [54/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2962 (0.3060)	loss 1.2635 (1.3645)	grad_norm 34.9061 (30.3063)	mem 4879MB
[2022-05-31 06:02:13 MetaFG_0] (main.py 265): INFO Train: [54/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2934 (0.3060)	loss 1.8433 (1.3653)	grad_norm 42.5574 (30.3264)	mem 4879MB
[2022-05-31 06:02:16 MetaFG_0] (main.py 265): INFO Train: [54/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2930 (0.3059)	loss 1.3400 (1.3657)	grad_norm 23.4871 (30.3007)	mem 4879MB
[2022-05-31 06:02:20 MetaFG_0] (main.py 265): INFO Train: [54/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2980 (0.3060)	loss 1.6108 (1.3661)	grad_norm 29.2280 (30.3185)	mem 4879MB
[2022-05-31 06:02:23 MetaFG_0] (main.py 265): INFO Train: [54/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2923 (0.3060)	loss 1.3286 (1.3661)	grad_norm 33.2607 (30.3039)	mem 4879MB
[2022-05-31 06:02:26 MetaFG_0] (main.py 265): INFO Train: [54/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2988 (0.3060)	loss 1.1830 (1.3664)	grad_norm 17.6117 (30.2620)	mem 4879MB
[2022-05-31 06:02:29 MetaFG_0] (main.py 265): INFO Train: [54/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2983 (0.3060)	loss 1.1849 (1.3657)	grad_norm 16.5677 (30.2329)	mem 4879MB
[2022-05-31 06:02:32 MetaFG_0] (main.py 265): INFO Train: [54/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2933 (0.3060)	loss 1.1613 (1.3655)	grad_norm 28.7329 (30.2256)	mem 4879MB
[2022-05-31 06:02:35 MetaFG_0] (main.py 265): INFO Train: [54/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2941 (0.3060)	loss 1.4219 (1.3656)	grad_norm 18.0948 (30.2903)	mem 4879MB
[2022-05-31 06:02:38 MetaFG_0] (main.py 265): INFO Train: [54/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2928 (0.3059)	loss 0.9847 (1.3654)	grad_norm 31.1984 (30.2887)	mem 4879MB
[2022-05-31 06:02:41 MetaFG_0] (main.py 265): INFO Train: [54/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2941 (0.3059)	loss 1.4628 (1.3653)	grad_norm 26.8175 (30.2449)	mem 4879MB
[2022-05-31 06:02:44 MetaFG_0] (main.py 265): INFO Train: [54/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2998 (0.3059)	loss 1.3455 (1.3655)	grad_norm 35.7857 (30.2175)	mem 4879MB
[2022-05-31 06:02:47 MetaFG_0] (main.py 265): INFO Train: [54/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2980 (0.3059)	loss 1.2792 (1.3651)	grad_norm 25.4390 (30.2211)	mem 4879MB
[2022-05-31 06:02:50 MetaFG_0] (main.py 265): INFO Train: [54/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.3002 (0.3059)	loss 1.3108 (1.3644)	grad_norm 47.6409 (30.2169)	mem 4879MB
[2022-05-31 06:02:53 MetaFG_0] (main.py 265): INFO Train: [54/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.3323 (0.3061)	loss 1.1022 (1.3644)	grad_norm 39.7192 (30.1881)	mem 4879MB
[2022-05-31 06:02:56 MetaFG_0] (main.py 265): INFO Train: [54/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2963 (0.3061)	loss 1.4057 (1.3651)	grad_norm 21.9399 (30.1838)	mem 4879MB
[2022-05-31 06:03:00 MetaFG_0] (main.py 265): INFO Train: [54/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2989 (0.3061)	loss 0.9756 (1.3652)	grad_norm 22.6307 (30.1832)	mem 4879MB
[2022-05-31 06:03:03 MetaFG_0] (main.py 265): INFO Train: [54/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2956 (0.3061)	loss 1.5508 (1.3657)	grad_norm 37.2268 (30.1368)	mem 4879MB
[2022-05-31 06:03:06 MetaFG_0] (main.py 265): INFO Train: [54/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2927 (0.3061)	loss 1.5273 (1.3657)	grad_norm 25.0328 (30.1262)	mem 4879MB
[2022-05-31 06:03:09 MetaFG_0] (main.py 265): INFO Train: [54/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2925 (0.3061)	loss 1.4111 (1.3649)	grad_norm 20.2697 (30.0959)	mem 4879MB
[2022-05-31 06:03:12 MetaFG_0] (main.py 265): INFO Train: [54/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2938 (0.3061)	loss 1.5708 (1.3655)	grad_norm 33.6706 (30.1107)	mem 4879MB
[2022-05-31 06:03:15 MetaFG_0] (main.py 265): INFO Train: [54/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2919 (0.3061)	loss 1.2001 (1.3655)	grad_norm 24.8672 (30.0861)	mem 4879MB
[2022-05-31 06:03:18 MetaFG_0] (main.py 265): INFO Train: [54/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2993 (0.3061)	loss 1.4661 (1.3656)	grad_norm 27.5688 (nan)	mem 4879MB
[2022-05-31 06:03:21 MetaFG_0] (main.py 265): INFO Train: [54/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2995 (0.3060)	loss 1.4544 (1.3655)	grad_norm 24.0950 (nan)	mem 4879MB
[2022-05-31 06:03:24 MetaFG_0] (main.py 265): INFO Train: [54/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.3064 (0.3060)	loss 1.6812 (1.3656)	grad_norm 33.2143 (nan)	mem 4879MB
[2022-05-31 06:03:27 MetaFG_0] (main.py 265): INFO Train: [54/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2978 (0.3060)	loss 0.9975 (1.3655)	grad_norm 50.8669 (nan)	mem 4879MB
[2022-05-31 06:03:30 MetaFG_0] (main.py 265): INFO Train: [54/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3062 (0.3060)	loss 1.4898 (1.3652)	grad_norm 32.1694 (nan)	mem 4879MB
[2022-05-31 06:03:33 MetaFG_0] (main.py 265): INFO Train: [54/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2980 (0.3060)	loss 1.2487 (1.3654)	grad_norm 23.4620 (nan)	mem 4879MB
[2022-05-31 06:03:36 MetaFG_0] (main.py 265): INFO Train: [54/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2970 (0.3060)	loss 1.5386 (1.3656)	grad_norm 23.9055 (nan)	mem 4879MB
[2022-05-31 06:03:39 MetaFG_0] (main.py 265): INFO Train: [54/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2927 (0.3060)	loss 1.5491 (1.3656)	grad_norm 31.2003 (nan)	mem 4879MB
[2022-05-31 06:03:42 MetaFG_0] (main.py 265): INFO Train: [54/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2991 (0.3060)	loss 1.4549 (1.3664)	grad_norm 28.5620 (nan)	mem 4879MB
[2022-05-31 06:03:45 MetaFG_0] (main.py 265): INFO Train: [54/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2990 (0.3060)	loss 1.5277 (1.3669)	grad_norm 29.0181 (nan)	mem 4879MB
[2022-05-31 06:03:48 MetaFG_0] (main.py 265): INFO Train: [54/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2924 (0.3060)	loss 1.2041 (1.3671)	grad_norm 42.1064 (nan)	mem 4879MB
[2022-05-31 06:03:51 MetaFG_0] (main.py 265): INFO Train: [54/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2922 (0.3060)	loss 1.3965 (1.3671)	grad_norm 18.5820 (nan)	mem 4879MB
[2022-05-31 06:03:54 MetaFG_0] (main.py 265): INFO Train: [54/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2991 (0.3060)	loss 1.2926 (1.3665)	grad_norm 13.4218 (nan)	mem 4879MB
[2022-05-31 06:03:57 MetaFG_0] (main.py 265): INFO Train: [54/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2995 (0.3060)	loss 1.4675 (1.3660)	grad_norm 32.0874 (nan)	mem 4879MB
[2022-05-31 06:04:00 MetaFG_0] (main.py 265): INFO Train: [54/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2922 (0.3060)	loss 0.9873 (1.3658)	grad_norm 26.7924 (nan)	mem 4879MB
[2022-05-31 06:04:01 MetaFG_0] (main.py 272): INFO EPOCH 54 training takes 0:07:58
[2022-05-31 06:04:01 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_54.pth saving......
[2022-05-31 06:04:02 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_54.pth saved !!!
[2022-05-31 06:04:02 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:04:03 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:04:03 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:04:04 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.653 (0.653)	Loss 0.4071 (0.4071)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:04:05 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.103 (0.148)	Loss 0.6176 (0.5650)	Acc@1 87.500 (86.648)	Acc@5 96.875 (99.432)	Mem 4879MB
[2022-05-31 06:04:06 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.122)	Loss 0.4236 (0.5776)	Acc@1 93.750 (86.756)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 06:04:07 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.114)	Loss 0.6974 (0.6080)	Acc@1 81.250 (86.089)	Acc@5 100.000 (98.790)	Mem 4879MB
[2022-05-31 06:04:08 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.109)	Loss 0.6811 (0.6263)	Acc@1 81.250 (85.366)	Acc@5 96.875 (98.628)	Mem 4879MB
[2022-05-31 06:04:09 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.102 (0.106)	Loss 0.6604 (0.6210)	Acc@1 87.500 (85.600)	Acc@5 96.875 (98.591)	Mem 4879MB
[2022-05-31 06:04:10 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.099 (0.104)	Loss 1.0593 (0.6231)	Acc@1 68.750 (85.861)	Acc@5 100.000 (98.668)	Mem 4879MB
[2022-05-31 06:04:11 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.102)	Loss 0.7557 (0.6256)	Acc@1 78.125 (85.871)	Acc@5 96.875 (98.636)	Mem 4879MB
[2022-05-31 06:04:12 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.089 (0.101)	Loss 0.6218 (0.6160)	Acc@1 87.500 (86.188)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 06:04:13 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.101)	Loss 0.3396 (0.6126)	Acc@1 93.750 (86.401)	Acc@5 100.000 (98.729)	Mem 4879MB
[2022-05-31 06:04:14 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.4490 (0.6060)	Acc@1 90.625 (86.665)	Acc@5 100.000 (98.731)	Mem 4879MB
[2022-05-31 06:04:15 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.097 (0.100)	Loss 0.4934 (0.6075)	Acc@1 90.625 (86.684)	Acc@5 100.000 (98.733)	Mem 4879MB
[2022-05-31 06:04:15 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.099)	Loss 0.4976 (0.6114)	Acc@1 90.625 (86.622)	Acc@5 96.875 (98.760)	Mem 4879MB
[2022-05-31 06:04:16 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.097 (0.099)	Loss 0.9776 (0.6109)	Acc@1 68.750 (86.641)	Acc@5 96.875 (98.736)	Mem 4879MB
[2022-05-31 06:04:17 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.4083 (0.6072)	Acc@1 93.750 (86.813)	Acc@5 100.000 (98.759)	Mem 4879MB
[2022-05-31 06:04:18 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.095 (0.099)	Loss 0.8335 (0.6103)	Acc@1 68.750 (86.631)	Acc@5 100.000 (98.738)	Mem 4879MB
[2022-05-31 06:04:19 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.5312 (0.6165)	Acc@1 87.500 (86.432)	Acc@5 100.000 (98.641)	Mem 4879MB
[2022-05-31 06:04:20 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.5032 (0.6149)	Acc@1 84.375 (86.422)	Acc@5 100.000 (98.666)	Mem 4879MB
[2022-05-31 06:04:21 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.092 (0.098)	Loss 0.4333 (0.6199)	Acc@1 96.875 (86.240)	Acc@5 100.000 (98.584)	Mem 4879MB
[2022-05-31 06:04:22 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.6067 (0.6192)	Acc@1 90.625 (86.224)	Acc@5 96.875 (98.609)	Mem 4879MB
[2022-05-31 06:04:23 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.098)	Loss 0.7914 (0.6212)	Acc@1 78.125 (86.225)	Acc@5 100.000 (98.616)	Mem 4879MB
[2022-05-31 06:04:24 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.098 (0.097)	Loss 0.6658 (0.6200)	Acc@1 84.375 (86.211)	Acc@5 96.875 (98.623)	Mem 4879MB
[2022-05-31 06:04:25 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.097)	Loss 0.3846 (0.6218)	Acc@1 93.750 (86.100)	Acc@5 100.000 (98.600)	Mem 4879MB
[2022-05-31 06:04:26 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.088 (0.097)	Loss 0.5246 (0.6219)	Acc@1 93.750 (86.174)	Acc@5 96.875 (98.607)	Mem 4879MB
[2022-05-31 06:04:27 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.6908 (0.6218)	Acc@1 87.500 (86.164)	Acc@5 96.875 (98.626)	Mem 4879MB
[2022-05-31 06:04:28 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.5792 (0.6189)	Acc@1 84.375 (86.255)	Acc@5 100.000 (98.643)	Mem 4879MB
[2022-05-31 06:04:29 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.091 (0.097)	Loss 0.5894 (0.6250)	Acc@1 90.625 (86.123)	Acc@5 100.000 (98.551)	Mem 4879MB
[2022-05-31 06:04:30 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.088 (0.097)	Loss 0.5434 (0.6210)	Acc@1 96.875 (86.266)	Acc@5 100.000 (98.605)	Mem 4879MB
[2022-05-31 06:04:31 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.101 (0.097)	Loss 0.5219 (0.6180)	Acc@1 87.500 (86.366)	Acc@5 100.000 (98.621)	Mem 4879MB
[2022-05-31 06:04:32 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.091 (0.096)	Loss 0.5436 (0.6203)	Acc@1 90.625 (86.276)	Acc@5 96.875 (98.625)	Mem 4879MB
[2022-05-31 06:04:32 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.088 (0.096)	Loss 0.4012 (0.6207)	Acc@1 90.625 (86.254)	Acc@5 100.000 (98.630)	Mem 4879MB
[2022-05-31 06:04:33 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7219 (0.6191)	Acc@1 81.250 (86.304)	Acc@5 100.000 (98.643)	Mem 4879MB
[2022-05-31 06:04:34 MetaFG_0] (main.py 330): INFO  * Acc@1 86.290 Acc@5 98.640
[2022-05-31 06:04:34 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.3%
[2022-05-31 06:04:34 MetaFG_0] (main.py 171): INFO Max accuracy: 86.29%
[2022-05-31 06:04:35 MetaFG_0] (main.py 265): INFO Train: [55/300][0/1562]	eta 0:25:45 lr 0.000006	time 0.9892 (0.9892)	loss 1.2104 (1.2104)	grad_norm 17.5290 (17.5290)	mem 4879MB
[2022-05-31 06:04:38 MetaFG_0] (main.py 265): INFO Train: [55/300][10/1562]	eta 0:09:38 lr 0.000006	time 0.2996 (0.3729)	loss 1.4116 (1.3106)	grad_norm 33.9121 (26.9243)	mem 4879MB
[2022-05-31 06:04:41 MetaFG_0] (main.py 265): INFO Train: [55/300][20/1562]	eta 0:08:45 lr 0.000006	time 0.3015 (0.3411)	loss 1.2058 (1.2984)	grad_norm 18.8497 (27.6789)	mem 4879MB
[2022-05-31 06:04:44 MetaFG_0] (main.py 265): INFO Train: [55/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2993 (0.3302)	loss 1.2724 (1.3135)	grad_norm 16.6217 (26.9990)	mem 4879MB
[2022-05-31 06:04:47 MetaFG_0] (main.py 265): INFO Train: [55/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.3013 (0.3242)	loss 1.3173 (1.3241)	grad_norm 33.4137 (28.1794)	mem 4879MB
[2022-05-31 06:04:50 MetaFG_0] (main.py 265): INFO Train: [55/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.3032 (0.3202)	loss 1.1057 (1.3309)	grad_norm 38.1695 (28.1977)	mem 4879MB
[2022-05-31 06:04:53 MetaFG_0] (main.py 265): INFO Train: [55/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2984 (0.3180)	loss 1.3316 (1.3247)	grad_norm 27.9426 (28.8184)	mem 4879MB
[2022-05-31 06:04:56 MetaFG_0] (main.py 265): INFO Train: [55/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2970 (0.3160)	loss 1.2928 (1.3374)	grad_norm 22.5122 (28.5411)	mem 4879MB
[2022-05-31 06:04:59 MetaFG_0] (main.py 265): INFO Train: [55/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2932 (0.3148)	loss 1.4767 (1.3476)	grad_norm 12.8229 (29.0810)	mem 4879MB
[2022-05-31 06:05:02 MetaFG_0] (main.py 265): INFO Train: [55/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2938 (0.3136)	loss 1.5583 (1.3507)	grad_norm 25.0623 (29.6167)	mem 4879MB
[2022-05-31 06:05:05 MetaFG_0] (main.py 265): INFO Train: [55/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2926 (0.3127)	loss 1.0977 (1.3539)	grad_norm 41.0891 (29.6472)	mem 4879MB
[2022-05-31 06:05:08 MetaFG_0] (main.py 265): INFO Train: [55/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.3049 (0.3119)	loss 1.2789 (1.3552)	grad_norm 20.9094 (29.5510)	mem 4879MB
[2022-05-31 06:05:11 MetaFG_0] (main.py 265): INFO Train: [55/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2984 (0.3113)	loss 0.9137 (1.3525)	grad_norm 17.8944 (30.0511)	mem 4879MB
[2022-05-31 06:05:14 MetaFG_0] (main.py 265): INFO Train: [55/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2929 (0.3107)	loss 0.8624 (1.3497)	grad_norm 17.2974 (30.0516)	mem 4879MB
[2022-05-31 06:05:17 MetaFG_0] (main.py 265): INFO Train: [55/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2936 (0.3101)	loss 1.0313 (1.3375)	grad_norm 21.8408 (30.2170)	mem 4879MB
[2022-05-31 06:05:20 MetaFG_0] (main.py 265): INFO Train: [55/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2924 (0.3098)	loss 0.8919 (1.3383)	grad_norm 28.7737 (29.8951)	mem 4879MB
[2022-05-31 06:05:23 MetaFG_0] (main.py 265): INFO Train: [55/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2940 (0.3095)	loss 1.7073 (1.3399)	grad_norm 28.1553 (29.7575)	mem 4879MB
[2022-05-31 06:05:26 MetaFG_0] (main.py 265): INFO Train: [55/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.3025 (0.3093)	loss 1.6047 (1.3360)	grad_norm 47.6738 (29.7480)	mem 4879MB
[2022-05-31 06:05:30 MetaFG_0] (main.py 265): INFO Train: [55/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.3005 (0.3092)	loss 1.4262 (1.3361)	grad_norm 25.2761 (29.3863)	mem 4879MB
[2022-05-31 06:05:33 MetaFG_0] (main.py 265): INFO Train: [55/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.3011 (0.3089)	loss 0.9839 (1.3365)	grad_norm 21.1541 (29.4166)	mem 4879MB
[2022-05-31 06:05:36 MetaFG_0] (main.py 265): INFO Train: [55/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2935 (0.3087)	loss 1.1472 (1.3364)	grad_norm 33.8748 (29.4469)	mem 4879MB
[2022-05-31 06:05:39 MetaFG_0] (main.py 265): INFO Train: [55/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2923 (0.3085)	loss 1.5921 (1.3395)	grad_norm 36.7819 (29.5028)	mem 4879MB
[2022-05-31 06:05:42 MetaFG_0] (main.py 265): INFO Train: [55/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.3015 (0.3083)	loss 1.1727 (1.3424)	grad_norm 33.0219 (29.8034)	mem 4879MB
[2022-05-31 06:05:45 MetaFG_0] (main.py 265): INFO Train: [55/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.3001 (0.3082)	loss 1.2387 (1.3391)	grad_norm 23.7302 (29.8675)	mem 4879MB
[2022-05-31 06:05:48 MetaFG_0] (main.py 265): INFO Train: [55/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2934 (0.3081)	loss 1.7421 (1.3377)	grad_norm 31.8918 (29.8913)	mem 4879MB
[2022-05-31 06:05:51 MetaFG_0] (main.py 265): INFO Train: [55/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2925 (0.3079)	loss 1.5057 (1.3405)	grad_norm 22.5647 (29.8344)	mem 4879MB
[2022-05-31 06:05:54 MetaFG_0] (main.py 265): INFO Train: [55/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2989 (0.3079)	loss 1.5462 (1.3364)	grad_norm 32.8407 (29.9419)	mem 4879MB
[2022-05-31 06:05:57 MetaFG_0] (main.py 265): INFO Train: [55/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2982 (0.3078)	loss 1.4231 (1.3386)	grad_norm 32.3009 (29.7758)	mem 4879MB
[2022-05-31 06:06:00 MetaFG_0] (main.py 265): INFO Train: [55/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2988 (0.3076)	loss 1.4775 (1.3363)	grad_norm 19.7771 (29.7392)	mem 4879MB
[2022-05-31 06:06:03 MetaFG_0] (main.py 265): INFO Train: [55/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.3001 (0.3076)	loss 1.6228 (1.3385)	grad_norm 26.8372 (29.7523)	mem 4879MB
[2022-05-31 06:06:06 MetaFG_0] (main.py 265): INFO Train: [55/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2924 (0.3075)	loss 1.4199 (1.3414)	grad_norm 21.2638 (29.7238)	mem 4879MB
[2022-05-31 06:06:09 MetaFG_0] (main.py 265): INFO Train: [55/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2930 (0.3075)	loss 1.3000 (1.3380)	grad_norm 36.0089 (29.5724)	mem 4879MB
[2022-05-31 06:06:12 MetaFG_0] (main.py 265): INFO Train: [55/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2951 (0.3074)	loss 1.0201 (1.3367)	grad_norm 48.7884 (29.7586)	mem 4879MB
[2022-05-31 06:06:15 MetaFG_0] (main.py 265): INFO Train: [55/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2993 (0.3074)	loss 1.3482 (1.3388)	grad_norm 24.2289 (29.7787)	mem 4879MB
[2022-05-31 06:06:18 MetaFG_0] (main.py 265): INFO Train: [55/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2929 (0.3073)	loss 1.1816 (1.3365)	grad_norm 23.3754 (29.8502)	mem 4879MB
[2022-05-31 06:06:21 MetaFG_0] (main.py 265): INFO Train: [55/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2950 (0.3072)	loss 1.5005 (1.3402)	grad_norm 60.9197 (29.9194)	mem 4879MB
[2022-05-31 06:06:25 MetaFG_0] (main.py 265): INFO Train: [55/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2984 (0.3072)	loss 0.8124 (1.3366)	grad_norm 25.0318 (30.1353)	mem 4879MB
[2022-05-31 06:06:28 MetaFG_0] (main.py 265): INFO Train: [55/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2994 (0.3072)	loss 1.4350 (1.3372)	grad_norm 40.1606 (30.0854)	mem 4879MB
[2022-05-31 06:06:31 MetaFG_0] (main.py 265): INFO Train: [55/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2934 (0.3072)	loss 1.4763 (1.3347)	grad_norm 27.8578 (30.0253)	mem 4879MB
[2022-05-31 06:06:34 MetaFG_0] (main.py 265): INFO Train: [55/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2987 (0.3071)	loss 1.5216 (1.3389)	grad_norm 18.5609 (30.0846)	mem 4879MB
[2022-05-31 06:06:37 MetaFG_0] (main.py 265): INFO Train: [55/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2993 (0.3071)	loss 1.5076 (1.3413)	grad_norm 31.3452 (30.0465)	mem 4879MB
[2022-05-31 06:06:40 MetaFG_0] (main.py 265): INFO Train: [55/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2985 (0.3070)	loss 1.7426 (1.3444)	grad_norm 19.9034 (30.0126)	mem 4879MB
[2022-05-31 06:06:43 MetaFG_0] (main.py 265): INFO Train: [55/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2941 (0.3070)	loss 1.3560 (1.3456)	grad_norm 32.9764 (30.0391)	mem 4879MB
[2022-05-31 06:06:46 MetaFG_0] (main.py 265): INFO Train: [55/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2931 (0.3069)	loss 1.4600 (1.3481)	grad_norm 20.0333 (29.9428)	mem 4879MB
[2022-05-31 06:06:49 MetaFG_0] (main.py 265): INFO Train: [55/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2934 (0.3069)	loss 1.3956 (1.3473)	grad_norm 24.6644 (29.8911)	mem 4879MB
[2022-05-31 06:06:52 MetaFG_0] (main.py 265): INFO Train: [55/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2995 (0.3069)	loss 1.6028 (1.3465)	grad_norm 21.4475 (29.9745)	mem 4879MB
[2022-05-31 06:06:55 MetaFG_0] (main.py 265): INFO Train: [55/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2929 (0.3068)	loss 1.3524 (1.3479)	grad_norm 26.3426 (29.9059)	mem 4879MB
[2022-05-31 06:06:58 MetaFG_0] (main.py 265): INFO Train: [55/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2926 (0.3068)	loss 1.0251 (1.3483)	grad_norm 31.4739 (29.9611)	mem 4879MB
[2022-05-31 06:07:01 MetaFG_0] (main.py 265): INFO Train: [55/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2929 (0.3067)	loss 0.9662 (1.3484)	grad_norm 22.8318 (30.0042)	mem 4879MB
[2022-05-31 06:07:04 MetaFG_0] (main.py 265): INFO Train: [55/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2988 (0.3067)	loss 0.9908 (1.3479)	grad_norm 45.0231 (30.0087)	mem 4879MB
[2022-05-31 06:07:07 MetaFG_0] (main.py 265): INFO Train: [55/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2938 (0.3067)	loss 1.3288 (1.3466)	grad_norm 25.8898 (30.0477)	mem 4879MB
[2022-05-31 06:07:10 MetaFG_0] (main.py 265): INFO Train: [55/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2994 (0.3066)	loss 1.7998 (1.3477)	grad_norm 24.1943 (30.1782)	mem 4879MB
[2022-05-31 06:07:13 MetaFG_0] (main.py 265): INFO Train: [55/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2923 (0.3066)	loss 1.3894 (1.3468)	grad_norm 37.9849 (30.1187)	mem 4879MB
[2022-05-31 06:07:16 MetaFG_0] (main.py 265): INFO Train: [55/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2991 (0.3066)	loss 1.5828 (1.3493)	grad_norm 31.4751 (30.1204)	mem 4879MB
[2022-05-31 06:07:19 MetaFG_0] (main.py 265): INFO Train: [55/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2927 (0.3065)	loss 1.0164 (1.3505)	grad_norm 44.3555 (30.1241)	mem 4879MB
[2022-05-31 06:07:22 MetaFG_0] (main.py 265): INFO Train: [55/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2984 (0.3065)	loss 1.6190 (1.3500)	grad_norm 31.6415 (30.0040)	mem 4879MB
[2022-05-31 06:07:26 MetaFG_0] (main.py 265): INFO Train: [55/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2987 (0.3065)	loss 1.1308 (1.3492)	grad_norm 22.1727 (30.0519)	mem 4879MB
[2022-05-31 06:07:29 MetaFG_0] (main.py 265): INFO Train: [55/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2928 (0.3065)	loss 1.4938 (1.3495)	grad_norm 24.2244 (30.0905)	mem 4879MB
[2022-05-31 06:07:32 MetaFG_0] (main.py 265): INFO Train: [55/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2957 (0.3065)	loss 1.2537 (1.3481)	grad_norm 26.5786 (30.0304)	mem 4879MB
[2022-05-31 06:07:35 MetaFG_0] (main.py 265): INFO Train: [55/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.3006 (0.3064)	loss 0.9309 (1.3444)	grad_norm 32.2784 (30.0441)	mem 4879MB
[2022-05-31 06:07:38 MetaFG_0] (main.py 265): INFO Train: [55/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2999 (0.3065)	loss 1.5922 (1.3458)	grad_norm 34.5292 (30.0950)	mem 4879MB
[2022-05-31 06:07:41 MetaFG_0] (main.py 265): INFO Train: [55/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2931 (0.3064)	loss 1.5797 (1.3458)	grad_norm 24.5965 (30.1596)	mem 4879MB
[2022-05-31 06:07:44 MetaFG_0] (main.py 265): INFO Train: [55/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2990 (0.3065)	loss 1.1901 (1.3457)	grad_norm 30.6314 (30.2090)	mem 4879MB
[2022-05-31 06:07:47 MetaFG_0] (main.py 265): INFO Train: [55/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2919 (0.3064)	loss 1.5250 (1.3462)	grad_norm 28.6407 (30.2113)	mem 4879MB
[2022-05-31 06:07:50 MetaFG_0] (main.py 265): INFO Train: [55/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2984 (0.3064)	loss 1.2279 (1.3461)	grad_norm 21.9105 (30.2166)	mem 4879MB
[2022-05-31 06:07:53 MetaFG_0] (main.py 265): INFO Train: [55/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2930 (0.3064)	loss 1.2479 (1.3451)	grad_norm 21.6075 (30.2180)	mem 4879MB
[2022-05-31 06:07:56 MetaFG_0] (main.py 265): INFO Train: [55/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2978 (0.3064)	loss 1.4110 (1.3442)	grad_norm 20.4641 (30.1565)	mem 4879MB
[2022-05-31 06:07:59 MetaFG_0] (main.py 265): INFO Train: [55/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2925 (0.3064)	loss 1.5342 (1.3456)	grad_norm 26.5496 (30.0964)	mem 4879MB
[2022-05-31 06:08:02 MetaFG_0] (main.py 265): INFO Train: [55/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.3001 (0.3064)	loss 0.8126 (1.3451)	grad_norm 32.6966 (30.1138)	mem 4879MB
[2022-05-31 06:08:05 MetaFG_0] (main.py 265): INFO Train: [55/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2984 (0.3064)	loss 1.2835 (1.3457)	grad_norm 36.1870 (30.0783)	mem 4879MB
[2022-05-31 06:08:08 MetaFG_0] (main.py 265): INFO Train: [55/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2991 (0.3063)	loss 1.3931 (1.3456)	grad_norm 23.8600 (29.9971)	mem 4879MB
[2022-05-31 06:08:12 MetaFG_0] (main.py 265): INFO Train: [55/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.3366 (0.3068)	loss 0.9971 (1.3453)	grad_norm 33.2741 (30.0010)	mem 4879MB
[2022-05-31 06:08:15 MetaFG_0] (main.py 265): INFO Train: [55/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2920 (0.3068)	loss 1.1349 (1.3453)	grad_norm 29.8375 (29.9997)	mem 4879MB
[2022-05-31 06:08:18 MetaFG_0] (main.py 265): INFO Train: [55/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2934 (0.3068)	loss 1.3757 (1.3457)	grad_norm 34.5929 (30.0215)	mem 4879MB
[2022-05-31 06:08:21 MetaFG_0] (main.py 265): INFO Train: [55/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2999 (0.3067)	loss 1.6625 (1.3448)	grad_norm 18.3299 (29.9879)	mem 4879MB
[2022-05-31 06:08:24 MetaFG_0] (main.py 265): INFO Train: [55/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2921 (0.3067)	loss 1.5329 (1.3456)	grad_norm 38.7967 (30.0041)	mem 4879MB
[2022-05-31 06:08:27 MetaFG_0] (main.py 265): INFO Train: [55/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2980 (0.3067)	loss 1.4662 (1.3448)	grad_norm 33.0517 (30.0093)	mem 4879MB
[2022-05-31 06:08:30 MetaFG_0] (main.py 265): INFO Train: [55/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2929 (0.3067)	loss 1.7460 (1.3457)	grad_norm 25.6504 (29.9865)	mem 4879MB
[2022-05-31 06:08:33 MetaFG_0] (main.py 265): INFO Train: [55/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2922 (0.3067)	loss 1.6755 (1.3453)	grad_norm 30.7013 (29.9469)	mem 4879MB
[2022-05-31 06:08:36 MetaFG_0] (main.py 265): INFO Train: [55/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2928 (0.3067)	loss 1.5571 (1.3440)	grad_norm 20.1266 (30.0206)	mem 4879MB
[2022-05-31 06:08:39 MetaFG_0] (main.py 265): INFO Train: [55/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2950 (0.3066)	loss 1.4367 (1.3445)	grad_norm 25.3496 (29.9534)	mem 4879MB
[2022-05-31 06:08:42 MetaFG_0] (main.py 265): INFO Train: [55/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2937 (0.3066)	loss 1.5135 (1.3451)	grad_norm 44.1656 (30.0195)	mem 4879MB
[2022-05-31 06:08:45 MetaFG_0] (main.py 265): INFO Train: [55/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.3001 (0.3066)	loss 1.4416 (1.3453)	grad_norm 23.5120 (29.9984)	mem 4879MB
[2022-05-31 06:08:48 MetaFG_0] (main.py 265): INFO Train: [55/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2940 (0.3066)	loss 1.5915 (1.3448)	grad_norm 22.1169 (30.0688)	mem 4879MB
[2022-05-31 06:08:51 MetaFG_0] (main.py 265): INFO Train: [55/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2997 (0.3066)	loss 1.4432 (1.3454)	grad_norm 25.7486 (30.0277)	mem 4879MB
[2022-05-31 06:08:54 MetaFG_0] (main.py 265): INFO Train: [55/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2934 (0.3066)	loss 0.7797 (1.3443)	grad_norm 37.7466 (30.0486)	mem 4879MB
[2022-05-31 06:08:58 MetaFG_0] (main.py 265): INFO Train: [55/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2949 (0.3065)	loss 1.1643 (1.3435)	grad_norm 27.4617 (29.9991)	mem 4879MB
[2022-05-31 06:09:01 MetaFG_0] (main.py 265): INFO Train: [55/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.2936 (0.3065)	loss 1.4519 (1.3427)	grad_norm 29.8868 (30.0365)	mem 4879MB
[2022-05-31 06:09:04 MetaFG_0] (main.py 265): INFO Train: [55/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2936 (0.3065)	loss 1.4839 (1.3444)	grad_norm 23.4597 (30.0309)	mem 4879MB
[2022-05-31 06:09:07 MetaFG_0] (main.py 265): INFO Train: [55/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2994 (0.3065)	loss 1.4705 (1.3437)	grad_norm 33.0763 (30.1313)	mem 4879MB
[2022-05-31 06:09:10 MetaFG_0] (main.py 265): INFO Train: [55/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2926 (0.3065)	loss 1.4687 (1.3431)	grad_norm 43.5782 (30.1270)	mem 4879MB
[2022-05-31 06:09:13 MetaFG_0] (main.py 265): INFO Train: [55/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2940 (0.3065)	loss 1.5842 (1.3439)	grad_norm 36.1112 (30.1764)	mem 4879MB
[2022-05-31 06:09:16 MetaFG_0] (main.py 265): INFO Train: [55/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2996 (0.3065)	loss 1.6419 (1.3450)	grad_norm 30.7282 (30.1847)	mem 4879MB
[2022-05-31 06:09:19 MetaFG_0] (main.py 265): INFO Train: [55/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2986 (0.3065)	loss 1.3078 (1.3450)	grad_norm 14.1277 (30.1323)	mem 4879MB
[2022-05-31 06:09:22 MetaFG_0] (main.py 265): INFO Train: [55/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2934 (0.3065)	loss 0.8474 (1.3445)	grad_norm 37.9603 (30.1563)	mem 4879MB
[2022-05-31 06:09:25 MetaFG_0] (main.py 265): INFO Train: [55/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3001 (0.3064)	loss 1.0152 (1.3435)	grad_norm 35.3532 (30.1791)	mem 4879MB
[2022-05-31 06:09:28 MetaFG_0] (main.py 265): INFO Train: [55/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2929 (0.3064)	loss 1.4351 (1.3440)	grad_norm 33.5611 (30.1900)	mem 4879MB
[2022-05-31 06:09:31 MetaFG_0] (main.py 265): INFO Train: [55/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2927 (0.3064)	loss 1.4803 (1.3438)	grad_norm 29.3916 (30.1401)	mem 4879MB
[2022-05-31 06:09:34 MetaFG_0] (main.py 265): INFO Train: [55/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2939 (0.3064)	loss 1.6811 (1.3454)	grad_norm 22.0814 (30.1173)	mem 4879MB
[2022-05-31 06:09:37 MetaFG_0] (main.py 265): INFO Train: [55/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2984 (0.3063)	loss 1.5823 (1.3459)	grad_norm 32.7397 (30.0952)	mem 4879MB
[2022-05-31 06:09:40 MetaFG_0] (main.py 265): INFO Train: [55/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.3003 (0.3063)	loss 1.4540 (1.3477)	grad_norm 23.5076 (30.0622)	mem 4879MB
[2022-05-31 06:09:43 MetaFG_0] (main.py 265): INFO Train: [55/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2985 (0.3063)	loss 1.4144 (1.3477)	grad_norm 24.6206 (30.0976)	mem 4879MB
[2022-05-31 06:09:46 MetaFG_0] (main.py 265): INFO Train: [55/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.3017 (0.3063)	loss 1.2514 (1.3485)	grad_norm 25.2665 (30.0609)	mem 4879MB
[2022-05-31 06:09:49 MetaFG_0] (main.py 265): INFO Train: [55/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2930 (0.3063)	loss 0.9996 (1.3492)	grad_norm 64.3290 (30.1233)	mem 4879MB
[2022-05-31 06:09:52 MetaFG_0] (main.py 265): INFO Train: [55/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2990 (0.3063)	loss 1.2758 (1.3494)	grad_norm 30.5144 (30.1530)	mem 4879MB
[2022-05-31 06:09:55 MetaFG_0] (main.py 265): INFO Train: [55/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2951 (0.3062)	loss 1.3729 (1.3490)	grad_norm 33.8772 (30.1816)	mem 4879MB
[2022-05-31 06:09:58 MetaFG_0] (main.py 265): INFO Train: [55/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2984 (0.3062)	loss 1.4074 (1.3487)	grad_norm 23.3243 (30.1502)	mem 4879MB
[2022-05-31 06:10:02 MetaFG_0] (main.py 265): INFO Train: [55/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2931 (0.3062)	loss 1.6004 (1.3500)	grad_norm 21.4462 (30.1412)	mem 4879MB
[2022-05-31 06:10:05 MetaFG_0] (main.py 265): INFO Train: [55/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2935 (0.3062)	loss 0.8827 (1.3502)	grad_norm 25.4128 (30.1295)	mem 4879MB
[2022-05-31 06:10:08 MetaFG_0] (main.py 265): INFO Train: [55/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2926 (0.3062)	loss 1.3373 (1.3501)	grad_norm 38.3573 (30.1446)	mem 4879MB
[2022-05-31 06:10:11 MetaFG_0] (main.py 265): INFO Train: [55/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2935 (0.3062)	loss 1.2320 (1.3489)	grad_norm 32.6481 (30.1625)	mem 4879MB
[2022-05-31 06:10:14 MetaFG_0] (main.py 265): INFO Train: [55/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2963 (0.3062)	loss 1.4888 (1.3476)	grad_norm 27.6842 (30.1750)	mem 4879MB
[2022-05-31 06:10:17 MetaFG_0] (main.py 265): INFO Train: [55/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2977 (0.3062)	loss 1.3566 (1.3478)	grad_norm 28.0019 (30.1683)	mem 4879MB
[2022-05-31 06:10:20 MetaFG_0] (main.py 265): INFO Train: [55/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2925 (0.3062)	loss 1.5232 (1.3485)	grad_norm 36.0835 (30.1734)	mem 4879MB
[2022-05-31 06:10:23 MetaFG_0] (main.py 265): INFO Train: [55/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2990 (0.3062)	loss 1.1238 (1.3481)	grad_norm 50.6867 (30.2067)	mem 4879MB
[2022-05-31 06:10:26 MetaFG_0] (main.py 265): INFO Train: [55/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2920 (0.3062)	loss 1.4873 (1.3480)	grad_norm 18.2701 (30.1666)	mem 4879MB
[2022-05-31 06:10:29 MetaFG_0] (main.py 265): INFO Train: [55/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2998 (0.3062)	loss 1.5665 (1.3479)	grad_norm 22.8668 (30.1227)	mem 4879MB
[2022-05-31 06:10:32 MetaFG_0] (main.py 265): INFO Train: [55/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2933 (0.3062)	loss 0.9504 (1.3469)	grad_norm 41.4048 (30.0924)	mem 4879MB
[2022-05-31 06:10:35 MetaFG_0] (main.py 265): INFO Train: [55/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2987 (0.3061)	loss 1.3310 (1.3469)	grad_norm 46.7244 (30.0810)	mem 4879MB
[2022-05-31 06:10:38 MetaFG_0] (main.py 265): INFO Train: [55/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2995 (0.3061)	loss 1.5043 (1.3471)	grad_norm 21.1997 (30.0707)	mem 4879MB
[2022-05-31 06:10:41 MetaFG_0] (main.py 265): INFO Train: [55/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2989 (0.3061)	loss 1.5742 (1.3463)	grad_norm 32.8821 (30.0448)	mem 4879MB
[2022-05-31 06:10:44 MetaFG_0] (main.py 265): INFO Train: [55/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2932 (0.3061)	loss 1.0483 (1.3476)	grad_norm 33.0458 (30.0379)	mem 4879MB
[2022-05-31 06:10:47 MetaFG_0] (main.py 265): INFO Train: [55/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2989 (0.3061)	loss 1.2016 (1.3476)	grad_norm 19.8281 (30.0166)	mem 4879MB
[2022-05-31 06:10:50 MetaFG_0] (main.py 265): INFO Train: [55/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3030 (0.3061)	loss 1.3021 (1.3474)	grad_norm 28.2724 (30.0701)	mem 4879MB
[2022-05-31 06:10:53 MetaFG_0] (main.py 265): INFO Train: [55/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2956 (0.3061)	loss 1.0806 (1.3471)	grad_norm 35.5563 (30.0584)	mem 4879MB
[2022-05-31 06:10:57 MetaFG_0] (main.py 265): INFO Train: [55/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2931 (0.3061)	loss 1.5355 (1.3473)	grad_norm 18.3984 (30.0705)	mem 4879MB
[2022-05-31 06:11:00 MetaFG_0] (main.py 265): INFO Train: [55/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2927 (0.3061)	loss 1.3405 (1.3469)	grad_norm 26.3854 (30.0846)	mem 4879MB
[2022-05-31 06:11:03 MetaFG_0] (main.py 265): INFO Train: [55/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2950 (0.3061)	loss 1.3243 (1.3477)	grad_norm 20.1067 (30.0789)	mem 4879MB
[2022-05-31 06:11:06 MetaFG_0] (main.py 265): INFO Train: [55/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2951 (0.3060)	loss 1.1154 (1.3488)	grad_norm 21.8801 (30.0761)	mem 4879MB
[2022-05-31 06:11:09 MetaFG_0] (main.py 265): INFO Train: [55/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.3022 (0.3060)	loss 1.2364 (1.3492)	grad_norm 18.1518 (30.0917)	mem 4879MB
[2022-05-31 06:11:12 MetaFG_0] (main.py 265): INFO Train: [55/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2998 (0.3060)	loss 1.4016 (1.3487)	grad_norm 17.1327 (30.0787)	mem 4879MB
[2022-05-31 06:11:15 MetaFG_0] (main.py 265): INFO Train: [55/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.3020 (0.3060)	loss 1.2811 (1.3486)	grad_norm 61.7769 (30.1141)	mem 4879MB
[2022-05-31 06:11:18 MetaFG_0] (main.py 265): INFO Train: [55/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2999 (0.3060)	loss 1.0977 (1.3482)	grad_norm 20.5680 (30.0833)	mem 4879MB
[2022-05-31 06:11:21 MetaFG_0] (main.py 265): INFO Train: [55/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2939 (0.3060)	loss 1.4599 (1.3485)	grad_norm 29.7627 (30.1428)	mem 4879MB
[2022-05-31 06:11:24 MetaFG_0] (main.py 265): INFO Train: [55/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2928 (0.3060)	loss 1.6398 (1.3489)	grad_norm 31.7892 (30.1344)	mem 4879MB
[2022-05-31 06:11:27 MetaFG_0] (main.py 265): INFO Train: [55/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2955 (0.3060)	loss 1.5294 (1.3496)	grad_norm 21.4338 (30.1306)	mem 4879MB
[2022-05-31 06:11:30 MetaFG_0] (main.py 265): INFO Train: [55/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2923 (0.3060)	loss 1.6435 (1.3502)	grad_norm 16.9592 (30.1244)	mem 4879MB
[2022-05-31 06:11:33 MetaFG_0] (main.py 265): INFO Train: [55/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2948 (0.3060)	loss 1.8304 (1.3498)	grad_norm 64.0655 (30.1934)	mem 4879MB
[2022-05-31 06:11:36 MetaFG_0] (main.py 265): INFO Train: [55/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2970 (0.3060)	loss 1.0072 (1.3499)	grad_norm 34.5962 (30.1998)	mem 4879MB
[2022-05-31 06:11:39 MetaFG_0] (main.py 265): INFO Train: [55/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2991 (0.3060)	loss 1.2894 (1.3497)	grad_norm 17.3465 (30.2048)	mem 4879MB
[2022-05-31 06:11:42 MetaFG_0] (main.py 265): INFO Train: [55/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2927 (0.3060)	loss 1.1234 (1.3500)	grad_norm 35.5246 (30.2038)	mem 4879MB
[2022-05-31 06:11:45 MetaFG_0] (main.py 265): INFO Train: [55/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2992 (0.3060)	loss 1.6718 (1.3504)	grad_norm 43.2579 (30.2304)	mem 4879MB
[2022-05-31 06:11:48 MetaFG_0] (main.py 265): INFO Train: [55/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2939 (0.3060)	loss 1.6297 (1.3508)	grad_norm 27.5340 (30.1817)	mem 4879MB
[2022-05-31 06:11:51 MetaFG_0] (main.py 265): INFO Train: [55/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2927 (0.3060)	loss 1.0563 (1.3507)	grad_norm 33.1614 (30.1599)	mem 4879MB
[2022-05-31 06:11:55 MetaFG_0] (main.py 265): INFO Train: [55/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2959 (0.3060)	loss 1.4974 (1.3507)	grad_norm 21.7356 (30.1298)	mem 4879MB
[2022-05-31 06:11:58 MetaFG_0] (main.py 265): INFO Train: [55/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2994 (0.3060)	loss 1.6168 (1.3509)	grad_norm 36.0419 (30.1549)	mem 4879MB
[2022-05-31 06:12:01 MetaFG_0] (main.py 265): INFO Train: [55/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2940 (0.3060)	loss 1.2156 (1.3517)	grad_norm 19.1177 (30.1603)	mem 4879MB
[2022-05-31 06:12:04 MetaFG_0] (main.py 265): INFO Train: [55/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2984 (0.3060)	loss 1.5161 (1.3512)	grad_norm 38.2740 (30.1424)	mem 4879MB
[2022-05-31 06:12:07 MetaFG_0] (main.py 265): INFO Train: [55/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2939 (0.3060)	loss 1.6166 (1.3515)	grad_norm 38.8953 (30.1364)	mem 4879MB
[2022-05-31 06:12:10 MetaFG_0] (main.py 265): INFO Train: [55/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2931 (0.3060)	loss 1.6161 (1.3513)	grad_norm 30.9399 (30.1433)	mem 4879MB
[2022-05-31 06:12:13 MetaFG_0] (main.py 265): INFO Train: [55/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2936 (0.3060)	loss 1.3842 (1.3512)	grad_norm 17.1812 (30.1731)	mem 4879MB
[2022-05-31 06:12:16 MetaFG_0] (main.py 265): INFO Train: [55/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2923 (0.3060)	loss 1.3938 (1.3513)	grad_norm 16.5273 (30.2019)	mem 4879MB
[2022-05-31 06:12:19 MetaFG_0] (main.py 265): INFO Train: [55/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2960 (0.3060)	loss 1.1195 (1.3517)	grad_norm 25.1202 (30.1805)	mem 4879MB
[2022-05-31 06:12:22 MetaFG_0] (main.py 265): INFO Train: [55/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2986 (0.3060)	loss 1.5686 (1.3512)	grad_norm 25.0055 (30.1727)	mem 4879MB
[2022-05-31 06:12:25 MetaFG_0] (main.py 265): INFO Train: [55/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2925 (0.3060)	loss 1.2475 (1.3518)	grad_norm 42.6718 (30.2013)	mem 4879MB
[2022-05-31 06:12:28 MetaFG_0] (main.py 265): INFO Train: [55/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2928 (0.3060)	loss 1.6171 (1.3521)	grad_norm 35.7882 (30.1982)	mem 4879MB
[2022-05-31 06:12:31 MetaFG_0] (main.py 265): INFO Train: [55/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2919 (0.3060)	loss 1.1671 (1.3514)	grad_norm 29.6411 (30.2139)	mem 4879MB
[2022-05-31 06:12:32 MetaFG_0] (main.py 272): INFO EPOCH 55 training takes 0:07:58
[2022-05-31 06:12:32 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_55.pth saving......
[2022-05-31 06:12:33 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_55.pth saved !!!
[2022-05-31 06:12:33 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:12:34 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:12:34 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:12:35 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.654 (0.654)	Loss 0.6868 (0.6868)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:12:36 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.147)	Loss 0.5737 (0.6492)	Acc@1 87.500 (88.068)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 06:12:37 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.121)	Loss 0.4609 (0.6344)	Acc@1 93.750 (87.500)	Acc@5 96.875 (98.363)	Mem 4879MB
[2022-05-31 06:12:38 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.098 (0.112)	Loss 0.8556 (0.6231)	Acc@1 68.750 (87.198)	Acc@5 100.000 (98.589)	Mem 4879MB
[2022-05-31 06:12:39 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.108)	Loss 0.7458 (0.6240)	Acc@1 84.375 (87.043)	Acc@5 96.875 (98.476)	Mem 4879MB
[2022-05-31 06:12:40 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.105)	Loss 0.8649 (0.6312)	Acc@1 81.250 (87.010)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 06:12:41 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.104)	Loss 1.0495 (0.6293)	Acc@1 78.1[2022-05-31 08:30:47 MetaFG_0] (main.py 265): INFO Train: [72/300][310/1562]	eta 0:06:25 lr 0.000005	time 0.2938 (0.3079)	loss 0.9405 (1.3041)	grad_norm 28.7378 (30.2599)	mem 4879MB
25 (87.039)	Acc@5 90.625 (98.514)	Mem 4879MB
[2022-05-31 06:12:42 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.103)	Loss 0.6052 (0.6287)	Acc@1 90.625 (86.884)	Acc@5 96.875 (98.636)	Mem 4879MB
[2022-05-31 06:12:43 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.101 (0.102)	Loss 0.6499 (0.6206)	Acc@1 84.375 (87.037)	Acc@5 96.875 (98.534)	Mem 4879MB
[2022-05-31 06:12:43 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.105 (0.101)	Loss 0.3861 (0.6121)	Acc@1 93.750 (87.294)	Acc@5 100.000 (98.592)	Mem 4879MB
[2022-05-31 06:12:44 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.099 (0.101)	Loss 0.5210 (0.6177)	Acc@1 87.500 (86.881)	Acc@5 100.000 (98.546)	Mem 4879MB
[2022-05-31 06:12:45 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.097 (0.100)	Loss 0.9041 (0.6232)	Acc@1 78.125 (86.486)	Acc@5 96.875 (98.564)	Mem 4879MB
[2022-05-31 06:12:46 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.099 (0.100)	Loss 0.8228 (0.6259)	Acc@1 78.125 (86.441)	Acc@5 100.000 (98.605)	Mem 4879MB
[2022-05-31 06:12:47 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.099 (0.099)	Loss 0.6049 (0.6322)	Acc@1 90.625 (86.140)	Acc@5 100.000 (98.593)	Mem 4879MB
[2022-05-31 06:12:48 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.5900 (0.6261)	Acc@1 84.375 (86.348)	Acc@5 100.000 (98.670)	Mem 4879MB
[2022-05-31 06:12:49 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.093 (0.099)	Loss 0.5554 (0.6259)	Acc@1 90.625 (86.341)	Acc@5 100.000 (98.655)	Mem 4879MB
[2022-05-31 06:12:50 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.098)	Loss 1.0005 (0.6313)	Acc@1 78.125 (86.200)	Acc@5 96.875 (98.622)	Mem 4879MB
[2022-05-31 06:12:51 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.091 (0.098)	Loss 0.9547 (0.6345)	Acc@1 75.000 (86.093)	Acc@5 100.000 (98.648)	Mem 4879MB
[2022-05-31 06:12:52 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.104 (0.098)	Loss 0.8837 (0.6329)	Acc@1 78.125 (86.136)	Acc@5 93.750 (98.636)	Mem 4879MB
[2022-05-31 06:12:53 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.098)	Loss 0.7708 (0.6317)	Acc@1 84.375 (86.126)	Acc@5 93.750 (98.658)	Mem 4879MB
[2022-05-31 06:12:54 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.098)	Loss 0.6112 (0.6333)	Acc@1 84.375 (86.147)	Acc@5 100.000 (98.632)	Mem 4879MB
[2022-05-31 06:12:55 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.6474 (0.6361)	Acc@1 87.500 (86.034)	Acc@5 96.875 (98.578)	Mem 4879MB
[2022-05-31 06:12:56 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.097 (0.097)	Loss 0.6341 (0.6372)	Acc@1 81.250 (85.973)	Acc@5 100.000 (98.586)	Mem 4879MB
[2022-05-31 06:12:57 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.103 (0.097)	Loss 0.9614 (0.6366)	Acc@1 78.125 (85.931)	Acc@5 96.875 (98.634)	Mem 4879MB
[2022-05-31 06:12:58 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.6387 (0.6367)	Acc@1 81.250 (85.970)	Acc@5 96.875 (98.626)	Mem 4879MB
[2022-05-31 06:12:59 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.097)	Loss 0.4418 (0.6339)	Acc@1 93.750 (86.043)	Acc@5 100.000 (98.643)	Mem 4879MB
[2022-05-31 06:13:00 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.102 (0.097)	Loss 0.6518 (0.6330)	Acc@1 84.375 (86.087)	Acc@5 96.875 (98.647)	Mem 4879MB
[2022-05-31 06:13:01 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.097)	Loss 0.5769 (0.6314)	Acc@1 87.500 (86.105)	Acc@5 100.000 (98.651)	Mem 4879MB
[2022-05-31 06:13:01 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.8436 (0.6281)	Acc@1 81.250 (86.188)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 06:13:02 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.097)	Loss 0.4202 (0.6248)	Acc@1 90.625 (86.351)	Acc@5 100.000 (98.690)	Mem 4879MB
[2022-05-31 06:13:03 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.095 (0.097)	Loss 0.6745 (0.6246)	Acc@1 81.250 (86.296)	Acc@5 96.875 (98.681)	Mem 4879MB
[2022-05-31 06:13:04 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7458 (0.6258)	Acc@1 87.500 (86.324)	Acc@5 96.875 (98.633)	Mem 4879MB
[2022-05-31 06:13:05 MetaFG_0] (main.py 330): INFO  * Acc@1 86.360 Acc@5 98.640
[2022-05-31 06:13:05 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.4%
[2022-05-31 06:13:05 MetaFG_0] (main.py 171): INFO Max accuracy: 86.36%
[2022-05-31 06:13:06 MetaFG_0] (main.py 265): INFO Train: [56/300][0/1562]	eta 0:28:47 lr 0.000006	time 1.1063 (1.1063)	loss 1.6119 (1.6119)	grad_norm 19.5982 (19.5982)	mem 4879MB
[2022-05-31 06:13:09 MetaFG_0] (main.py 265): INFO Train: [56/300][10/1562]	eta 0:09:52 lr 0.000006	time 0.2935 (0.3818)	loss 1.4410 (1.3306)	grad_norm 21.0621 (26.5218)	mem 4879MB
[2022-05-31 06:13:12 MetaFG_0] (main.py 265): INFO Train: [56/300][20/1562]	eta 0:08:53 lr 0.000006	time 0.2981 (0.3459)	loss 1.4779 (1.3864)	grad_norm 19.6041 (25.9586)	mem 4879MB
[2022-05-31 06:13:15 MetaFG_0] (main.py 265): INFO Train: [56/300][30/1562]	eta 0:08:29 lr 0.000006	time 0.2929 (0.3323)	loss 1.1462 (1.3751)	grad_norm 27.0701 (26.1029)	mem 4879MB
[2022-05-31 06:13:18 MetaFG_0] (main.py 265): INFO Train: [56/300][40/1562]	eta 0:08:16 lr 0.000006	time 0.2956 (0.3265)	loss 1.6097 (1.3499)	grad_norm 28.6557 (27.9249)	mem 4879MB
[2022-05-31 06:13:21 MetaFG_0] (main.py 265): INFO Train: [56/300][50/1562]	eta 0:08:06 lr 0.000006	time 0.2921 (0.3220)	loss 1.3109 (1.3313)	grad_norm 36.1210 (28.6500)	mem 4879MB
[2022-05-31 06:13:24 MetaFG_0] (main.py 265): INFO Train: [56/300][60/1562]	eta 0:07:59 lr 0.000006	time 0.2925 (0.3193)	loss 1.4182 (1.3582)	grad_norm 14.8609 (28.7256)	mem 4879MB
[2022-05-31 06:13:27 MetaFG_0] (main.py 265): INFO Train: [56/300][70/1562]	eta 0:07:53 lr 0.000006	time 0.2954 (0.3174)	loss 0.8691 (1.3535)	grad_norm 28.8677 (29.1407)	mem 4879MB
[2022-05-31 06:13:30 MetaFG_0] (main.py 265): INFO Train: [56/300][80/1562]	eta 0:07:48 lr 0.000006	time 0.2990 (0.3160)	loss 1.2253 (1.3547)	grad_norm 45.8383 (29.3273)	mem 4879MB
[2022-05-31 06:13:33 MetaFG_0] (main.py 265): INFO Train: [56/300][90/1562]	eta 0:07:44 lr 0.000006	time 0.3365 (0.3157)	loss 1.6907 (1.3587)	grad_norm 51.0390 (29.2933)	mem 4879MB
[2022-05-31 06:13:37 MetaFG_0] (main.py 265): INFO Train: [56/300][100/1562]	eta 0:07:42 lr 0.000006	time 0.2927 (0.3165)	loss 1.2465 (1.3608)	grad_norm 20.6772 (29.0302)	mem 4879MB
[2022-05-31 06:13:40 MetaFG_0] (main.py 265): INFO Train: [56/300][110/1562]	eta 0:07:38 lr 0.000006	time 0.2991 (0.3155)	loss 1.1969 (1.3702)	grad_norm 55.3053 (29.1807)	mem 4879MB
[2022-05-31 06:13:43 MetaFG_0] (main.py 265): INFO Train: [56/300][120/1562]	eta 0:07:33 lr 0.000006	time 0.2941 (0.3146)	loss 1.2524 (1.3617)	grad_norm 20.8374 (29.1449)	mem 4879MB
[2022-05-31 06:13:46 MetaFG_0] (main.py 265): INFO Train: [56/300][130/1562]	eta 0:07:29 lr 0.000006	time 0.3007 (0.3140)	loss 1.3268 (1.3618)	grad_norm 31.5078 (29.6420)	mem 4879MB
[2022-05-31 06:13:49 MetaFG_0] (main.py 265): INFO Train: [56/300][140/1562]	eta 0:07:25 lr 0.000006	time 0.2922 (0.3133)	loss 1.5417 (1.3675)	grad_norm 27.6314 (29.7706)	mem 4879MB
[2022-05-31 06:13:52 MetaFG_0] (main.py 265): INFO Train: [56/300][150/1562]	eta 0:07:21 lr 0.000006	time 0.2934 (0.3128)	loss 1.3729 (1.3637)	grad_norm 24.9335 (29.8149)	mem 4879MB
[2022-05-31 06:13:55 MetaFG_0] (main.py 265): INFO Train: [56/300][160/1562]	eta 0:07:17 lr 0.000006	time 0.2960 (0.3122)	loss 1.4078 (1.3632)	grad_norm 26.2467 (30.0439)	mem 4879MB
[2022-05-31 06:13:58 MetaFG_0] (main.py 265): INFO Train: [56/300][170/1562]	eta 0:07:14 lr 0.000006	time 0.2987 (0.3118)	loss 0.8517 (1.3585)	grad_norm 32.5073 (29.9347)	mem 4879MB
[2022-05-31 06:14:01 MetaFG_0] (main.py 265): INFO Train: [56/300][180/1562]	eta 0:07:10 lr 0.000006	time 0.2997 (0.3115)	loss 1.1175 (1.3576)	grad_norm 41.9417 (29.9932)	mem 4879MB
[2022-05-31 06:14:04 MetaFG_0] (main.py 265): INFO Train: [56/300][190/1562]	eta 0:07:06 lr 0.000006	time 0.2930 (0.3111)	loss 1.5647 (1.3593)	grad_norm 35.0052 (29.7901)	mem 4879MB
[2022-05-31 06:14:07 MetaFG_0] (main.py 265): INFO Train: [56/300][200/1562]	eta 0:07:03 lr 0.000006	time 0.2926 (0.3108)	loss 1.4835 (1.3559)	grad_norm 16.0081 (29.7179)	mem 4879MB
[2022-05-31 06:14:10 MetaFG_0] (main.py 265): INFO Train: [56/300][210/1562]	eta 0:06:59 lr 0.000006	time 0.2929 (0.3105)	loss 1.2003 (1.3518)	grad_norm 46.3091 (29.9230)	mem 4879MB
[2022-05-31 06:14:13 MetaFG_0] (main.py 265): INFO Train: [56/300][220/1562]	eta 0:06:56 lr 0.000006	time 0.3033 (0.3102)	loss 1.3284 (1.3509)	grad_norm 22.5850 (29.8373)	mem 4879MB
[2022-05-31 06:14:16 MetaFG_0] (main.py 265): INFO Train: [56/300][230/1562]	eta 0:06:52 lr 0.000006	time 0.2931 (0.3100)	loss 1.4588 (1.3527)	grad_norm 20.5340 (29.7263)	mem 4879MB
[2022-05-31 06:14:19 MetaFG_0] (main.py 265): INFO Train: [56/300][240/1562]	eta 0:06:49 lr 0.000006	time 0.2984 (0.3098)	loss 1.5196 (1.3527)	grad_norm 18.9444 (29.7663)	mem 4879MB
[2022-05-31 06:14:22 MetaFG_0] (main.py 265): INFO Train: [56/300][250/1562]	eta 0:06:46 lr 0.000006	time 0.2931 (0.3096)	loss 1.3459 (1.3556)	grad_norm 22.2398 (29.7771)	mem 4879MB
[2022-05-31 06:14:25 MetaFG_0] (main.py 265): INFO Train: [56/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.3012 (0.3095)	loss 1.3884 (1.3558)	grad_norm 19.5247 (29.8652)	mem 4879MB
[2022-05-31 06:14:28 MetaFG_0] (main.py 265): INFO Train: [56/300][270/1562]	eta 0:06:39 lr 0.000006	time 0.2930 (0.3093)	loss 1.2549 (1.3531)	grad_norm 40.9574 (29.8141)	mem 4879MB
[2022-05-31 06:14:31 MetaFG_0] (main.py 265): INFO Train: [56/300][280/1562]	eta 0:06:36 lr 0.000006	time 0.2946 (0.3091)	loss 1.7062 (1.3554)	grad_norm 28.7621 (29.7454)	mem 4879MB
[2022-05-31 06:14:34 MetaFG_0] (main.py 265): INFO Train: [56/300][290/1562]	eta 0:06:33 lr 0.000006	time 0.2983 (0.3090)	loss 1.3190 (1.3585)	grad_norm 26.4824 (29.7094)	mem 4879MB
[2022-05-31 06:14:38 MetaFG_0] (main.py 265): INFO Train: [56/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.3012 (0.3089)	loss 1.6373 (1.3575)	grad_norm 24.0753 (29.9122)	mem 4879MB
[2022-05-31 06:14:41 MetaFG_0] (main.py 265): INFO Train: [56/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.2937 (0.3088)	loss 1.3008 (1.3558)	grad_norm 24.1379 (29.8973)	mem 4879MB
[2022-05-31 06:14:44 MetaFG_0] (main.py 265): INFO Train: [56/300][320/1562]	eta 0:06:23 lr 0.000006	time 0.2940 (0.3087)	loss 1.0270 (1.3541)	grad_norm 23.6596 (29.8142)	mem 4879MB
[2022-05-31 06:14:47 MetaFG_0] (main.py 265): INFO Train: [56/300][330/1562]	eta 0:06:20 lr 0.000006	time 0.2928 (0.3085)	loss 1.5323 (1.3552)	grad_norm 29.4625 (29.7947)	mem 4879MB
[2022-05-31 06:14:50 MetaFG_0] (main.py 265): INFO Train: [56/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2927 (0.3084)	loss 1.2174 (1.3543)	grad_norm 28.1246 (29.9296)	mem 4879MB
[2022-05-31 06:14:53 MetaFG_0] (main.py 265): INFO Train: [56/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2990 (0.3083)	loss 1.5795 (1.3532)	grad_norm 30.2667 (29.7774)	mem 4879MB
[2022-05-31 06:14:56 MetaFG_0] (main.py 265): INFO Train: [56/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.2951 (0.3082)	loss 1.5129 (1.3531)	grad_norm 25.2477 (29.7021)	mem 4879MB
[2022-05-31 06:14:59 MetaFG_0] (main.py 265): INFO Train: [56/300][370/1562]	eta 0:06:07 lr 0.000006	time 0.2989 (0.3082)	loss 1.1213 (1.3513)	grad_norm 27.4128 (29.7405)	mem 4879MB
[2022-05-31 06:15:02 MetaFG_0] (main.py 265): INFO Train: [56/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2926 (0.3080)	loss 1.4937 (1.3518)	grad_norm 15.4492 (29.7037)	mem 4879MB
[2022-05-31 06:15:05 MetaFG_0] (main.py 265): INFO Train: [56/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2941 (0.3079)	loss 1.5457 (1.3507)	grad_norm 25.1097 (29.7752)	mem 4879MB
[2022-05-31 06:15:08 MetaFG_0] (main.py 265): INFO Train: [56/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2986 (0.3078)	loss 1.3962 (1.3525)	grad_norm 20.8470 (29.7407)	mem 4879MB
[2022-05-31 06:15:11 MetaFG_0] (main.py 265): INFO Train: [56/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2987 (0.3077)	loss 1.5478 (1.3530)	grad_norm 31.5741 (29.6832)	mem 4879MB
[2022-05-31 06:15:14 MetaFG_0] (main.py 265): INFO Train: [56/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2939 (0.3077)	loss 1.3742 (1.3543)	grad_norm 31.7190 (29.7018)	mem 4879MB
[2022-05-31 06:15:17 MetaFG_0] (main.py 265): INFO Train: [56/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2925 (0.3076)	loss 1.4248 (1.3535)	grad_norm 35.9655 (29.7542)	mem 4879MB
[2022-05-31 06:15:20 MetaFG_0] (main.py 265): INFO Train: [56/300][440/1562]	eta 0:05:45 lr 0.000006	time 0.2921 (0.3075)	loss 1.4549 (1.3509)	grad_norm 34.5004 (29.8631)	mem 4879MB
[2022-05-31 06:15:23 MetaFG_0] (main.py 265): INFO Train: [56/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2977 (0.3075)	loss 1.4429 (1.3510)	grad_norm 26.1525 (29.7501)	mem 4879MB
[2022-05-31 06:15:26 MetaFG_0] (main.py 265): INFO Train: [56/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2920 (0.3074)	loss 1.1677 (1.3515)	grad_norm 52.7372 (29.8146)	mem 4879MB
[2022-05-31 06:15:29 MetaFG_0] (main.py 265): INFO Train: [56/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2926 (0.3073)	loss 1.5052 (1.3508)	grad_norm 18.0413 (29.8250)	mem 4879MB
[2022-05-31 06:15:32 MetaFG_0] (main.py 265): INFO Train: [56/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2995 (0.3073)	loss 1.6096 (1.3515)	grad_norm 32.3765 (29.8305)	mem 4879MB
[2022-05-31 06:15:35 MetaFG_0] (main.py 265): INFO Train: [56/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2934 (0.3073)	loss 1.3789 (1.3527)	grad_norm 21.8481 (29.7778)	mem 4879MB
[2022-05-31 06:15:38 MetaFG_0] (main.py 265): INFO Train: [56/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.3013 (0.3073)	loss 1.4054 (1.3502)	grad_norm 42.1045 (29.7664)	mem 4879MB
[2022-05-31 06:15:42 MetaFG_0] (main.py 265): INFO Train: [56/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2995 (0.3072)	loss 1.6481 (1.3542)	grad_norm 30.2350 (29.7343)	mem 4879MB
[2022-05-31 06:15:45 MetaFG_0] (main.py 265): INFO Train: [56/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2921 (0.3072)	loss 1.4996 (1.3549)	grad_norm 18.3985 (29.7509)	mem 4879MB
[2022-05-31 06:15:48 MetaFG_0] (main.py 265): INFO Train: [56/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2941 (0.3072)	loss 1.2908 (1.3541)	grad_norm 28.5706 (29.6966)	mem 4879MB
[2022-05-31 06:15:51 MetaFG_0] (main.py 265): INFO Train: [56/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2944 (0.3071)	loss 1.1172 (1.3526)	grad_norm 14.4605 (29.5805)	mem 4879MB
[2022-05-31 06:15:54 MetaFG_0] (main.py 265): INFO Train: [56/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2926 (0.3070)	loss 1.4952 (1.3508)	grad_norm 59.3257 (29.7477)	mem 4879MB
[2022-05-31 06:15:57 MetaFG_0] (main.py 265): INFO Train: [56/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2994 (0.3070)	loss 1.4966 (1.3499)	grad_norm 32.9279 (29.7110)	mem 4879MB
[2022-05-31 06:16:00 MetaFG_0] (main.py 265): INFO Train: [56/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2942 (0.3070)	loss 1.0292 (1.3492)	grad_norm 32.5433 (29.7789)	mem 4879MB
[2022-05-31 06:16:03 MetaFG_0] (main.py 265): INFO Train: [56/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.3006 (0.3069)	loss 1.2824 (1.3496)	grad_norm 35.1454 (29.7381)	mem 4879MB
[2022-05-31 06:16:06 MetaFG_0] (main.py 265): INFO Train: [56/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2978 (0.3069)	loss 1.3634 (1.3477)	grad_norm 30.0861 (29.7213)	mem 4879MB
[2022-05-31 06:16:09 MetaFG_0] (main.py 265): INFO Train: [56/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.3022 (0.3068)	loss 1.0419 (1.3479)	grad_norm 31.5249 (29.6508)	mem 4879MB
[2022-05-31 06:16:12 MetaFG_0] (main.py 265): INFO Train: [56/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2989 (0.3068)	loss 1.4451 (1.3484)	grad_norm 30.2863 (29.6232)	mem 4879MB
[2022-05-31 06:16:15 MetaFG_0] (main.py 265): INFO Train: [56/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2998 (0.3068)	loss 1.2213 (1.3471)	grad_norm 33.1800 (29.7392)	mem 4879MB
[2022-05-31 06:16:18 MetaFG_0] (main.py 265): INFO Train: [56/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2968 (0.3067)	loss 1.5370 (1.3469)	grad_norm 24.9608 (29.6914)	mem 4879MB
[2022-05-31 06:16:21 MetaFG_0] (main.py 265): INFO Train: [56/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2921 (0.3067)	loss 1.1828 (1.3461)	grad_norm 41.1034 (29.7236)	mem 4879MB
[2022-05-31 06:16:24 MetaFG_0] (main.py 265): INFO Train: [56/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2928 (0.3067)	loss 1.5254 (1.3452)	grad_norm 45.8501 (29.6779)	mem 4879MB
[2022-05-31 06:16:27 MetaFG_0] (main.py 265): INFO Train: [56/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.3013 (0.3067)	loss 1.5447 (1.3455)	grad_norm 57.3180 (29.7755)	mem 4879MB
[2022-05-31 06:16:30 MetaFG_0] (main.py 265): INFO Train: [56/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2925 (0.3067)	loss 1.6532 (1.3442)	grad_norm 21.8342 (29.8623)	mem 4879MB
[2022-05-31 06:16:33 MetaFG_0] (main.py 265): INFO Train: [56/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2924 (0.3066)	loss 1.4666 (1.3445)	grad_norm 39.4656 (29.8604)	mem 4879MB
[2022-05-31 06:16:36 MetaFG_0] (main.py 265): INFO Train: [56/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2939 (0.3066)	loss 0.8890 (1.3441)	grad_norm 32.9801 (29.8190)	mem 4879MB
[2022-05-31 06:16:39 MetaFG_0] (main.py 265): INFO Train: [56/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2925 (0.3065)	loss 1.7765 (1.3469)	grad_norm 48.3569 (29.8789)	mem 4879MB
[2022-05-31 06:16:42 MetaFG_0] (main.py 265): INFO Train: [56/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2978 (0.3065)	loss 1.3114 (1.3461)	grad_norm 15.5246 (29.8280)	mem 4879MB
[2022-05-31 06:16:46 MetaFG_0] (main.py 265): INFO Train: [56/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.3050 (0.3065)	loss 1.4355 (1.3478)	grad_norm 23.8140 (29.8556)	mem 4879MB
[2022-05-31 06:16:49 MetaFG_0] (main.py 265): INFO Train: [56/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3058 (0.3065)	loss 1.4388 (1.3488)	grad_norm 28.9416 (29.8994)	mem 4879MB
[2022-05-31 06:16:52 MetaFG_0] (main.py 265): INFO Train: [56/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.3053 (0.3065)	loss 1.1304 (1.3476)	grad_norm 29.6696 (29.9399)	mem 4879MB
[2022-05-31 06:16:55 MetaFG_0] (main.py 265): INFO Train: [56/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2994 (0.3065)	loss 0.7949 (1.3474)	grad_norm 28.3241 (29.9443)	mem 4879MB
[2022-05-31 06:16:58 MetaFG_0] (main.py 265): INFO Train: [56/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2931 (0.3064)	loss 1.4312 (1.3460)	grad_norm 28.5896 (30.0907)	mem 4879MB
[2022-05-31 06:17:01 MetaFG_0] (main.py 265): INFO Train: [56/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2989 (0.3064)	loss 1.3412 (1.3458)	grad_norm 42.5987 (30.0837)	mem 4879MB
[2022-05-31 06:17:04 MetaFG_0] (main.py 265): INFO Train: [56/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2936 (0.3064)	loss 1.4658 (1.3459)	grad_norm 40.6237 (30.2211)	mem 4879MB
[2022-05-31 06:17:07 MetaFG_0] (main.py 265): INFO Train: [56/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2995 (0.3064)	loss 1.4201 (1.3450)	grad_norm 26.2666 (30.1993)	mem 4879MB
[2022-05-31 06:17:10 MetaFG_0] (main.py 265): INFO Train: [56/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.3001 (0.3064)	loss 1.4575 (1.3462)	grad_norm 22.0617 (30.2022)	mem 4879MB
[2022-05-31 06:17:13 MetaFG_0] (main.py 265): INFO Train: [56/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2943 (0.3064)	loss 1.4040 (1.3467)	grad_norm 21.3360 (30.2903)	mem 4879MB
[2022-05-31 06:17:16 MetaFG_0] (main.py 265): INFO Train: [56/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2923 (0.3064)	loss 1.3443 (1.3469)	grad_norm 84.3924 (30.3047)	mem 4879MB
[2022-05-31 06:17:19 MetaFG_0] (main.py 265): INFO Train: [56/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.3014 (0.3063)	loss 1.1654 (1.3453)	grad_norm 57.5550 (30.3172)	mem 4879MB
[2022-05-31 06:17:22 MetaFG_0] (main.py 265): INFO Train: [56/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2928 (0.3063)	loss 1.4888 (1.3455)	grad_norm 34.0470 (30.2849)	mem 4879MB
[2022-05-31 06:17:25 MetaFG_0] (main.py 265): INFO Train: [56/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2925 (0.3063)	loss 1.6796 (1.3459)	grad_norm 45.2469 (30.2924)	mem 4879MB
[2022-05-31 06:17:28 MetaFG_0] (main.py 265): INFO Train: [56/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2928 (0.3063)	loss 1.5191 (1.3456)	grad_norm 21.6775 (30.2913)	mem 4879MB
[2022-05-31 06:17:31 MetaFG_0] (main.py 265): INFO Train: [56/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2983 (0.3063)	loss 1.4789 (1.3451)	grad_norm 48.2320 (30.3289)	mem 4879MB
[2022-05-31 06:17:34 MetaFG_0] (main.py 265): INFO Train: [56/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2923 (0.3063)	loss 1.4077 (1.3456)	grad_norm 23.7945 (30.3151)	mem 4879MB
[2022-05-31 06:17:37 MetaFG_0] (main.py 265): INFO Train: [56/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2991 (0.3063)	loss 1.4470 (1.3462)	grad_norm 37.8769 (30.2910)	mem 4879MB
[2022-05-31 06:17:40 MetaFG_0] (main.py 265): INFO Train: [56/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2999 (0.3063)	loss 1.4730 (1.3471)	grad_norm 35.2370 (30.2630)	mem 4879MB
[2022-05-31 06:17:44 MetaFG_0] (main.py 265): INFO Train: [56/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.3011 (0.3063)	loss 1.5616 (1.3476)	grad_norm 22.7800 (30.2106)	mem 4879MB
[2022-05-31 06:17:47 MetaFG_0] (main.py 265): INFO Train: [56/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2996 (0.3063)	loss 1.2452 (1.3475)	grad_norm 26.9319 (30.2158)	mem 4879MB
[2022-05-31 06:17:50 MetaFG_0] (main.py 265): INFO Train: [56/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2927 (0.3063)	loss 1.6128 (1.3486)	grad_norm 17.2226 (30.2365)	mem 4879MB
[2022-05-31 06:17:53 MetaFG_0] (main.py 265): INFO Train: [56/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2922 (0.3062)	loss 1.2406 (1.3485)	grad_norm 23.2812 (30.1940)	mem 4879MB
[2022-05-31 06:17:56 MetaFG_0] (main.py 265): INFO Train: [56/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2921 (0.3062)	loss 1.2871 (1.3484)	grad_norm 20.9322 (30.1453)	mem 4879MB
[2022-05-31 06:17:59 MetaFG_0] (main.py 265): INFO Train: [56/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2924 (0.3062)	loss 1.1870 (1.3490)	grad_norm 33.8252 (30.0968)	mem 4879MB
[2022-05-31 06:18:02 MetaFG_0] (main.py 265): INFO Train: [56/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2924 (0.3062)	loss 1.2388 (1.3490)	grad_norm 26.1206 (30.0723)	mem 4879MB
[2022-05-31 06:18:05 MetaFG_0] (main.py 265): INFO Train: [56/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2922 (0.3061)	loss 1.2945 (1.3498)	grad_norm 21.3674 (30.0515)	mem 4879MB
[2022-05-31 06:18:08 MetaFG_0] (main.py 265): INFO Train: [56/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2926 (0.3061)	loss 1.4478 (1.3497)	grad_norm 22.8378 (30.0311)	mem 4879MB
[2022-05-31 06:18:11 MetaFG_0] (main.py 265): INFO Train: [56/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2932 (0.3061)	loss 1.5853 (1.3503)	grad_norm 30.3666 (30.0005)	mem 4879MB
[2022-05-31 06:18:14 MetaFG_0] (main.py 265): INFO Train: [56/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2924 (0.3061)	loss 1.5728 (1.3507)	grad_norm 32.8247 (29.9873)	mem 4879MB
[2022-05-31 06:18:17 MetaFG_0] (main.py 265): INFO Train: [56/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2940 (0.3061)	loss 1.0727 (1.3504)	grad_norm 42.0991 (29.9368)	mem 4879MB
[2022-05-31 06:18:20 MetaFG_0] (main.py 265): INFO Train: [56/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2941 (0.3060)	loss 1.2556 (1.3503)	grad_norm 21.8593 (29.9394)	mem 4879MB
[2022-05-31 06:18:23 MetaFG_0] (main.py 265): INFO Train: [56/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2994 (0.3060)	loss 1.2262 (1.3512)	grad_norm 29.2824 (29.9193)	mem 4879MB
[2022-05-31 06:18:26 MetaFG_0] (main.py 265): INFO Train: [56/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2935 (0.3060)	loss 1.3548 (1.3518)	grad_norm 27.0622 (29.9437)	mem 4879MB
[2022-05-31 06:18:29 MetaFG_0] (main.py 265): INFO Train: [56/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2932 (0.3060)	loss 1.2686 (1.3516)	grad_norm 22.7304 (29.9064)	mem 4879MB
[2022-05-31 06:18:32 MetaFG_0] (main.py 265): INFO Train: [56/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2937 (0.3060)	loss 1.3101 (1.3518)	grad_norm 23.5466 (29.9203)	mem 4879MB
[2022-05-31 06:18:35 MetaFG_0] (main.py 265): INFO Train: [56/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2941 (0.3060)	loss 1.1412 (1.3517)	grad_norm 30.3370 (29.9500)	mem 4879MB
[2022-05-31 06:18:38 MetaFG_0] (main.py 265): INFO Train: [56/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2938 (0.3060)	loss 1.0807 (1.3515)	grad_norm 48.4410 (29.9209)	mem 4879MB
[2022-05-31 06:18:41 MetaFG_0] (main.py 265): INFO Train: [56/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2923 (0.3060)	loss 1.2800 (1.3518)	grad_norm 30.2951 (29.9391)	mem 4879MB
[2022-05-31 06:18:44 MetaFG_0] (main.py 265): INFO Train: [56/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2928 (0.3059)	loss 1.5386 (1.3516)	grad_norm 41.6122 (29.9351)	mem 4879MB
[2022-05-31 06:18:47 MetaFG_0] (main.py 265): INFO Train: [56/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2942 (0.3059)	loss 1.4159 (1.3528)	grad_norm 24.9723 (29.9338)	mem 4879MB
[2022-05-31 06:18:51 MetaFG_0] (main.py 265): INFO Train: [56/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2937 (0.3059)	loss 1.1176 (1.3538)	grad_norm 33.0404 (29.9410)	mem 4879MB
[2022-05-31 06:18:54 MetaFG_0] (main.py 265): INFO Train: [56/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2936 (0.3059)	loss 1.6062 (1.3539)	grad_norm 30.3734 (30.0042)	mem 4879MB
[2022-05-31 06:18:57 MetaFG_0] (main.py 265): INFO Train: [56/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2956 (0.3059)	loss 1.3595 (1.3547)	grad_norm 19.5263 (29.9680)	mem 4879MB
[2022-05-31 06:19:00 MetaFG_0] (main.py 265): INFO Train: [56/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2926 (0.3059)	loss 1.4251 (1.3553)	grad_norm 20.2394 (29.9559)	mem 4879MB
[2022-05-31 06:19:03 MetaFG_0] (main.py 265): INFO Train: [56/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.3006 (0.3062)	loss 1.3751 (1.3550)	grad_norm 16.9241 (29.9241)	mem 4879MB
[2022-05-31 06:19:06 MetaFG_0] (main.py 265): INFO Train: [56/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2929 (0.3062)	loss 1.0523 (1.3553)	grad_norm 23.7600 (29.8930)	mem 4879MB
[2022-05-31 06:19:09 MetaFG_0] (main.py 265): INFO Train: [56/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2998 (0.3061)	loss 1.4290 (1.3550)	grad_norm 34.3157 (29.8652)	mem 4879MB
[2022-05-31 06:19:12 MetaFG_0] (main.py 265): INFO Train: [56/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2946 (0.3061)	loss 1.6785 (1.3550)	grad_norm 25.3756 (29.8587)	mem 4879MB
[2022-05-31 06:19:15 MetaFG_0] (main.py 265): INFO Train: [56/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2992 (0.3061)	loss 0.9212 (1.3552)	grad_norm 27.1482 (29.8297)	mem 4879MB
[2022-05-31 06:19:18 MetaFG_0] (main.py 265): INFO Train: [56/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2922 (0.3061)	loss 1.4962 (1.3558)	grad_norm 23.4172 (29.8414)	mem 4879MB
[2022-05-31 06:19:21 MetaFG_0] (main.py 265): INFO Train: [56/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2925 (0.3061)	loss 0.9619 (1.3557)	grad_norm 33.3688 (29.8287)	mem 4879MB
[2022-05-31 06:19:24 MetaFG_0] (main.py 265): INFO Train: [56/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.3007 (0.3061)	loss 0.9751 (1.3557)	grad_norm 41.7843 (29.8449)	mem 4879MB
[2022-05-31 06:19:27 MetaFG_0] (main.py 265): INFO Train: [56/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3028 (0.3061)	loss 1.5137 (1.3562)	grad_norm 65.9439 (29.9196)	mem 4879MB
[2022-05-31 06:19:31 MetaFG_0] (main.py 265): INFO Train: [56/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2962 (0.3061)	loss 1.1532 (1.3558)	grad_norm 18.4151 (29.9128)	mem 4879MB
[2022-05-31 06:19:34 MetaFG_0] (main.py 265): INFO Train: [56/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2963 (0.3061)	loss 1.4431 (1.3556)	grad_norm 26.7661 (29.9357)	mem 4879MB
[2022-05-31 06:19:37 MetaFG_0] (main.py 265): INFO Train: [56/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2920 (0.3061)	loss 1.3849 (1.3551)	grad_norm 22.6001 (29.9536)	mem 4879MB
[2022-05-31 06:19:40 MetaFG_0] (main.py 265): INFO Train: [56/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2940 (0.3061)	loss 1.6209 (1.3557)	grad_norm 30.9005 (29.9748)	mem 4879MB
[2022-05-31 06:19:43 MetaFG_0] (main.py 265): INFO Train: [56/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2943 (0.3061)	loss 1.3301 (1.3550)	grad_norm 24.3243 (29.9562)	mem 4879MB
[2022-05-31 06:19:46 MetaFG_0] (main.py 265): INFO Train: [56/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2975 (0.3060)	loss 0.9441 (1.3540)	grad_norm 26.9708 (29.9429)	mem 4879MB
[2022-05-31 06:19:49 MetaFG_0] (main.py 265): INFO Train: [56/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.3007 (0.3060)	loss 1.1860 (1.3540)	grad_norm 23.6256 (29.9497)	mem 4879MB
[2022-05-31 06:19:52 MetaFG_0] (main.py 265): INFO Train: [56/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2936 (0.3060)	loss 1.3132 (1.3540)	grad_norm 27.8490 (29.9591)	mem 4879MB
[2022-05-31 06:19:55 MetaFG_0] (main.py 265): INFO Train: [56/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2949 (0.3060)	loss 1.4381 (1.3537)	grad_norm 20.6324 (29.9301)	mem 4879MB
[2022-05-31 06:19:58 MetaFG_0] (main.py 265): INFO Train: [56/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2935 (0.3060)	loss 1.2066 (1.3532)	grad_norm 39.7546 (29.9333)	mem 4879MB
[2022-05-31 06:20:01 MetaFG_0] (main.py 265): INFO Train: [56/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2982 (0.3060)	loss 1.5018 (1.3544)	grad_norm 36.1229 (29.9694)	mem 4879MB
[2022-05-31 06:20:04 MetaFG_0] (main.py 265): INFO Train: [56/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2933 (0.3060)	loss 1.1587 (1.3548)	grad_norm 32.9078 (29.9735)	mem 4879MB
[2022-05-31 06:20:07 MetaFG_0] (main.py 265): INFO Train: [56/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.3001 (0.3060)	loss 1.3491 (1.3542)	grad_norm 25.7280 (29.9705)	mem 4879MB
[2022-05-31 06:20:10 MetaFG_0] (main.py 265): INFO Train: [56/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2925 (0.3060)	loss 1.3006 (1.3548)	grad_norm 24.4091 (30.0356)	mem 4879MB
[2022-05-31 06:20:13 MetaFG_0] (main.py 265): INFO Train: [56/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2921 (0.3060)	loss 1.3611 (1.3544)	grad_norm 36.8545 (29.9957)	mem 4879MB
[2022-05-31 06:20:16 MetaFG_0] (main.py 265): INFO Train: [56/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3000 (0.3060)	loss 1.4439 (1.3548)	grad_norm 18.8788 (30.0174)	mem 4879MB
[2022-05-31 06:20:19 MetaFG_0] (main.py 265): INFO Train: [56/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2937 (0.3060)	loss 1.0710 (1.3545)	grad_norm 23.1076 (30.0136)	mem 4879MB
[2022-05-31 06:20:22 MetaFG_0] (main.py 265): INFO Train: [56/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2928 (0.3060)	loss 1.2973 (1.3541)	grad_norm 28.2841 (30.0135)	mem 4879MB
[2022-05-31 06:20:25 MetaFG_0] (main.py 265): INFO Train: [56/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2930 (0.3060)	loss 1.0327 (1.3540)	grad_norm 26.8477 (30.0124)	mem 4879MB
[2022-05-31 06:20:28 MetaFG_0] (main.py 265): INFO Train: [56/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2918 (0.3060)	loss 1.5390 (1.3544)	grad_norm 31.1098 (30.0277)	mem 4879MB
[2022-05-31 06:20:32 MetaFG_0] (main.py 265): INFO Train: [56/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3001 (0.3060)	loss 1.4374 (1.3549)	grad_norm 39.9974 (30.0133)	mem 4879MB
[2022-05-31 06:20:35 MetaFG_0] (main.py 265): INFO Train: [56/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2939 (0.3059)	loss 0.9712 (1.3546)	grad_norm 28.7644 (29.9802)	mem 4879MB
[2022-05-31 06:20:38 MetaFG_0] (main.py 265): INFO Train: [56/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2959 (0.3059)	loss 1.6383 (1.3543)	grad_norm 30.3569 (29.9423)	mem 4879MB
[2022-05-31 06:20:41 MetaFG_0] (main.py 265): INFO Train: [56/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2941 (0.3059)	loss 1.1846 (1.3544)	grad_norm 22.6805 (29.9580)	mem 4879MB
[2022-05-31 06:20:44 MetaFG_0] (main.py 265): INFO Train: [56/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2933 (0.3059)	loss 1.4171 (1.3546)	grad_norm 20.4857 (29.9624)	mem 4879MB
[2022-05-31 06:20:47 MetaFG_0] (main.py 265): INFO Train: [56/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2937 (0.3059)	loss 1.2236 (1.3542)	grad_norm 27.7333 (29.9504)	mem 4879MB
[2022-05-31 06:20:50 MetaFG_0] (main.py 265): INFO Train: [56/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2971 (0.3059)	loss 1.5973 (1.3545)	grad_norm 37.7163 (29.9902)	mem 4879MB
[2022-05-31 06:20:53 MetaFG_0] (main.py 265): INFO Train: [56/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2921 (0.3059)	loss 1.6128 (1.3553)	grad_norm 33.9106 (29.9757)	mem 4879MB
[2022-05-31 06:20:56 MetaFG_0] (main.py 265): INFO Train: [56/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2925 (0.3058)	loss 0.8544 (1.3544)	grad_norm 20.6332 (29.9912)	mem 4879MB
[2022-05-31 06:20:59 MetaFG_0] (main.py 265): INFO Train: [56/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2988 (0.3058)	loss 0.9081 (1.3533)	grad_norm 16.8467 (29.9670)	mem 4879MB
[2022-05-31 06:21:02 MetaFG_0] (main.py 265): INFO Train: [56/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2932 (0.3058)	loss 1.5683 (1.3538)	grad_norm 44.1137 (29.9698)	mem 4879MB
[2022-05-31 06:21:02 MetaFG_0] (main.py 272): INFO EPOCH 56 training takes 0:07:57
[2022-05-31 06:21:02 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_56.pth saving......
[2022-05-31 06:21:03 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_56.pth saved !!!
[2022-05-31 06:21:03 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:21:05 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:21:05 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:21:05 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.617 (0.617)	Loss 0.3694 (0.3694)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:21:06 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.088 (0.144)	Loss 0.7316 (0.6114)	Acc@1 87.500 (86.080)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 06:21:07 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.101 (0.121)	Loss 0.8337 (0.5921)	Acc@1 78.125 (87.351)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 06:21:08 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.112)	Loss 0.7485 (0.5914)	Acc@1 81.250 (87.198)	Acc@5 96.875 (98.992)	Mem 4879MB
[2022-05-31 06:21:09 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.110)	Loss 0.6336 (0.6112)	Acc@1 84.375 (86.357)	Acc@5 96.875 (98.857)	Mem 4879MB
[2022-05-31 06:21:10 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.106)	Loss 0.5796 (0.6100)	Acc@1 87.500 (86.581)	Acc@5 100.000 (98.897)	Mem 4879MB
[2022-05-31 06:21:11 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.100 (0.104)	Loss 0.4347 (0.6025)	Acc@1 96.875 (86.834)	Acc@5 96.875 (98.873)	Mem 4879MB
[2022-05-31 06:21:12 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.103)	Loss 0.8979 (0.6075)	Acc@1 75.000 (86.356)	Acc@5 96.875 (98.812)	Mem 4879MB
[2022-05-31 06:21:13 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 0.6595 (0.6178)	Acc@1 81.250 (86.150)	Acc@5 100.000 (98.611)	Mem 4879MB
[2022-05-31 06:21:14 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.7356 (0.6133)	Acc@1 78.125 (86.332)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 06:21:15 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.4859 (0.6023)	Acc@1 93.750 (86.665)	Acc@5 100.000 (98.731)	Mem 4879MB
[2022-05-31 06:21:16 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.099 (0.100)	Loss 0.9188 (0.6079)	Acc@1 78.125 (86.599)	Acc@5 93.750 (98.677)	Mem 4879MB
[2022-05-31 06:21:17 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.097 (0.100)	Loss 0.5979 (0.6088)	Acc@1 84.375 (86.493)	Acc@5 100.000 (98.554)	Mem 4879MB
[2022-05-31 06:21:18 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.100 (0.099)	Loss 0.7431 (0.6167)	Acc@1 78.125 (86.260)	Acc@5 100.000 (98.545)	Mem 4879MB
[2022-05-31 06:21:19 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.099 (0.099)	Loss 0.8104 (0.6154)	Acc@1 78.125 (86.170)	Acc@5 96.875 (98.559)	Mem 4879MB
[2022-05-31 06:21:20 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 0.6579 (0.6092)	Acc@1 87.500 (86.424)	Acc@5 100.000 (98.613)	Mem 4879MB
[2022-05-31 06:21:21 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.092 (0.098)	Loss 0.6774 (0.6083)	Acc@1 87.500 (86.568)	Acc@5 96.875 (98.641)	Mem 4879MB
[2022-05-31 06:21:21 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.088 (0.098)	Loss 0.5988 (0.6113)	Acc@1 84.375 (86.440)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 06:21:22 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.097)	Loss 0.5750 (0.6075)	Acc@1 93.750 (86.585)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 06:21:23 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.097)	Loss 0.4181 (0.6061)	Acc@1 93.750 (86.682)	Acc@5 100.000 (98.740)	Mem 4879MB
[2022-05-31 06:21:24 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.090 (0.097)	Loss 0.6315 (0.6082)	Acc@1 81.250 (86.629)	Acc@5 100.000 (98.741)	Mem 4879MB
[2022-05-31 06:21:25 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.4622 (0.6086)	Acc@1 90.625 (86.641)	Acc@5 100.000 (98.711)	Mem 4879MB
[2022-05-31 06:21:26 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.097)	Loss 0.9168 (0.6113)	Acc@1 78.125 (86.496)	Acc@5 96.875 (98.699)	Mem 4879MB
[2022-05-31 06:21:27 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.095 (0.097)	Loss 0.8220 (0.6139)	Acc@1 75.000 (86.310)	Acc@5 100.000 (98.715)	Mem 4879MB
[2022-05-31 06:21:28 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.7786 (0.6202)	Acc@1 78.125 (86.151)	Acc@5 96.875 (98.651)	Mem 4879MB
[2022-05-31 06:21:29 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.096)	Loss 0.5910 (0.6185)	Acc@1 90.625 (86.218)	Acc@5 96.875 (98.668)	Mem 4879MB
[2022-05-31 06:21:30 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.121 (0.096)	Loss 0.6231 (0.6145)	Acc@1 87.500 (86.363)	Acc@5 96.875 (98.695)	Mem 4879MB
[2022-05-31 06:21:31 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 0.5645 (0.6146)	Acc@1 93.750 (86.324)	Acc@5 96.875 (98.651)	Mem 4879MB
[2022-05-31 06:21:32 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.8163 (0.6142)	Acc@1 87.500 (86.399)	Acc@5 93.750 (98.643)	Mem 4879MB
[2022-05-31 06:21:33 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.9119 (0.6160)	Acc@1 81.250 (86.405)	Acc@5 100.000 (98.636)	Mem 4879MB
[2022-05-31 06:21:34 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 0.5303 (0.6142)	Acc@1 90.625 (86.483)	Acc@5 96.875 (98.640)	Mem 4879MB
[2022-05-31 06:21:34 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.4826 (0.6107)	Acc@1 90.625 (86.576)	Acc@5 100.000 (98.674)	Mem 4879MB
[2022-05-31 06:21:35 MetaFG_0] (main.py 330): INFO  * Acc@1 86.540 Acc@5 98.680
[2022-05-31 06:21:35 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.5%
[2022-05-31 06:21:35 MetaFG_0] (main.py 171): INFO Max accuracy: 86.54%
[2022-05-31 06:21:36 MetaFG_0] (main.py 265): INFO Train: [57/300][0/1562]	eta 0:26:55 lr 0.000006	time 1.0345 (1.0345)	loss 1.3908 (1.3908)	grad_norm 27.0127 (27.0127)	mem 4879MB
[2022-05-31 06:21:39 MetaFG_0] (main.py 265): INFO Train: [57/300][10/1562]	eta 0:09:41 lr 0.000006	time 0.2930 (0.3744)	loss 1.5601 (1.2930)	grad_norm 37.4342 (33.8162)	mem 4879MB
[2022-05-31 06:21:42 MetaFG_0] (main.py 265): INFO Train: [57/300][20/1562]	eta 0:08:44 lr 0.000006	time 0.2924 (0.3402)	loss 1.2474 (1.3136)	grad_norm 16.6693 (31.2086)	mem 4879MB
[2022-05-31 06:21:45 MetaFG_0] (main.py 265): INFO Train: [57/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.2967 (0.3281)	loss 0.8613 (1.3009)	grad_norm 37.5386 (29.8548)	mem 4879MB
[2022-05-31 06:21:48 MetaFG_0] (main.py 265): INFO Train: [57/300][40/1562]	eta 0:08:10 lr 0.000006	time 0.2982 (0.3222)	loss 1.4150 (1.2912)	grad_norm 23.1702 (29.9733)	mem 4879MB
[2022-05-31 06:21:51 MetaFG_0] (main.py 265): INFO Train: [57/300][50/1562]	eta 0:08:01 lr 0.000006	time 0.2997 (0.3186)	loss 1.4665 (1.3019)	grad_norm 29.0383 (30.9154)	mem 4879MB
[2022-05-31 06:21:54 MetaFG_0] (main.py 265): INFO Train: [57/300][60/1562]	eta 0:07:54 lr 0.000006	time 0.3009 (0.3161)	loss 1.4530 (1.3203)	grad_norm 58.9125 (31.1907)	mem 4879MB
[2022-05-31 06:21:57 MetaFG_0] (main.py 265): INFO Train: [57/300][70/1562]	eta 0:07:49 lr 0.000006	time 0.3143 (0.3146)	loss 1.5675 (1.2984)	grad_norm 26.5465 (30.8798)	mem 4879MB
[2022-05-31 06:22:00 MetaFG_0] (main.py 265): INFO Train: [57/300][80/1562]	eta 0:07:44 lr 0.000006	time 0.3025 (0.3133)	loss 1.4129 (1.3045)	grad_norm 29.9227 (31.1196)	mem 4879MB
[2022-05-31 06:22:03 MetaFG_0] (main.py 265): INFO Train: [57/300][90/1562]	eta 0:07:39 lr 0.000006	time 0.3041 (0.3124)	loss 1.3717 (1.3129)	grad_norm 13.3726 (31.3294)	mem 4879MB
[2022-05-31 06:22:06 MetaFG_0] (main.py 265): INFO Train: [57/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.3020 (0.3116)	loss 1.3307 (1.3170)	grad_norm 34.3720 (31.4127)	mem 4879MB
[2022-05-31 06:22:09 MetaFG_0] (main.py 265): INFO Train: [57/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2921 (0.3107)	loss 1.1299 (1.3207)	grad_norm 36.7443 (31.4093)	mem 4879MB
[2022-05-31 06:22:12 MetaFG_0] (main.py 265): INFO Train: [57/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2925 (0.3100)	loss 1.5252 (1.3224)	grad_norm 25.7001 (31.0577)	mem 4879MB
[2022-05-31 06:22:15 MetaFG_0] (main.py 265): INFO Train: [57/300][130/1562]	eta 0:07:23 lr 0.000006	time 0.3008 (0.3097)	loss 1.4392 (1.3160)	grad_norm 21.9068 (30.7060)	mem 4879MB
[2022-05-31 06:22:18 MetaFG_0] (main.py 265): INFO Train: [57/300][140/1562]	eta 0:07:19 lr 0.000006	time 0.2985 (0.3093)	loss 1.5559 (1.3077)	grad_norm 42.1888 (30.6060)	mem 4879MB
[2022-05-31 06:22:21 MetaFG_0] (main.py 265): INFO Train: [57/300][150/1562]	eta 0:07:16 lr 0.000006	time 0.3035 (0.3094)	loss 1.5323 (1.3035)	grad_norm 23.4649 (30.9321)	mem 4879MB
[2022-05-31 06:22:25 MetaFG_0] (main.py 265): INFO Train: [57/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3017 (0.3095)	loss 1.3370 (1.3049)	grad_norm 36.6973 (31.0730)	mem 4879MB
[2022-05-31 06:22:28 MetaFG_0] (main.py 265): INFO Train: [57/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.3049 (0.3094)	loss 1.4524 (1.3047)	grad_norm 18.9245 (31.0875)	mem 4879MB
[2022-05-31 06:22:31 MetaFG_0] (main.py 265): INFO Train: [57/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2990 (0.3092)	loss 0.9283 (1.3100)	grad_norm 19.7845 (30.9794)	mem 4879MB
[2022-05-31 06:22:34 MetaFG_0] (main.py 265): INFO Train: [57/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2927 (0.3088)	loss 1.0314 (1.3039)	grad_norm 39.1306 (31.0279)	mem 4879MB
[2022-05-31 06:22:37 MetaFG_0] (main.py 265): INFO Train: [57/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2991 (0.3086)	loss 1.1463 (1.3054)	grad_norm 25.4095 (31.3076)	mem 4879MB
[2022-05-31 06:22:40 MetaFG_0] (main.py 265): INFO Train: [57/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2935 (0.3084)	loss 1.3716 (1.3070)	grad_norm 35.4082 (31.2670)	mem 4879MB
[2022-05-31 06:22:43 MetaFG_0] (main.py 265): INFO Train: [57/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.3000 (0.3083)	loss 1.4106 (1.3061)	grad_norm 21.7558 (31.2453)	mem 4879MB
[2022-05-31 06:22:46 MetaFG_0] (main.py 265): INFO Train: [57/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2989 (0.3083)	loss 1.4885 (1.3082)	grad_norm 24.9764 (31.2926)	mem 4879MB
[2022-05-31 06:22:49 MetaFG_0] (main.py 265): INFO Train: [57/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2928 (0.3080)	loss 1.6109 (1.3100)	grad_norm 43.3057 (nan)	mem 4879MB
[2022-05-31 06:22:52 MetaFG_0] (main.py 265): INFO Train: [57/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2927 (0.3079)	loss 1.5379 (1.3146)	grad_norm 33.9012 (nan)	mem 4879MB
[2022-05-31 06:22:55 MetaFG_0] (main.py 265): INFO Train: [57/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2993 (0.3078)	loss 1.6626 (1.3182)	grad_norm 17.6665 (nan)	mem 4879MB
[2022-05-31 06:22:58 MetaFG_0] (main.py 265): INFO Train: [57/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2945 (0.3076)	loss 1.0654 (1.3193)	grad_norm 39.1602 (nan)	mem 4879MB
[2022-05-31 06:23:01 MetaFG_0] (main.py 265): INFO Train: [57/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2941 (0.3076)	loss 1.0593 (1.3214)	grad_norm 46.8661 (nan)	mem 4879MB
[2022-05-31 06:23:04 MetaFG_0] (main.py 265): INFO Train: [57/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.3074 (0.3075)	loss 1.3224 (1.3214)	grad_norm 16.8885 (nan)	mem 4879MB
[2022-05-31 06:23:07 MetaFG_0] (main.py 265): INFO Train: [57/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.3018 (0.3076)	loss 0.9115 (1.3206)	grad_norm 30.2088 (nan)	mem 4879MB
[2022-05-31 06:23:10 MetaFG_0] (main.py 265): INFO Train: [57/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.3003 (0.3077)	loss 0.9926 (1.3210)	grad_norm 19.0345 (nan)	mem 4879MB
[2022-05-31 06:23:14 MetaFG_0] (main.py 265): INFO Train: [57/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2955 (0.3078)	loss 1.5256 (1.3218)	grad_norm 19.5630 (nan)	mem 4879MB
[2022-05-31 06:23:17 MetaFG_0] (main.py 265): INFO Train: [57/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.3026 (0.3078)	loss 1.6022 (1.3229)	grad_norm 18.2614 (nan)	mem 4879MB
[2022-05-31 06:23:20 MetaFG_0] (main.py 265): INFO Train: [57/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2998 (0.3078)	loss 1.6457 (1.3205)	grad_norm 36.0150 (nan)	mem 4879MB
[2022-05-31 06:23:23 MetaFG_0] (main.py 265): INFO Train: [57/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.3028 (0.3078)	loss 1.3323 (1.3232)	grad_norm 18.1524 (nan)	mem 4879MB
[2022-05-31 06:23:26 MetaFG_0] (main.py 265): INFO Train: [57/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2946 (0.3077)	loss 0.8417 (1.3246)	grad_norm 24.9739 (nan)	mem 4879MB
[2022-05-31 06:23:29 MetaFG_0] (main.py 265): INFO Train: [57/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.3026 (0.3077)	loss 1.5136 (1.3218)	grad_norm 17.5249 (nan)	mem 4879MB
[2022-05-31 06:23:32 MetaFG_0] (main.py 265): INFO Train: [57/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.3012 (0.3077)	loss 1.1949 (1.3228)	grad_norm 20.4502 (nan)	mem 4879MB
[2022-05-31 06:23:35 MetaFG_0] (main.py 265): INFO Train: [57/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.3040 (0.3077)	loss 1.5428 (1.3262)	grad_norm 50.1225 (nan)	mem 4879MB
[2022-05-31 06:23:38 MetaFG_0] (main.py 265): INFO Train: [57/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.3043 (0.3077)	loss 1.3258 (1.3296)	grad_norm 47.9921 (nan)	mem 4879MB
[2022-05-31 06:23:41 MetaFG_0] (main.py 265): INFO Train: [57/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.3027 (0.3077)	loss 1.5954 (1.3304)	grad_norm 60.5660 (nan)	mem 4879MB
[2022-05-31 06:23:44 MetaFG_0] (main.py 265): INFO Train: [57/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2933 (0.3076)	loss 1.6415 (1.3333)	grad_norm 34.7201 (nan)	mem 4879MB
[2022-05-31 06:23:47 MetaFG_0] (main.py 265): INFO Train: [57/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2984 (0.3075)	loss 1.0640 (1.3318)	grad_norm 81.7784 (nan)	mem 4879MB
[2022-05-31 06:23:50 MetaFG_0] (main.py 265): INFO Train: [57/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.3021 (0.3075)	loss 0.8797 (1.3333)	grad_norm 25.3153 (nan)	mem 4879MB
[2022-05-31 06:23:53 MetaFG_0] (main.py 265): INFO Train: [57/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2938 (0.3074)	loss 1.0736 (1.3343)	grad_norm 20.3758 (nan)	mem 4879MB
[2022-05-31 06:23:56 MetaFG_0] (main.py 265): INFO Train: [57/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.3010 (0.3074)	loss 1.5908 (1.3369)	grad_norm 41.4810 (nan)	mem 4879MB
[2022-05-31 06:24:00 MetaFG_0] (main.py 265): INFO Train: [57/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2963 (0.3073)	loss 1.4299 (1.3383)	grad_norm 37.9896 (nan)	mem 4879MB
[2022-05-31 06:24:03 MetaFG_0] (main.py 265): INFO Train: [57/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2933 (0.3073)	loss 1.4141 (1.3389)	grad_norm 31.0300 (nan)	mem 4879MB
[2022-05-31 06:24:06 MetaFG_0] (main.py 265): INFO Train: [57/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2933 (0.3073)	loss 1.0597 (1.3392)	grad_norm 40.3050 (nan)	mem 4879MB
[2022-05-31 06:24:09 MetaFG_0] (main.py 265): INFO Train: [57/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2934 (0.3072)	loss 1.4547 (1.3380)	grad_norm 26.1687 (nan)	mem 4879MB
[2022-05-31 06:24:12 MetaFG_0] (main.py 265): INFO Train: [57/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2929 (0.3072)	loss 1.4422 (1.3386)	grad_norm 26.6064 (nan)	mem 4879MB
[2022-05-31 06:24:15 MetaFG_0] (main.py 265): INFO Train: [57/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2931 (0.3072)	loss 1.4899 (1.3403)	grad_norm 25.8436 (nan)	mem 4879MB
[2022-05-31 06:24:18 MetaFG_0] (main.py 265): INFO Train: [57/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2955 (0.3072)	loss 1.5498 (1.3401)	grad_norm 22.2823 (nan)	mem 4879MB
[2022-05-31 06:24:21 MetaFG_0] (main.py 265): INFO Train: [57/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2984 (0.3072)	loss 1.5358 (1.3398)	grad_norm 23.6295 (nan)	mem 4879MB
[2022-05-31 06:24:24 MetaFG_0] (main.py 265): INFO Train: [57/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2923 (0.3071)	loss 1.3367 (1.3400)	grad_norm 16.4987 (nan)	mem 4879MB
[2022-05-31 06:24:27 MetaFG_0] (main.py 265): INFO Train: [57/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.3040 (0.3072)	loss 1.4599 (1.3426)	grad_norm 24.9558 (nan)	mem 4879MB
[2022-05-31 06:24:30 MetaFG_0] (main.py 265): INFO Train: [57/300][570/1562]	eta 0:05:05 lr 0.000006	time 0.3140 (0.3078)	loss 1.4042 (1.3428)	grad_norm 31.0472 (nan)	mem 4879MB
[2022-05-31 06:24:34 MetaFG_0] (main.py 265): INFO Train: [57/300][580/1562]	eta 0:05:02 lr 0.000006	time 0.2989 (0.3078)	loss 1.4924 (1.3440)	grad_norm 22.6922 (nan)	mem 4879MB
[2022-05-31 06:24:37 MetaFG_0] (main.py 265): INFO Train: [57/300][590/1562]	eta 0:04:59 lr 0.000006	time 0.2942 (0.3077)	loss 1.0153 (1.3465)	grad_norm 75.0456 (nan)	mem 4879MB
[2022-05-31 06:24:40 MetaFG_0] (main.py 265): INFO Train: [57/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.3007 (0.3077)	loss 1.2084 (1.3435)	grad_norm 42.9052 (nan)	mem 4879MB
[2022-05-31 06:24:43 MetaFG_0] (main.py 265): INFO Train: [57/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.3048 (0.3076)	loss 1.4266 (1.3450)	grad_norm 45.9836 (nan)	mem 4879MB
[2022-05-31 06:24:46 MetaFG_0] (main.py 265): INFO Train: [57/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2938 (0.3076)	loss 1.3220 (1.3474)	grad_norm 26.4374 (nan)	mem 4879MB
[2022-05-31 06:24:49 MetaFG_0] (main.py 265): INFO Train: [57/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2937 (0.3076)	loss 1.4134 (1.3476)	grad_norm 12.5218 (nan)	mem 4879MB
[2022-05-31 06:24:52 MetaFG_0] (main.py 265): INFO Train: [57/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2927 (0.3075)	loss 1.7165 (1.3479)	grad_norm 19.2760 (nan)	mem 4879MB
[2022-05-31 06:24:55 MetaFG_0] (main.py 265): INFO Train: [57/300][650/1562]	eta 0:04:40 lr 0.000006	time 0.3000 (0.3075)	loss 1.5351 (1.3468)	grad_norm 34.4647 (nan)	mem 4879MB
[2022-05-31 06:24:58 MetaFG_0] (main.py 265): INFO Train: [57/300][660/1562]	eta 0:04:37 lr 0.000006	time 0.3012 (0.3075)	loss 1.2875 (1.3459)	grad_norm 18.3494 (nan)	mem 4879MB
[2022-05-31 06:25:01 MetaFG_0] (main.py 265): INFO Train: [57/300][670/1562]	eta 0:04:34 lr 0.000006	time 0.2951 (0.3075)	loss 1.5839 (1.3473)	grad_norm 14.2205 (nan)	mem 4879MB
[2022-05-31 06:25:04 MetaFG_0] (main.py 265): INFO Train: [57/300][680/1562]	eta 0:04:31 lr 0.000006	time 0.2927 (0.3074)	loss 1.4080 (1.3448)	grad_norm 25.6858 (nan)	mem 4879MB
[2022-05-31 06:25:07 MetaFG_0] (main.py 265): INFO Train: [57/300][690/1562]	eta 0:04:28 lr 0.000006	time 0.3002 (0.3074)	loss 1.5844 (1.3459)	grad_norm 19.7754 (nan)	mem 4879MB
[2022-05-31 06:25:10 MetaFG_0] (main.py 265): INFO Train: [57/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2977 (0.3074)	loss 0.9814 (1.3435)	grad_norm 20.6909 (nan)	mem 4879MB
[2022-05-31 06:25:13 MetaFG_0] (main.py 265): INFO Train: [57/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2932 (0.3073)	loss 1.4734 (1.3435)	grad_norm 25.9856 (nan)	mem 4879MB
[2022-05-31 06:25:16 MetaFG_0] (main.py 265): INFO Train: [57/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.3005 (0.3074)	loss 1.4163 (1.3441)	grad_norm 35.3534 (nan)	mem 4879MB
[2022-05-31 06:25:19 MetaFG_0] (main.py 265): INFO Train: [57/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2987 (0.3074)	loss 1.5130 (1.3450)	grad_norm 35.3433 (nan)	mem 4879MB
[2022-05-31 06:25:23 MetaFG_0] (main.py 265): INFO Train: [57/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.3013 (0.3074)	loss 0.9863 (1.3452)	grad_norm 22.1150 (nan)	mem 4879MB
[2022-05-31 06:25:26 MetaFG_0] (main.py 265): INFO Train: [57/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.3003 (0.3074)	loss 1.3573 (1.3454)	grad_norm 29.7813 (nan)	mem 4879MB
[2022-05-31 06:25:29 MetaFG_0] (main.py 265): INFO Train: [57/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.2946 (0.3074)	loss 1.7266 (1.3458)	grad_norm 30.7251 (nan)	mem 4879MB
[2022-05-31 06:25:32 MetaFG_0] (main.py 265): INFO Train: [57/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.3004 (0.3074)	loss 1.6034 (1.3461)	grad_norm 22.1485 (nan)	mem 4879MB
[2022-05-31 06:25:35 MetaFG_0] (main.py 265): INFO Train: [57/300][780/1562]	eta 0:04:00 lr 0.000006	time 0.3096 (0.3074)	loss 1.5135 (1.3471)	grad_norm 28.0689 (nan)	mem 4879MB
[2022-05-31 06:25:38 MetaFG_0] (main.py 265): INFO Train: [57/300][790/1562]	eta 0:03:57 lr 0.000006	time 0.2938 (0.3074)	loss 1.1644 (1.3468)	grad_norm 22.9530 (nan)	mem 4879MB
[2022-05-31 06:25:41 MetaFG_0] (main.py 265): INFO Train: [57/300][800/1562]	eta 0:03:54 lr 0.000006	time 0.3007 (0.3074)	loss 0.9541 (1.3468)	grad_norm 42.8271 (nan)	mem 4879MB
[2022-05-31 06:25:44 MetaFG_0] (main.py 265): INFO Train: [57/300][810/1562]	eta 0:03:51 lr 0.000006	time 0.2999 (0.3074)	loss 1.5203 (1.3468)	grad_norm 28.1388 (nan)	mem 4879MB
[2022-05-31 06:25:47 MetaFG_0] (main.py 265): INFO Train: [57/300][820/1562]	eta 0:03:48 lr 0.000006	time 0.3012 (0.3074)	loss 1.3663 (1.3462)	grad_norm 29.9148 (nan)	mem 4879MB
[2022-05-31 06:25:50 MetaFG_0] (main.py 265): INFO Train: [57/300][830/1562]	eta 0:03:45 lr 0.000006	time 0.2993 (0.3074)	loss 1.0530 (1.3450)	grad_norm 13.4114 (nan)	mem 4879MB
[2022-05-31 06:25:53 MetaFG_0] (main.py 265): INFO Train: [57/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.3002 (0.3074)	loss 1.3716 (1.3454)	grad_norm 53.6203 (nan)	mem 4879MB
[2022-05-31 06:25:56 MetaFG_0] (main.py 265): INFO Train: [57/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2997 (0.3074)	loss 1.0276 (1.3453)	grad_norm 46.2858 (nan)	mem 4879MB
[2022-05-31 06:25:59 MetaFG_0] (main.py 265): INFO Train: [57/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2999 (0.3074)	loss 1.3280 (1.3457)	grad_norm 26.2808 (nan)	mem 4879MB
[2022-05-31 06:26:02 MetaFG_0] (main.py 265): INFO Train: [57/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3009 (0.3074)	loss 1.2646 (1.3457)	grad_norm 38.7763 (nan)	mem 4879MB
[2022-05-31 06:26:06 MetaFG_0] (main.py 265): INFO Train: [57/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2939 (0.3074)	loss 0.9237 (1.3459)	grad_norm 33.0941 (nan)	mem 4879MB
[2022-05-31 06:26:09 MetaFG_0] (main.py 265): INFO Train: [57/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.3000 (0.3074)	loss 1.0038 (1.3456)	grad_norm 26.8414 (nan)	mem 4879MB
[2022-05-31 06:26:12 MetaFG_0] (main.py 265): INFO Train: [57/300][900/1562]	eta 0:03:23 lr 0.000006	time 0.3016 (0.3073)	loss 1.0877 (1.3443)	grad_norm 21.7747 (nan)	mem 4879MB
[2022-05-31 06:26:15 MetaFG_0] (main.py 265): INFO Train: [57/300][910/1562]	eta 0:03:20 lr 0.000006	time 0.2994 (0.3074)	loss 1.4626 (1.3456)	grad_norm 42.0891 (nan)	mem 4879MB
[2022-05-31 06:26:18 MetaFG_0] (main.py 265): INFO Train: [57/300][920/1562]	eta 0:03:17 lr 0.000006	time 0.3000 (0.3074)	loss 1.1534 (1.3463)	grad_norm 27.6509 (nan)	mem 4879MB
[2022-05-31 06:26:21 MetaFG_0] (main.py 265): INFO Train: [57/300][930/1562]	eta 0:03:14 lr 0.000006	time 0.2997 (0.3074)	loss 1.4131 (1.3464)	grad_norm 33.8359 (nan)	mem 4879MB
[2022-05-31 06:26:24 MetaFG_0] (main.py 265): INFO Train: [57/300][940/1562]	eta 0:03:11 lr 0.000006	time 0.2995 (0.3074)	loss 1.7044 (1.3482)	grad_norm 26.3250 (nan)	mem 4879MB
[2022-05-31 06:26:27 MetaFG_0] (main.py 265): INFO Train: [57/300][950/1562]	eta 0:03:08 lr 0.000006	time 0.2947 (0.3074)	loss 1.1614 (1.3472)	grad_norm 22.0762 (nan)	mem 4879MB
[2022-05-31 06:26:30 MetaFG_0] (main.py 265): INFO Train: [57/300][960/1562]	eta 0:03:05 lr 0.000006	time 0.2989 (0.3074)	loss 1.0575 (1.3464)	grad_norm 21.8221 (nan)	mem 4879MB
[2022-05-31 06:26:33 MetaFG_0] (main.py 265): INFO Train: [57/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.3001 (0.3074)	loss 1.6271 (1.3453)	grad_norm 26.1723 (nan)	mem 4879MB
[2022-05-31 06:26:36 MetaFG_0] (main.py 265): INFO Train: [57/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2996 (0.3074)	loss 1.4042 (1.3439)	grad_norm 21.9873 (nan)	mem 4879MB
[2022-05-31 06:26:39 MetaFG_0] (main.py 265): INFO Train: [57/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.3001 (0.3074)	loss 1.5117 (1.3445)	grad_norm 33.3909 (nan)	mem 4879MB
[2022-05-31 06:26:42 MetaFG_0] (main.py 265): INFO Train: [57/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2943 (0.3074)	loss 1.3473 (1.3438)	grad_norm 29.7941 (nan)	mem 4879MB
[2022-05-31 06:26:45 MetaFG_0] (main.py 265): INFO Train: [57/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2967 (0.3074)	loss 1.5277 (1.3446)	grad_norm 19.8230 (nan)	mem 4879MB
[2022-05-31 06:26:49 MetaFG_0] (main.py 265): INFO Train: [57/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2936 (0.3073)	loss 0.9838 (1.3440)	grad_norm 39.9218 (nan)	mem 4879MB
[2022-05-31 06:26:52 MetaFG_0] (main.py 265): INFO Train: [57/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2997 (0.3074)	loss 1.4402 (1.3442)	grad_norm 28.7945 (nan)	mem 4879MB
[2022-05-31 06:26:55 MetaFG_0] (main.py 265): INFO Train: [57/300][1040/1562]	eta 0:02:40 lr 0.000006	time 0.3004 (0.3074)	loss 1.1956 (1.3440)	grad_norm 16.1485 (nan)	mem 4879MB
[2022-05-31 06:26:58 MetaFG_0] (main.py 265): INFO Train: [57/300][1050/1562]	eta 0:02:37 lr 0.000006	time 0.3055 (0.3074)	loss 1.3974 (1.3448)	grad_norm 16.2427 (nan)	mem 4879MB
[2022-05-31 06:27:01 MetaFG_0] (main.py 265): INFO Train: [57/300][1060/1562]	eta 0:02:34 lr 0.000006	time 0.2949 (0.3073)	loss 1.0352 (1.3446)	grad_norm 37.8397 (nan)	mem 4879MB
[2022-05-31 06:27:04 MetaFG_0] (main.py 265): INFO Train: [57/300][1070/1562]	eta 0:02:31 lr 0.000006	time 0.3000 (0.3074)	loss 1.4967 (1.3452)	grad_norm 19.5758 (nan)	mem 4879MB
[2022-05-31 06:27:07 MetaFG_0] (main.py 265): INFO Train: [57/300][1080/1562]	eta 0:02:28 lr 0.000006	time 0.2967 (0.3073)	loss 0.9913 (1.3450)	grad_norm 29.1163 (nan)	mem 4879MB
[2022-05-31 06:27:10 MetaFG_0] (main.py 265): INFO Train: [57/300][1090/1562]	eta 0:02:25 lr 0.000006	time 0.3001 (0.3074)	loss 1.5337 (1.3461)	grad_norm 22.6590 (nan)	mem 4879MB
[2022-05-31 06:27:13 MetaFG_0] (main.py 265): INFO Train: [57/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2932 (0.3074)	loss 1.2729 (1.3469)	grad_norm 22.1301 (nan)	mem 4879MB
[2022-05-31 06:27:16 MetaFG_0] (main.py 265): INFO Train: [57/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2930 (0.3073)	loss 1.4310 (1.3465)	grad_norm 30.3335 (nan)	mem 4879MB
[2022-05-31 06:27:19 MetaFG_0] (main.py 265): INFO Train: [57/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2954 (0.3073)	loss 0.9979 (1.3457)	grad_norm 38.5762 (nan)	mem 4879MB
[2022-05-31 06:27:22 MetaFG_0] (main.py 265): INFO Train: [57/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2931 (0.3073)	loss 0.9771 (1.3440)	grad_norm 23.3563 (nan)	mem 4879MB
[2022-05-31 06:27:25 MetaFG_0] (main.py 265): INFO Train: [57/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2933 (0.3073)	loss 1.5112 (1.3444)	grad_norm 32.9763 (nan)	mem 4879MB
[2022-05-31 06:27:28 MetaFG_0] (main.py 265): INFO Train: [57/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.3001 (0.3073)	loss 0.9717 (1.3450)	grad_norm 35.7525 (nan)	mem 4879MB
[2022-05-31 06:27:32 MetaFG_0] (main.py 265): INFO Train: [57/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2949 (0.3073)	loss 1.5251 (1.3454)	grad_norm 24.9285 (nan)	mem 4879MB
[2022-05-31 06:27:35 MetaFG_0] (main.py 265): INFO Train: [57/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2935 (0.3073)	loss 1.6081 (1.3447)	grad_norm 31.8323 (nan)	mem 4879MB
[2022-05-31 06:27:38 MetaFG_0] (main.py 265): INFO Train: [57/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.2937 (0.3073)	loss 0.8178 (1.3442)	grad_norm 20.0822 (nan)	mem 4879MB
[2022-05-31 06:27:41 MetaFG_0] (main.py 265): INFO Train: [57/300][1190/1562]	eta 0:01:54 lr 0.000006	time 0.2925 (0.3073)	loss 1.5464 (1.3437)	grad_norm 29.7348 (nan)	mem 4879MB
[2022-05-31 06:27:44 MetaFG_0] (main.py 265): INFO Train: [57/300][1200/1562]	eta 0:01:51 lr 0.000006	time 0.2943 (0.3072)	loss 1.3774 (1.3434)	grad_norm 33.3114 (nan)	mem 4879MB
[2022-05-31 06:27:47 MetaFG_0] (main.py 265): INFO Train: [57/300][1210/1562]	eta 0:01:48 lr 0.000006	time 0.3005 (0.3072)	loss 1.0314 (1.3435)	grad_norm 58.1916 (nan)	mem 4879MB
[2022-05-31 06:27:50 MetaFG_0] (main.py 265): INFO Train: [57/300][1220/1562]	eta 0:01:45 lr 0.000006	time 0.2990 (0.3072)	loss 1.3023 (1.3440)	grad_norm 16.1762 (nan)	mem 4879MB
[2022-05-31 06:27:53 MetaFG_0] (main.py 265): INFO Train: [57/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2950 (0.3072)	loss 1.4920 (1.3448)	grad_norm 24.1090 (nan)	mem 4879MB
[2022-05-31 06:27:56 MetaFG_0] (main.py 265): INFO Train: [57/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2988 (0.3072)	loss 1.2437 (1.3448)	grad_norm 37.3062 (nan)	mem 4879MB
[2022-05-31 06:27:59 MetaFG_0] (main.py 265): INFO Train: [57/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3043 (0.3072)	loss 1.7488 (1.3457)	grad_norm 32.6375 (nan)	mem 4879MB
[2022-05-31 06:28:02 MetaFG_0] (main.py 265): INFO Train: [57/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3004 (0.3072)	loss 1.2934 (1.3455)	grad_norm 18.9391 (nan)	mem 4879MB
[2022-05-31 06:28:05 MetaFG_0] (main.py 265): INFO Train: [57/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.3005 (0.3072)	loss 1.3219 (1.3451)	grad_norm 34.6300 (nan)	mem 4879MB
[2022-05-31 06:28:08 MetaFG_0] (main.py 265): INFO Train: [57/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2930 (0.3071)	loss 1.4218 (1.3445)	grad_norm 13.6854 (nan)	mem 4879MB
[2022-05-31 06:28:11 MetaFG_0] (main.py 265): INFO Train: [57/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.3012 (0.3071)	loss 1.5113 (1.3447)	grad_norm 32.4087 (nan)	mem 4879MB
[2022-05-31 06:28:14 MetaFG_0] (main.py 265): INFO Train: [57/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2935 (0.3071)	loss 1.4548 (1.3444)	grad_norm 28.9958 (nan)	mem 4879MB
[2022-05-31 06:28:17 MetaFG_0] (main.py 265): INFO Train: [57/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2938 (0.3071)	loss 1.5001 (1.3437)	grad_norm 32.9549 (nan)	mem 4879MB
[2022-05-31 06:28:20 MetaFG_0] (main.py 265): INFO Train: [57/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2936 (0.3071)	loss 1.8011 (1.3443)	grad_norm 30.1173 (nan)	mem 4879MB
[2022-05-31 06:28:23 MetaFG_0] (main.py 265): INFO Train: [57/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2945 (0.3071)	loss 1.5752 (1.3435)	grad_norm 21.2595 (nan)	mem 4879MB
[2022-05-31 06:28:26 MetaFG_0] (main.py 265): INFO Train: [57/300][1340/1562]	eta 0:01:08 lr 0.000006	time 0.2984 (0.3070)	loss 1.2335 (1.3442)	grad_norm 29.0443 (nan)	mem 4879MB
[2022-05-31 06:28:30 MetaFG_0] (main.py 265): INFO Train: [57/300][1350/1562]	eta 0:01:05 lr 0.000006	time 0.2956 (0.3070)	loss 0.9692 (1.3438)	grad_norm 18.6414 (nan)	mem 4879MB
[2022-05-31 06:28:33 MetaFG_0] (main.py 265): INFO Train: [57/300][1360/1562]	eta 0:01:02 lr 0.000006	time 0.2952 (0.3070)	loss 0.8911 (1.3434)	grad_norm 27.8836 (nan)	mem 4879MB
[2022-05-31 06:28:36 MetaFG_0] (main.py 265): INFO Train: [57/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2976 (0.3070)	loss 1.4221 (1.3437)	grad_norm 51.8588 (nan)	mem 4879MB
[2022-05-31 06:28:39 MetaFG_0] (main.py 265): INFO Train: [57/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2990 (0.3070)	loss 1.2675 (1.3448)	grad_norm 19.4080 (nan)	mem 4879MB
[2022-05-31 06:28:42 MetaFG_0] (main.py 265): INFO Train: [57/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2989 (0.3070)	loss 1.3720 (1.3455)	grad_norm 30.3701 (nan)	mem 4879MB
[2022-05-31 06:28:45 MetaFG_0] (main.py 265): INFO Train: [57/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.3006 (0.3069)	loss 1.0489 (1.3454)	grad_norm 24.9349 (nan)	mem 4879MB
[2022-05-31 06:28:48 MetaFG_0] (main.py 265): INFO Train: [57/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2934 (0.3069)	loss 1.6764 (1.3462)	grad_norm 19.5539 (nan)	mem 4879MB
[2022-05-31 06:28:51 MetaFG_0] (main.py 265): INFO Train: [57/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2994 (0.3069)	loss 1.4468 (1.3466)	grad_norm 31.6401 (nan)	mem 4879MB
[2022-05-31 06:28:54 MetaFG_0] (main.py 265): INFO Train: [57/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2926 (0.3069)	loss 1.6352 (1.3462)	grad_norm 17.5403 (nan)	mem 4879MB
[2022-05-31 06:28:57 MetaFG_0] (main.py 265): INFO Train: [57/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2932 (0.3069)	loss 1.4528 (1.3464)	grad_norm 45.0547 (nan)	mem 4879MB
[2022-05-31 06:29:00 MetaFG_0] (main.py 265): INFO Train: [57/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2935 (0.3069)	loss 1.5619 (1.3470)	grad_norm 34.5565 (nan)	mem 4879MB
[2022-05-31 06:29:03 MetaFG_0] (main.py 265): INFO Train: [57/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2982 (0.3069)	loss 1.4975 (1.3470)	grad_norm 29.2712 (nan)	mem 4879MB
[2022-05-31 06:29:06 MetaFG_0] (main.py 265): INFO Train: [57/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2926 (0.3068)	loss 1.6173 (1.3467)	grad_norm 28.8605 (nan)	mem 4879MB
[2022-05-31 06:29:09 MetaFG_0] (main.py 265): INFO Train: [57/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.3005 (0.3068)	loss 0.8143 (1.3459)	grad_norm 24.9557 (nan)	mem 4879MB
[2022-05-31 06:29:12 MetaFG_0] (main.py 265): INFO Train: [57/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2929 (0.3068)	loss 1.6549 (1.3459)	grad_norm 62.6866 (nan)	mem 4879MB
[2022-05-31 06:29:15 MetaFG_0] (main.py 265): INFO Train: [57/300][1500/1562]	eta 0:00:19 lr 0.000006	time 0.2987 (0.3068)	loss 1.3716 (1.3469)	grad_norm 16.6892 (nan)	mem 4879MB
[2022-05-31 06:29:18 MetaFG_0] (main.py 265): INFO Train: [57/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2991 (0.3068)	loss 1.4588 (1.3465)	grad_norm 37.7991 (nan)	mem 4879MB
[2022-05-31 06:29:21 MetaFG_0] (main.py 265): INFO Train: [57/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2993 (0.3068)	loss 1.2816 (1.3472)	grad_norm 30.0988 (nan)	mem 4879MB
[2022-05-31 06:29:24 MetaFG_0] (main.py 265): INFO Train: [57/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2937 (0.3068)	loss 1.1765 (1.3470)	grad_norm 21.9640 (nan)	mem 4879MB
[2022-05-31 06:29:27 MetaFG_0] (main.py 265): INFO Train: [57/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2945 (0.3067)	loss 1.4277 (1.3463)	grad_norm 25.5486 (nan)	mem 4879MB
[2022-05-31 06:29:30 MetaFG_0] (main.py 265): INFO Train: [57/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2934 (0.3067)	loss 1.1207 (1.3454)	grad_norm 35.4041 (nan)	mem 4879MB
[2022-05-31 06:29:33 MetaFG_0] (main.py 265): INFO Train: [57/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2927 (0.3067)	loss 1.5618 (1.3455)	grad_norm 21.4858 (nan)	mem 4879MB
[2022-05-31 06:29:34 MetaFG_0] (main.py 272): INFO EPOCH 57 training takes 0:07:59
[2022-05-31 06:29:34 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_57.pth saving......
[2022-05-31 06:29:35 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_57.pth saved !!!
[2022-05-31 06:29:35 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:29:36 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:29:36 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:29:37 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.601 (0.601)	Loss 0.4585 (0.4585)	Acc@1 96.875 (96.875)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 06:29:38 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.106 (0.144)	Loss 0.5299 (0.6121)	Acc@1 93.750 (87.784)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 06:29:39 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.120)	Loss 0.4418 (0.6006)	Acc@1 87.500 (87.798)	Acc@5 100.000 (98.065)	Mem 4879MB
[2022-05-31 06:29:40 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.100 (0.112)	Loss 0.5948 (0.6148)	Acc@1 87.500 (87.399)	Acc@5 100.000 (98.185)	Mem 4879MB
[2022-05-31 06:29:41 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.107)	Loss 0.5768 (0.5983)	Acc@1 87.500 (87.576)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 06:29:42 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.095 (0.105)	Loss 0.6642 (0.6003)	Acc@1 81.250 (87.255)	Acc@5 100.000 (98.713)	Mem 4879MB
[2022-05-31 06:29:42 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.098 (0.103)	Loss 0.4633 (0.5981)	Acc@1 90.625 (86.936)	Acc@5 100.000 (98.822)	Mem 4879MB
[2022-05-31 06:29:43 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.6886 (0.5959)	Acc@1 81.250 (86.620)	Acc@5 100.000 (98.900)	Mem 4879MB
[2022-05-31 06:29:44 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 0.4201 (0.5952)	Acc@1 93.750 (86.690)	Acc@5 100.000 (98.881)	Mem 4879MB
[2022-05-31 06:29:45 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.107 (0.100)	Loss 0.5652 (0.5943)	Acc@1 81.250 (86.676)	Acc@5 100.000 (98.832)	Mem 4879MB
[2022-05-31 06:29:46 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.099)	Loss 0.5634 (0.5921)	Acc@1 90.625 (86.788)	Acc@5 100.000 (98.855)	Mem 4879MB
[2022-05-31 06:29:47 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.090 (0.099)	Loss 0.7782 (0.5884)	Acc@1 81.250 (86.937)	Acc@5 100.000 (98.902)	Mem 4879MB
[2022-05-31 06:29:48 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.099)	Loss 0.4784 (0.5902)	Acc@1 90.625 (86.854)	Acc@5 96.875 (98.812)	Mem 4879MB
[2022-05-31 06:29:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.098)	Loss 0.5576 (0.5944)	Acc@1 90.625 (86.689)	Acc@5 100.000 (98.783)	Mem 4879MB
[2022-05-31 06:29:50 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.098)	Loss 0.5812 (0.5975)	Acc@1 87.500 (86.613)	Acc@5 100.000 (98.781)	Mem 4879MB
[2022-05-31 06:29:51 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.5651 (0.5968)	Acc@1 81.250 (86.651)	Acc@5 100.000 (98.758)	Mem 4879MB
[2022-05-31 06:29:52 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 0.5166 (0.5982)	Acc@1 93.750 (86.665)	Acc@5 100.000 (98.797)	Mem 4879MB
[2022-05-31 06:29:53 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.097)	Loss 0.6812 (0.6004)	Acc@1 87.500 (86.696)	Acc@5 96.875 (98.721)	Mem 4879MB
[2022-05-31 06:29:54 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.097)	Loss 0.4350 (0.5998)	Acc@1 90.625 (86.740)	Acc@5 100.000 (98.774)	Mem 4879MB
[2022-05-31 06:29:55 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.096 (0.097)	Loss 0.6364 (0.6006)	Acc@1 81.250 (86.649)	Acc@5 96.875 (98.789)	Mem 4879MB
[2022-05-31 06:29:56 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 0.5764 (0.6028)	Acc@1 87.500 (86.660)	Acc@5 100.000 (98.741)	Mem 4879MB
[2022-05-31 06:29:57 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.097)	Loss 0.5152 (0.6030)	Acc@1 87.500 (86.582)	Acc@5 96.875 (98.741)	Mem 4879MB
[2022-05-31 06:29:58 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.097)	Loss 0.6072 (0.6027)	Acc@1 87.500 (86.581)	Acc@5 96.875 (98.742)	Mem 4879MB
[2022-05-31 06:29:59 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.8254 (0.6053)	Acc@1 78.125 (86.499)	Acc@5 93.750 (98.715)	Mem 4879MB
[2022-05-31 06:29:59 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.092 (0.097)	Loss 0.4255 (0.6046)	Acc@1 93.750 (86.553)	Acc@5 100.000 (98.742)	Mem 4879MB
[2022-05-31 06:30:00 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.089 (0.096)	Loss 0.8067 (0.6054)	Acc@1 87.500 (86.616)	Acc@5 96.875 (98.730)	Mem 4879MB
[2022-05-31 06:30:01 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 0.4223 (0.6040)	Acc@1 90.625 (86.662)	Acc@5 100.000 (98.719)	Mem 4879MB
[2022-05-31 06:30:02 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.088 (0.096)	Loss 0.7174 (0.6093)	Acc@1 84.375 (86.508)	Acc@5 93.750 (98.628)	Mem 4879MB
[2022-05-31 06:30:03 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.088 (0.096)	Loss 0.5611 (0.6056)	Acc@1 87.500 (86.644)	Acc@5 100.000 (98.665)	Mem 4879MB
[2022-05-31 06:30:04 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.093 (0.096)	Loss 0.4838 (0.6045)	Acc@1 93.750 (86.716)	Acc@5 100.000 (98.658)	Mem 4879MB
[2022-05-31 06:30:05 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.127 (0.096)	Loss 0.7002 (0.6075)	Acc@1 87.500 (86.524)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 06:30:06 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8094 (0.6097)	Acc@1 68.750 (86.375)	Acc@5 100.000 (98.654)	Mem 4879MB
[2022-05-31 06:30:06 MetaFG_0] (main.py 330): INFO  * Acc@1 86.380 Acc@5 98.650
[2022-05-31 06:30:06 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.4%
[2022-05-31 06:30:06 MetaFG_0] (main.py 171): INFO Max accuracy: 86.54%
[2022-05-31 06:30:08 MetaFG_0] (main.py 265): INFO Train: [58/300][0/1562]	eta 0:32:02 lr 0.000006	time 1.2309 (1.2309)	loss 1.3474 (1.3474)	grad_norm 27.1971 (27.1971)	mem 4879MB
[2022-05-31 06:30:11 MetaFG_0] (main.py 265): INFO Train: [58/300][10/1562]	eta 0:10:09 lr 0.000006	time 0.2976 (0.3927)	loss 1.2849 (1.3174)	grad_norm 41.3875 (30.7255)	mem 4879MB
[2022-05-31 06:30:14 MetaFG_0] (main.py 265): INFO Train: [58/300][20/1562]	eta 0:09:00 lr 0.000006	time 0.3000 (0.3505)	loss 1.3234 (1.3300)	grad_norm 19.6922 (31.8205)	mem 4879MB
[2022-05-31 06:30:17 MetaFG_0] (main.py 265): INFO Train: [58/300][30/1562]	eta 0:08:34 lr 0.000006	time 0.2959 (0.3358)	loss 1.3791 (1.3085)	grad_norm 18.9856 (31.8389)	mem 4879MB
[2022-05-31 06:30:20 MetaFG_0] (main.py 265): INFO Train: [58/300][40/1562]	eta 0:08:19 lr 0.000006	time 0.2985 (0.3279)	loss 0.9489 (1.3296)	grad_norm 42.4595 (31.4185)	mem 4879MB
[2022-05-31 06:30:23 MetaFG_0] (main.py 265): INFO Train: [58/300][50/1562]	eta 0:08:08 lr 0.000006	time 0.2984 (0.3231)	loss 1.2435 (1.3133)	grad_norm 22.2243 (31.6362)	mem 4879MB
[2022-05-31 06:30:26 MetaFG_0] (main.py 265): INFO Train: [58/300][60/1562]	eta 0:08:00 lr 0.000006	time 0.2996 (0.3202)	loss 1.4312 (1.3268)	grad_norm 38.1727 (31.4264)	mem 4879MB
[2022-05-31 06:30:29 MetaFG_0] (main.py 265): INFO Train: [58/300][70/1562]	eta 0:07:54 lr 0.000006	time 0.2918 (0.3181)	loss 1.0057 (1.3126)	grad_norm 22.3789 (31.6939)	mem 4879MB
[2022-05-31 06:30:32 MetaFG_0] (main.py 265): INFO Train: [58/300][80/1562]	eta 0:07:48 lr 0.000006	time 0.2935 (0.3162)	loss 1.2755 (1.3065)	grad_norm 38.9105 (31.4550)	mem 4879MB
[2022-05-31 06:30:35 MetaFG_0] (main.py 265): INFO Train: [58/300][90/1562]	eta 0:07:43 lr 0.000006	time 0.2970 (0.3147)	loss 1.1918 (1.3093)	grad_norm 51.1873 (31.4089)	mem 4879MB
[2022-05-31 06:30:38 MetaFG_0] (main.py 265): INFO Train: [58/300][100/1562]	eta 0:07:38 lr 0.000006	time 0.2931 (0.3138)	loss 1.4645 (1.3180)	grad_norm 19.6200 (30.6652)	mem 4879MB
[2022-05-31 06:30:41 MetaFG_0] (main.py 265): INFO Train: [58/300][110/1562]	eta 0:07:34 lr 0.000006	time 0.2931 (0.3129)	loss 1.4502 (1.3278)	grad_norm 28.3561 (30.7631)	mem 4879MB
[2022-05-31 06:30:44 MetaFG_0] (main.py 265): INFO Train: [58/300][120/1562]	eta 0:07:30 lr 0.000006	time 0.2999 (0.3122)	loss 1.5761 (1.3408)	grad_norm 36.4736 (30.6083)	mem 4879MB
[2022-05-31 06:30:47 MetaFG_0] (main.py 265): INFO Train: [58/300][130/1562]	eta 0:07:26 lr 0.000006	time 0.2950 (0.3117)	loss 1.5746 (1.3452)	grad_norm 31.9228 (30.5353)	mem 4879MB
[2022-05-31 06:30:50 MetaFG_0] (main.py 265): INFO Train: [58/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.2940 (0.3111)	loss 1.2814 (1.3388)	grad_norm 24.1034 (30.4427)	mem 4879MB
[2022-05-31 06:30:53 MetaFG_0] (main.py 265): INFO Train: [58/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2982 (0.3107)	loss 1.2094 (1.3401)	grad_norm 33.3272 (30.5298)	mem 4879MB
[2022-05-31 06:30:56 MetaFG_0] (main.py 265): INFO Train: [58/300][160/1562]	eta 0:07:14 lr 0.000006	time 0.2919 (0.3103)	loss 1.5180 (1.3370)	grad_norm 24.1542 (30.3515)	mem 4879MB
[2022-05-31 06:30:59 MetaFG_0] (main.py 265): INFO Train: [58/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2995 (0.3099)	loss 1.3736 (1.3401)	grad_norm 53.2494 (30.7886)	mem 4879MB
[2022-05-31 06:31:02 MetaFG_0] (main.py 265): INFO Train: [58/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2937 (0.3096)	loss 1.1759 (1.3431)	grad_norm 42.4704 (31.3214)	mem 4879MB
[2022-05-31 06:31:06 MetaFG_0] (main.py 265): INFO Train: [58/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2925 (0.3093)	loss 1.4520 (1.3430)	grad_norm 36.9998 (31.0907)	mem 4879MB
[2022-05-31 06:31:09 MetaFG_0] (main.py 265): INFO Train: [58/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2986 (0.3091)	loss 1.3362 (1.3428)	grad_norm 25.7547 (30.9866)	mem 4879MB
[2022-05-31 06:31:12 MetaFG_0] (main.py 265): INFO Train: [58/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2922 (0.3088)	loss 1.5626 (1.3425)	grad_norm 36.9364 (30.9271)	mem 4879MB
[2022-05-31 06:31:15 MetaFG_0] (main.py 265): INFO Train: [58/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2927 (0.3085)	loss 1.3154 (1.3410)	grad_norm 20.5851 (31.1799)	mem 4879MB
[2022-05-31 06:31:18 MetaFG_0] (main.py 265): INFO Train: [58/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2985 (0.3084)	loss 1.4571 (1.3436)	grad_norm 35.7053 (31.0487)	mem 4879MB
[2022-05-31 06:31:21 MetaFG_0] (main.py 265): INFO Train: [58/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2979 (0.3082)	loss 1.5880 (1.3427)	grad_norm 50.3046 (31.0168)	mem 4879MB
[2022-05-31 06:31:24 MetaFG_0] (main.py 265): INFO Train: [58/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2940 (0.3080)	loss 0.9806 (1.3407)	grad_norm 20.3816 (30.8973)	mem 4879MB
[2022-05-31 06:31:27 MetaFG_0] (main.py 265): INFO Train: [58/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.3002 (0.3080)	loss 1.8435 (1.3456)	grad_norm 34.7680 (30.8706)	mem 4879MB
[2022-05-31 06:31:30 MetaFG_0] (main.py 265): INFO Train: [58/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2998 (0.3079)	loss 1.3630 (1.3454)	grad_norm 20.5820 (30.7691)	mem 4879MB
[2022-05-31 06:31:33 MetaFG_0] (main.py 265): INFO Train: [58/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.3001 (0.3078)	loss 1.4791 (1.3508)	grad_norm 24.3536 (30.7665)	mem 4879MB
[2022-05-31 06:31:36 MetaFG_0] (main.py 265): INFO Train: [58/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2980 (0.3077)	loss 1.8777 (1.3564)	grad_norm 159.6816 (31.1529)	mem 4879MB
[2022-05-31 06:31:39 MetaFG_0] (main.py 265): INFO Train: [58/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.3012 (0.3076)	loss 1.4799 (1.3556)	grad_norm 32.7715 (30.9792)	mem 4879MB
[2022-05-31 06:31:42 MetaFG_0] (main.py 265): INFO Train: [58/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2986 (0.3074)	loss 0.7866 (1.3537)	grad_norm 18.9421 (30.8823)	mem 4879MB
[2022-05-31 06:31:45 MetaFG_0] (main.py 265): INFO Train: [58/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2941 (0.3073)	loss 1.6107 (1.3555)	grad_norm 36.9518 (30.8313)	mem 4879MB
[2022-05-31 06:31:48 MetaFG_0] (main.py 265): INFO Train: [58/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2992 (0.3072)	loss 1.7289 (1.3571)	grad_norm 40.4418 (30.8081)	mem 4879MB
[2022-05-31 06:31:51 MetaFG_0] (main.py 265): INFO Train: [58/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2990 (0.3071)	loss 1.0983 (1.3541)	grad_norm 47.1085 (30.7081)	mem 4879MB
[2022-05-31 06:31:54 MetaFG_0] (main.py 265): INFO Train: [58/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2932 (0.3070)	loss 1.4344 (1.3551)	grad_norm 25.2465 (30.4850)	mem 4879MB
[2022-05-31 06:31:57 MetaFG_0] (main.py 265): INFO Train: [58/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2924 (0.3069)	loss 1.1878 (1.3540)	grad_norm 29.0668 (30.5482)	mem 4879MB
[2022-05-31 06:32:00 MetaFG_0] (main.py 265): INFO Train: [58/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2990 (0.3068)	loss 1.4643 (1.3550)	grad_norm 21.1825 (30.5752)	mem 4879MB
[2022-05-31 06:32:03 MetaFG_0] (main.py 265): INFO Train: [58/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.3056 (0.3067)	loss 0.8970 (1.3509)	grad_norm 55.3759 (30.5231)	mem 4879MB
[2022-05-31 06:32:06 MetaFG_0] (main.py 265): INFO Train: [58/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2967 (0.3067)	loss 1.3712 (1.3533)	grad_norm 15.4549 (30.4111)	mem 4879MB
[2022-05-31 06:32:09 MetaFG_0] (main.py 265): INFO Train: [58/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2927 (0.3066)	loss 1.7223 (1.3531)	grad_norm 18.4945 (30.2853)	mem 4879MB
[2022-05-31 06:32:12 MetaFG_0] (main.py 265): INFO Train: [58/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2926 (0.3065)	loss 1.2680 (1.3511)	grad_norm 25.2824 (30.1662)	mem 4879MB
[2022-05-31 06:32:15 MetaFG_0] (main.py 265): INFO Train: [58/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2951 (0.3064)	loss 1.4642 (1.3464)	grad_norm 14.9673 (30.0550)	mem 4879MB
[2022-05-31 06:32:19 MetaFG_0] (main.py 265): INFO Train: [58/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2933 (0.3064)	loss 1.3788 (1.3465)	grad_norm 27.1631 (30.1350)	mem 4879MB
[2022-05-31 06:32:22 MetaFG_0] (main.py 265): INFO Train: [58/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.2977 (0.3063)	loss 1.5410 (1.3479)	grad_norm 28.5807 (30.0981)	mem 4879MB
[2022-05-31 06:32:25 MetaFG_0] (main.py 265): INFO Train: [58/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2987 (0.3063)	loss 1.0328 (1.3472)	grad_norm 25.3605 (30.0543)	mem 4879MB
[2022-05-31 06:32:28 MetaFG_0] (main.py 265): INFO Train: [58/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2933 (0.3062)	loss 1.4035 (1.3486)	grad_norm 21.4710 (29.9932)	mem 4879MB
[2022-05-31 06:32:31 MetaFG_0] (main.py 265): INFO Train: [58/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2989 (0.3062)	loss 0.9314 (1.3474)	grad_norm 16.1545 (30.0648)	mem 4879MB
[2022-05-31 06:32:34 MetaFG_0] (main.py 265): INFO Train: [58/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2966 (0.3061)	loss 1.6242 (1.3487)	grad_norm 32.0871 (30.0521)	mem 4879MB
[2022-05-31 06:32:37 MetaFG_0] (main.py 265): INFO Train: [58/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2979 (0.3060)	loss 1.0509 (1.3495)	grad_norm 25.0285 (29.9912)	mem 4879MB
[2022-05-31 06:32:40 MetaFG_0] (main.py 265): INFO Train: [58/300][500/1562]	eta 0:05:24 lr 0.000006	time 0.2930 (0.3060)	loss 1.3479 (1.3498)	grad_norm 40.1699 (29.9465)	mem 4879MB
[2022-05-31 06:32:43 MetaFG_0] (main.py 265): INFO Train: [58/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2927 (0.3060)	loss 1.5258 (1.3504)	grad_norm 22.7618 (30.0421)	mem 4879MB
[2022-05-31 06:32:46 MetaFG_0] (main.py 265): INFO Train: [58/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2987 (0.3060)	loss 1.3261 (1.3488)	grad_norm 18.6516 (30.1292)	mem 4879MB
[2022-05-31 06:32:49 MetaFG_0] (main.py 265): INFO Train: [58/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2982 (0.3060)	loss 1.4572 (1.3469)	grad_norm 30.2521 (30.0261)	mem 4879MB
[2022-05-31 06:32:52 MetaFG_0] (main.py 265): INFO Train: [58/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2941 (0.3059)	loss 1.3190 (1.3479)	grad_norm 28.5471 (29.9131)	mem 4879MB
[2022-05-31 06:32:55 MetaFG_0] (main.py 265): INFO Train: [58/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2985 (0.3059)	loss 1.2774 (1.3501)	grad_norm 23.1980 (30.0250)	mem 4879MB
[2022-05-31 06:32:58 MetaFG_0] (main.py 265): INFO Train: [58/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2924 (0.3058)	loss 1.4629 (1.3515)	grad_norm 25.2235 (30.0013)	mem 4879MB
[2022-05-31 06:33:01 MetaFG_0] (main.py 265): INFO Train: [58/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2933 (0.3058)	loss 1.4259 (1.3524)	grad_norm 27.6105 (30.0359)	mem 4879MB
[2022-05-31 06:33:04 MetaFG_0] (main.py 265): INFO Train: [58/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2917 (0.3058)	loss 1.5401 (1.3539)	grad_norm 25.2411 (29.9824)	mem 4879MB
[2022-05-31 06:33:07 MetaFG_0] (main.py 265): INFO Train: [58/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2934 (0.3057)	loss 1.6900 (1.3526)	grad_norm 22.0291 (29.9921)	mem 4879MB
[2022-05-31 06:33:10 MetaFG_0] (main.py 265): INFO Train: [58/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2929 (0.3057)	loss 1.3643 (1.3515)	grad_norm 51.2275 (29.9936)	mem 4879MB
[2022-05-31 06:33:13 MetaFG_0] (main.py 265): INFO Train: [58/300][610/1562]	eta 0:04:50 lr 0.000006	time 0.2981 (0.3057)	loss 1.1923 (1.3525)	grad_norm 34.0010 (30.0761)	mem 4879MB
[2022-05-31 06:33:16 MetaFG_0] (main.py 265): INFO Train: [58/300][620/1562]	eta 0:04:47 lr 0.000006	time 0.2955 (0.3057)	loss 1.6983 (1.3515)	grad_norm 28.2106 (29.9986)	mem 4879MB
[2022-05-31 06:33:19 MetaFG_0] (main.py 265): INFO Train: [58/300][630/1562]	eta 0:04:44 lr 0.000006	time 0.2938 (0.3056)	loss 1.2256 (1.3506)	grad_norm 13.5346 (29.9725)	mem 4879MB
[2022-05-31 06:33:22 MetaFG_0] (main.py 265): INFO Train: [58/300][640/1562]	eta 0:04:41 lr 0.000006	time 0.2984 (0.3056)	loss 0.9951 (1.3497)	grad_norm 26.6895 (30.0564)	mem 4879MB
[2022-05-31 06:33:25 MetaFG_0] (main.py 265): INFO Train: [58/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2937 (0.3055)	loss 0.9232 (1.3490)	grad_norm 28.5879 (30.0167)	mem 4879MB
[2022-05-31 06:33:28 MetaFG_0] (main.py 265): INFO Train: [58/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2928 (0.3055)	loss 1.4663 (1.3496)	grad_norm 24.2392 (30.0587)	mem 4879MB
[2022-05-31 06:33:31 MetaFG_0] (main.py 265): INFO Train: [58/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2929 (0.3055)	loss 1.2701 (1.3501)	grad_norm 47.2141 (30.0632)	mem 4879MB
[2022-05-31 06:33:35 MetaFG_0] (main.py 265): INFO Train: [58/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2998 (0.3055)	loss 1.2354 (1.3503)	grad_norm 31.3998 (30.0701)	mem 4879MB
[2022-05-31 06:33:38 MetaFG_0] (main.py 265): INFO Train: [58/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2993 (0.3055)	loss 1.3018 (1.3495)	grad_norm 43.3496 (30.1021)	mem 4879MB
[2022-05-31 06:33:41 MetaFG_0] (main.py 265): INFO Train: [58/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2939 (0.3054)	loss 1.3632 (1.3489)	grad_norm 19.3115 (30.0943)	mem 4879MB
[2022-05-31 06:33:44 MetaFG_0] (main.py 265): INFO Train: [58/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2979 (0.3054)	loss 1.5178 (1.3509)	grad_norm 30.4482 (30.0688)	mem 4879MB
[2022-05-31 06:33:47 MetaFG_0] (main.py 265): INFO Train: [58/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2984 (0.3054)	loss 1.1064 (1.3501)	grad_norm 35.9681 (30.0547)	mem 4879MB
[2022-05-31 06:33:50 MetaFG_0] (main.py 265): INFO Train: [58/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.3008 (0.3054)	loss 1.1454 (1.3502)	grad_norm 22.2086 (30.0120)	mem 4879MB
[2022-05-31 06:33:53 MetaFG_0] (main.py 265): INFO Train: [58/300][740/1562]	eta 0:04:10 lr 0.000006	time 0.2935 (0.3053)	loss 1.6146 (1.3492)	grad_norm 45.6920 (30.0335)	mem 4879MB
[2022-05-31 06:33:56 MetaFG_0] (main.py 265): INFO Train: [58/300][750/1562]	eta 0:04:07 lr 0.000006	time 0.2984 (0.3053)	loss 1.1717 (1.3494)	grad_norm 30.5120 (30.0889)	mem 4879MB
[2022-05-31 06:33:59 MetaFG_0] (main.py 265): INFO Train: [58/300][760/1562]	eta 0:04:04 lr 0.000006	time 0.2964 (0.3052)	loss 1.5355 (1.3473)	grad_norm 21.6458 (30.0507)	mem 4879MB
[2022-05-31 06:34:02 MetaFG_0] (main.py 265): INFO Train: [58/300][770/1562]	eta 0:04:01 lr 0.000006	time 0.2952 (0.3052)	loss 1.5774 (1.3458)	grad_norm 29.3178 (30.0474)	mem 4879MB
[2022-05-31 06:34:05 MetaFG_0] (main.py 265): INFO Train: [58/300][780/1562]	eta 0:03:58 lr 0.000006	time 0.2980 (0.3052)	loss 1.2328 (1.3462)	grad_norm 23.2042 (30.0543)	mem 4879MB
[2022-05-31 06:34:08 MetaFG_0] (main.py 265): INFO Train: [58/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2932 (0.3052)	loss 1.5404 (1.3473)	grad_norm 27.2442 (30.0375)	mem 4879MB
[2022-05-31 06:34:11 MetaFG_0] (main.py 265): INFO Train: [58/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2919 (0.3052)	loss 1.5484 (1.3462)	grad_norm 21.0333 (29.9968)	mem 4879MB
[2022-05-31 06:34:14 MetaFG_0] (main.py 265): INFO Train: [58/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2918 (0.3051)	loss 1.2640 (1.3468)	grad_norm 27.9857 (30.0442)	mem 4879MB
[2022-05-31 06:34:17 MetaFG_0] (main.py 265): INFO Train: [58/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2936 (0.3051)	loss 1.3852 (1.3464)	grad_norm 33.7187 (30.0457)	mem 4879MB
[2022-05-31 06:34:20 MetaFG_0] (main.py 265): INFO Train: [58/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2933 (0.3051)	loss 1.4915 (1.3461)	grad_norm 29.7228 (30.0646)	mem 4879MB
[2022-05-31 06:34:23 MetaFG_0] (main.py 265): INFO Train: [58/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2924 (0.3051)	loss 1.3218 (1.3459)	grad_norm 27.0803 (30.0857)	mem 4879MB
[2022-05-31 06:34:26 MetaFG_0] (main.py 265): INFO Train: [58/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2935 (0.3051)	loss 1.2560 (1.3456)	grad_norm 47.2984 (30.0904)	mem 4879MB
[2022-05-31 06:34:29 MetaFG_0] (main.py 265): INFO Train: [58/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2949 (0.3051)	loss 1.4610 (1.3467)	grad_norm 45.3541 (30.1116)	mem 4879MB
[2022-05-31 06:34:32 MetaFG_0] (main.py 265): INFO Train: [58/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2996 (0.3050)	loss 1.7596 (1.3471)	grad_norm 33.0598 (30.1018)	mem 4879MB
[2022-05-31 06:34:35 MetaFG_0] (main.py 265): INFO Train: [58/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2981 (0.3050)	loss 1.0874 (1.3477)	grad_norm 23.9086 (30.0887)	mem 4879MB
[2022-05-31 06:34:38 MetaFG_0] (main.py 265): INFO Train: [58/300][890/1562]	eta 0:03:24 lr 0.000006	time 0.2928 (0.3050)	loss 1.3098 (1.3477)	grad_norm 21.8490 (30.0727)	mem 4879MB
[2022-05-31 06:34:41 MetaFG_0] (main.py 265): INFO Train: [58/300][900/1562]	eta 0:03:21 lr 0.000006	time 0.2937 (0.3050)	loss 1.6223 (1.3473)	grad_norm 34.8149 (30.0858)	mem 4879MB
[2022-05-31 06:34:44 MetaFG_0] (main.py 265): INFO Train: [58/300][910/1562]	eta 0:03:18 lr 0.000006	time 0.2932 (0.3050)	loss 0.8332 (1.3462)	grad_norm 23.9851 (30.0543)	mem 4879MB
[2022-05-31 06:34:47 MetaFG_0] (main.py 265): INFO Train: [58/300][920/1562]	eta 0:03:15 lr 0.000006	time 0.2924 (0.3050)	loss 1.7014 (1.3464)	grad_norm 51.3087 (30.1202)	mem 4879MB
[2022-05-31 06:34:50 MetaFG_0] (main.py 265): INFO Train: [58/300][930/1562]	eta 0:03:12 lr 0.000006	time 0.2933 (0.3050)	loss 1.7278 (1.3473)	grad_norm 38.0226 (30.2170)	mem 4879MB
[2022-05-31 06:34:53 MetaFG_0] (main.py 265): INFO Train: [58/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2938 (0.3050)	loss 1.3446 (1.3478)	grad_norm 12.1660 (30.1969)	mem 4879MB
[2022-05-31 06:34:56 MetaFG_0] (main.py 265): INFO Train: [58/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2938 (0.3049)	loss 1.2833 (1.3485)	grad_norm 35.1007 (30.2089)	mem 4879MB
[2022-05-31 06:35:00 MetaFG_0] (main.py 265): INFO Train: [58/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2942 (0.3049)	loss 1.1518 (1.3476)	grad_norm 24.8353 (30.2675)	mem 4879MB
[2022-05-31 06:35:03 MetaFG_0] (main.py 265): INFO Train: [58/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2979 (0.3049)	loss 1.4939 (1.3479)	grad_norm 21.3291 (30.2273)	mem 4879MB
[2022-05-31 06:35:06 MetaFG_0] (main.py 265): INFO Train: [58/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3021 (0.3049)	loss 1.3412 (1.3487)	grad_norm 16.2858 (30.1427)	mem 4879MB
[2022-05-31 06:35:09 MetaFG_0] (main.py 265): INFO Train: [58/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2943 (0.3049)	loss 1.4800 (1.3482)	grad_norm 15.9870 (30.0737)	mem 4879MB
[2022-05-31 06:35:12 MetaFG_0] (main.py 265): INFO Train: [58/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.3004 (0.3049)	loss 1.4584 (1.3490)	grad_norm 28.7723 (30.0696)	mem 4879MB
[2022-05-31 06:35:15 MetaFG_0] (main.py 265): INFO Train: [58/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.3322 (0.3050)	loss 1.3275 (1.3480)	grad_norm 30.1376 (30.0505)	mem 4879MB
[2022-05-31 06:35:18 MetaFG_0] (main.py 265): INFO Train: [58/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2982 (0.3052)	loss 1.5569 (1.3494)	grad_norm 34.9837 (30.0377)	mem 4879MB
[2022-05-31 06:35:21 MetaFG_0] (main.py 265): INFO Train: [58/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2920 (0.3051)	loss 1.6013 (1.3488)	grad_norm 29.5374 (30.0184)	mem 4879MB
[2022-05-31 06:35:24 MetaFG_0] (main.py 265): INFO Train: [58/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2929 (0.3051)	loss 1.3269 (1.3489)	grad_norm 74.3808 (30.0339)	mem 4879MB
[2022-05-31 06:35:27 MetaFG_0] (main.py 265): INFO Train: [58/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2990 (0.3051)	loss 1.5095 (1.3483)	grad_norm 29.5534 (30.0293)	mem 4879MB
[2022-05-31 06:35:30 MetaFG_0] (main.py 265): INFO Train: [58/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2929 (0.3051)	loss 1.1238 (1.3490)	grad_norm 31.3837 (30.0218)	mem 4879MB
[2022-05-31 06:35:33 MetaFG_0] (main.py 265): INFO Train: [58/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2922 (0.3051)	loss 0.9291 (1.3488)	grad_norm 59.8841 (30.0297)	mem 4879MB
[2022-05-31 06:35:36 MetaFG_0] (main.py 265): INFO Train: [58/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3010 (0.3051)	loss 1.3439 (1.3484)	grad_norm 22.4215 (30.0337)	mem 4879MB
[2022-05-31 06:35:39 MetaFG_0] (main.py 265): INFO Train: [58/300][1090/1562]	eta 0:02:23 lr 0.000006	time 0.2921 (0.3051)	loss 1.4841 (1.3491)	grad_norm 30.4910 (29.9881)	mem 4879MB
[2022-05-31 06:35:42 MetaFG_0] (main.py 265): INFO Train: [58/300][1100/1562]	eta 0:02:20 lr 0.000006	time 0.2924 (0.3051)	loss 1.2641 (1.3482)	grad_norm 33.5844 (29.9973)	mem 4879MB
[2022-05-31 06:35:45 MetaFG_0] (main.py 265): INFO Train: [58/300][1110/1562]	eta 0:02:17 lr 0.000006	time 0.2983 (0.3050)	loss 0.9675 (1.3481)	grad_norm 34.3890 (30.0124)	mem 4879MB
[2022-05-31 06:35:48 MetaFG_0] (main.py 265): INFO Train: [58/300][1120/1562]	eta 0:02:14 lr 0.000006	time 0.2918 (0.3050)	loss 1.2203 (1.3482)	grad_norm 30.2735 (30.0400)	mem 4879MB
[2022-05-31 06:35:51 MetaFG_0] (main.py 265): INFO Train: [58/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2984 (0.3050)	loss 1.4243 (1.3473)	grad_norm 29.6170 (30.0780)	mem 4879MB
[2022-05-31 06:35:54 MetaFG_0] (main.py 265): INFO Train: [58/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2950 (0.3050)	loss 1.1519 (1.3471)	grad_norm 56.5994 (30.1006)	mem 4879MB
[2022-05-31 06:35:58 MetaFG_0] (main.py 265): INFO Train: [58/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2930 (0.3050)	loss 1.3772 (1.3458)	grad_norm 18.5817 (30.1415)	mem 4879MB
[2022-05-31 06:36:01 MetaFG_0] (main.py 265): INFO Train: [58/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2929 (0.3050)	loss 1.2006 (1.3463)	grad_norm 47.4875 (30.1450)	mem 4879MB
[2022-05-31 06:36:04 MetaFG_0] (main.py 265): INFO Train: [58/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2985 (0.3050)	loss 1.2188 (1.3466)	grad_norm 39.3943 (30.1324)	mem 4879MB
[2022-05-31 06:36:07 MetaFG_0] (main.py 265): INFO Train: [58/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2994 (0.3050)	loss 1.3814 (1.3460)	grad_norm 28.7405 (30.1564)	mem 4879MB
[2022-05-31 06:36:10 MetaFG_0] (main.py 265): INFO Train: [58/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2998 (0.3050)	loss 1.6732 (1.3464)	grad_norm 22.2845 (30.1295)	mem 4879MB
[2022-05-31 06:36:13 MetaFG_0] (main.py 265): INFO Train: [58/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2992 (0.3050)	loss 1.6445 (1.3460)	grad_norm 15.8993 (30.0970)	mem 4879MB
[2022-05-31 06:36:16 MetaFG_0] (main.py 265): INFO Train: [58/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2981 (0.3049)	loss 1.0489 (1.3447)	grad_norm 43.3832 (30.0692)	mem 4879MB
[2022-05-31 06:36:19 MetaFG_0] (main.py 265): INFO Train: [58/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2926 (0.3049)	loss 1.5447 (1.3451)	grad_norm 21.6788 (30.0480)	mem 4879MB
[2022-05-31 06:36:22 MetaFG_0] (main.py 265): INFO Train: [58/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2926 (0.3050)	loss 1.2198 (1.3443)	grad_norm 25.9410 (30.0287)	mem 4879MB
[2022-05-31 06:36:25 MetaFG_0] (main.py 265): INFO Train: [58/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2930 (0.3049)	loss 1.6332 (1.3443)	grad_norm 29.6454 (30.0134)	mem 4879MB
[2022-05-31 06:36:28 MetaFG_0] (main.py 265): INFO Train: [58/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3009 (0.3049)	loss 1.5822 (1.3441)	grad_norm 27.0841 (30.0189)	mem 4879MB
[2022-05-31 06:36:31 MetaFG_0] (main.py 265): INFO Train: [58/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2936 (0.3049)	loss 1.4898 (1.3434)	grad_norm 33.2100 (30.0427)	mem 4879MB
[2022-05-31 06:36:34 MetaFG_0] (main.py 265): INFO Train: [58/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2923 (0.3049)	loss 1.1979 (1.3426)	grad_norm 26.6229 (30.0065)	mem 4879MB
[2022-05-31 06:36:37 MetaFG_0] (main.py 265): INFO Train: [58/300][1280/1562]	eta 0:01:25 lr 0.000006	time 0.2926 (0.3049)	loss 0.9524 (1.3420)	grad_norm 26.4791 (30.0762)	mem 4879MB
[2022-05-31 06:36:40 MetaFG_0] (main.py 265): INFO Train: [58/300][1290/1562]	eta 0:01:22 lr 0.000006	time 0.2935 (0.3049)	loss 1.4103 (1.3419)	grad_norm 31.1052 (30.0769)	mem 4879MB
[2022-05-31 06:36:43 MetaFG_0] (main.py 265): INFO Train: [58/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2926 (0.3049)	loss 1.2272 (1.3415)	grad_norm 21.8326 (30.0868)	mem 4879MB
[2022-05-31 06:36:46 MetaFG_0] (main.py 265): INFO Train: [58/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2929 (0.3049)	loss 1.3541 (1.3416)	grad_norm 29.8602 (inf)	mem 4879MB
[2022-05-31 06:36:49 MetaFG_0] (main.py 265): INFO Train: [58/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2983 (0.3049)	loss 1.3949 (1.3414)	grad_norm 16.9077 (inf)	mem 4879MB
[2022-05-31 06:36:52 MetaFG_0] (main.py 265): INFO Train: [58/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2979 (0.3049)	loss 1.7512 (1.3417)	grad_norm 44.9423 (inf)	mem 4879MB
[2022-05-31 06:36:55 MetaFG_0] (main.py 265): INFO Train: [58/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2930 (0.3049)	loss 1.5491 (1.3419)	grad_norm 29.3738 (inf)	mem 4879MB
[2022-05-31 06:36:58 MetaFG_0] (main.py 265): INFO Train: [58/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2926 (0.3049)	loss 1.6204 (1.3419)	grad_norm 54.5518 (inf)	mem 4879MB
[2022-05-31 06:37:01 MetaFG_0] (main.py 265): INFO Train: [58/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2985 (0.3049)	loss 0.9695 (1.3416)	grad_norm 36.1136 (inf)	mem 4879MB
[2022-05-31 06:37:04 MetaFG_0] (main.py 265): INFO Train: [58/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2932 (0.3048)	loss 1.6653 (1.3418)	grad_norm 30.2142 (inf)	mem 4879MB
[2022-05-31 06:37:07 MetaFG_0] (main.py 265): INFO Train: [58/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2926 (0.3048)	loss 1.6118 (1.3422)	grad_norm 24.8124 (inf)	mem 4879MB
[2022-05-31 06:37:11 MetaFG_0] (main.py 265): INFO Train: [58/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2982 (0.3048)	loss 1.4868 (1.3424)	grad_norm 26.4536 (inf)	mem 4879MB
[2022-05-31 06:37:14 MetaFG_0] (main.py 265): INFO Train: [58/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2999 (0.3049)	loss 1.3589 (1.3420)	grad_norm 20.4956 (inf)	mem 4879MB
[2022-05-31 06:37:17 MetaFG_0] (main.py 265): INFO Train: [58/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2922 (0.3048)	loss 1.4694 (1.3423)	grad_norm 22.6957 (inf)	mem 4879MB
[2022-05-31 06:37:20 MetaFG_0] (main.py 265): INFO Train: [58/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2934 (0.3048)	loss 1.5411 (1.3429)	grad_norm 27.6363 (inf)	mem 4879MB
[2022-05-31 06:37:23 MetaFG_0] (main.py 265): INFO Train: [58/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2928 (0.3048)	loss 1.5481 (1.3417)	grad_norm 38.0200 (inf)	mem 4879MB
[2022-05-31 06:37:26 MetaFG_0] (main.py 265): INFO Train: [58/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2936 (0.3048)	loss 0.8651 (1.3412)	grad_norm 47.7573 (inf)	mem 4879MB
[2022-05-31 06:37:29 MetaFG_0] (main.py 265): INFO Train: [58/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2940 (0.3048)	loss 1.6674 (1.3420)	grad_norm 24.1096 (inf)	mem 4879MB
[2022-05-31 06:37:32 MetaFG_0] (main.py 265): INFO Train: [58/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2932 (0.3048)	loss 1.5208 (1.3414)	grad_norm 35.9081 (inf)	mem 4879MB
[2022-05-31 06:37:35 MetaFG_0] (main.py 265): INFO Train: [58/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2980 (0.3048)	loss 1.3244 (1.3417)	grad_norm 45.8988 (inf)	mem 4879MB
[2022-05-31 06:37:38 MetaFG_0] (main.py 265): INFO Train: [58/300][1480/1562]	eta 0:00:24 lr 0.000006	time 0.2986 (0.3048)	loss 1.5889 (1.3418)	grad_norm 32.2749 (inf)	mem 4879MB
[2022-05-31 06:37:41 MetaFG_0] (main.py 265): INFO Train: [58/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2989 (0.3048)	loss 1.5078 (1.3414)	grad_norm 22.1092 (inf)	mem 4879MB
[2022-05-31 06:37:44 MetaFG_0] (main.py 265): INFO Train: [58/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2983 (0.3048)	loss 1.6039 (1.3410)	grad_norm 36.5730 (inf)	mem 4879MB
[2022-05-31 06:37:47 MetaFG_0] (main.py 265): INFO Train: [58/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2998 (0.3048)	loss 0.8191 (1.3411)	grad_norm 48.8100 (inf)	mem 4879MB
[2022-05-31 06:37:50 MetaFG_0] (main.py 265): INFO Train: [58/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2945 (0.3048)	loss 1.1881 (1.3406)	grad_norm 25.8235 (inf)	mem 4879MB
[2022-05-31 06:37:53 MetaFG_0] (main.py 265): INFO Train: [58/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2973 (0.3048)	loss 1.1896 (1.3399)	grad_norm 48.5528 (inf)	mem 4879MB
[2022-05-31 06:37:56 MetaFG_0] (main.py 265): INFO Train: [58/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2994 (0.3048)	loss 1.4088 (1.3402)	grad_norm 21.7839 (inf)	mem 4879MB
[2022-05-31 06:37:59 MetaFG_0] (main.py 265): INFO Train: [58/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2966 (0.3048)	loss 1.4366 (1.3400)	grad_norm 23.1241 (inf)	mem 4879MB
[2022-05-31 06:38:02 MetaFG_0] (main.py 265): INFO Train: [58/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2912 (0.3048)	loss 1.5369 (1.3405)	grad_norm 36.8576 (inf)	mem 4879MB
[2022-05-31 06:38:03 MetaFG_0] (main.py 272): INFO EPOCH 58 training takes 0:07:56
[2022-05-31 06:38:03 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_58.pth saving......
[2022-05-31 06:38:03 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_58.pth saved !!!
[2022-05-31 06:38:03 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:38:05 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:38:05 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:38:06 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.737 (0.737)	Loss 0.4113 (0.4113)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:38:07 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.156)	Loss 0.6303 (0.6080)	Acc@1 87.500 (86.648)	Acc@5 100.000 (98.580)	Mem 4879MB
[2022-05-31 06:38:08 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.128)	Loss 0.6983 (0.6095)	Acc@1 87.500 (87.500)	Acc@5 93.750 (98.363)	Mem 4879MB
[2022-05-31 06:38:09 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.118)	Loss 0.4979 (0.6078)	Acc@1 90.625 (87.903)	Acc@5 100.000 (98.690)	Mem 4879MB
[2022-05-31 06:38:10 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.112)	Loss 0.6216 (0.6022)	Acc@1 84.375 (87.729)	Acc@5 100.000 (98.933)	Mem 4879MB
[2022-05-31 06:38:11 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.109)	Loss 0.5830 (0.6132)	Acc@1 87.500 (87.377)	Acc@5 100.000 (98.836)	Mem 4879MB
[2022-05-31 06:38:12 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.098 (0.106)	Loss 1.0033 (0.6244)	Acc@1 75.000 (87.039)	Acc@5 96.875 (98.873)	Mem 4879MB
[2022-05-31 06:38:13 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.105)	Loss 0.7442 (0.6435)	Acc@1 81.250 (86.488)	Acc@5 100.000 (98.812)	Mem 4879MB
[2022-05-31 06:38:14 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.095 (0.104)	Loss 0.5939 (0.6413)	Acc@1 87.500 (86.535)	Acc@5 96.875 (98.650)	Mem 4879MB
[2022-05-31 06:38:15 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.103)	Loss 0.5173 (0.6362)	Acc@1 90.625 (86.745)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 06:38:16 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.102)	Loss 0.4563 (0.6292)	Acc@1 90.625 (87.005)	Acc@5 100.000 (98.670)	Mem 4879MB
[2022-05-31 06:38:17 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.101)	Loss 0.8605 (0.6265)	Acc@1 81.250 (87.162)	Acc@5 96.875 (98.705)	Mem 4879MB
[2022-05-31 06:38:17 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.098 (0.101)	Loss 0.4854 (0.6267)	Acc@1 93.750 (87.190)	Acc@5 100.000 (98.631)	Mem 4879MB
[2022-05-31 06:38:18 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.099 (0.100)	Loss 0.5495 (0.6232)	Acc@1 87.500 (87.333)	Acc@5 100.000 (98.712)	Mem 4879MB
[2022-05-31 06:38:19 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.100)	Loss 0.7143 (0.6232)	Acc@1 84.375 (87.345)	Acc@5 100.000 (98.737)	Mem 4879MB
[2022-05-31 06:38:20 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.103 (0.099)	Loss 0.6137 (0.6256)	Acc@1 90.625 (87.376)	Acc@5 100.000 (98.675)	Mem 4879MB
[2022-05-31 06:38:21 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.099)	Loss 0.6918 (0.6269)	Acc@1 84.375 (87.286)	Acc@5 100.000 (98.719)	Mem 4879MB
[2022-05-31 06:38:22 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.099)	Loss 0.7556 (0.6304)	Acc@1 81.250 (87.189)	Acc@5 96.875 (98.702)	Mem 4879MB
[2022-05-31 06:38:23 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.4664 (0.6331)	Acc@1 90.625 (87.017)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 06:38:24 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.098)	Loss 0.4638 (0.6340)	Acc@1 93.750 (87.042)	Acc@5 100.000 (98.691)	Mem 4879MB
[2022-05-31 06:38:25 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.097 (0.098)	Loss 0.5610 (0.6334)	Acc@1 87.500 (87.065)	Acc@5 100.000 (98.710)	Mem 4879MB
[2022-05-31 06:38:26 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.098)	Loss 0.6017 (0.6350)	Acc@1 90.625 (87.056)	Acc@5 96.875 (98.652)	Mem 4879MB
[2022-05-31 06:38:27 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.098)	Loss 0.5582 (0.6318)	Acc@1 90.625 (87.175)	Acc@5 100.000 (98.699)	Mem 4879MB
[2022-05-31 06:38:28 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 0.9081 (0.6335)	Acc@1 71.875 (87.040)	Acc@5 96.875 (98.715)	Mem 4879MB
[2022-05-31 06:38:29 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.7409 (0.6354)	Acc@1 87.500 (86.955)	Acc@5 96.875 (98.716)	Mem 4879MB
[2022-05-31 06:38:30 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.101 (0.097)	Loss 0.7595 (0.6392)	Acc@1 81.250 (86.828)	Acc@5 96.875 (98.718)	Mem 4879MB
[2022-05-31 06:38:31 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.4790 (0.6421)	Acc@1 96.875 (86.806)	Acc@5 96.875 (98.671)	Mem 4879MB
[2022-05-31 06:38:32 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.088 (0.097)	Loss 0.4987 (0.6414)	Acc@1 93.750 (86.820)	Acc@5 100.000 (98.685)	Mem 4879MB
[2022-05-31 06:38:33 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.088 (0.097)	Loss 0.5450 (0.6407)	Acc@1 87.500 (86.855)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 06:38:33 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.100 (0.097)	Loss 0.6183 (0.6420)	Acc@1 90.625 (86.802)	Acc@5 96.875 (98.658)	Mem 4879MB
[2022-05-31 06:38:34 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.097)	Loss 0.7359 (0.6431)	Acc@1 87.500 (86.732)	Acc@5 96.875 (98.681)	Mem 4879MB
[2022-05-31 06:38:35 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.6067 (0.6431)	Acc@1 87.500 (86.706)	Acc@5 100.000 (98.694)	Mem 4879MB
[2022-05-31 06:38:36 MetaFG_0] (main.py 330): INFO  * Acc@1 86.690 Acc@5 98.690
[2022-05-31 06:38:36 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.7%
[2022-05-31 06:38:36 MetaFG_0] (main.py 171): INFO Max accuracy: 86.69%
[2022-05-31 06:38:36 MetaFG_0] (main.py 265): INFO Train: [59/300][0/1562]	eta 0:20:37 lr 0.000006	time 0.7922 (0.7922)	loss 1.5606 (1.5606)	grad_norm 42.3849 (42.3849)	mem 4879MB
[2022-05-31 06:38:40 MetaFG_0] (main.py 265): INFO Train: [59/300][10/1562]	eta 0:09:36 lr 0.000006	time 0.2982 (0.3712)	loss 1.5838 (1.4707)	grad_norm 29.7036 (35.8743)	mem 4879MB
[2022-05-31 06:38:43 MetaFG_0] (main.py 265): INFO Train: [59/300][20/1562]	eta 0:08:42 lr 0.000006	time 0.2936 (0.3389)	loss 1.2573 (1.3934)	grad_norm 24.4674 (30.2242)	mem 4879MB
[2022-05-31 06:38:46 MetaFG_0] (main.py 265): INFO Train: [59/300][30/1562]	eta 0:08:22 lr 0.000006	time 0.3017 (0.3283)	loss 1.0290 (1.3412)	grad_norm 29.9290 (32.1119)	mem 4879MB
[2022-05-31 06:38:49 MetaFG_0] (main.py 265): INFO Train: [59/300][40/1562]	eta 0:08:11 lr 0.000006	time 0.2924 (0.3228)	loss 1.5502 (1.3588)	grad_norm 32.4982 (31.0816)	mem 4879MB
[2022-05-31 06:38:52 MetaFG_0] (main.py 265): INFO Train: [59/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.3007 (0.3195)	loss 1.4373 (1.3650)	grad_norm 45.3955 (32.0469)	mem 4879MB
[2022-05-31 06:38:55 MetaFG_0] (main.py 265): INFO Train: [59/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.3033 (0.3171)	loss 1.5007 (1.3660)	grad_norm 35.4618 (32.0001)	mem 4879MB
[2022-05-31 06:38:58 MetaFG_0] (main.py 265): INFO Train: [59/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2925 (0.3156)	loss 1.1029 (1.3671)	grad_norm 51.9241 (31.9623)	mem 4879MB
[2022-05-31 06:39:01 MetaFG_0] (main.py 265): INFO Train: [59/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.2989 (0.3141)	loss 0.9781 (1.3638)	grad_norm 28.0524 (31.5882)	mem 4879MB
[2022-05-31 06:39:04 MetaFG_0] (main.py 265): INFO Train: [59/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2996 (0.3135)	loss 1.4667 (1.3556)	grad_norm 55.7326 (32.6485)	mem 4879MB
[2022-05-31 06:39:07 MetaFG_0] (main.py 265): INFO Train: [59/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2932 (0.3127)	loss 1.2874 (1.3598)	grad_norm 19.2545 (32.0049)	mem 4879MB
[2022-05-31 06:39:10 MetaFG_0] (main.py 265): INFO Train: [59/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2985 (0.3121)	loss 1.1828 (1.3611)	grad_norm 33.4443 (31.7397)	mem 4879MB
[2022-05-31 06:39:13 MetaFG_0] (main.py 265): INFO Train: [59/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2927 (0.3115)	loss 1.3046 (1.3601)	grad_norm 18.0749 (31.1126)	mem 4879MB
[2022-05-31 06:39:16 MetaFG_0] (main.py 265): INFO Train: [59/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2931 (0.3112)	loss 1.4051 (1.3438)	grad_norm 22.5892 (31.4882)	mem 4879MB
[2022-05-31 06:39:19 MetaFG_0] (main.py 265): INFO Train: [59/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.2925 (0.3108)	loss 1.6370 (1.3411)	grad_norm 28.8363 (31.3332)	mem 4879MB
[2022-05-31 06:39:22 MetaFG_0] (main.py 265): INFO Train: [59/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2995 (0.3106)	loss 1.2860 (1.3429)	grad_norm 18.3045 (31.5661)	mem 4879MB
[2022-05-31 06:39:26 MetaFG_0] (main.py 265): INFO Train: [59/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.3053 (0.3103)	loss 1.6309 (1.3438)	grad_norm 16.9810 (30.9985)	mem 4879MB
[2022-05-31 06:39:29 MetaFG_0] (main.py 265): INFO Train: [59/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2926 (0.3101)	loss 1.5327 (1.3419)	grad_norm 27.5547 (31.0496)	mem 4879MB
[2022-05-31 06:39:32 MetaFG_0] (main.py 265): INFO Train: [59/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2928 (0.3097)	loss 1.4128 (1.3460)	grad_norm 29.8972 (30.8124)	mem 4879MB
[2022-05-31 06:39:35 MetaFG_0] (main.py 265): INFO Train: [59/300][190/1562]	eta 0:07:04 lr 0.000006	time 0.2924 (0.3094)	loss 1.4232 (1.3467)	grad_norm 27.2857 (30.7780)	mem 4879MB
[2022-05-31 06:39:38 MetaFG_0] (main.py 265): INFO Train: [59/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2995 (0.3092)	loss 1.4427 (1.3473)	grad_norm 32.0594 (30.6773)	mem 4879MB
[2022-05-31 06:39:41 MetaFG_0] (main.py 265): INFO Train: [59/300][210/1562]	eta 0:06:57 lr 0.000006	time 0.2928 (0.3090)	loss 1.4466 (1.3468)	grad_norm 30.4827 (30.4336)	mem 4879MB
[2022-05-31 06:39:44 MetaFG_0] (main.py 265): INFO Train: [59/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2982 (0.3090)	loss 1.4109 (1.3408)	grad_norm 22.9140 (30.3223)	mem 4879MB
[2022-05-31 06:39:47 MetaFG_0] (main.py 265): INFO Train: [59/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2983 (0.3088)	loss 1.5151 (1.3388)	grad_norm 18.8404 (30.1799)	mem 4879MB
[2022-05-31 06:39:50 MetaFG_0] (main.py 265): INFO Train: [59/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2928 (0.3087)	loss 1.6442 (1.3414)	grad_norm 28.7215 (30.3061)	mem 4879MB
[2022-05-31 06:39:53 MetaFG_0] (main.py 265): INFO Train: [59/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2923 (0.3085)	loss 1.5924 (1.3419)	grad_norm 29.6083 (30.3022)	mem 4879MB
[2022-05-31 06:39:56 MetaFG_0] (main.py 265): INFO Train: [59/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.3001 (0.3085)	loss 1.2805 (1.3347)	grad_norm 29.7936 (30.3538)	mem 4879MB
[2022-05-31 06:39:59 MetaFG_0] (main.py 265): INFO Train: [59/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.3000 (0.3083)	loss 1.2693 (1.3350)	grad_norm 37.5524 (30.3796)	mem 4879MB
[2022-05-31 06:40:02 MetaFG_0] (main.py 265): INFO Train: [59/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.2934 (0.3082)	loss 0.9747 (1.3333)	grad_norm 27.2009 (30.2389)	mem 4879MB
[2022-05-31 06:40:05 MetaFG_0] (main.py 265): INFO Train: [59/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2992 (0.3081)	loss 0.8936 (1.3333)	grad_norm 33.9900 (30.3914)	mem 4879MB
[2022-05-31 06:40:08 MetaFG_0] (main.py 265): INFO Train: [59/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2940 (0.3079)	loss 1.0553 (1.3364)	grad_norm 20.0143 (30.4377)	mem 4879MB
[2022-05-31 06:40:11 MetaFG_0] (main.py 265): INFO Train: [59/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2997 (0.3078)	loss 1.1737 (1.3378)	grad_norm 31.5660 (30.4039)	mem 4879MB
[2022-05-31 06:40:14 MetaFG_0] (main.py 265): INFO Train: [59/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2965 (0.3077)	loss 1.6631 (1.3368)	grad_norm 64.5597 (30.5215)	mem 4879MB
[2022-05-31 06:40:17 MetaFG_0] (main.py 265): INFO Train: [59/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2991 (0.3076)	loss 1.3614 (1.3374)	grad_norm 30.0668 (30.4844)	mem 4879MB
[2022-05-31 06:40:20 MetaFG_0] (main.py 265): INFO Train: [59/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2989 (0.3075)	loss 1.5081 (1.3328)	grad_norm 34.4892 (30.5042)	mem 4879MB
[2022-05-31 06:40:23 MetaFG_0] (main.py 265): INFO Train: [59/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2949 (0.3075)	loss 1.3566 (1.3343)	grad_norm 22.2661 (30.3806)	mem 4879MB
[2022-05-31 06:40:27 MetaFG_0] (main.py 265): INFO Train: [59/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2983 (0.3074)	loss 1.5509 (1.3366)	grad_norm 32.0498 (30.3957)	mem 4879MB
[2022-05-31 06:40:30 MetaFG_0] (main.py 265): INFO Train: [59/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.3000 (0.3075)	loss 1.4735 (1.3350)	grad_norm 25.3508 (30.3478)	mem 4879MB
[2022-05-31 06:40:33 MetaFG_0] (main.py 265): INFO Train: [59/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2968 (0.3074)	loss 1.3576 (1.3334)	grad_norm 19.7427 (30.3510)	mem 4879MB
[2022-05-31 06:40:36 MetaFG_0] (main.py 265): INFO Train: [59/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2920 (0.3073)	loss 1.6454 (1.3345)	grad_norm 23.3584 (30.2924)	mem 4879MB
[2022-05-31 06:40:39 MetaFG_0] (main.py 265): INFO Train: [59/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2964 (0.3073)	loss 1.5679 (1.3355)	grad_norm 19.2923 (30.3173)	mem 4879MB
[2022-05-31 06:40:42 MetaFG_0] (main.py 265): INFO Train: [59/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2939 (0.3073)	loss 1.6150 (1.3389)	grad_norm 30.2927 (30.2904)	mem 4879MB
[2022-05-31 06:40:45 MetaFG_0] (main.py 265): INFO Train: [59/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2934 (0.3072)	loss 1.3212 (1.3362)	grad_norm 25.7823 (30.3266)	mem 4879MB
[2022-05-31 06:40:48 MetaFG_0] (main.py 265): INFO Train: [59/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2958 (0.3072)	loss 1.5580 (1.3379)	grad_norm 33.1388 (30.3845)	mem 4879MB
[2022-05-31 06:40:51 MetaFG_0] (main.py 265): INFO Train: [59/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.3138 (0.3073)	loss 1.2393 (1.3340)	grad_norm 24.3530 (30.3461)	mem 4879MB
[2022-05-31 06:40:54 MetaFG_0] (main.py 265): INFO Train: [59/300][450/1562]	eta 0:05:42 lr 0.000006	time 0.2989 (0.3078)	loss 1.7773 (1.3324)	grad_norm 42.9135 (30.3093)	mem 4879MB
[2022-05-31 06:40:57 MetaFG_0] (main.py 265): INFO Train: [59/300][460/1562]	eta 0:05:39 lr 0.000006	time 0.2995 (0.3077)	loss 1.3235 (1.3329)	grad_norm 26.9666 (30.3558)	mem 4879MB
[2022-05-31 06:41:00 MetaFG_0] (main.py 265): INFO Train: [59/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2930 (0.3077)	loss 1.1425 (1.3324)	grad_norm 27.7941 (30.2783)	mem 4879MB
[2022-05-31 06:41:03 MetaFG_0] (main.py 265): INFO Train: [59/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2989 (0.3076)	loss 1.2036 (1.3339)	grad_norm 26.9343 (30.1644)	mem 4879MB
[2022-05-31 06:41:07 MetaFG_0] (main.py 265): INFO Train: [59/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2927 (0.3075)	loss 1.4523 (1.3346)	grad_norm 25.1978 (30.1478)	mem 4879MB
[2022-05-31 06:41:10 MetaFG_0] (main.py 265): INFO Train: [59/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.3079 (0.3075)	loss 1.1659 (1.3333)	grad_norm 33.9066 (30.1274)	mem 4879MB
[2022-05-31 06:41:13 MetaFG_0] (main.py 265): INFO Train: [59/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2947 (0.3075)	loss 1.6150 (1.3366)	grad_norm 25.9685 (30.2450)	mem 4879MB
[2022-05-31 06:41:16 MetaFG_0] (main.py 265): INFO Train: [59/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2985 (0.3075)	loss 1.5010 (1.3373)	grad_norm 21.6688 (30.2483)	mem 4879MB
[2022-05-31 06:41:19 MetaFG_0] (main.py 265): INFO Train: [59/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2987 (0.3074)	loss 1.4731 (1.3371)	grad_norm 34.7202 (30.3202)	mem 4879MB
[2022-05-31 06:41:22 MetaFG_0] (main.py 265): INFO Train: [59/300][540/1562]	eta 0:05:14 lr 0.000006	time 0.3056 (0.3074)	loss 1.5314 (1.3380)	grad_norm 49.3595 (30.3104)	mem 4879MB
[2022-05-31 06:41:25 MetaFG_0] (main.py 265): INFO Train: [59/300][550/1562]	eta 0:05:11 lr 0.000006	time 0.3051 (0.3074)	loss 1.2586 (1.3379)	grad_norm 29.4275 (30.3261)	mem 4879MB
[2022-05-31 06:41:28 MetaFG_0] (main.py 265): INFO Train: [59/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2989 (0.3074)	loss 1.6452 (1.3397)	grad_norm 24.6099 (30.2968)	mem 4879MB
[2022-05-31 06:41:31 MetaFG_0] (main.py 265): INFO Train: [59/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2925 (0.3074)	loss 1.6593 (1.3405)	grad_norm 42.1232 (30.3256)	mem 4879MB
[2022-05-31 06:41:34 MetaFG_0] (main.py 265): INFO Train: [59/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2927 (0.3073)	loss 1.3502 (1.3399)	grad_norm 24.4233 (30.3727)	mem 4879MB
[2022-05-31 06:41:37 MetaFG_0] (main.py 265): INFO Train: [59/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.3014 (0.3073)	loss 1.3690 (1.3399)	grad_norm 26.3327 (30.3620)	mem 4879MB
[2022-05-31 06:41:40 MetaFG_0] (main.py 265): INFO Train: [59/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2928 (0.3073)	loss 1.1532 (1.3404)	grad_norm 26.0323 (30.3064)	mem 4879MB
[2022-05-31 06:41:43 MetaFG_0] (main.py 265): INFO Train: [59/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2991 (0.3073)	loss 1.5265 (1.3417)	grad_norm 21.6145 (30.3591)	mem 4879MB
[2022-05-31 06:41:46 MetaFG_0] (main.py 265): INFO Train: [59/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2988 (0.3073)	loss 1.0441 (1.3413)	grad_norm 58.5114 (30.4044)	mem 4879MB
[2022-05-31 06:41:49 MetaFG_0] (main.py 265): INFO Train: [59/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2934 (0.3073)	loss 1.2854 (1.3410)	grad_norm 66.7938 (30.4470)	mem 4879MB
[2022-05-31 06:41:52 MetaFG_0] (main.py 265): INFO Train: [59/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2975 (0.3072)	loss 1.4409 (1.3405)	grad_norm 32.9802 (30.3819)	mem 4879MB
[2022-05-31 06:41:56 MetaFG_0] (main.py 265): INFO Train: [59/300][650/1562]	eta 0:04:40 lr 0.000006	time 0.3059 (0.3072)	loss 1.2934 (1.3399)	grad_norm 14.8229 (30.3060)	mem 4879MB
[2022-05-31 06:41:59 MetaFG_0] (main.py 265): INFO Train: [59/300][660/1562]	eta 0:04:37 lr 0.000006	time 0.2932 (0.3072)	loss 0.9825 (1.3372)	grad_norm 35.3518 (30.2759)	mem 4879MB
[2022-05-31 06:42:02 MetaFG_0] (main.py 265): INFO Train: [59/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2992 (0.3072)	loss 1.4287 (1.3372)	grad_norm 49.6344 (30.3373)	mem 4879MB
[2022-05-31 06:42:05 MetaFG_0] (main.py 265): INFO Train: [59/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2992 (0.3071)	loss 1.7038 (1.3368)	grad_norm 18.6156 (30.3189)	mem 4879MB
[2022-05-31 06:42:08 MetaFG_0] (main.py 265): INFO Train: [59/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2997 (0.3071)	loss 1.1275 (1.3371)	grad_norm 36.3144 (30.4001)	mem 4879MB
[2022-05-31 06:42:11 MetaFG_0] (main.py 265): INFO Train: [59/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2947 (0.3070)	loss 1.3463 (1.3370)	grad_norm 29.7643 (30.4010)	mem 4879MB
[2022-05-31 06:42:14 MetaFG_0] (main.py 265): INFO Train: [59/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2984 (0.3070)	loss 1.3579 (1.3368)	grad_norm 24.7679 (30.5429)	mem 4879MB
[2022-05-31 06:42:17 MetaFG_0] (main.py 265): INFO Train: [59/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2983 (0.3070)	loss 1.3738 (1.3351)	grad_norm 27.7855 (30.5863)	mem 4879MB
[2022-05-31 06:42:20 MetaFG_0] (main.py 265): INFO Train: [59/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2929 (0.3069)	loss 1.2656 (1.3351)	grad_norm 39.7904 (30.6299)	mem 4879MB
[2022-05-31 06:42:23 MetaFG_0] (main.py 265): INFO Train: [59/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.3054 (0.3069)	loss 1.4575 (1.3357)	grad_norm 42.2839 (30.6573)	mem 4879MB
[2022-05-31 06:42:26 MetaFG_0] (main.py 265): INFO Train: [59/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2952 (0.3069)	loss 1.2283 (1.3348)	grad_norm 21.5845 (30.6903)	mem 4879MB
[2022-05-31 06:42:29 MetaFG_0] (main.py 265): INFO Train: [59/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.3011 (0.3068)	loss 1.7571 (1.3360)	grad_norm 22.9002 (30.7273)	mem 4879MB
[2022-05-31 06:42:32 MetaFG_0] (main.py 265): INFO Train: [59/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.2935 (0.3068)	loss 1.5144 (1.3355)	grad_norm 27.2916 (30.7025)	mem 4879MB
[2022-05-31 06:42:35 MetaFG_0] (main.py 265): INFO Train: [59/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.3000 (0.3068)	loss 1.4686 (1.3359)	grad_norm 32.6678 (30.7398)	mem 4879MB
[2022-05-31 06:42:38 MetaFG_0] (main.py 265): INFO Train: [59/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2930 (0.3068)	loss 1.6293 (1.3379)	grad_norm 29.8138 (30.7974)	mem 4879MB
[2022-05-31 06:42:41 MetaFG_0] (main.py 265): INFO Train: [59/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2951 (0.3068)	loss 1.3815 (1.3390)	grad_norm 32.4694 (30.8075)	mem 4879MB
[2022-05-31 06:42:44 MetaFG_0] (main.py 265): INFO Train: [59/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2927 (0.3068)	loss 1.0317 (1.3404)	grad_norm 41.4356 (30.8065)	mem 4879MB
[2022-05-31 06:42:47 MetaFG_0] (main.py 265): INFO Train: [59/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2995 (0.3068)	loss 1.5558 (1.3409)	grad_norm 27.0184 (30.7553)	mem 4879MB
[2022-05-31 06:42:50 MetaFG_0] (main.py 265): INFO Train: [59/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2997 (0.3067)	loss 1.1836 (1.3402)	grad_norm 27.0654 (30.7088)	mem 4879MB
[2022-05-31 06:42:53 MetaFG_0] (main.py 265): INFO Train: [59/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2935 (0.3067)	loss 1.0331 (1.3401)	grad_norm 24.2285 (30.7209)	mem 4879MB
[2022-05-31 06:42:57 MetaFG_0] (main.py 265): INFO Train: [59/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2929 (0.3067)	loss 1.4956 (1.3416)	grad_norm 25.1265 (30.7590)	mem 4879MB
[2022-05-31 06:43:00 MetaFG_0] (main.py 265): INFO Train: [59/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.3040 (0.3066)	loss 1.6012 (1.3432)	grad_norm 26.4508 (30.6896)	mem 4879MB
[2022-05-31 06:43:03 MetaFG_0] (main.py 265): INFO Train: [59/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3000 (0.3066)	loss 1.4848 (1.3437)	grad_norm 38.8436 (30.6746)	mem 4879MB
[2022-05-31 06:43:06 MetaFG_0] (main.py 265): INFO Train: [59/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.3069 (0.3066)	loss 1.3832 (1.3429)	grad_norm 30.4363 (30.6649)	mem 4879MB
[2022-05-31 06:43:09 MetaFG_0] (main.py 265): INFO Train: [59/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.2983 (0.3066)	loss 1.7535 (1.3437)	grad_norm 33.9095 (30.6741)	mem 4879MB
[2022-05-31 06:43:12 MetaFG_0] (main.py 265): INFO Train: [59/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2978 (0.3066)	loss 1.5192 (1.3432)	grad_norm 20.9422 (30.6408)	mem 4879MB
[2022-05-31 06:43:15 MetaFG_0] (main.py 265): INFO Train: [59/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2936 (0.3066)	loss 1.4585 (1.3418)	grad_norm 18.7255 (30.6743)	mem 4879MB
[2022-05-31 06:43:18 MetaFG_0] (main.py 265): INFO Train: [59/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2924 (0.3066)	loss 1.3573 (1.3415)	grad_norm 29.9791 (30.6901)	mem 4879MB
[2022-05-31 06:43:21 MetaFG_0] (main.py 265): INFO Train: [59/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2933 (0.3065)	loss 1.6157 (1.3415)	grad_norm 22.4342 (30.6610)	mem 4879MB
[2022-05-31 06:43:24 MetaFG_0] (main.py 265): INFO Train: [59/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3021 (0.3065)	loss 1.5916 (1.3425)	grad_norm 24.2464 (30.6573)	mem 4879MB
[2022-05-31 06:43:27 MetaFG_0] (main.py 265): INFO Train: [59/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2993 (0.3065)	loss 1.5864 (1.3433)	grad_norm 40.6031 (30.6270)	mem 4879MB
[2022-05-31 06:43:30 MetaFG_0] (main.py 265): INFO Train: [59/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2998 (0.3065)	loss 1.1512 (1.3438)	grad_norm 37.1513 (30.6502)	mem 4879MB
[2022-05-31 06:43:33 MetaFG_0] (main.py 265): INFO Train: [59/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2934 (0.3065)	loss 1.4462 (1.3445)	grad_norm 24.9303 (30.5856)	mem 4879MB
[2022-05-31 06:43:36 MetaFG_0] (main.py 265): INFO Train: [59/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2994 (0.3065)	loss 1.3780 (1.3451)	grad_norm 30.4859 (30.5679)	mem 4879MB
[2022-05-31 06:43:39 MetaFG_0] (main.py 265): INFO Train: [59/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2925 (0.3065)	loss 1.1611 (1.3453)	grad_norm 27.1343 (30.5371)	mem 4879MB
[2022-05-31 06:43:42 MetaFG_0] (main.py 265): INFO Train: [59/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2984 (0.3065)	loss 1.5415 (1.3457)	grad_norm 19.5587 (30.4844)	mem 4879MB
[2022-05-31 06:43:45 MetaFG_0] (main.py 265): INFO Train: [59/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2996 (0.3065)	loss 1.5144 (1.3457)	grad_norm 22.5127 (30.4463)	mem 4879MB
[2022-05-31 06:43:48 MetaFG_0] (main.py 265): INFO Train: [59/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2932 (0.3064)	loss 1.2694 (1.3448)	grad_norm 30.7597 (30.4407)	mem 4879MB
[2022-05-31 06:43:51 MetaFG_0] (main.py 265): INFO Train: [59/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2928 (0.3064)	loss 0.9924 (1.3440)	grad_norm 37.3089 (30.4879)	mem 4879MB
[2022-05-31 06:43:55 MetaFG_0] (main.py 265): INFO Train: [59/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2993 (0.3064)	loss 0.9556 (1.3430)	grad_norm 31.6075 (30.4510)	mem 4879MB
[2022-05-31 06:43:58 MetaFG_0] (main.py 265): INFO Train: [59/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2938 (0.3064)	loss 1.0983 (1.3426)	grad_norm 48.7746 (30.4512)	mem 4879MB
[2022-05-31 06:44:01 MetaFG_0] (main.py 265): INFO Train: [59/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2935 (0.3064)	loss 1.0513 (1.3426)	grad_norm 40.8471 (30.4601)	mem 4879MB
[2022-05-31 06:44:04 MetaFG_0] (main.py 265): INFO Train: [59/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2945 (0.3064)	loss 1.6327 (1.3417)	grad_norm 16.2098 (30.4693)	mem 4879MB
[2022-05-31 06:44:07 MetaFG_0] (main.py 265): INFO Train: [59/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2990 (0.3064)	loss 1.4045 (1.3413)	grad_norm 37.8915 (30.4827)	mem 4879MB
[2022-05-31 06:44:10 MetaFG_0] (main.py 265): INFO Train: [59/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2988 (0.3063)	loss 1.4689 (1.3409)	grad_norm 33.7067 (30.5882)	mem 4879MB
[2022-05-31 06:44:13 MetaFG_0] (main.py 265): INFO Train: [59/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.3002 (0.3063)	loss 1.4868 (1.3416)	grad_norm 20.8425 (30.6266)	mem 4879MB
[2022-05-31 06:44:16 MetaFG_0] (main.py 265): INFO Train: [59/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2998 (0.3063)	loss 1.3553 (1.3421)	grad_norm 40.2536 (30.6121)	mem 4879MB
[2022-05-31 06:44:19 MetaFG_0] (main.py 265): INFO Train: [59/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2932 (0.3063)	loss 1.0232 (1.3416)	grad_norm 27.3537 (30.5774)	mem 4879MB
[2022-05-31 06:44:22 MetaFG_0] (main.py 265): INFO Train: [59/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2986 (0.3063)	loss 1.5674 (1.3427)	grad_norm 31.8114 (30.5698)	mem 4879MB
[2022-05-31 06:44:25 MetaFG_0] (main.py 265): INFO Train: [59/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2934 (0.3063)	loss 1.0335 (1.3413)	grad_norm 34.9692 (30.5980)	mem 4879MB
[2022-05-31 06:44:28 MetaFG_0] (main.py 265): INFO Train: [59/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2940 (0.3063)	loss 1.1381 (1.3410)	grad_norm 25.8858 (30.6120)	mem 4879MB
[2022-05-31 06:44:31 MetaFG_0] (main.py 265): INFO Train: [59/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2998 (0.3063)	loss 1.0561 (1.3410)	grad_norm 40.8855 (30.6445)	mem 4879MB
[2022-05-31 06:44:34 MetaFG_0] (main.py 265): INFO Train: [59/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.3005 (0.3063)	loss 1.5776 (1.3413)	grad_norm 22.3963 (30.6263)	mem 4879MB
[2022-05-31 06:44:37 MetaFG_0] (main.py 265): INFO Train: [59/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.2949 (0.3063)	loss 1.5341 (1.3419)	grad_norm 27.6752 (30.5917)	mem 4879MB
[2022-05-31 06:44:40 MetaFG_0] (main.py 265): INFO Train: [59/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2939 (0.3063)	loss 1.2630 (1.3416)	grad_norm 27.7799 (30.5423)	mem 4879MB
[2022-05-31 06:44:43 MetaFG_0] (main.py 265): INFO Train: [59/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2923 (0.3063)	loss 1.3222 (1.3423)	grad_norm 17.3526 (30.5338)	mem 4879MB
[2022-05-31 06:44:46 MetaFG_0] (main.py 265): INFO Train: [59/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2989 (0.3062)	loss 1.6151 (1.3421)	grad_norm 49.6381 (30.5380)	mem 4879MB
[2022-05-31 06:44:49 MetaFG_0] (main.py 265): INFO Train: [59/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2980 (0.3062)	loss 1.1955 (1.3417)	grad_norm 19.8576 (nan)	mem 4879MB
[2022-05-31 06:44:53 MetaFG_0] (main.py 265): INFO Train: [59/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2949 (0.3062)	loss 0.9998 (1.3418)	grad_norm 18.0320 (nan)	mem 4879MB
[2022-05-31 06:44:56 MetaFG_0] (main.py 265): INFO Train: [59/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2992 (0.3062)	loss 1.2296 (1.3410)	grad_norm 23.7512 (nan)	mem 4879MB
[2022-05-31 06:44:59 MetaFG_0] (main.py 265): INFO Train: [59/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2928 (0.3062)	loss 1.0944 (1.3403)	grad_norm 46.9098 (nan)	mem 4879MB
[2022-05-31 06:45:02 MetaFG_0] (main.py 265): INFO Train: [59/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3019 (0.3062)	loss 1.0962 (1.3400)	grad_norm 31.9611 (nan)	mem 4879MB
[2022-05-31 06:45:05 MetaFG_0] (main.py 265): INFO Train: [59/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2934 (0.3062)	loss 0.9902 (1.3399)	grad_norm 24.8482 (nan)	mem 4879MB
[2022-05-31 06:45:08 MetaFG_0] (main.py 265): INFO Train: [59/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2934 (0.3062)	loss 1.4544 (1.3410)	grad_norm 15.9505 (nan)	mem 4879MB
[2022-05-31 06:45:11 MetaFG_0] (main.py 265): INFO Train: [59/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.3008 (0.3062)	loss 1.7493 (1.3414)	grad_norm 40.7380 (nan)	mem 4879MB
[2022-05-31 06:45:14 MetaFG_0] (main.py 265): INFO Train: [59/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2943 (0.3062)	loss 1.5765 (1.3424)	grad_norm 25.9442 (nan)	mem 4879MB
[2022-05-31 06:45:17 MetaFG_0] (main.py 265): INFO Train: [59/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2949 (0.3062)	loss 1.2738 (1.3421)	grad_norm 22.3823 (nan)	mem 4879MB
[2022-05-31 06:45:20 MetaFG_0] (main.py 265): INFO Train: [59/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2937 (0.3062)	loss 1.5396 (1.3426)	grad_norm 27.8794 (nan)	mem 4879MB
[2022-05-31 06:45:23 MetaFG_0] (main.py 265): INFO Train: [59/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2922 (0.3061)	loss 1.5944 (1.3427)	grad_norm 25.5815 (nan)	mem 4879MB
[2022-05-31 06:45:26 MetaFG_0] (main.py 265): INFO Train: [59/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2920 (0.3061)	loss 1.3102 (1.3429)	grad_norm 34.5922 (nan)	mem 4879MB
[2022-05-31 06:45:29 MetaFG_0] (main.py 265): INFO Train: [59/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2930 (0.3061)	loss 1.0327 (1.3430)	grad_norm 20.3264 (nan)	mem 4879MB
[2022-05-31 06:45:32 MetaFG_0] (main.py 265): INFO Train: [59/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2937 (0.3061)	loss 1.5739 (1.3432)	grad_norm 31.2734 (nan)	mem 4879MB
[2022-05-31 06:45:35 MetaFG_0] (main.py 265): INFO Train: [59/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2996 (0.3061)	loss 1.1981 (1.3440)	grad_norm 48.9310 (nan)	mem 4879MB
[2022-05-31 06:45:38 MetaFG_0] (main.py 265): INFO Train: [59/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2931 (0.3061)	loss 1.6538 (1.3442)	grad_norm 33.8544 (nan)	mem 4879MB
[2022-05-31 06:45:41 MetaFG_0] (main.py 265): INFO Train: [59/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2956 (0.3061)	loss 1.5274 (1.3439)	grad_norm 77.6537 (nan)	mem 4879MB
[2022-05-31 06:45:44 MetaFG_0] (main.py 265): INFO Train: [59/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2986 (0.3061)	loss 1.1466 (1.3441)	grad_norm 18.7193 (nan)	mem 4879MB
[2022-05-31 06:45:47 MetaFG_0] (main.py 265): INFO Train: [59/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3007 (0.3061)	loss 1.3915 (1.3440)	grad_norm 28.0503 (nan)	mem 4879MB
[2022-05-31 06:45:51 MetaFG_0] (main.py 265): INFO Train: [59/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2981 (0.3061)	loss 1.3123 (1.3440)	grad_norm 57.4395 (nan)	mem 4879MB
[2022-05-31 06:45:54 MetaFG_0] (main.py 265): INFO Train: [59/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2929 (0.3061)	loss 1.1892 (1.3434)	grad_norm 27.2662 (nan)	mem 4879MB
[2022-05-31 06:45:57 MetaFG_0] (main.py 265): INFO Train: [59/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2984 (0.3061)	loss 0.8100 (1.3429)	grad_norm 28.3937 (nan)	mem 4879MB
[2022-05-31 06:46:00 MetaFG_0] (main.py 265): INFO Train: [59/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2938 (0.3061)	loss 1.6282 (1.3431)	grad_norm 23.6046 (nan)	mem 4879MB
[2022-05-31 06:46:03 MetaFG_0] (main.py 265): INFO Train: [59/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2980 (0.3061)	loss 1.5955 (1.3424)	grad_norm 35.7872 (nan)	mem 4879MB
[2022-05-31 06:46:06 MetaFG_0] (main.py 265): INFO Train: [59/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2931 (0.3061)	loss 1.3983 (1.3425)	grad_norm 32.4356 (nan)	mem 4879MB
[2022-05-31 06:46:09 MetaFG_0] (main.py 265): INFO Train: [59/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2998 (0.3061)	loss 1.1949 (1.3426)	grad_norm 24.3370 (nan)	mem 4879MB
[2022-05-31 06:46:12 MetaFG_0] (main.py 265): INFO Train: [59/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2941 (0.3061)	loss 1.5449 (1.3432)	grad_norm 21.7459 (nan)	mem 4879MB
[2022-05-31 06:46:15 MetaFG_0] (main.py 265): INFO Train: [59/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3000 (0.3061)	loss 1.3442 (1.3436)	grad_norm 20.1495 (nan)	mem 4879MB
[2022-05-31 06:46:18 MetaFG_0] (main.py 265): INFO Train: [59/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2985 (0.3061)	loss 1.3125 (1.3438)	grad_norm 34.2239 (nan)	mem 4879MB
[2022-05-31 06:46:21 MetaFG_0] (main.py 265): INFO Train: [59/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2989 (0.3061)	loss 1.6713 (1.3435)	grad_norm 34.8417 (nan)	mem 4879MB
[2022-05-31 06:46:24 MetaFG_0] (main.py 265): INFO Train: [59/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2942 (0.3061)	loss 1.3357 (1.3437)	grad_norm 39.0310 (nan)	mem 4879MB
[2022-05-31 06:46:27 MetaFG_0] (main.py 265): INFO Train: [59/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2925 (0.3061)	loss 1.1302 (1.3435)	grad_norm 37.1490 (nan)	mem 4879MB
[2022-05-31 06:46:30 MetaFG_0] (main.py 265): INFO Train: [59/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2990 (0.3061)	loss 1.5964 (1.3443)	grad_norm 21.9144 (nan)	mem 4879MB
[2022-05-31 06:46:33 MetaFG_0] (main.py 265): INFO Train: [59/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2932 (0.3061)	loss 1.1108 (1.3440)	grad_norm 37.0356 (nan)	mem 4879MB
[2022-05-31 06:46:34 MetaFG_0] (main.py 272): INFO EPOCH 59 training takes 0:07:58
[2022-05-31 06:46:34 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_59.pth saving......
[2022-05-31 06:46:35 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_59.pth saved !!!
[2022-05-31 06:46:35 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:46:36 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:46:36 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:46:37 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.737 (0.737)	Loss 0.5011 (0.5011)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:46:38 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.156)	Loss 0.6678 (0.6110)	Acc@1 84.375 (87.216)	Acc@5 100.000 (99.432)	Mem 4879MB
[2022-05-31 06:46:39 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.127)	Loss 0.6765 (0.6257)	Acc@1 87.500 (87.202)	Acc@5 96.875 (98.958)	Mem 4879MB
[2022-05-31 06:46:40 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.103 (0.116)	Loss 0.7221 (0.6427)	Acc@1 84.375 (86.593)	Acc@5 100.000 (98.589)	Mem 4879MB
[2022-05-31 06:46:41 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.106 (0.110)	Loss 0.5373 (0.6369)	Acc@1 90.625 (87.043)	Acc@5 100.000 (98.780)	Mem 4879MB
[2022-05-31 06:46:42 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.107)	Loss 0.6884 (0.6312)	Acc@1 84.375 (87.010)	Acc@5 100.000 (98.775)	Mem 4879MB
[2022-05-31 06:46:42 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.105)	Loss 0.7737 (0.6358)	Acc@1 84.375 (86.834)	Acc@5 96.875 (98.822)	Mem 4879MB
[2022-05-31 06:46:43 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.097 (0.104)	Loss 0.4753 (0.6412)	Acc@1 93.750 (86.620)	Acc@5 96.875 (98.680)	Mem 4879MB
[2022-05-31 06:46:44 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.102)	Loss 0.6055 (0.6439)	Acc@1 84.375 (86.690)	Acc@5 96.875 (98.573)	Mem 4879MB
[2022-05-31 06:46:45 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.102)	Loss 0.5616 (0.6448)	Acc@1 90.625 (86.573)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 06:46:46 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.101)	Loss 0.6484 (0.6445)	Acc@1 84.375 (86.541)	Acc@5 100.000 (98.484)	Mem 4879MB
[2022-05-31 06:46:47 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.090 (0.100)	Loss 0.5083 (0.6389)	Acc@1 93.750 (86.655)	Acc@5 100.000 (98.536)	Mem 4879MB
[2022-05-31 06:46:48 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.103 (0.100)	Loss 0.7615 (0.6322)	Acc@1 81.250 (86.777)	Acc@5 96.875 (98.631)	Mem 4879MB
[2022-05-31 06:46:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.6661 (0.6390)	Acc@1 84.375 (86.498)	Acc@5 100.000 (98.664)	Mem 4879MB
[2022-05-31 06:46:50 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.099)	Loss 0.4057 (0.6352)	Acc@1 93.750 (86.613)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 06:46:51 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.099)	Loss 1.0002 (0.6354)	Acc@1 71.875 (86.569)	Acc@5 100.000 (98.655)	Mem 4879MB
[2022-05-31 06:46:52 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.093 (0.098)	Loss 0.5088 (0.6374)	Acc@1 90.625 (86.607)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 06:46:53 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.5284 (0.6324)	Acc@1 81.250 (86.714)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 06:46:54 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.128 (0.099)	Loss 0.7235 (0.6302)	Acc@1 81.250 (86.740)	Acc@5 93.750 (98.688)	Mem 4879MB
[2022-05-31 06:46:55 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.145 (0.101)	Loss 0.8062 (0.6290)	Acc@1 78.125 (86.846)	Acc@5 100.000 (98.691)	Mem 4879MB
[2022-05-31 06:46:56 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.103 (0.101)	Loss 0.6777 (0.6332)	Acc@1 84.375 (86.723)	Acc@5 100.000 (98.647)	Mem 4879MB
[2022-05-31 06:46:57 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.101)	Loss 0.5679 (0.6334)	Acc@1 84.375 (86.685)	Acc@5 100.000 (98.667)	Mem 4879MB
[2022-05-31 06:46:58 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.097 (0.100)	Loss 0.6779 (0.6341)	Acc@1 87.500 (86.609)	Acc@5 100.000 (98.657)	Mem 4879MB
[2022-05-31 06:46:59 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.100)	Loss 0.8708 (0.6319)	Acc@1 78.125 (86.688)	Acc@5 96.875 (98.701)	Mem 4879MB
[2022-05-31 06:47:00 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.104 (0.100)	Loss 0.4443 (0.6317)	Acc@1 96.875 (86.735)	Acc@5 100.000 (98.703)	Mem 4879MB
[2022-05-31 06:47:01 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.104 (0.100)	Loss 0.6803 (0.6365)	Acc@1 81.250 (86.529)	Acc@5 100.000 (98.668)	Mem 4879MB
[2022-05-31 06:47:02 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.088 (0.099)	Loss 0.5637 (0.6353)	Acc@1 87.500 (86.614)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 06:47:03 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.099)	Loss 0.8491 (0.6338)	Acc@1 81.250 (86.658)	Acc@5 96.875 (98.697)	Mem 4879MB
[2022-05-31 06:47:04 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.099)	Loss 0.5866 (0.6370)	Acc@1 90.625 (86.532)	Acc@5 100.000 (98.710)	Mem 4879MB
[2022-05-31 06:47:05 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.099)	Loss 0.7999 (0.6363)	Acc@1 84.375 (86.523)	Acc@5 96.875 (98.722)	Mem 4879MB
[2022-05-31 06:47:06 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.103 (0.099)	Loss 0.4471 (0.6342)	Acc@1 93.750 (86.597)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 06:47:07 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.098)	Loss 0.5752 (0.6346)	Acc@1 87.500 (86.626)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 06:47:07 MetaFG_0] (main.py 330): INFO  * Acc@1 86.560 Acc@5 98.740
[2022-05-31 06:47:07 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.6%
[2022-05-31 06:47:07 MetaFG_0] (main.py 171): INFO Max accuracy: 86.69%
[2022-05-31 06:47:08 MetaFG_0] (main.py 265): INFO Train: [60/300][0/1562]	eta 0:28:21 lr 0.000006	time 1.0894 (1.0894)	loss 1.0560 (1.0560)	grad_norm 22.3154 (22.3154)	mem 4879MB
[2022-05-31 06:47:11 MetaFG_0] (main.py 265): INFO Train: [60/300][10/1562]	eta 0:09:51 lr 0.000006	time 0.3002 (0.3809)	loss 1.6661 (1.3676)	grad_norm 48.2331 (31.9168)	mem 4879MB
[2022-05-31 06:47:14 MetaFG_0] (main.py 265): INFO Train: [60/300][20/1562]	eta 0:08:54 lr 0.000006	time 0.2993 (0.3463)	loss 1.6179 (1.3501)	grad_norm 32.6079 (32.8498)	mem 4879MB
[2022-05-31 06:47:17 MetaFG_0] (main.py 265): INFO Train: [60/300][30/1562]	eta 0:08:30 lr 0.000006	time 0.2945 (0.3332)	loss 1.6147 (1.3841)	grad_norm 22.1013 (31.2515)	mem 4879MB
[2022-05-31 06:47:20 MetaFG_0] (main.py 265): INFO Train: [60/300][40/1562]	eta 0:08:17 lr 0.000006	time 0.2989 (0.3266)	loss 0.9846 (1.3277)	grad_norm 44.5097 (30.2051)	mem 4879MB
[2022-05-31 06:47:23 MetaFG_0] (main.py 265): INFO Train: [60/300][50/1562]	eta 0:08:08 lr 0.000006	time 0.3069 (0.3230)	loss 1.2715 (1.3270)	grad_norm 20.8585 (29.6794)	mem 4879MB
[2022-05-31 06:47:26 MetaFG_0] (main.py 265): INFO Train: [60/300][60/1562]	eta 0:08:00 lr 0.000006	time 0.2923 (0.3198)	loss 1.4503 (1.3247)	grad_norm 27.0797 (29.9417)	mem 4879MB
[2022-05-31 06:47:30 MetaFG_0] (main.py 265): INFO Train: [60/300][70/1562]	eta 0:07:54 lr 0.000006	time 0.2993 (0.3180)	loss 1.4320 (1.3452)	grad_norm 17.7627 (30.0085)	mem 4879MB
[2022-05-31 06:47:33 MetaFG_0] (main.py 265): INFO Train: [60/300][80/1562]	eta 0:07:48 lr 0.000006	time 0.2994 (0.3164)	loss 1.4490 (1.3537)	grad_norm 33.8661 (29.7785)	mem 4879MB
[2022-05-31 06:47:36 MetaFG_0] (main.py 265): INFO Train: [60/300][90/1562]	eta 0:07:43 lr 0.000006	time 0.2940 (0.3151)	loss 1.2247 (1.3481)	grad_norm 58.7050 (29.6970)	mem 4879MB
[2022-05-31 06:47:39 MetaFG_0] (main.py 265): INFO Train: [60/300][100/1562]	eta 0:07:39 lr 0.000006	time 0.2997 (0.3140)	loss 1.1971 (1.3528)	grad_norm 34.7835 (29.8739)	mem 4879MB
[2022-05-31 06:47:42 MetaFG_0] (main.py 265): INFO Train: [60/300][110/1562]	eta 0:07:34 lr 0.000006	time 0.2938 (0.3133)	loss 0.9951 (1.3580)	grad_norm 44.1941 (30.0882)	mem 4879MB
[2022-05-31 06:47:45 MetaFG_0] (main.py 265): INFO Train: [60/300][120/1562]	eta 0:07:31 lr 0.000006	time 0.2924 (0.3128)	loss 0.7975 (1.3581)	grad_norm 52.2704 (30.0473)	mem 4879MB
[2022-05-31 06:47:48 MetaFG_0] (main.py 265): INFO Train: [60/300][130/1562]	eta 0:07:27 lr 0.000006	time 0.2932 (0.3122)	loss 1.6094 (1.3595)	grad_norm 22.2043 (29.7437)	mem 4879MB
[2022-05-31 06:47:51 MetaFG_0] (main.py 265): INFO Train: [60/300][140/1562]	eta 0:07:23 lr 0.000006	time 0.2928 (0.3116)	loss 1.4703 (1.3597)	grad_norm 47.9399 (30.0992)	mem 4879MB
[2022-05-31 06:47:54 MetaFG_0] (main.py 265): INFO Train: [60/300][150/1562]	eta 0:07:19 lr 0.000006	time 0.2928 (0.3111)	loss 1.0796 (1.3623)	grad_norm 67.1960 (30.0952)	mem 4879MB
[2022-05-31 06:47:57 MetaFG_0] (main.py 265): INFO Train: [60/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.2979 (0.3106)	loss 0.9022 (1.3573)	grad_norm 23.3084 (29.8310)	mem 4879MB
[2022-05-31 06:48:00 MetaFG_0] (main.py 265): INFO Train: [60/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2976 (0.3103)	loss 1.1556 (1.3615)	grad_norm 37.6227 (30.0404)	mem 4879MB
[2022-05-31 06:48:03 MetaFG_0] (main.py 265): INFO Train: [60/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2997 (0.3100)	loss 1.5124 (1.3505)	grad_norm 45.8761 (30.2734)	mem 4879MB
[2022-05-31 06:48:06 MetaFG_0] (main.py 265): INFO Train: [60/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2928 (0.3099)	loss 1.5043 (1.3489)	grad_norm 38.7474 (30.1068)	mem 4879MB
[2022-05-31 06:48:09 MetaFG_0] (main.py 265): INFO Train: [60/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2934 (0.3097)	loss 1.6264 (1.3486)	grad_norm 21.1536 (29.8848)	mem 4879MB
[2022-05-31 06:48:12 MetaFG_0] (main.py 265): INFO Train: [60/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2950 (0.3095)	loss 1.0234 (1.3515)	grad_norm 30.0038 (29.7779)	mem 4879MB
[2022-05-31 06:48:15 MetaFG_0] (main.py 265): INFO Train: [60/300][220/1562]	eta 0:06:55 lr 0.000006	time 0.3005 (0.3094)	loss 0.8946 (1.3523)	grad_norm 22.9913 (29.8757)	mem 4879MB
[2022-05-31 06:48:18 MetaFG_0] (main.py 265): INFO Train: [60/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2926 (0.3092)	loss 1.4574 (1.3515)	grad_norm 17.4083 (29.8598)	mem 4879MB
[2022-05-31 06:48:21 MetaFG_0] (main.py 265): INFO Train: [60/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2993 (0.3091)	loss 1.5219 (1.3539)	grad_norm 41.9011 (29.8377)	mem 4879MB
[2022-05-31 06:48:24 MetaFG_0] (main.py 265): INFO Train: [60/300][250/1562]	eta 0:06:45 lr 0.000006	time 0.2929 (0.3089)	loss 1.2418 (1.3574)	grad_norm 47.1972 (29.7686)	mem 4879MB
[2022-05-31 06:48:28 MetaFG_0] (main.py 265): INFO Train: [60/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.2938 (0.3089)	loss 1.6119 (1.3560)	grad_norm 39.5957 (29.5959)	mem 4879MB
[2022-05-31 06:48:31 MetaFG_0] (main.py 265): INFO Train: [60/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2991 (0.3087)	loss 1.4604 (1.3575)	grad_norm 17.3791 (29.3956)	mem 4879MB
[2022-05-31 06:48:34 MetaFG_0] (main.py 265): INFO Train: [60/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.3006 (0.3087)	loss 1.4017 (1.3615)	grad_norm 37.8045 (29.5062)	mem 4879MB
[2022-05-31 06:48:37 MetaFG_0] (main.py 265): INFO Train: [60/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2994 (0.3087)	loss 1.4380 (1.3564)	grad_norm 60.5728 (29.5751)	mem 4879MB
[2022-05-31 06:48:40 MetaFG_0] (main.py 265): INFO Train: [60/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.2942 (0.3086)	loss 1.4039 (1.3587)	grad_norm 28.4107 (29.4986)	mem 4879MB
[2022-05-31 06:48:43 MetaFG_0] (main.py 265): INFO Train: [60/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.2934 (0.3085)	loss 1.4652 (1.3576)	grad_norm 56.2297 (29.7222)	mem 4879MB
[2022-05-31 06:48:46 MetaFG_0] (main.py 265): INFO Train: [60/300][320/1562]	eta 0:06:23 lr 0.000006	time 0.3097 (0.3085)	loss 1.0174 (1.3572)	grad_norm 31.8944 (29.7182)	mem 4879MB
[2022-05-31 06:48:49 MetaFG_0] (main.py 265): INFO Train: [60/300][330/1562]	eta 0:06:20 lr 0.000006	time 0.3014 (0.3085)	loss 1.1764 (1.3574)	grad_norm 38.8727 (29.6773)	mem 4879MB
[2022-05-31 06:48:52 MetaFG_0] (main.py 265): INFO Train: [60/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.3005 (0.3084)	loss 1.4271 (1.3582)	grad_norm 55.3681 (29.6785)	mem 4879MB
[2022-05-31 06:48:55 MetaFG_0] (main.py 265): INFO Train: [60/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2922 (0.3083)	loss 1.4435 (1.3567)	grad_norm 19.3442 (29.7035)	mem 4879MB
[2022-05-31 06:48:58 MetaFG_0] (main.py 265): INFO Train: [60/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.3004 (0.3082)	loss 1.3422 (1.3555)	grad_norm 22.4969 (29.5356)	mem 4879MB
[2022-05-31 06:49:01 MetaFG_0] (main.py 265): INFO Train: [60/300][370/1562]	eta 0:06:07 lr 0.000006	time 0.2945 (0.3081)	loss 1.4626 (1.3543)	grad_norm 25.3046 (29.7922)	mem 4879MB
[2022-05-31 06:49:04 MetaFG_0] (main.py 265): INFO Train: [60/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2985 (0.3081)	loss 1.4301 (1.3538)	grad_norm 22.9832 (29.7116)	mem 4879MB
[2022-05-31 06:49:07 MetaFG_0] (main.py 265): INFO Train: [60/300][390/1562]	eta 0:06:01 lr 0.000006	time 0.3063 (0.3080)	loss 1.4837 (1.3578)	grad_norm 24.0331 (29.6789)	mem 4879MB
[2022-05-31 06:49:10 MetaFG_0] (main.py 265): INFO Train: [60/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2939 (0.3080)	loss 1.2756 (1.3591)	grad_norm 53.7012 (29.7762)	mem 4879MB
[2022-05-31 06:49:14 MetaFG_0] (main.py 265): INFO Train: [60/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2988 (0.3080)	loss 1.1683 (1.3576)	grad_norm 27.8160 (29.7760)	mem 4879MB
[2022-05-31 06:49:17 MetaFG_0] (main.py 265): INFO Train: [60/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2927 (0.3079)	loss 1.4198 (1.3565)	grad_norm 39.8248 (29.8783)	mem 4879MB
[2022-05-31 06:49:20 MetaFG_0] (main.py 265): INFO Train: [60/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2946 (0.3078)	loss 1.3367 (1.3551)	grad_norm 35.4579 (29.9002)	mem 4879MB
[2022-05-31 06:49:23 MetaFG_0] (main.py 265): INFO Train: [60/300][440/1562]	eta 0:05:45 lr 0.000006	time 0.2938 (0.3078)	loss 1.5344 (1.3577)	grad_norm 35.1742 (30.0166)	mem 4879MB
[2022-05-31 06:49:26 MetaFG_0] (main.py 265): INFO Train: [60/300][450/1562]	eta 0:05:42 lr 0.000006	time 0.3003 (0.3078)	loss 1.0290 (1.3577)	grad_norm 15.9083 (29.9189)	mem 4879MB
[2022-05-31 06:49:29 MetaFG_0] (main.py 265): INFO Train: [60/300][460/1562]	eta 0:05:39 lr 0.000006	time 0.2935 (0.3077)	loss 1.3446 (1.3590)	grad_norm 43.0436 (29.9849)	mem 4879MB
[2022-05-31 06:49:32 MetaFG_0] (main.py 265): INFO Train: [60/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2935 (0.3077)	loss 1.4248 (1.3599)	grad_norm 22.2520 (29.9217)	mem 4879MB
[2022-05-31 06:49:35 MetaFG_0] (main.py 265): INFO Train: [60/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2955 (0.3076)	loss 1.2380 (1.3575)	grad_norm 23.3959 (29.9922)	mem 4879MB
[2022-05-31 06:49:38 MetaFG_0] (main.py 265): INFO Train: [60/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2978 (0.3076)	loss 1.3121 (1.3572)	grad_norm 42.0738 (29.9195)	mem 4879MB
[2022-05-31 06:49:41 MetaFG_0] (main.py 265): INFO Train: [60/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.2992 (0.3075)	loss 1.6953 (1.3586)	grad_norm 43.3353 (29.9388)	mem 4879MB
[2022-05-31 06:49:44 MetaFG_0] (main.py 265): INFO Train: [60/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2943 (0.3075)	loss 1.4156 (1.3584)	grad_norm 21.4494 (30.0127)	mem 4879MB
[2022-05-31 06:49:47 MetaFG_0] (main.py 265): INFO Train: [60/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2992 (0.3075)	loss 1.3321 (1.3576)	grad_norm 18.3606 (29.9321)	mem 4879MB
[2022-05-31 06:49:50 MetaFG_0] (main.py 265): INFO Train: [60/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2925 (0.3074)	loss 1.4501 (1.3578)	grad_norm 40.3933 (29.9813)	mem 4879MB
[2022-05-31 06:49:53 MetaFG_0] (main.py 265): INFO Train: [60/300][540/1562]	eta 0:05:14 lr 0.000006	time 0.3001 (0.3074)	loss 1.2723 (1.3580)	grad_norm 17.8675 (30.0809)	mem 4879MB
[2022-05-31 06:49:56 MetaFG_0] (main.py 265): INFO Train: [60/300][550/1562]	eta 0:05:11 lr 0.000006	time 0.2931 (0.3073)	loss 1.6269 (1.3599)	grad_norm 35.6300 (30.0823)	mem 4879MB
[2022-05-31 06:49:59 MetaFG_0] (main.py 265): INFO Train: [60/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.3018 (0.3073)	loss 1.3834 (1.3586)	grad_norm 17.9407 (30.0100)	mem 4879MB
[2022-05-31 06:50:02 MetaFG_0] (main.py 265): INFO Train: [60/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.3040 (0.3073)	loss 1.1594 (1.3576)	grad_norm 20.8667 (29.9660)	mem 4879MB
[2022-05-31 06:50:05 MetaFG_0] (main.py 265): INFO Train: [60/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2993 (0.3072)	loss 1.5453 (1.3586)	grad_norm 39.9691 (29.9350)	mem 4879MB
[2022-05-31 06:50:09 MetaFG_0] (main.py 265): INFO Train: [60/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.3002 (0.3072)	loss 1.6296 (1.3575)	grad_norm 30.2552 (30.0259)	mem 4879MB
[2022-05-31 06:50:12 MetaFG_0] (main.py 265): INFO Train: [60/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.3012 (0.3072)	loss 1.3066 (1.3575)	grad_norm 35.9539 (29.9730)	mem 4879MB
[2022-05-31 06:50:15 MetaFG_0] (main.py 265): INFO Train: [60/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2978 (0.3072)	loss 1.3413 (1.3569)	grad_norm 32.5588 (30.0503)	mem 4879MB
[2022-05-31 06:50:18 MetaFG_0] (main.py 265): INFO Train: [60/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2988 (0.3072)	loss 1.0692 (1.3565)	grad_norm 27.2268 (30.0133)	mem 4879MB
[2022-05-31 06:50:21 MetaFG_0] (main.py 265): INFO Train: [60/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2996 (0.3072)	loss 1.3584 (1.3567)	grad_norm 28.7063 (30.1288)	mem 4879MB
[2022-05-31 06:50:24 MetaFG_0] (main.py 265): INFO Train: [60/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2978 (0.3072)	loss 1.8528 (1.3591)	grad_norm 32.2780 (30.1424)	mem 4879MB
[2022-05-31 06:50:27 MetaFG_0] (main.py 265): INFO Train: [60/300][650/1562]	eta 0:04:40 lr 0.000006	time 0.2949 (0.3072)	loss 1.4227 (1.3588)	grad_norm 29.1269 (30.0899)	mem 4879MB
[2022-05-31 06:50:30 MetaFG_0] (main.py 265): INFO Train: [60/300][660/1562]	eta 0:04:37 lr 0.000006	time 0.2937 (0.3071)	loss 1.6404 (1.3584)	grad_norm 23.1889 (30.1170)	mem 4879MB
[2022-05-31 06:50:33 MetaFG_0] (main.py 265): INFO Train: [60/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.3000 (0.3071)	loss 1.4044 (1.3582)	grad_norm 21.4419 (30.1330)	mem 4879MB
[2022-05-31 06:50:36 MetaFG_0] (main.py 265): INFO Train: [60/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2987 (0.3071)	loss 1.2202 (1.3602)	grad_norm 23.7376 (30.0976)	mem 4879MB
[2022-05-31 06:50:39 MetaFG_0] (main.py 265): INFO Train: [60/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2989 (0.3071)	loss 1.5982 (1.3616)	grad_norm 43.8725 (30.1789)	mem 4879MB
[2022-05-31 06:50:42 MetaFG_0] (main.py 265): INFO Train: [60/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2953 (0.3070)	loss 0.8073 (1.3612)	grad_norm 32.9626 (30.2060)	mem 4879MB
[2022-05-31 06:50:45 MetaFG_0] (main.py 265): INFO Train: [60/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2996 (0.3070)	loss 1.5052 (1.3620)	grad_norm 36.3715 (30.2115)	mem 4879MB
[2022-05-31 06:50:48 MetaFG_0] (main.py 265): INFO Train: [60/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.3002 (0.3070)	loss 1.2201 (1.3627)	grad_norm 35.7257 (30.2234)	mem 4879MB
[2022-05-31 06:50:51 MetaFG_0] (main.py 265): INFO Train: [60/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.3016 (0.3070)	loss 1.4426 (1.3627)	grad_norm 18.5606 (30.1832)	mem 4879MB
[2022-05-31 06:50:54 MetaFG_0] (main.py 265): INFO Train: [60/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.3004 (0.3070)	loss 1.3603 (1.3627)	grad_norm 31.6485 (30.1329)	mem 4879MB
[2022-05-31 06:50:58 MetaFG_0] (main.py 265): INFO Train: [60/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2980 (0.3070)	loss 1.5951 (1.3630)	grad_norm 36.0640 (30.0715)	mem 4879MB
[2022-05-31 06:51:01 MetaFG_0] (main.py 265): INFO Train: [60/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.2953 (0.3070)	loss 1.9263 (1.3634)	grad_norm 52.3248 (30.0966)	mem 4879MB
[2022-05-31 06:51:04 MetaFG_0] (main.py 265): INFO Train: [60/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.2924 (0.3069)	loss 1.4844 (1.3639)	grad_norm 26.3912 (30.0832)	mem 4879MB
[2022-05-31 06:51:07 MetaFG_0] (main.py 265): INFO Train: [60/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2942 (0.3069)	loss 1.1588 (1.3629)	grad_norm 20.8204 (30.0578)	mem 4879MB
[2022-05-31 06:51:10 MetaFG_0] (main.py 265): INFO Train: [60/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2927 (0.3069)	loss 1.5940 (1.3638)	grad_norm 33.8890 (30.0258)	mem 4879MB
[2022-05-31 06:51:13 MetaFG_0] (main.py 265): INFO Train: [60/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2986 (0.3069)	loss 1.4266 (1.3643)	grad_norm 20.7647 (30.0644)	mem 4879MB
[2022-05-31 06:51:16 MetaFG_0] (main.py 265): INFO Train: [60/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.3002 (0.3069)	loss 1.3656 (1.3629)	grad_norm 20.5689 (30.0571)	mem 4879MB
[2022-05-31 06:51:19 MetaFG_0] (main.py 265): INFO Train: [60/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.3015 (0.3068)	loss 1.8066 (1.3618)	grad_norm 20.3057 (30.0685)	mem 4879MB
[2022-05-31 06:51:22 MetaFG_0] (main.py 265): INFO Train: [60/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2987 (0.3068)	loss 1.4757 (1.3612)	grad_norm 16.2228 (30.0976)	mem 4879MB
[2022-05-31 06:51:25 MetaFG_0] (main.py 265): INFO Train: [60/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2929 (0.3068)	loss 1.0905 (1.3608)	grad_norm 57.6047 (30.0987)	mem 4879MB
[2022-05-31 06:51:28 MetaFG_0] (main.py 265): INFO Train: [60/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2927 (0.3068)	loss 1.3688 (1.3596)	grad_norm 16.8014 (30.1175)	mem 4879MB
[2022-05-31 06:51:31 MetaFG_0] (main.py 265): INFO Train: [60/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.3008 (0.3067)	loss 1.0635 (1.3584)	grad_norm 24.2654 (30.0423)	mem 4879MB
[2022-05-31 06:51:34 MetaFG_0] (main.py 265): INFO Train: [60/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3014 (0.3067)	loss 1.3114 (1.3588)	grad_norm 27.1087 (30.0069)	mem 4879MB
[2022-05-31 06:51:37 MetaFG_0] (main.py 265): INFO Train: [60/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2985 (0.3067)	loss 1.4429 (1.3589)	grad_norm 33.4330 (29.9593)	mem 4879MB
[2022-05-31 06:51:40 MetaFG_0] (main.py 265): INFO Train: [60/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.2944 (0.3067)	loss 1.4878 (1.3596)	grad_norm 20.3404 (29.9827)	mem 4879MB
[2022-05-31 06:51:43 MetaFG_0] (main.py 265): INFO Train: [60/300][900/1562]	eta 0:03:23 lr 0.000006	time 0.2941 (0.3067)	loss 1.1738 (1.3608)	grad_norm 38.8903 (29.9792)	mem 4879MB
[2022-05-31 06:51:46 MetaFG_0] (main.py 265): INFO Train: [60/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2999 (0.3066)	loss 1.2627 (1.3607)	grad_norm 36.6275 (29.9710)	mem 4879MB
[2022-05-31 06:51:49 MetaFG_0] (main.py 265): INFO Train: [60/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.3004 (0.3066)	loss 1.3611 (1.3601)	grad_norm 25.6742 (29.9963)	mem 4879MB
[2022-05-31 06:51:52 MetaFG_0] (main.py 265): INFO Train: [60/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2992 (0.3066)	loss 1.5490 (1.3605)	grad_norm 33.2110 (29.9415)	mem 4879MB
[2022-05-31 06:51:55 MetaFG_0] (main.py 265): INFO Train: [60/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.3028 (0.3066)	loss 1.4145 (1.3599)	grad_norm 29.3716 (29.9795)	mem 4879MB
[2022-05-31 06:51:59 MetaFG_0] (main.py 265): INFO Train: [60/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2935 (0.3066)	loss 1.2967 (1.3606)	grad_norm 43.2525 (30.0160)	mem 4879MB
[2022-05-31 06:52:02 MetaFG_0] (main.py 265): INFO Train: [60/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2939 (0.3066)	loss 1.0035 (1.3596)	grad_norm 18.0454 (30.0891)	mem 4879MB
[2022-05-31 06:52:05 MetaFG_0] (main.py 265): INFO Train: [60/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2929 (0.3066)	loss 1.4540 (1.3593)	grad_norm 29.6447 (30.0565)	mem 4879MB
[2022-05-31 06:52:08 MetaFG_0] (main.py 265): INFO Train: [60/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2997 (0.3065)	loss 1.6377 (1.3597)	grad_norm 39.5668 (30.0616)	mem 4879MB
[2022-05-31 06:52:11 MetaFG_0] (main.py 265): INFO Train: [60/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2927 (0.3065)	loss 1.4263 (1.3604)	grad_norm 31.8081 (30.0156)	mem 4879MB
[2022-05-31 06:52:14 MetaFG_0] (main.py 265): INFO Train: [60/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.3048 (0.3065)	loss 1.4210 (1.3605)	grad_norm 22.8044 (30.0266)	mem 4879MB
[2022-05-31 06:52:17 MetaFG_0] (main.py 265): INFO Train: [60/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2999 (0.3065)	loss 1.0547 (1.3587)	grad_norm 25.6575 (30.0762)	mem 4879MB
[2022-05-31 06:52:20 MetaFG_0] (main.py 265): INFO Train: [60/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2929 (0.3065)	loss 1.3389 (1.3587)	grad_norm 48.2512 (30.0975)	mem 4879MB
[2022-05-31 06:52:23 MetaFG_0] (main.py 265): INFO Train: [60/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2926 (0.3065)	loss 1.1226 (1.3586)	grad_norm 49.0279 (30.1177)	mem 4879MB
[2022-05-31 06:52:26 MetaFG_0] (main.py 265): INFO Train: [60/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2930 (0.3065)	loss 1.5988 (1.3594)	grad_norm 21.0014 (30.0720)	mem 4879MB
[2022-05-31 06:52:29 MetaFG_0] (main.py 265): INFO Train: [60/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2944 (0.3065)	loss 1.2957 (1.3582)	grad_norm 39.3597 (30.1117)	mem 4879MB
[2022-05-31 06:52:32 MetaFG_0] (main.py 265): INFO Train: [60/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2926 (0.3065)	loss 1.3975 (1.3591)	grad_norm 21.2838 (30.0706)	mem 4879MB
[2022-05-31 06:52:35 MetaFG_0] (main.py 265): INFO Train: [60/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.3215 (0.3066)	loss 1.4279 (1.3594)	grad_norm 33.5998 (30.1361)	mem 4879MB
[2022-05-31 06:52:39 MetaFG_0] (main.py 265): INFO Train: [60/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2982 (0.3067)	loss 1.3168 (1.3589)	grad_norm 21.2624 (30.1346)	mem 4879MB
[2022-05-31 06:52:42 MetaFG_0] (main.py 265): INFO Train: [60/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.3007 (0.3067)	loss 1.5024 (1.3597)	grad_norm 36.6645 (30.1082)	mem 4879MB
[2022-05-31 06:52:45 MetaFG_0] (main.py 265): INFO Train: [60/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.3075 (0.3067)	loss 1.2276 (1.3591)	grad_norm 19.0184 (30.0865)	mem 4879MB
[2022-05-31 06:52:48 MetaFG_0] (main.py 265): INFO Train: [60/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2932 (0.3067)	loss 1.5431 (1.3595)	grad_norm 26.4886 (30.0615)	mem 4879MB
[2022-05-31 06:52:51 MetaFG_0] (main.py 265): INFO Train: [60/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2952 (0.3067)	loss 1.4341 (1.3604)	grad_norm 22.3375 (30.1033)	mem 4879MB
[2022-05-31 06:52:54 MetaFG_0] (main.py 265): INFO Train: [60/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2932 (0.3066)	loss 1.4566 (1.3613)	grad_norm 39.7424 (30.0557)	mem 4879MB
[2022-05-31 06:52:57 MetaFG_0] (main.py 265): INFO Train: [60/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2953 (0.3066)	loss 1.1942 (1.3609)	grad_norm 52.0627 (30.0921)	mem 4879MB
[2022-05-31 06:53:00 MetaFG_0] (main.py 265): INFO Train: [60/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2959 (0.3066)	loss 1.7332 (1.3613)	grad_norm 19.5514 (30.0799)	mem 4879MB
[2022-05-31 06:53:03 MetaFG_0] (main.py 265): INFO Train: [60/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2986 (0.3066)	loss 1.5146 (1.3616)	grad_norm 26.1207 (30.0763)	mem 4879MB
[2022-05-31 06:53:06 MetaFG_0] (main.py 265): INFO Train: [60/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2988 (0.3066)	loss 1.7175 (1.3616)	grad_norm 30.0291 (30.0673)	mem 4879MB
[2022-05-31 06:53:09 MetaFG_0] (main.py 265): INFO Train: [60/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.3008 (0.3066)	loss 1.2422 (1.3613)	grad_norm 33.4638 (30.1070)	mem 4879MB
[2022-05-31 06:53:12 MetaFG_0] (main.py 265): INFO Train: [60/300][1190/1562]	eta 0:01:54 lr 0.000006	time 0.2923 (0.3066)	loss 1.2557 (1.3610)	grad_norm 41.2824 (30.1506)	mem 4879MB
[2022-05-31 06:53:15 MetaFG_0] (main.py 265): INFO Train: [60/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2937 (0.3066)	loss 1.2386 (1.3602)	grad_norm 29.2790 (30.1207)	mem 4879MB
[2022-05-31 06:53:18 MetaFG_0] (main.py 265): INFO Train: [60/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2996 (0.3066)	loss 1.4460 (1.3597)	grad_norm 29.5095 (30.1040)	mem 4879MB
[2022-05-31 06:53:21 MetaFG_0] (main.py 265): INFO Train: [60/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2997 (0.3066)	loss 1.1852 (1.3595)	grad_norm 32.6333 (30.1129)	mem 4879MB
[2022-05-31 06:53:24 MetaFG_0] (main.py 265): INFO Train: [60/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.3000 (0.3066)	loss 1.6969 (1.3590)	grad_norm 25.7772 (30.1891)	mem 4879MB
[2022-05-31 06:53:27 MetaFG_0] (main.py 265): INFO Train: [60/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2934 (0.3066)	loss 1.4369 (1.3594)	grad_norm 18.6161 (30.1673)	mem 4879MB
[2022-05-31 06:53:30 MetaFG_0] (main.py 265): INFO Train: [60/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2982 (0.3065)	loss 1.1321 (1.3576)	grad_norm 37.2735 (30.2023)	mem 4879MB
[2022-05-31 06:53:34 MetaFG_0] (main.py 265): INFO Train: [60/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3003 (0.3065)	loss 1.7493 (1.3578)	grad_norm 40.3424 (30.2360)	mem 4879MB
[2022-05-31 06:53:37 MetaFG_0] (main.py 265): INFO Train: [60/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2941 (0.3065)	loss 1.5828 (1.3570)	grad_norm 39.2975 (30.2529)	mem 4879MB
[2022-05-31 06:53:40 MetaFG_0] (main.py 265): INFO Train: [60/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2937 (0.3065)	loss 1.4054 (1.3573)	grad_norm 66.2803 (30.2843)	mem 4879MB
[2022-05-31 06:53:43 MetaFG_0] (main.py 265): INFO Train: [60/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2960 (0.3065)	loss 1.6949 (1.3583)	grad_norm 30.6824 (30.2963)	mem 4879MB
[2022-05-31 06:53:46 MetaFG_0] (main.py 265): INFO Train: [60/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2986 (0.3065)	loss 1.0984 (1.3572)	grad_norm 33.5718 (30.2869)	mem 4879MB
[2022-05-31 06:53:49 MetaFG_0] (main.py 265): INFO Train: [60/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2928 (0.3065)	loss 1.2818 (1.3567)	grad_norm 59.8036 (30.3260)	mem 4879MB
[2022-05-31 06:53:52 MetaFG_0] (main.py 265): INFO Train: [60/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2994 (0.3065)	loss 1.6017 (1.3570)	grad_norm 16.2580 (30.2978)	mem 4879MB
[2022-05-31 06:53:55 MetaFG_0] (main.py 265): INFO Train: [60/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2918 (0.3065)	loss 1.2168 (1.3574)	grad_norm 51.6541 (30.3661)	mem 4879MB
[2022-05-31 06:53:58 MetaFG_0] (main.py 265): INFO Train: [60/300][1340/1562]	eta 0:01:08 lr 0.000006	time 0.2992 (0.3065)	loss 1.5474 (1.3573)	grad_norm 26.8411 (30.3903)	mem 4879MB
[2022-05-31 06:54:01 MetaFG_0] (main.py 265): INFO Train: [60/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3002 (0.3065)	loss 1.4488 (1.3572)	grad_norm 43.4047 (30.4336)	mem 4879MB
[2022-05-31 06:54:04 MetaFG_0] (main.py 265): INFO Train: [60/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2945 (0.3065)	loss 1.1373 (1.3566)	grad_norm 33.6886 (30.4765)	mem 4879MB
[2022-05-31 06:54:07 MetaFG_0] (main.py 265): INFO Train: [60/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2941 (0.3065)	loss 1.2290 (1.3567)	grad_norm 23.7376 (30.4974)	mem 4879MB
[2022-05-31 06:54:10 MetaFG_0] (main.py 265): INFO Train: [60/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2994 (0.3065)	loss 1.4526 (1.3569)	grad_norm 34.0526 (30.4966)	mem 4879MB
[2022-05-31 06:54:13 MetaFG_0] (main.py 265): INFO Train: [60/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2931 (0.3064)	loss 1.5817 (1.3578)	grad_norm 20.4221 (30.4668)	mem 4879MB
[2022-05-31 06:54:16 MetaFG_0] (main.py 265): INFO Train: [60/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2938 (0.3064)	loss 1.3919 (1.3572)	grad_norm 29.5880 (30.4432)	mem 4879MB
[2022-05-31 06:54:19 MetaFG_0] (main.py 265): INFO Train: [60/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2930 (0.3064)	loss 1.4318 (1.3570)	grad_norm 21.3233 (30.4125)	mem 4879MB
[2022-05-31 06:54:22 MetaFG_0] (main.py 265): INFO Train: [60/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3003 (0.3064)	loss 1.1577 (1.3569)	grad_norm 32.6931 (30.4162)	mem 4879MB
[2022-05-31 06:54:25 MetaFG_0] (main.py 265): INFO Train: [60/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2996 (0.3064)	loss 1.1567 (1.3569)	grad_norm 30.3824 (30.3959)	mem 4879MB
[2022-05-31 06:54:28 MetaFG_0] (main.py 265): INFO Train: [60/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2996 (0.3064)	loss 1.4082 (1.3571)	grad_norm 26.3891 (30.3868)	mem 4879MB
[2022-05-31 06:54:32 MetaFG_0] (main.py 265): INFO Train: [60/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2993 (0.3064)	loss 1.0277 (1.3573)	grad_norm 29.9722 (30.4175)	mem 4879MB
[2022-05-31 06:54:35 MetaFG_0] (main.py 265): INFO Train: [60/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3002 (0.3064)	loss 1.6007 (1.3579)	grad_norm 39.3708 (30.4103)	mem 4879MB
[2022-05-31 06:54:38 MetaFG_0] (main.py 265): INFO Train: [60/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.3016 (0.3064)	loss 1.2260 (1.3584)	grad_norm 39.8605 (30.4131)	mem 4879MB
[2022-05-31 06:54:41 MetaFG_0] (main.py 265): INFO Train: [60/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2928 (0.3064)	loss 1.5382 (1.3590)	grad_norm 34.6972 (30.3944)	mem 4879MB
[2022-05-31 06:54:44 MetaFG_0] (main.py 265): INFO Train: [60/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2947 (0.3064)	loss 1.5286 (1.3596)	grad_norm 33.2851 (30.3970)	mem 4879MB
[2022-05-31 06:54:47 MetaFG_0] (main.py 265): INFO Train: [60/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3009 (0.3064)	loss 1.4714 (1.3590)	grad_norm 24.0912 (30.3804)	mem 4879MB
[2022-05-31 06:54:50 MetaFG_0] (main.py 265): INFO Train: [60/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2931 (0.3064)	loss 1.5313 (1.3589)	grad_norm 26.6649 (30.3841)	mem 4879MB
[2022-05-31 06:54:53 MetaFG_0] (main.py 265): INFO Train: [60/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2983 (0.3064)	loss 1.2405 (1.3586)	grad_norm 33.8998 (30.3814)	mem 4879MB
[2022-05-31 06:54:56 MetaFG_0] (main.py 265): INFO Train: [60/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2979 (0.3064)	loss 1.2841 (1.3580)	grad_norm 16.7870 (30.3553)	mem 4879MB
[2022-05-31 06:54:59 MetaFG_0] (main.py 265): INFO Train: [60/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2933 (0.3064)	loss 1.3343 (1.3577)	grad_norm 21.2340 (30.3427)	mem 4879MB
[2022-05-31 06:55:02 MetaFG_0] (main.py 265): INFO Train: [60/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2942 (0.3064)	loss 1.1873 (1.3581)	grad_norm 16.2767 (30.3327)	mem 4879MB
[2022-05-31 06:55:05 MetaFG_0] (main.py 265): INFO Train: [60/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2929 (0.3063)	loss 1.4737 (1.3579)	grad_norm 28.1798 (30.3356)	mem 4879MB
[2022-05-31 06:55:06 MetaFG_0] (main.py 272): INFO EPOCH 60 training takes 0:07:58
[2022-05-31 06:55:06 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_60.pth saving......
[2022-05-31 06:55:06 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_60.pth saved !!!
[2022-05-31 06:55:06 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 06:55:08 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 06:55:08 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 06:55:09 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.479 (0.479)	Loss 0.4484 (0.4484)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 06:55:10 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.092 (0.141)	Loss 0.4567 (0.5963)	Acc@1 93.750 (87.216)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 06:55:11 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.120)	Loss 0.5838 (0.6236)	Acc@1 87.500 (86.905)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 06:55:12 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.091 (0.112)	Loss 0.3113 (0.6088)	Acc@1 100.000 (87.601)	Acc@5 100.000 (98.790)	Mem 4879MB
[2022-05-31 06:55:12 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.090 (0.107)	Loss 0.5540 (0.6097)	Acc@1 87.500 (87.424)	Acc@5 100.000 (98.857)	Mem 4879MB
[2022-05-31 06:55:13 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.088 (0.104)	Loss 0.4466 (0.6146)	Acc@1 93.750 (87.439)	Acc@5 100.000 (99.020)	Mem 4879MB
[2022-05-31 06:55:14 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.099 (0.103)	Loss 0.5979 (0.6140)	Acc@1 87.500 (87.398)	Acc@5 100.000 (99.078)	Mem 4879MB
[2022-05-31 06:55:15 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.097 (0.102)	Loss 0.9090 (0.6152)	Acc@1 81.250 (87.412)	Acc@5 96.875 (99.120)	Mem 4879MB
[2022-05-31 06:55:16 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.099 (0.101)	Loss 0.7292 (0.6119)	Acc@1 84.375 (87.577)	Acc@5 100.000 (99.190)	Mem 4879MB
[2022-05-31 06:55:17 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.094 (0.101)	Loss 0.6738 (0.6167)	Acc@1 78.125 (87.431)	Acc@5 100.000 (99.176)	Mem 4879MB
[2022-05-31 06:55:18 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.094 (0.100)	Loss 0.8372 (0.6128)	Acc@1 81.250 (87.778)	Acc@5 96.875 (99.103)	Mem 4879MB
[2022-05-31 06:55:19 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.099)	Loss 0.7910 (0.6239)	Acc@1 84.375 (87.528)	Acc@5 96.875 (99.043)	Mem 4879MB
[2022-05-31 06:55:20 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.8463 (0.6349)	Acc@1 78.125 (87.035)	Acc@5 93.750 (98.838)	Mem 4879MB
[2022-05-31 06:55:21 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.097 (0.099)	Loss 0.8049 (0.6302)	Acc@1 81.250 (87.094)	Acc@5 100.000 (98.855)	Mem 4879MB
[2022-05-31 06:55:22 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.098)	Loss 0.7935 (0.6360)	Acc@1 81.250 (86.924)	Acc@5 93.750 (98.781)	Mem 4879MB
[2022-05-31 06:55:23 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 0.6021 (0.6374)	Acc@1 90.625 (86.858)	Acc@5 100.000 (98.800)	Mem 4879MB
[2022-05-31 06:55:24 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.6539 (0.6324)	Acc@1 87.500 (87.015)	Acc@5 100.000 (98.835)	Mem 4879MB
[2022-05-31 06:55:25 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.088 (0.098)	Loss 0.8073 (0.6313)	Acc@1 81.250 (86.933)	Acc@5 100.000 (98.904)	Mem 4879MB
[2022-05-31 06:55:26 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.097)	Loss 0.7955 (0.6332)	Acc@1 84.375 (86.896)	Acc@5 96.875 (98.912)	Mem 4879MB
[2022-05-31 06:55:27 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.097)	Loss 0.4431 (0.6326)	Acc@1 93.750 (86.911)	Acc@5 100.000 (98.937)	Mem 4879MB
[2022-05-31 06:55:28 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 0.9827 (0.6346)	Acc@1 71.875 (86.847)	Acc@5 96.875 (98.881)	Mem 4879MB
[2022-05-31 06:55:29 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 0.8114 (0.6376)	Acc@1 75.000 (86.715)	Acc@5 100.000 (98.889)	Mem 4879MB
[2022-05-31 06:55:30 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.103 (0.097)	Loss 0.7942 (0.6354)	Acc@1 84.375 (86.920)	Acc@5 96.875 (98.883)	Mem 4879MB
[2022-05-31 06:55:31 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.092 (0.097)	Loss 0.6480 (0.6372)	Acc@1 84.375 (86.851)	Acc@5 100.000 (98.877)	Mem 4879MB
[2022-05-31 06:55:31 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.6169 (0.6393)	Acc@1 84.375 (86.761)	Acc@5 100.000 (98.872)	Mem 4879MB
[2022-05-31 06:55:32 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.097)	Loss 0.5916 (0.6426)	Acc@1 87.500 (86.666)	Acc@5 96.875 (98.879)	Mem 4879MB
[2022-05-31 06:55:33 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.4365 (0.6457)	Acc@1 96.875 (86.650)	Acc@5 100.000 (98.851)	Mem 4879MB
[2022-05-31 06:55:34 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.097)	Loss 0.7400 (0.6479)	Acc@1 81.250 (86.531)	Acc@5 96.875 (98.824)	Mem 4879MB
[2022-05-31 06:55:35 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.095 (0.097)	Loss 0.5776 (0.6480)	Acc@1 87.500 (86.488)	Acc@5 96.875 (98.810)	Mem 4879MB
[2022-05-31 06:55:36 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 0.7001 (0.6499)	Acc@1 87.500 (86.469)	Acc@5 93.750 (98.722)	Mem 4879MB
[2022-05-31 06:55:37 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.103 (0.097)	Loss 0.7275 (0.6515)	Acc@1 84.375 (86.431)	Acc@5 96.875 (98.681)	Mem 4879MB
[2022-05-31 06:55:38 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.5114 (0.6481)	Acc@1 90.625 (86.566)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 06:55:38 MetaFG_0] (main.py 330): INFO  * Acc@1 86.560 Acc@5 98.680
[2022-05-31 06:55:38 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.6%
[2022-05-31 06:55:38 MetaFG_0] (main.py 171): INFO Max accuracy: 86.69%
[2022-05-31 06:55:39 MetaFG_0] (main.py 265): INFO Train: [61/300][0/1562]	eta 0:26:02 lr 0.000006	time 1.0005 (1.0005)	loss 1.4924 (1.4924)	grad_norm 24.7160 (24.7160)	mem 4879MB
[2022-05-31 06:55:43 MetaFG_0] (main.py 265): INFO Train: [61/300][10/1562]	eta 0:09:45 lr 0.000006	time 0.3002 (0.3773)	loss 1.3933 (1.2915)	grad_norm 23.6219 (29.8035)	mem 4879MB
[2022-05-31 06:55:46 MetaFG_0] (main.py 265): INFO Train: [61/300][20/1562]	eta 0:08:47 lr 0.000006	time 0.2928 (0.3421)	loss 1.6941 (1.2971)	grad_norm 28.8072 (29.5495)	mem 4879MB
[2022-05-31 06:55:49 MetaFG_0] (main.py 265): INFO Train: [61/300][30/1562]	eta 0:08:26 lr 0.000006	time 0.2996 (0.3305)	loss 1.5217 (1.3329)	grad_norm 24.4927 (28.2869)	mem 4879MB
[2022-05-31 06:55:52 MetaFG_0] (main.py 265): INFO Train: [61/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.3000 (0.3244)	loss 1.1173 (1.3507)	grad_norm 42.5718 (28.3057)	mem 4879MB
[2022-05-31 06:55:55 MetaFG_0] (main.py 265): INFO Train: [61/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2923 (0.3205)	loss 1.3258 (1.3077)	grad_norm 28.8835 (29.0542)	mem 4879MB
[2022-05-31 06:55:58 MetaFG_0] (main.py 265): INFO Train: [61/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2938 (0.3180)	loss 1.6358 (1.3294)	grad_norm 19.2995 (29.5782)	mem 4879MB
[2022-05-31 06:56:01 MetaFG_0] (main.py 265): INFO Train: [61/300][70/1562]	eta 0:07:52 lr 0.000006	time 0.2990 (0.3166)	loss 1.5255 (1.3511)	grad_norm 25.0121 (29.5163)	mem 4879MB
[2022-05-31 06:56:04 MetaFG_0] (main.py 265): INFO Train: [61/300][80/1562]	eta 0:07:47 lr 0.000006	time 0.2922 (0.3152)	loss 1.6379 (1.3497)	grad_norm 40.8154 (30.1969)	mem 4879MB
[2022-05-31 06:56:07 MetaFG_0] (main.py 265): INFO Train: [61/300][90/1562]	eta 0:07:42 lr 0.000006	time 0.2929 (0.3139)	loss 1.0654 (1.3465)	grad_norm 30.4549 (32.3105)	mem 4879MB
[2022-05-31 06:56:10 MetaFG_0] (main.py 265): INFO Train: [61/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2985 (0.3132)	loss 1.3333 (1.3495)	grad_norm 27.5823 (32.6728)	mem 4879MB
[2022-05-31 06:56:13 MetaFG_0] (main.py 265): INFO Train: [61/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2924 (0.3127)	loss 1.7347 (1.3563)	grad_norm 37.5046 (32.9918)	mem 4879MB
[2022-05-31 06:56:16 MetaFG_0] (main.py 265): INFO Train: [61/300][120/1562]	eta 0:07:30 lr 0.000006	time 0.2933 (0.3122)	loss 1.3901 (1.3556)	grad_norm 26.3011 (33.0831)	mem 4879MB
[2022-05-31 06:56:19 MetaFG_0] (main.py 265): INFO Train: [61/300][130/1562]	eta 0:07:26 lr 0.000006	time 0.2986 (0.3117)	loss 1.2499 (1.3468)	grad_norm 36.4974 (33.0704)	mem 4879MB
[2022-05-31 06:56:22 MetaFG_0] (main.py 265): INFO Train: [61/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.2926 (0.3111)	loss 1.5960 (1.3455)	grad_norm 66.9070 (33.0628)	mem 4879MB
[2022-05-31 06:56:25 MetaFG_0] (main.py 265): INFO Train: [61/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2935 (0.3107)	loss 1.0188 (1.3515)	grad_norm 23.2684 (33.0565)	mem 4879MB
[2022-05-31 06:56:28 MetaFG_0] (main.py 265): INFO Train: [61/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.3056 (0.3105)	loss 1.3526 (1.3509)	grad_norm 29.1350 (32.9862)	mem 4879MB
[2022-05-31 06:56:31 MetaFG_0] (main.py 265): INFO Train: [61/300][170/1562]	eta 0:07:12 lr 0.000006	time 0.2942 (0.3104)	loss 1.3521 (1.3431)	grad_norm 24.7685 (32.9613)	mem 4879MB
[2022-05-31 06:56:35 MetaFG_0] (main.py 265): INFO Train: [61/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.3029 (0.3103)	loss 1.0135 (1.3358)	grad_norm 73.5766 (32.6792)	mem 4879MB
[2022-05-31 06:56:38 MetaFG_0] (main.py 265): INFO Train: [61/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2969 (0.3100)	loss 1.3659 (1.3371)	grad_norm 25.9374 (32.4970)	mem 4879MB
[2022-05-31 06:56:41 MetaFG_0] (main.py 265): INFO Train: [61/300][200/1562]	eta 0:07:02 lr 0.000006	time 0.2996 (0.3099)	loss 1.4854 (1.3392)	grad_norm 39.9267 (32.3219)	mem 4879MB
[2022-05-31 06:56:44 MetaFG_0] (main.py 265): INFO Train: [61/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2992 (0.3098)	loss 1.5120 (1.3388)	grad_norm 15.3528 (32.0609)	mem 4879MB
[2022-05-31 06:56:47 MetaFG_0] (main.py 265): INFO Train: [61/300][220/1562]	eta 0:06:55 lr 0.000006	time 0.2932 (0.3096)	loss 1.1506 (1.3396)	grad_norm 19.4370 (32.0189)	mem 4879MB
[2022-05-31 06:56:50 MetaFG_0] (main.py 265): INFO Train: [61/300][230/1562]	eta 0:06:52 lr 0.000006	time 0.2982 (0.3095)	loss 1.4622 (1.3449)	grad_norm 16.2036 (31.7954)	mem 4879MB
[2022-05-31 06:56:53 MetaFG_0] (main.py 265): INFO Train: [61/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2968 (0.3093)	loss 1.0633 (1.3437)	grad_norm 47.4281 (32.0363)	mem 4879MB
[2022-05-31 06:56:56 MetaFG_0] (main.py 265): INFO Train: [61/300][250/1562]	eta 0:06:45 lr 0.000006	time 0.2988 (0.3093)	loss 1.4924 (1.3436)	grad_norm 21.5560 (31.7234)	mem 4879MB
[2022-05-31 06:56:59 MetaFG_0] (main.py 265): INFO Train: [61/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.2985 (0.3091)	loss 1.2791 (1.3390)	grad_norm 26.8419 (31.6520)	mem 4879MB
[2022-05-31 06:57:02 MetaFG_0] (main.py 265): INFO Train: [61/300][270/1562]	eta 0:06:39 lr 0.000006	time 0.2936 (0.3089)	loss 1.1512 (1.3371)	grad_norm 31.6032 (31.5153)	mem 4879MB
[2022-05-31 06:57:05 MetaFG_0] (main.py 265): INFO Train: [61/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.2937 (0.3088)	loss 1.4796 (1.3422)	grad_norm 17.2802 (31.5464)	mem 4879MB
[2022-05-31 06:57:08 MetaFG_0] (main.py 265): INFO Train: [61/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2936 (0.3087)	loss 1.0292 (1.3438)	grad_norm 34.7180 (31.2823)	mem 4879MB
[2022-05-31 06:57:11 MetaFG_0] (main.py 265): INFO Train: [61/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.2921 (0.3086)	loss 1.4446 (1.3410)	grad_norm 17.7918 (31.0896)	mem 4879MB
[2022-05-31 06:57:14 MetaFG_0] (main.py 265): INFO Train: [61/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.2983 (0.3085)	loss 1.3132 (1.3416)	grad_norm 18.7115 (30.9511)	mem 4879MB
[2022-05-31 06:57:17 MetaFG_0] (main.py 265): INFO Train: [61/300][320/1562]	eta 0:06:23 lr 0.000006	time 0.2923 (0.3084)	loss 1.4684 (1.3432)	grad_norm 23.9668 (30.8086)	mem 4879MB
[2022-05-31 06:57:20 MetaFG_0] (main.py 265): INFO Train: [61/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.3002 (0.3084)	loss 1.4462 (1.3462)	grad_norm 24.5142 (30.6697)	mem 4879MB
[2022-05-31 06:57:24 MetaFG_0] (main.py 265): INFO Train: [61/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2926 (0.3083)	loss 1.3737 (1.3472)	grad_norm 27.6838 (30.7923)	mem 4879MB
[2022-05-31 06:57:27 MetaFG_0] (main.py 265): INFO Train: [61/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2983 (0.3083)	loss 1.5877 (1.3498)	grad_norm 161.7697 (31.2851)	mem 4879MB
[2022-05-31 06:57:30 MetaFG_0] (main.py 265): INFO Train: [61/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.3063 (0.3083)	loss 1.4378 (1.3485)	grad_norm 49.9521 (31.3595)	mem 4879MB
[2022-05-31 06:57:33 MetaFG_0] (main.py 265): INFO Train: [61/300][370/1562]	eta 0:06:07 lr 0.000006	time 0.3009 (0.3082)	loss 0.9377 (1.3480)	grad_norm 26.9006 (31.2557)	mem 4879MB
[2022-05-31 06:57:36 MetaFG_0] (main.py 265): INFO Train: [61/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2943 (0.3082)	loss 1.6215 (1.3486)	grad_norm 19.2022 (31.3212)	mem 4879MB
[2022-05-31 06:57:39 MetaFG_0] (main.py 265): INFO Train: [61/300][390/1562]	eta 0:06:01 lr 0.000006	time 0.2942 (0.3081)	loss 1.2396 (1.3457)	grad_norm 25.8686 (31.3012)	mem 4879MB
[2022-05-31 06:57:42 MetaFG_0] (main.py 265): INFO Train: [61/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2955 (0.3081)	loss 1.5159 (1.3463)	grad_norm 21.2423 (31.1477)	mem 4879MB
[2022-05-31 06:57:45 MetaFG_0] (main.py 265): INFO Train: [61/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2928 (0.3080)	loss 1.3349 (1.3445)	grad_norm 39.7805 (31.1223)	mem 4879MB
[2022-05-31 06:57:48 MetaFG_0] (main.py 265): INFO Train: [61/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2936 (0.3079)	loss 1.3971 (1.3471)	grad_norm 27.3120 (31.0667)	mem 4879MB
[2022-05-31 06:57:51 MetaFG_0] (main.py 265): INFO Train: [61/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2928 (0.3079)	loss 1.4015 (1.3470)	grad_norm 30.9238 (31.0295)	mem 4879MB
[2022-05-31 06:57:54 MetaFG_0] (main.py 265): INFO Train: [61/300][440/1562]	eta 0:05:45 lr 0.000006	time 0.2946 (0.3079)	loss 1.3917 (1.3480)	grad_norm 76.8896 (31.1149)	mem 4879MB
[2022-05-31 06:57:57 MetaFG_0] (main.py 265): INFO Train: [61/300][450/1562]	eta 0:05:42 lr 0.000006	time 0.2993 (0.3078)	loss 1.1708 (1.3512)	grad_norm 31.7022 (31.0930)	mem 4879MB
[2022-05-31 06:58:00 MetaFG_0] (main.py 265): INFO Train: [61/300][460/1562]	eta 0:05:39 lr 0.000006	time 0.2989 (0.3078)	loss 1.4163 (1.3511)	grad_norm 34.5983 (30.9941)	mem 4879MB
[2022-05-31 06:58:03 MetaFG_0] (main.py 265): INFO Train: [61/300][470/1562]	eta 0:05:36 lr 0.000006	time 0.3022 (0.3078)	loss 1.5776 (1.3516)	grad_norm 29.6281 (31.1233)	mem 4879MB
[2022-05-31 06:58:07 MetaFG_0] (main.py 265): INFO Train: [61/300][480/1562]	eta 0:05:33 lr 0.000006	time 0.2941 (0.3084)	loss 0.9939 (1.3493)	grad_norm 29.9155 (31.0243)	mem 4879MB
[2022-05-31 06:58:10 MetaFG_0] (main.py 265): INFO Train: [61/300][490/1562]	eta 0:05:30 lr 0.000006	time 0.2933 (0.3084)	loss 1.3434 (1.3509)	grad_norm 33.2534 (30.9811)	mem 4879MB
[2022-05-31 06:58:13 MetaFG_0] (main.py 265): INFO Train: [61/300][500/1562]	eta 0:05:27 lr 0.000006	time 0.2989 (0.3083)	loss 1.2628 (1.3506)	grad_norm 27.6631 (30.9288)	mem 4879MB
[2022-05-31 06:58:16 MetaFG_0] (main.py 265): INFO Train: [61/300][510/1562]	eta 0:05:24 lr 0.000006	time 0.2954 (0.3083)	loss 1.3948 (1.3521)	grad_norm 49.2995 (30.9436)	mem 4879MB
[2022-05-31 06:58:19 MetaFG_0] (main.py 265): INFO Train: [61/300][520/1562]	eta 0:05:21 lr 0.000006	time 0.2929 (0.3082)	loss 1.2076 (1.3519)	grad_norm 31.9641 (30.8126)	mem 4879MB
[2022-05-31 06:58:22 MetaFG_0] (main.py 265): INFO Train: [61/300][530/1562]	eta 0:05:18 lr 0.000006	time 0.2939 (0.3082)	loss 1.6070 (1.3523)	grad_norm 29.5597 (30.7717)	mem 4879MB
[2022-05-31 06:58:25 MetaFG_0] (main.py 265): INFO Train: [61/300][540/1562]	eta 0:05:14 lr 0.000006	time 0.2960 (0.3081)	loss 1.5312 (1.3539)	grad_norm 19.9106 (30.7012)	mem 4879MB
[2022-05-31 06:58:28 MetaFG_0] (main.py 265): INFO Train: [61/300][550/1562]	eta 0:05:11 lr 0.000006	time 0.2985 (0.3081)	loss 1.2538 (1.3520)	grad_norm 38.1488 (30.8017)	mem 4879MB
[2022-05-31 06:58:31 MetaFG_0] (main.py 265): INFO Train: [61/300][560/1562]	eta 0:05:08 lr 0.000006	time 0.2985 (0.3080)	loss 1.0840 (1.3530)	grad_norm 27.8051 (30.7888)	mem 4879MB
[2022-05-31 06:58:34 MetaFG_0] (main.py 265): INFO Train: [61/300][570/1562]	eta 0:05:05 lr 0.000006	time 0.2991 (0.3080)	loss 1.1238 (1.3523)	grad_norm 34.4209 (30.8310)	mem 4879MB
[2022-05-31 06:58:37 MetaFG_0] (main.py 265): INFO Train: [61/300][580/1562]	eta 0:05:02 lr 0.000006	time 0.3001 (0.3080)	loss 0.8705 (1.3505)	grad_norm 32.4805 (30.8582)	mem 4879MB
[2022-05-31 06:58:40 MetaFG_0] (main.py 265): INFO Train: [61/300][590/1562]	eta 0:04:59 lr 0.000006	time 0.2961 (0.3080)	loss 1.7395 (1.3514)	grad_norm 67.0244 (30.8443)	mem 4879MB
[2022-05-31 06:58:43 MetaFG_0] (main.py 265): INFO Train: [61/300][600/1562]	eta 0:04:56 lr 0.000006	time 0.2997 (0.3079)	loss 1.5076 (1.3523)	grad_norm 22.5047 (30.8162)	mem 4879MB
[2022-05-31 06:58:47 MetaFG_0] (main.py 265): INFO Train: [61/300][610/1562]	eta 0:04:53 lr 0.000006	time 0.2986 (0.3079)	loss 1.0372 (1.3515)	grad_norm 18.0197 (30.6914)	mem 4879MB
[2022-05-31 06:58:50 MetaFG_0] (main.py 265): INFO Train: [61/300][620/1562]	eta 0:04:50 lr 0.000006	time 0.2927 (0.3079)	loss 1.2255 (1.3517)	grad_norm 17.1756 (30.6663)	mem 4879MB
[2022-05-31 06:58:53 MetaFG_0] (main.py 265): INFO Train: [61/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2936 (0.3078)	loss 1.1826 (1.3535)	grad_norm 33.8175 (30.6554)	mem 4879MB
[2022-05-31 06:58:56 MetaFG_0] (main.py 265): INFO Train: [61/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2997 (0.3078)	loss 1.5681 (1.3533)	grad_norm 17.6700 (30.6592)	mem 4879MB
[2022-05-31 06:58:59 MetaFG_0] (main.py 265): INFO Train: [61/300][650/1562]	eta 0:04:40 lr 0.000006	time 0.2999 (0.3078)	loss 0.9348 (1.3523)	grad_norm 22.8048 (30.6194)	mem 4879MB
[2022-05-31 06:59:02 MetaFG_0] (main.py 265): INFO Train: [61/300][660/1562]	eta 0:04:37 lr 0.000006	time 0.2930 (0.3078)	loss 1.4303 (1.3510)	grad_norm 23.5759 (30.5804)	mem 4879MB
[2022-05-31 06:59:05 MetaFG_0] (main.py 265): INFO Train: [61/300][670/1562]	eta 0:04:34 lr 0.000006	time 0.2987 (0.3078)	loss 0.8617 (1.3496)	grad_norm 36.0925 (30.6116)	mem 4879MB
[2022-05-31 06:59:08 MetaFG_0] (main.py 265): INFO Train: [61/300][680/1562]	eta 0:04:31 lr 0.000006	time 0.2988 (0.3078)	loss 1.5243 (1.3498)	grad_norm 20.6949 (30.5338)	mem 4879MB
[2022-05-31 06:59:11 MetaFG_0] (main.py 265): INFO Train: [61/300][690/1562]	eta 0:04:28 lr 0.000006	time 0.2938 (0.3078)	loss 1.6715 (1.3506)	grad_norm 30.7092 (30.4743)	mem 4879MB
[2022-05-31 06:59:14 MetaFG_0] (main.py 265): INFO Train: [61/300][700/1562]	eta 0:04:25 lr 0.000006	time 0.2980 (0.3077)	loss 1.1772 (1.3487)	grad_norm 28.8074 (30.4414)	mem 4879MB
[2022-05-31 06:59:17 MetaFG_0] (main.py 265): INFO Train: [61/300][710/1562]	eta 0:04:22 lr 0.000006	time 0.3001 (0.3077)	loss 1.1890 (1.3461)	grad_norm 48.1289 (30.4100)	mem 4879MB
[2022-05-31 06:59:20 MetaFG_0] (main.py 265): INFO Train: [61/300][720/1562]	eta 0:04:19 lr 0.000006	time 0.2939 (0.3076)	loss 1.0335 (1.3451)	grad_norm 22.5140 (30.3060)	mem 4879MB
[2022-05-31 06:59:23 MetaFG_0] (main.py 265): INFO Train: [61/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2998 (0.3076)	loss 1.0758 (1.3444)	grad_norm 35.7397 (30.3897)	mem 4879MB
[2022-05-31 06:59:26 MetaFG_0] (main.py 265): INFO Train: [61/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.3008 (0.3076)	loss 1.5808 (1.3441)	grad_norm 77.8944 (30.5229)	mem 4879MB
[2022-05-31 06:59:29 MetaFG_0] (main.py 265): INFO Train: [61/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2999 (0.3076)	loss 1.3790 (1.3450)	grad_norm 27.8597 (30.5125)	mem 4879MB
[2022-05-31 06:59:32 MetaFG_0] (main.py 265): INFO Train: [61/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.3065 (0.3075)	loss 1.3233 (1.3454)	grad_norm 15.2866 (30.5325)	mem 4879MB
[2022-05-31 06:59:35 MetaFG_0] (main.py 265): INFO Train: [61/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.2999 (0.3075)	loss 1.4780 (1.3456)	grad_norm 37.2397 (30.5774)	mem 4879MB
[2022-05-31 06:59:39 MetaFG_0] (main.py 265): INFO Train: [61/300][780/1562]	eta 0:04:00 lr 0.000006	time 0.2987 (0.3075)	loss 1.4104 (1.3456)	grad_norm 23.3685 (30.5903)	mem 4879MB
[2022-05-31 06:59:42 MetaFG_0] (main.py 265): INFO Train: [61/300][790/1562]	eta 0:03:57 lr 0.000006	time 0.2946 (0.3075)	loss 1.6489 (1.3456)	grad_norm 34.6470 (30.5609)	mem 4879MB
[2022-05-31 06:59:45 MetaFG_0] (main.py 265): INFO Train: [61/300][800/1562]	eta 0:03:54 lr 0.000006	time 0.3027 (0.3074)	loss 1.0051 (1.3451)	grad_norm 19.6052 (30.4971)	mem 4879MB
[2022-05-31 06:59:48 MetaFG_0] (main.py 265): INFO Train: [61/300][810/1562]	eta 0:03:51 lr 0.000006	time 0.2933 (0.3074)	loss 1.2458 (1.3447)	grad_norm 18.2606 (30.4565)	mem 4879MB
[2022-05-31 06:59:51 MetaFG_0] (main.py 265): INFO Train: [61/300][820/1562]	eta 0:03:48 lr 0.000006	time 0.2945 (0.3074)	loss 1.0605 (1.3446)	grad_norm 31.4227 (30.4421)	mem 4879MB
[2022-05-31 06:59:54 MetaFG_0] (main.py 265): INFO Train: [61/300][830/1562]	eta 0:03:45 lr 0.000006	time 0.2929 (0.3074)	loss 1.4540 (1.3447)	grad_norm 18.3055 (30.4140)	mem 4879MB
[2022-05-31 06:59:57 MetaFG_0] (main.py 265): INFO Train: [61/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2982 (0.3074)	loss 1.5713 (1.3457)	grad_norm 25.1113 (30.4108)	mem 4879MB
[2022-05-31 07:00:00 MetaFG_0] (main.py 265): INFO Train: [61/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2925 (0.3074)	loss 1.6744 (1.3454)	grad_norm 30.7445 (30.4232)	mem 4879MB
[2022-05-31 07:00:03 MetaFG_0] (main.py 265): INFO Train: [61/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2929 (0.3073)	loss 1.4446 (1.3461)	grad_norm 59.0180 (30.4318)	mem 4879MB
[2022-05-31 07:00:06 MetaFG_0] (main.py 265): INFO Train: [61/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3009 (0.3073)	loss 1.5335 (1.3470)	grad_norm 27.5272 (30.4257)	mem 4879MB
[2022-05-31 07:00:09 MetaFG_0] (main.py 265): INFO Train: [61/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.3052 (0.3073)	loss 1.5842 (1.3482)	grad_norm 28.7828 (30.4060)	mem 4879MB
[2022-05-31 07:00:12 MetaFG_0] (main.py 265): INFO Train: [61/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.2939 (0.3073)	loss 1.2034 (1.3478)	grad_norm 59.2207 (30.3958)	mem 4879MB
[2022-05-31 07:00:15 MetaFG_0] (main.py 265): INFO Train: [61/300][900/1562]	eta 0:03:23 lr 0.000006	time 0.3024 (0.3073)	loss 1.4317 (1.3470)	grad_norm 31.3459 (30.4275)	mem 4879MB
[2022-05-31 07:00:18 MetaFG_0] (main.py 265): INFO Train: [61/300][910/1562]	eta 0:03:20 lr 0.000006	time 0.2942 (0.3073)	loss 1.4122 (1.3465)	grad_norm 34.5970 (30.4896)	mem 4879MB
[2022-05-31 07:00:21 MetaFG_0] (main.py 265): INFO Train: [61/300][920/1562]	eta 0:03:17 lr 0.000006	time 0.3019 (0.3073)	loss 0.9382 (1.3451)	grad_norm 21.7308 (30.4196)	mem 4879MB
[2022-05-31 07:00:24 MetaFG_0] (main.py 265): INFO Train: [61/300][930/1562]	eta 0:03:14 lr 0.000006	time 0.3037 (0.3073)	loss 1.3918 (1.3436)	grad_norm 36.3768 (30.3892)	mem 4879MB
[2022-05-31 07:00:28 MetaFG_0] (main.py 265): INFO Train: [61/300][940/1562]	eta 0:03:11 lr 0.000006	time 0.3021 (0.3072)	loss 1.0976 (1.3423)	grad_norm 26.0806 (30.3241)	mem 4879MB
[2022-05-31 07:00:31 MetaFG_0] (main.py 265): INFO Train: [61/300][950/1562]	eta 0:03:08 lr 0.000006	time 0.2996 (0.3072)	loss 1.6270 (1.3424)	grad_norm 52.5319 (30.3501)	mem 4879MB
[2022-05-31 07:00:34 MetaFG_0] (main.py 265): INFO Train: [61/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2989 (0.3072)	loss 1.5283 (1.3419)	grad_norm 40.9601 (30.3818)	mem 4879MB
[2022-05-31 07:00:37 MetaFG_0] (main.py 265): INFO Train: [61/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.3015 (0.3072)	loss 1.3736 (1.3416)	grad_norm 22.3632 (30.3666)	mem 4879MB
[2022-05-31 07:00:40 MetaFG_0] (main.py 265): INFO Train: [61/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2998 (0.3072)	loss 1.6013 (1.3410)	grad_norm 42.6325 (30.3191)	mem 4879MB
[2022-05-31 07:00:43 MetaFG_0] (main.py 265): INFO Train: [61/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.3012 (0.3072)	loss 1.5726 (1.3410)	grad_norm 43.1878 (30.2946)	mem 4879MB
[2022-05-31 07:00:46 MetaFG_0] (main.py 265): INFO Train: [61/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.3018 (0.3072)	loss 1.5126 (1.3410)	grad_norm 21.0825 (30.2672)	mem 4879MB
[2022-05-31 07:00:49 MetaFG_0] (main.py 265): INFO Train: [61/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.3013 (0.3072)	loss 1.1617 (1.3405)	grad_norm 21.2990 (30.2843)	mem 4879MB
[2022-05-31 07:00:52 MetaFG_0] (main.py 265): INFO Train: [61/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2988 (0.3072)	loss 1.3122 (1.3412)	grad_norm 44.3912 (30.3208)	mem 4879MB
[2022-05-31 07:00:55 MetaFG_0] (main.py 265): INFO Train: [61/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2940 (0.3072)	loss 1.0339 (1.3409)	grad_norm 22.3498 (30.3295)	mem 4879MB
[2022-05-31 07:00:58 MetaFG_0] (main.py 265): INFO Train: [61/300][1040/1562]	eta 0:02:40 lr 0.000006	time 0.2998 (0.3071)	loss 1.4827 (1.3413)	grad_norm 28.1503 (30.2948)	mem 4879MB
[2022-05-31 07:01:01 MetaFG_0] (main.py 265): INFO Train: [61/300][1050/1562]	eta 0:02:37 lr 0.000006	time 0.2933 (0.3071)	loss 1.1294 (1.3398)	grad_norm 30.5939 (30.2644)	mem 4879MB
[2022-05-31 07:01:04 MetaFG_0] (main.py 265): INFO Train: [61/300][1060/1562]	eta 0:02:34 lr 0.000006	time 0.2949 (0.3071)	loss 1.0374 (1.3393)	grad_norm 22.6612 (30.2519)	mem 4879MB
[2022-05-31 07:01:07 MetaFG_0] (main.py 265): INFO Train: [61/300][1070/1562]	eta 0:02:31 lr 0.000006	time 0.2940 (0.3071)	loss 1.4217 (1.3388)	grad_norm 21.3659 (30.3025)	mem 4879MB
[2022-05-31 07:01:10 MetaFG_0] (main.py 265): INFO Train: [61/300][1080/1562]	eta 0:02:28 lr 0.000006	time 0.2939 (0.3071)	loss 1.2072 (1.3380)	grad_norm 27.1706 (30.2758)	mem 4879MB
[2022-05-31 07:01:13 MetaFG_0] (main.py 265): INFO Train: [61/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2932 (0.3071)	loss 1.4104 (1.3389)	grad_norm 44.0007 (30.2634)	mem 4879MB
[2022-05-31 07:01:17 MetaFG_0] (main.py 265): INFO Train: [61/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2992 (0.3071)	loss 1.0587 (1.3387)	grad_norm 17.5768 (30.3442)	mem 4879MB
[2022-05-31 07:01:20 MetaFG_0] (main.py 265): INFO Train: [61/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2986 (0.3071)	loss 1.4906 (1.3391)	grad_norm 96.5451 (30.4258)	mem 4879MB
[2022-05-31 07:01:23 MetaFG_0] (main.py 265): INFO Train: [61/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2924 (0.3071)	loss 1.6546 (1.3400)	grad_norm 35.6557 (30.4146)	mem 4879MB
[2022-05-31 07:01:26 MetaFG_0] (main.py 265): INFO Train: [61/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2940 (0.3071)	loss 1.1958 (1.3403)	grad_norm 20.5357 (30.3756)	mem 4879MB
[2022-05-31 07:01:29 MetaFG_0] (main.py 265): INFO Train: [61/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2995 (0.3071)	loss 1.4087 (1.3417)	grad_norm 51.5141 (30.3851)	mem 4879MB
[2022-05-31 07:01:32 MetaFG_0] (main.py 265): INFO Train: [61/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2994 (0.3071)	loss 1.6544 (1.3425)	grad_norm 131.4198 (30.5080)	mem 4879MB
[2022-05-31 07:01:35 MetaFG_0] (main.py 265): INFO Train: [61/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2936 (0.3070)	loss 0.8937 (1.3415)	grad_norm 28.8713 (30.5553)	mem 4879MB
[2022-05-31 07:01:38 MetaFG_0] (main.py 265): INFO Train: [61/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2939 (0.3070)	loss 1.4528 (1.3418)	grad_norm 22.6602 (30.5633)	mem 4879MB
[2022-05-31 07:01:41 MetaFG_0] (main.py 265): INFO Train: [61/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.2930 (0.3070)	loss 1.5322 (1.3419)	grad_norm 22.8721 (30.5433)	mem 4879MB
[2022-05-31 07:01:44 MetaFG_0] (main.py 265): INFO Train: [61/300][1190/1562]	eta 0:01:54 lr 0.000006	time 0.2983 (0.3070)	loss 1.4162 (1.3426)	grad_norm 24.7749 (30.5178)	mem 4879MB
[2022-05-31 07:01:47 MetaFG_0] (main.py 265): INFO Train: [61/300][1200/1562]	eta 0:01:51 lr 0.000006	time 0.2984 (0.3070)	loss 1.4108 (1.3418)	grad_norm 24.9379 (30.5357)	mem 4879MB
[2022-05-31 07:01:50 MetaFG_0] (main.py 265): INFO Train: [61/300][1210/1562]	eta 0:01:48 lr 0.000006	time 0.2942 (0.3070)	loss 1.0280 (1.3428)	grad_norm 32.5254 (30.5259)	mem 4879MB
[2022-05-31 07:01:53 MetaFG_0] (main.py 265): INFO Train: [61/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2993 (0.3070)	loss 1.6818 (1.3428)	grad_norm 30.9959 (30.5302)	mem 4879MB
[2022-05-31 07:01:56 MetaFG_0] (main.py 265): INFO Train: [61/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2980 (0.3070)	loss 1.6000 (1.3433)	grad_norm 62.8068 (30.5158)	mem 4879MB
[2022-05-31 07:01:59 MetaFG_0] (main.py 265): INFO Train: [61/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2934 (0.3070)	loss 1.4453 (1.3437)	grad_norm 22.1586 (30.5004)	mem 4879MB
[2022-05-31 07:02:02 MetaFG_0] (main.py 265): INFO Train: [61/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2925 (0.3070)	loss 1.6977 (1.3430)	grad_norm 35.5999 (30.5587)	mem 4879MB
[2022-05-31 07:02:05 MetaFG_0] (main.py 265): INFO Train: [61/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2946 (0.3070)	loss 0.7763 (1.3426)	grad_norm 17.8416 (30.5366)	mem 4879MB
[2022-05-31 07:02:09 MetaFG_0] (main.py 265): INFO Train: [61/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.3005 (0.3070)	loss 1.2813 (1.3427)	grad_norm 23.6657 (30.4884)	mem 4879MB
[2022-05-31 07:02:12 MetaFG_0] (main.py 265): INFO Train: [61/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2994 (0.3070)	loss 1.6840 (1.3434)	grad_norm 19.9634 (30.4519)	mem 4879MB
[2022-05-31 07:02:15 MetaFG_0] (main.py 265): INFO Train: [61/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.3026 (0.3070)	loss 1.2558 (1.3433)	grad_norm 19.1528 (30.4389)	mem 4879MB
[2022-05-31 07:02:18 MetaFG_0] (main.py 265): INFO Train: [61/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2927 (0.3070)	loss 1.2405 (1.3435)	grad_norm 34.4792 (30.4365)	mem 4879MB
[2022-05-31 07:02:21 MetaFG_0] (main.py 265): INFO Train: [61/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2992 (0.3070)	loss 1.5127 (1.3443)	grad_norm 29.0292 (30.4251)	mem 4879MB
[2022-05-31 07:02:24 MetaFG_0] (main.py 265): INFO Train: [61/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2954 (0.3070)	loss 1.2406 (1.3445)	grad_norm 30.1920 (30.4362)	mem 4879MB
[2022-05-31 07:02:27 MetaFG_0] (main.py 265): INFO Train: [61/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2936 (0.3070)	loss 1.3323 (1.3446)	grad_norm 26.7068 (30.3952)	mem 4879MB
[2022-05-31 07:02:30 MetaFG_0] (main.py 265): INFO Train: [61/300][1340/1562]	eta 0:01:08 lr 0.000006	time 0.2982 (0.3069)	loss 1.2035 (1.3444)	grad_norm 42.8584 (30.3873)	mem 4879MB
[2022-05-31 07:02:33 MetaFG_0] (main.py 265): INFO Train: [61/300][1350/1562]	eta 0:01:05 lr 0.000006	time 0.3007 (0.3069)	loss 1.1121 (1.3439)	grad_norm 28.5462 (30.3610)	mem 4879MB
[2022-05-31 07:02:36 MetaFG_0] (main.py 265): INFO Train: [61/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2988 (0.3069)	loss 1.3220 (1.3435)	grad_norm 54.0158 (30.3648)	mem 4879MB
[2022-05-31 07:02:39 MetaFG_0] (main.py 265): INFO Train: [61/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2989 (0.3069)	loss 1.4791 (1.3438)	grad_norm 18.4597 (30.4017)	mem 4879MB
[2022-05-31 07:02:42 MetaFG_0] (main.py 265): INFO Train: [61/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2936 (0.3069)	loss 1.6666 (1.3435)	grad_norm 51.0096 (30.3877)	mem 4879MB
[2022-05-31 07:02:45 MetaFG_0] (main.py 265): INFO Train: [61/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2987 (0.3069)	loss 1.2107 (1.3440)	grad_norm 27.4212 (30.3576)	mem 4879MB
[2022-05-31 07:02:48 MetaFG_0] (main.py 265): INFO Train: [61/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2952 (0.3069)	loss 1.3979 (1.3442)	grad_norm 12.8850 (30.3580)	mem 4879MB
[2022-05-31 07:02:51 MetaFG_0] (main.py 265): INFO Train: [61/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2927 (0.3069)	loss 1.4426 (1.3439)	grad_norm 22.4643 (30.3402)	mem 4879MB
[2022-05-31 07:02:54 MetaFG_0] (main.py 265): INFO Train: [61/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2982 (0.3068)	loss 1.7777 (1.3447)	grad_norm 27.0505 (30.2983)	mem 4879MB
[2022-05-31 07:02:57 MetaFG_0] (main.py 265): INFO Train: [61/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2937 (0.3068)	loss 1.2288 (1.3450)	grad_norm 15.7370 (30.2909)	mem 4879MB
[2022-05-31 07:03:01 MetaFG_0] (main.py 265): INFO Train: [61/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.3033 (0.3068)	loss 1.6280 (1.3448)	grad_norm 16.6348 (30.2485)	mem 4879MB
[2022-05-31 07:03:04 MetaFG_0] (main.py 265): INFO Train: [61/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2944 (0.3068)	loss 1.2281 (1.3452)	grad_norm 31.5461 (30.2211)	mem 4879MB
[2022-05-31 07:03:07 MetaFG_0] (main.py 265): INFO Train: [61/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2936 (0.3068)	loss 1.3360 (1.3456)	grad_norm 33.7625 (30.2452)	mem 4879MB
[2022-05-31 07:03:10 MetaFG_0] (main.py 265): INFO Train: [61/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2947 (0.3068)	loss 1.6010 (1.3460)	grad_norm 38.4874 (30.2706)	mem 4879MB
[2022-05-31 07:03:13 MetaFG_0] (main.py 265): INFO Train: [61/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2916 (0.3068)	loss 1.2414 (1.3458)	grad_norm 17.0556 (30.2648)	mem 4879MB
[2022-05-31 07:03:16 MetaFG_0] (main.py 265): INFO Train: [61/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2929 (0.3068)	loss 0.9020 (1.3462)	grad_norm 34.9362 (30.2404)	mem 4879MB
[2022-05-31 07:03:19 MetaFG_0] (main.py 265): INFO Train: [61/300][1500/1562]	eta 0:00:19 lr 0.000006	time 0.2950 (0.3068)	loss 1.3248 (1.3469)	grad_norm 49.6527 (30.2570)	mem 4879MB
[2022-05-31 07:03:22 MetaFG_0] (main.py 265): INFO Train: [61/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2917 (0.3067)	loss 1.3250 (1.3469)	grad_norm 36.8367 (30.2697)	mem 4879MB
[2022-05-31 07:03:25 MetaFG_0] (main.py 265): INFO Train: [61/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2993 (0.3067)	loss 1.4149 (1.3467)	grad_norm 24.8582 (30.3037)	mem 4879MB
[2022-05-31 07:03:28 MetaFG_0] (main.py 265): INFO Train: [61/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2925 (0.3067)	loss 1.3256 (1.3471)	grad_norm 21.4040 (30.2938)	mem 4879MB
[2022-05-31 07:03:31 MetaFG_0] (main.py 265): INFO Train: [61/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.3051 (0.3067)	loss 1.0842 (1.3467)	grad_norm 32.9935 (nan)	mem 4879MB
[2022-05-31 07:03:34 MetaFG_0] (main.py 265): INFO Train: [61/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2989 (0.3067)	loss 1.4802 (1.3468)	grad_norm 27.6244 (nan)	mem 4879MB
[2022-05-31 07:03:37 MetaFG_0] (main.py 265): INFO Train: [61/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2919 (0.3067)	loss 1.1294 (1.3462)	grad_norm 48.0277 (nan)	mem 4879MB
[2022-05-31 07:03:38 MetaFG_0] (main.py 272): INFO EPOCH 61 training takes 0:07:59
[2022-05-31 07:03:38 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_61.pth saving......
[2022-05-31 07:03:38 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_61.pth saved !!!
[2022-05-31 07:03:38 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:03:40 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:03:40 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:03:41 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.876 (0.876)	Loss 0.8794 (0.8794)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 07:03:42 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.167)	Loss 0.6568 (0.7062)	Acc@1 87.500 (82.386)	Acc@5 96.875 (97.727)	Mem 4879MB
[2022-05-31 07:03:43 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.132)	Loss 0.4185 (0.6289)	Acc@1 90.625 (85.714)	Acc@5 100.000 (98.214)	Mem 4879MB
[2022-05-31 07:03:44 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.095 (0.120)	Loss 0.4922 (0.6083)	Acc@1 90.625 (85.988)	Acc@5 96.875 (98.387)	Mem 4879MB
[2022-05-31 07:03:45 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.114)	Loss 0.4163 (0.5868)	Acc@1 90.625 (86.509)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 07:03:46 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.110)	Loss 0.3749 (0.5831)	Acc@1 90.625 (86.458)	Acc@5 100.000 (98.591)	Mem 4879MB
[2022-05-31 07:03:47 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.088 (0.107)	Loss 0.4703 (0.5847)	Acc@1 90.625 (86.885)	Acc@5 100.000 (98.514)	Mem 4879MB
[2022-05-31 07:03:48 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.105)	Loss 0.4925 (0.5807)	Acc@1 93.750 (86.928)	Acc@5 96.875 (98.592)	Mem 4879MB
[2022-05-31 07:03:49 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.104)	Loss 0.9363 (0.5856)	Acc@1 75.000 (86.883)	Acc@5 96.875 (98.611)	Mem 4879MB
[2022-05-31 07:03:49 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.103)	Loss 0.3825 (0.5889)	Acc@1 93.750 (86.813)	Acc@5 100.000 (98.558)	Mem 4879MB
[2022-05-31 07:03:50 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.102)	Loss 0.5813 (0.5867)	Acc@1 90.625 (86.726)	Acc@5 100.000 (98.639)	Mem 4879MB
[2022-05-31 07:03:51 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.7115 (0.5864)	Acc@1 81.250 (86.712)	Acc@5 96.875 (98.620)	Mem 4879MB
[2022-05-31 07:03:52 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.100)	Loss 0.5608 (0.5805)	Acc@1 84.375 (86.829)	Acc@5 96.875 (98.683)	Mem 4879MB
[2022-05-31 07:03:53 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.100)	Loss 0.4877 (0.5788)	Acc@1 87.500 (86.927)	Acc@5 96.875 (98.664)	Mem 4879MB
[2022-05-31 07:03:54 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.100)	Loss 0.4117 (0.5759)	Acc@1 90.625 (87.012)	Acc@5 100.000 (98.715)	Mem 4879MB
[2022-05-31 07:03:55 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.093 (0.099)	Loss 0.5412 (0.5764)	Acc@1 90.625 (86.962)	Acc@5 100.000 (98.717)	Mem 4879MB
[2022-05-31 07:03:56 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.099)	Loss 0.5044 (0.5796)	Acc@1 90.625 (86.782)	Acc@5 100.000 (98.719)	Mem 4879MB
[2022-05-31 07:03:57 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.099)	Loss 0.5913 (0.5789)	Acc@1 84.375 (86.605)	Acc@5 96.875 (98.739)	Mem 4879MB
[2022-05-31 07:03:58 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.111 (0.099)	Loss 0.6326 (0.5761)	Acc@1 81.250 (86.775)	Acc@5 100.000 (98.774)	Mem 4879MB
[2022-05-31 07:03:59 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.137 (0.099)	Loss 0.5692 (0.5722)	Acc@1 84.375 (86.878)	Acc@5 100.000 (98.806)	Mem 4879MB
[2022-05-31 07:04:00 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.131 (0.100)	Loss 0.5504 (0.5728)	Acc@1 90.625 (86.816)	Acc@5 100.000 (98.787)	Mem 4879MB
[2022-05-31 07:04:01 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.092 (0.101)	Loss 0.2750 (0.5724)	Acc@1 100.000 (86.863)	Acc@5 100.000 (98.800)	Mem 4879MB
[2022-05-31 07:04:02 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.096 (0.101)	Loss 0.6201 (0.5720)	Acc@1 81.250 (86.835)	Acc@5 96.875 (98.798)	Mem 4879MB
[2022-05-31 07:04:03 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.101)	Loss 0.3062 (0.5730)	Acc@1 96.875 (86.783)	Acc@5 100.000 (98.796)	Mem 4879MB
[2022-05-31 07:04:04 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.098 (0.100)	Loss 0.7639 (0.5737)	Acc@1 78.125 (86.748)	Acc@5 100.000 (98.807)	Mem 4879MB
[2022-05-31 07:04:05 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.100)	Loss 0.6025 (0.5761)	Acc@1 84.375 (86.741)	Acc@5 100.000 (98.780)	Mem 4879MB
[2022-05-31 07:04:06 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.106 (0.100)	Loss 0.3555 (0.5738)	Acc@1 96.875 (86.806)	Acc@5 100.000 (98.791)	Mem 4879MB
[2022-05-31 07:04:07 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.095 (0.100)	Loss 0.4734 (0.5739)	Acc@1 81.250 (86.762)	Acc@5 100.000 (98.778)	Mem 4879MB
[2022-05-31 07:04:08 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.099)	Loss 0.7140 (0.5750)	Acc@1 87.500 (86.822)	Acc@5 96.875 (98.743)	Mem 4879MB
[2022-05-31 07:04:09 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.106 (0.099)	Loss 0.7290 (0.5755)	Acc@1 84.375 (86.866)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 07:04:10 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.099)	Loss 0.4827 (0.5754)	Acc@1 90.625 (86.908)	Acc@5 96.875 (98.744)	Mem 4879MB
[2022-05-31 07:04:11 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.099)	Loss 0.6533 (0.5760)	Acc@1 87.500 (86.857)	Acc@5 100.000 (98.754)	Mem 4879MB
[2022-05-31 07:04:11 MetaFG_0] (main.py 330): INFO  * Acc@1 86.840 Acc@5 98.760
[2022-05-31 07:04:11 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.8%
[2022-05-31 07:04:11 MetaFG_0] (main.py 171): INFO Max accuracy: 86.84%
[2022-05-31 07:04:12 MetaFG_0] (main.py 265): INFO Train: [62/300][0/1562]	eta 0:27:27 lr 0.000006	time 1.0550 (1.0550)	loss 1.3198 (1.3198)	grad_norm 22.3343 (22.3343)	mem 4879MB
[2022-05-31 07:04:15 MetaFG_0] (main.py 265): INFO Train: [62/300][10/1562]	eta 0:09:49 lr 0.000006	time 0.3017 (0.3799)	loss 1.1909 (1.2844)	grad_norm 27.6053 (30.2639)	mem 4879MB
[2022-05-31 07:04:18 MetaFG_0] (main.py 265): INFO Train: [62/300][20/1562]	eta 0:08:50 lr 0.000006	time 0.2940 (0.3440)	loss 1.1884 (1.2902)	grad_norm 29.9535 (32.0114)	mem 4879MB
[2022-05-31 07:04:21 MetaFG_0] (main.py 265): INFO Train: [62/300][30/1562]	eta 0:08:27 lr 0.000006	time 0.2937 (0.3312)	loss 1.2089 (1.3292)	grad_norm 50.3354 (31.9940)	mem 4879MB
[2022-05-31 07:04:24 MetaFG_0] (main.py 265): INFO Train: [62/300][40/1562]	eta 0:08:15 lr 0.000006	time 0.3005 (0.3255)	loss 1.2261 (1.3414)	grad_norm 32.3140 (32.0271)	mem 4879MB
[2022-05-31 07:04:28 MetaFG_0] (main.py 265): INFO Train: [62/300][50/1562]	eta 0:08:06 lr 0.000006	time 0.3001 (0.3220)	loss 1.4336 (1.3433)	grad_norm 30.4393 (31.5050)	mem 4879MB
[2022-05-31 07:04:31 MetaFG_0] (main.py 265): INFO Train: [62/300][60/1562]	eta 0:07:59 lr 0.000006	time 0.2924 (0.3191)	loss 1.2282 (1.3418)	grad_norm 25.1234 (31.2866)	mem 4879MB
[2022-05-31 07:04:34 MetaFG_0] (main.py 265): INFO Train: [62/300][70/1562]	eta 0:07:53 lr 0.000006	time 0.2949 (0.3175)	loss 1.8175 (1.3401)	grad_norm 32.2123 (31.2153)	mem 4879MB
[2022-05-31 07:04:37 MetaFG_0] (main.py 265): INFO Train: [62/300][80/1562]	eta 0:07:48 lr 0.000006	time 0.2994 (0.3162)	loss 1.6721 (1.3522)	grad_norm 21.6552 (30.6378)	mem 4879MB
[2022-05-31 07:04:40 MetaFG_0] (main.py 265): INFO Train: [62/300][90/1562]	eta 0:07:44 lr 0.000006	time 0.2923 (0.3152)	loss 1.3990 (1.3582)	grad_norm 25.8803 (31.0904)	mem 4879MB
[2022-05-31 07:04:43 MetaFG_0] (main.py 265): INFO Train: [62/300][100/1562]	eta 0:07:39 lr 0.000006	time 0.2997 (0.3144)	loss 1.5354 (1.3551)	grad_norm 33.3070 (31.4265)	mem 4879MB
[2022-05-31 07:04:46 MetaFG_0] (main.py 265): INFO Train: [62/300][110/1562]	eta 0:07:35 lr 0.000006	time 0.3013 (0.3139)	loss 1.5047 (1.3550)	grad_norm 27.2860 (31.2898)	mem 4879MB
[2022-05-31 07:04:49 MetaFG_0] (main.py 265): INFO Train: [62/300][120/1562]	eta 0:07:31 lr 0.000006	time 0.2987 (0.3133)	loss 1.3510 (1.3544)	grad_norm 31.6950 (31.0736)	mem 4879MB
[2022-05-31 07:04:52 MetaFG_0] (main.py 265): INFO Train: [62/300][130/1562]	eta 0:07:27 lr 0.000006	time 0.2927 (0.3126)	loss 0.9531 (1.3497)	grad_norm 34.5963 (31.0755)	mem 4879MB
[2022-05-31 07:04:55 MetaFG_0] (main.py 265): INFO Train: [62/300][140/1562]	eta 0:07:23 lr 0.000006	time 0.3029 (0.3122)	loss 0.9689 (1.3489)	grad_norm 36.9722 (30.8344)	mem 4879MB
[2022-05-31 07:04:58 MetaFG_0] (main.py 265): INFO Train: [62/300][150/1562]	eta 0:07:20 lr 0.000006	time 0.2980 (0.3118)	loss 1.2635 (1.3422)	grad_norm 23.9495 (30.7903)	mem 4879MB
[2022-05-31 07:05:01 MetaFG_0] (main.py 265): INFO Train: [62/300][160/1562]	eta 0:07:16 lr 0.000006	time 0.2940 (0.3114)	loss 1.3741 (1.3465)	grad_norm 19.3781 (30.6595)	mem 4879MB
[2022-05-31 07:05:04 MetaFG_0] (main.py 265): INFO Train: [62/300][170/1562]	eta 0:07:12 lr 0.000006	time 0.2940 (0.3110)	loss 1.2827 (1.3498)	grad_norm 17.1392 (30.2722)	mem 4879MB
[2022-05-31 07:05:07 MetaFG_0] (main.py 265): INFO Train: [62/300][180/1562]	eta 0:07:09 lr 0.000006	time 0.2998 (0.3106)	loss 1.5683 (1.3485)	grad_norm 29.6267 (30.3077)	mem 4879MB
[2022-05-31 07:05:10 MetaFG_0] (main.py 265): INFO Train: [62/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2992 (0.3103)	loss 0.9611 (1.3506)	grad_norm 36.0197 (30.1604)	mem 4879MB
[2022-05-31 07:05:13 MetaFG_0] (main.py 265): INFO Train: [62/300][200/1562]	eta 0:07:02 lr 0.000006	time 0.2944 (0.3099)	loss 1.1173 (1.3454)	grad_norm 42.1658 (30.1935)	mem 4879MB
[2022-05-31 07:05:16 MetaFG_0] (main.py 265): INFO Train: [62/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2940 (0.3097)	loss 1.4324 (1.3441)	grad_norm 27.6497 (30.1301)	mem 4879MB
[2022-05-31 07:05:19 MetaFG_0] (main.py 265): INFO Train: [62/300][220/1562]	eta 0:06:55 lr 0.000006	time 0.2929 (0.3094)	loss 1.2435 (1.3445)	grad_norm 28.8262 (30.1516)	mem 4879MB
[2022-05-31 07:05:23 MetaFG_0] (main.py 265): INFO Train: [62/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2923 (0.3092)	loss 1.1003 (1.3446)	grad_norm 31.5909 (30.2784)	mem 4879MB
[2022-05-31 07:05:26 MetaFG_0] (main.py 265): INFO Train: [62/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2920 (0.3090)	loss 1.0325 (1.3440)	grad_norm 23.0530 (30.0395)	mem 4879MB
[2022-05-31 07:05:29 MetaFG_0] (main.py 265): INFO Train: [62/300][250/1562]	eta 0:06:45 lr 0.000006	time 0.2960 (0.3089)	loss 1.5537 (1.3445)	grad_norm 17.8200 (29.9599)	mem 4879MB
[2022-05-31 07:05:32 MetaFG_0] (main.py 265): INFO Train: [62/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.3014 (0.3088)	loss 1.3178 (1.3462)	grad_norm 22.9304 (29.9220)	mem 4879MB
[2022-05-31 07:05:35 MetaFG_0] (main.py 265): INFO Train: [62/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2994 (0.3087)	loss 1.3972 (1.3482)	grad_norm 26.9294 (29.7806)	mem 4879MB
[2022-05-31 07:05:38 MetaFG_0] (main.py 265): INFO Train: [62/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.2995 (0.3087)	loss 1.3293 (1.3475)	grad_norm 30.4189 (30.0482)	mem 4879MB
[2022-05-31 07:05:41 MetaFG_0] (main.py 265): INFO Train: [62/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2933 (0.3086)	loss 1.1447 (1.3445)	grad_norm 30.6496 (29.8781)	mem 4879MB
[2022-05-31 07:05:44 MetaFG_0] (main.py 265): INFO Train: [62/300][300/1562]	eta 0:06:29 lr 0.000006	time 0.2986 (0.3084)	loss 0.9748 (1.3442)	grad_norm 29.0353 (29.7898)	mem 4879MB
[2022-05-31 07:05:47 MetaFG_0] (main.py 265): INFO Train: [62/300][310/1562]	eta 0:06:26 lr 0.000006	time 0.2991 (0.3084)	loss 1.5961 (1.3455)	grad_norm 28.1167 (29.6468)	mem 4879MB
[2022-05-31 07:05:50 MetaFG_0] (main.py 265): INFO Train: [62/300][320/1562]	eta 0:06:23 lr 0.000006	time 0.2990 (0.3084)	loss 1.5263 (1.3460)	grad_norm 23.7605 (29.5731)	mem 4879MB
[2022-05-31 07:05:53 MetaFG_0] (main.py 265): INFO Train: [62/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2921 (0.3084)	loss 0.9175 (1.3427)	grad_norm 34.7688 (29.6001)	mem 4879MB
[2022-05-31 07:05:56 MetaFG_0] (main.py 265): INFO Train: [62/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.3015 (0.3083)	loss 1.5361 (1.3456)	grad_norm 16.8288 (29.4568)	mem 4879MB
[2022-05-31 07:05:59 MetaFG_0] (main.py 265): INFO Train: [62/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2948 (0.3083)	loss 1.2231 (1.3433)	grad_norm 25.9580 (29.3679)	mem 4879MB
[2022-05-31 07:06:02 MetaFG_0] (main.py 265): INFO Train: [62/300][360/1562]	eta 0:06:10 lr 0.000006	time 0.3037 (0.3082)	loss 1.0769 (1.3423)	grad_norm 25.2310 (29.2846)	mem 4879MB
[2022-05-31 07:06:05 MetaFG_0] (main.py 265): INFO Train: [62/300][370/1562]	eta 0:06:07 lr 0.000006	time 0.2941 (0.3081)	loss 1.6755 (1.3426)	grad_norm 27.5146 (29.3250)	mem 4879MB
[2022-05-31 07:06:08 MetaFG_0] (main.py 265): INFO Train: [62/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2939 (0.3081)	loss 1.9501 (1.3437)	grad_norm 98.3971 (29.3882)	mem 4879MB
[2022-05-31 07:06:12 MetaFG_0] (main.py 265): INFO Train: [62/300][390/1562]	eta 0:06:01 lr 0.000006	time 0.2941 (0.3080)	loss 1.4501 (1.3450)	grad_norm 30.8326 (29.3625)	mem 4879MB
[2022-05-31 07:06:15 MetaFG_0] (main.py 265): INFO Train: [62/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2964 (0.3080)	loss 1.4820 (1.3444)	grad_norm 43.8125 (29.4137)	mem 4879MB
[2022-05-31 07:06:18 MetaFG_0] (main.py 265): INFO Train: [62/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2934 (0.3079)	loss 1.5614 (1.3426)	grad_norm 30.7792 (29.4006)	mem 4879MB
[2022-05-31 07:06:21 MetaFG_0] (main.py 265): INFO Train: [62/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2925 (0.3078)	loss 0.9884 (1.3420)	grad_norm 52.6737 (29.3863)	mem 4879MB
[2022-05-31 07:06:24 MetaFG_0] (main.py 265): INFO Train: [62/300][430/1562]	eta 0:05:48 lr 0.000006	time 0.2924 (0.3077)	loss 1.4307 (1.3437)	grad_norm 16.1647 (29.3647)	mem 4879MB
[2022-05-31 07:06:27 MetaFG_0] (main.py 265): INFO Train: [62/300][440/1562]	eta 0:05:45 lr 0.000006	time 0.2994 (0.3077)	loss 1.3202 (1.3426)	grad_norm 30.1224 (29.4118)	mem 4879MB
[2022-05-31 07:06:30 MetaFG_0] (main.py 265): INFO Train: [62/300][450/1562]	eta 0:05:42 lr 0.000006	time 0.2922 (0.3076)	loss 1.4287 (1.3457)	grad_norm 19.5622 (29.3906)	mem 4879MB
[2022-05-31 07:06:33 MetaFG_0] (main.py 265): INFO Train: [62/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2923 (0.3076)	loss 1.2623 (1.3459)	grad_norm 21.7184 (29.6422)	mem 4879MB
[2022-05-31 07:06:36 MetaFG_0] (main.py 265): INFO Train: [62/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2918 (0.3075)	loss 1.5345 (1.3461)	grad_norm 39.0355 (29.6276)	mem 4879MB
[2022-05-31 07:06:39 MetaFG_0] (main.py 265): INFO Train: [62/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.2942 (0.3075)	loss 1.5771 (1.3449)	grad_norm 28.3581 (29.6383)	mem 4879MB
[2022-05-31 07:06:42 MetaFG_0] (main.py 265): INFO Train: [62/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.3014 (0.3074)	loss 1.4394 (1.3464)	grad_norm 18.8879 (29.6256)	mem 4879MB
[2022-05-31 07:06:45 MetaFG_0] (main.py 265): INFO Train: [62/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.3015 (0.3074)	loss 1.5707 (1.3491)	grad_norm 36.3085 (29.5928)	mem 4879MB
[2022-05-31 07:06:48 MetaFG_0] (main.py 265): INFO Train: [62/300][510/1562]	eta 0:05:23 lr 0.000006	time 0.2982 (0.3074)	loss 1.3743 (1.3522)	grad_norm 25.6145 (29.5423)	mem 4879MB
[2022-05-31 07:06:51 MetaFG_0] (main.py 265): INFO Train: [62/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2945 (0.3073)	loss 1.3741 (1.3538)	grad_norm 26.0924 (29.5654)	mem 4879MB
[2022-05-31 07:06:54 MetaFG_0] (main.py 265): INFO Train: [62/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2936 (0.3073)	loss 1.3016 (1.3529)	grad_norm 23.5942 (29.5587)	mem 4879MB
[2022-05-31 07:06:57 MetaFG_0] (main.py 265): INFO Train: [62/300][540/1562]	eta 0:05:14 lr 0.000006	time 0.2939 (0.3073)	loss 1.1250 (1.3534)	grad_norm 50.9866 (29.5275)	mem 4879MB
[2022-05-31 07:07:00 MetaFG_0] (main.py 265): INFO Train: [62/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2954 (0.3072)	loss 1.4517 (1.3547)	grad_norm 51.1516 (29.5169)	mem 4879MB
[2022-05-31 07:07:03 MetaFG_0] (main.py 265): INFO Train: [62/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2986 (0.3072)	loss 1.3519 (1.3535)	grad_norm 21.2090 (29.5429)	mem 4879MB
[2022-05-31 07:07:06 MetaFG_0] (main.py 265): INFO Train: [62/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2951 (0.3072)	loss 1.5617 (1.3534)	grad_norm 30.0678 (29.5184)	mem 4879MB
[2022-05-31 07:07:10 MetaFG_0] (main.py 265): INFO Train: [62/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.2931 (0.3071)	loss 0.7531 (1.3515)	grad_norm 27.8793 (29.4601)	mem 4879MB
[2022-05-31 07:07:13 MetaFG_0] (main.py 265): INFO Train: [62/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2935 (0.3071)	loss 1.3229 (1.3527)	grad_norm 23.1091 (29.4409)	mem 4879MB
[2022-05-31 07:07:16 MetaFG_0] (main.py 265): INFO Train: [62/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2919 (0.3071)	loss 1.2406 (1.3535)	grad_norm 35.4061 (29.5135)	mem 4879MB
[2022-05-31 07:07:19 MetaFG_0] (main.py 265): INFO Train: [62/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2924 (0.3071)	loss 1.4082 (1.3523)	grad_norm 21.6234 (29.5204)	mem 4879MB
[2022-05-31 07:07:22 MetaFG_0] (main.py 265): INFO Train: [62/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2938 (0.3070)	loss 1.2847 (1.3510)	grad_norm 28.3535 (29.5453)	mem 4879MB
[2022-05-31 07:07:25 MetaFG_0] (main.py 265): INFO Train: [62/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2983 (0.3070)	loss 1.4073 (1.3509)	grad_norm 22.4923 (29.5131)	mem 4879MB
[2022-05-31 07:07:28 MetaFG_0] (main.py 265): INFO Train: [62/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2987 (0.3070)	loss 1.1359 (1.3484)	grad_norm 25.5442 (29.5468)	mem 4879MB
[2022-05-31 07:07:31 MetaFG_0] (main.py 265): INFO Train: [62/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.3007 (0.3070)	loss 1.1058 (1.3465)	grad_norm 26.4868 (29.5011)	mem 4879MB
[2022-05-31 07:07:34 MetaFG_0] (main.py 265): INFO Train: [62/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2932 (0.3070)	loss 1.3336 (1.3461)	grad_norm 40.3102 (29.5291)	mem 4879MB
[2022-05-31 07:07:37 MetaFG_0] (main.py 265): INFO Train: [62/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2933 (0.3070)	loss 1.5619 (1.3441)	grad_norm 27.3077 (29.4754)	mem 4879MB
[2022-05-31 07:07:40 MetaFG_0] (main.py 265): INFO Train: [62/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.3001 (0.3070)	loss 1.4632 (1.3424)	grad_norm 30.6401 (29.5037)	mem 4879MB
[2022-05-31 07:07:43 MetaFG_0] (main.py 265): INFO Train: [62/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2983 (0.3070)	loss 0.9041 (1.3429)	grad_norm 29.3555 (29.5936)	mem 4879MB
[2022-05-31 07:07:46 MetaFG_0] (main.py 265): INFO Train: [62/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.2934 (0.3070)	loss 0.7824 (1.3433)	grad_norm 19.3013 (29.5839)	mem 4879MB
[2022-05-31 07:07:49 MetaFG_0] (main.py 265): INFO Train: [62/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2948 (0.3069)	loss 1.6328 (1.3441)	grad_norm 46.7976 (29.5494)	mem 4879MB
[2022-05-31 07:07:52 MetaFG_0] (main.py 265): INFO Train: [62/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2997 (0.3069)	loss 1.1553 (1.3432)	grad_norm 17.0689 (29.6121)	mem 4879MB
[2022-05-31 07:07:55 MetaFG_0] (main.py 265): INFO Train: [62/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2952 (0.3069)	loss 1.1979 (1.3446)	grad_norm 34.6291 (29.5668)	mem 4879MB
[2022-05-31 07:07:58 MetaFG_0] (main.py 265): INFO Train: [62/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2955 (0.3068)	loss 1.4959 (1.3444)	grad_norm 19.1302 (29.4889)	mem 4879MB
[2022-05-31 07:08:02 MetaFG_0] (main.py 265): INFO Train: [62/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2936 (0.3068)	loss 1.5486 (1.3463)	grad_norm 19.7796 (29.4343)	mem 4879MB
[2022-05-31 07:08:05 MetaFG_0] (main.py 265): INFO Train: [62/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.2932 (0.3068)	loss 1.2997 (1.3463)	grad_norm 35.0695 (29.4809)	mem 4879MB
[2022-05-31 07:08:08 MetaFG_0] (main.py 265): INFO Train: [62/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2978 (0.3068)	loss 1.1385 (1.3465)	grad_norm 20.4830 (29.4994)	mem 4879MB
[2022-05-31 07:08:11 MetaFG_0] (main.py 265): INFO Train: [62/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2933 (0.3067)	loss 1.3809 (1.3452)	grad_norm 18.8827 (29.4962)	mem 4879MB
[2022-05-31 07:08:14 MetaFG_0] (main.py 265): INFO Train: [62/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2985 (0.3067)	loss 0.8367 (1.3421)	grad_norm 45.1265 (29.5092)	mem 4879MB
[2022-05-31 07:08:17 MetaFG_0] (main.py 265): INFO Train: [62/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2985 (0.3068)	loss 0.9167 (1.3401)	grad_norm 29.8115 (29.5351)	mem 4879MB
[2022-05-31 07:08:20 MetaFG_0] (main.py 265): INFO Train: [62/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2922 (0.3067)	loss 1.6270 (1.3401)	grad_norm 23.2601 (29.5504)	mem 4879MB
[2022-05-31 07:08:23 MetaFG_0] (main.py 265): INFO Train: [62/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2926 (0.3067)	loss 1.1055 (1.3402)	grad_norm 21.2514 (29.5536)	mem 4879MB
[2022-05-31 07:08:26 MetaFG_0] (main.py 265): INFO Train: [62/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2918 (0.3067)	loss 1.3975 (1.3410)	grad_norm 23.7839 (29.5258)	mem 4879MB
[2022-05-31 07:08:29 MetaFG_0] (main.py 265): INFO Train: [62/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2930 (0.3066)	loss 1.4920 (1.3420)	grad_norm 29.3425 (29.5484)	mem 4879MB
[2022-05-31 07:08:32 MetaFG_0] (main.py 265): INFO Train: [62/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2989 (0.3066)	loss 1.1712 (1.3418)	grad_norm 28.6608 (29.5220)	mem 4879MB
[2022-05-31 07:08:35 MetaFG_0] (main.py 265): INFO Train: [62/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.2928 (0.3066)	loss 1.5304 (1.3429)	grad_norm 30.9990 (29.5085)	mem 4879MB
[2022-05-31 07:08:38 MetaFG_0] (main.py 265): INFO Train: [62/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.2985 (0.3066)	loss 1.6389 (1.3437)	grad_norm 25.4138 (29.5373)	mem 4879MB
[2022-05-31 07:08:41 MetaFG_0] (main.py 265): INFO Train: [62/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2997 (0.3066)	loss 1.3134 (1.3422)	grad_norm 27.0839 (29.5887)	mem 4879MB
[2022-05-31 07:08:44 MetaFG_0] (main.py 265): INFO Train: [62/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.3007 (0.3066)	loss 1.3228 (1.3434)	grad_norm 25.4177 (29.5797)	mem 4879MB
[2022-05-31 07:08:47 MetaFG_0] (main.py 265): INFO Train: [62/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2953 (0.3065)	loss 1.0915 (1.3425)	grad_norm 128.3943 (29.7502)	mem 4879MB
[2022-05-31 07:08:50 MetaFG_0] (main.py 265): INFO Train: [62/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2957 (0.3065)	loss 0.8717 (1.3415)	grad_norm 21.4163 (29.8053)	mem 4879MB
[2022-05-31 07:08:53 MetaFG_0] (main.py 265): INFO Train: [62/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.3032 (0.3066)	loss 1.5905 (1.3412)	grad_norm 18.8576 (29.8329)	mem 4879MB
[2022-05-31 07:08:57 MetaFG_0] (main.py 265): INFO Train: [62/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3022 (0.3066)	loss 1.5946 (1.3399)	grad_norm 25.8062 (29.8017)	mem 4879MB
[2022-05-31 07:09:00 MetaFG_0] (main.py 265): INFO Train: [62/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2933 (0.3066)	loss 1.4990 (1.3405)	grad_norm 14.1058 (29.7508)	mem 4879MB
[2022-05-31 07:09:03 MetaFG_0] (main.py 265): INFO Train: [62/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2952 (0.3065)	loss 1.5236 (1.3399)	grad_norm 28.3726 (29.7326)	mem 4879MB
[2022-05-31 07:09:06 MetaFG_0] (main.py 265): INFO Train: [62/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.3007 (0.3065)	loss 0.8300 (1.3405)	grad_norm 34.9248 (29.7191)	mem 4879MB
[2022-05-31 07:09:09 MetaFG_0] (main.py 265): INFO Train: [62/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2940 (0.3065)	loss 1.4719 (1.3394)	grad_norm 19.8387 (29.6711)	mem 4879MB
[2022-05-31 07:09:12 MetaFG_0] (main.py 265): INFO Train: [62/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2966 (0.3065)	loss 0.9930 (1.3389)	grad_norm 35.4208 (29.6928)	mem 4879MB
[2022-05-31 07:09:15 MetaFG_0] (main.py 265): INFO Train: [62/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2969 (0.3065)	loss 1.6268 (1.3388)	grad_norm 21.3571 (29.6878)	mem 4879MB
[2022-05-31 07:09:18 MetaFG_0] (main.py 265): INFO Train: [62/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2975 (0.3065)	loss 1.7198 (1.3392)	grad_norm 26.8105 (29.6498)	mem 4879MB
[2022-05-31 07:09:21 MetaFG_0] (main.py 265): INFO Train: [62/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.3002 (0.3065)	loss 1.3601 (1.3398)	grad_norm 20.2822 (29.6302)	mem 4879MB
[2022-05-31 07:09:24 MetaFG_0] (main.py 265): INFO Train: [62/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2992 (0.3065)	loss 0.9713 (1.3392)	grad_norm 23.9109 (29.6433)	mem 4879MB
[2022-05-31 07:09:27 MetaFG_0] (main.py 265): INFO Train: [62/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2928 (0.3065)	loss 1.6299 (1.3401)	grad_norm 43.6093 (29.7144)	mem 4879MB
[2022-05-31 07:09:30 MetaFG_0] (main.py 265): INFO Train: [62/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3020 (0.3065)	loss 1.6869 (1.3398)	grad_norm 29.5575 (29.7851)	mem 4879MB
[2022-05-31 07:09:33 MetaFG_0] (main.py 265): INFO Train: [62/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2998 (0.3065)	loss 1.1340 (1.3395)	grad_norm 23.7295 (29.7809)	mem 4879MB
[2022-05-31 07:09:36 MetaFG_0] (main.py 265): INFO Train: [62/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2934 (0.3064)	loss 0.9725 (1.3389)	grad_norm 32.0404 (29.7799)	mem 4879MB
[2022-05-31 07:09:39 MetaFG_0] (main.py 265): INFO Train: [62/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2987 (0.3064)	loss 1.2756 (1.3385)	grad_norm 24.9317 (29.7776)	mem 4879MB
[2022-05-31 07:09:42 MetaFG_0] (main.py 265): INFO Train: [62/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3009 (0.3064)	loss 1.4315 (1.3385)	grad_norm 30.7450 (29.7522)	mem 4879MB
[2022-05-31 07:09:45 MetaFG_0] (main.py 265): INFO Train: [62/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2934 (0.3064)	loss 1.4252 (1.3381)	grad_norm 29.7287 (29.7432)	mem 4879MB
[2022-05-31 07:09:48 MetaFG_0] (main.py 265): INFO Train: [62/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2936 (0.3064)	loss 1.5141 (1.3383)	grad_norm 18.4058 (29.7003)	mem 4879MB
[2022-05-31 07:09:52 MetaFG_0] (main.py 265): INFO Train: [62/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2999 (0.3064)	loss 1.2570 (1.3388)	grad_norm 48.9190 (29.7142)	mem 4879MB
[2022-05-31 07:09:55 MetaFG_0] (main.py 265): INFO Train: [62/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2994 (0.3064)	loss 1.4602 (1.3390)	grad_norm 38.5267 (29.6989)	mem 4879MB
[2022-05-31 07:09:58 MetaFG_0] (main.py 265): INFO Train: [62/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2925 (0.3064)	loss 1.2697 (1.3378)	grad_norm 23.7470 (29.6762)	mem 4879MB
[2022-05-31 07:10:01 MetaFG_0] (main.py 265): INFO Train: [62/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2934 (0.3064)	loss 1.3509 (1.3388)	grad_norm 27.6929 (29.7081)	mem 4879MB
[2022-05-31 07:10:04 MetaFG_0] (main.py 265): INFO Train: [62/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2938 (0.3064)	loss 1.5520 (1.3387)	grad_norm 31.7035 (29.7133)	mem 4879MB
[2022-05-31 07:10:07 MetaFG_0] (main.py 265): INFO Train: [62/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2937 (0.3064)	loss 0.7795 (1.3384)	grad_norm 20.0670 (29.7280)	mem 4879MB
[2022-05-31 07:10:10 MetaFG_0] (main.py 265): INFO Train: [62/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2986 (0.3064)	loss 1.4212 (1.3386)	grad_norm 22.8930 (29.7152)	mem 4879MB
[2022-05-31 07:10:13 MetaFG_0] (main.py 265): INFO Train: [62/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.3005 (0.3064)	loss 1.5540 (1.3386)	grad_norm 30.5277 (29.7149)	mem 4879MB
[2022-05-31 07:10:16 MetaFG_0] (main.py 265): INFO Train: [62/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2922 (0.3064)	loss 1.1191 (1.3383)	grad_norm 44.0279 (29.7370)	mem 4879MB
[2022-05-31 07:10:19 MetaFG_0] (main.py 265): INFO Train: [62/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.3168 (0.3066)	loss 1.5018 (1.3380)	grad_norm 31.1417 (29.7142)	mem 4879MB
[2022-05-31 07:10:22 MetaFG_0] (main.py 265): INFO Train: [62/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.3020 (0.3066)	loss 1.3594 (1.3378)	grad_norm 34.6861 (29.7165)	mem 4879MB
[2022-05-31 07:10:25 MetaFG_0] (main.py 265): INFO Train: [62/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2938 (0.3066)	loss 1.5119 (1.3387)	grad_norm 18.3187 (29.7120)	mem 4879MB
[2022-05-31 07:10:29 MetaFG_0] (main.py 265): INFO Train: [62/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2939 (0.3066)	loss 1.4530 (1.3387)	grad_norm 24.2298 (29.7298)	mem 4879MB
[2022-05-31 07:10:32 MetaFG_0] (main.py 265): INFO Train: [62/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2979 (0.3066)	loss 1.2120 (1.3394)	grad_norm 25.5166 (29.7379)	mem 4879MB
[2022-05-31 07:10:35 MetaFG_0] (main.py 265): INFO Train: [62/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3017 (0.3066)	loss 1.3990 (1.3392)	grad_norm 29.9637 (29.7515)	mem 4879MB
[2022-05-31 07:10:38 MetaFG_0] (main.py 265): INFO Train: [62/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2936 (0.3066)	loss 1.6683 (1.3388)	grad_norm 32.7105 (29.7436)	mem 4879MB
[2022-05-31 07:10:41 MetaFG_0] (main.py 265): INFO Train: [62/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2987 (0.3065)	loss 1.4477 (1.3395)	grad_norm 40.7439 (29.7641)	mem 4879MB
[2022-05-31 07:10:44 MetaFG_0] (main.py 265): INFO Train: [62/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.3004 (0.3065)	loss 1.1681 (1.3388)	grad_norm 25.5630 (29.7543)	mem 4879MB
[2022-05-31 07:10:47 MetaFG_0] (main.py 265): INFO Train: [62/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2942 (0.3065)	loss 1.5234 (1.3381)	grad_norm 24.4137 (29.7406)	mem 4879MB
[2022-05-31 07:10:50 MetaFG_0] (main.py 265): INFO Train: [62/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2988 (0.3065)	loss 1.0058 (1.3370)	grad_norm 30.6084 (29.7563)	mem 4879MB
[2022-05-31 07:10:53 MetaFG_0] (main.py 265): INFO Train: [62/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.3000 (0.3065)	loss 1.3582 (1.3372)	grad_norm 28.3269 (29.7505)	mem 4879MB
[2022-05-31 07:10:56 MetaFG_0] (main.py 265): INFO Train: [62/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.3005 (0.3065)	loss 1.4795 (1.3378)	grad_norm 54.6213 (29.7457)	mem 4879MB
[2022-05-31 07:10:59 MetaFG_0] (main.py 265): INFO Train: [62/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2924 (0.3065)	loss 0.9467 (1.3369)	grad_norm 30.7605 (29.7410)	mem 4879MB
[2022-05-31 07:11:02 MetaFG_0] (main.py 265): INFO Train: [62/300][1340/1562]	eta 0:01:08 lr 0.000006	time 0.3009 (0.3065)	loss 1.3724 (1.3362)	grad_norm 42.2997 (29.7756)	mem 4879MB
[2022-05-31 07:11:05 MetaFG_0] (main.py 265): INFO Train: [62/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.3000 (0.3065)	loss 0.7674 (1.3357)	grad_norm 19.6325 (29.7385)	mem 4879MB
[2022-05-31 07:11:08 MetaFG_0] (main.py 265): INFO Train: [62/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2934 (0.3065)	loss 1.3559 (1.3356)	grad_norm 26.9581 (29.7296)	mem 4879MB
[2022-05-31 07:11:11 MetaFG_0] (main.py 265): INFO Train: [62/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2970 (0.3065)	loss 1.1974 (1.3347)	grad_norm 20.4062 (29.7712)	mem 4879MB
[2022-05-31 07:11:14 MetaFG_0] (main.py 265): INFO Train: [62/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2958 (0.3064)	loss 0.9459 (1.3345)	grad_norm 24.8272 (29.7989)	mem 4879MB
[2022-05-31 07:11:17 MetaFG_0] (main.py 265): INFO Train: [62/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2982 (0.3065)	loss 1.4437 (1.3345)	grad_norm 41.0203 (29.8158)	mem 4879MB
[2022-05-31 07:11:20 MetaFG_0] (main.py 265): INFO Train: [62/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2991 (0.3065)	loss 1.2676 (1.3343)	grad_norm 48.2688 (29.8071)	mem 4879MB
[2022-05-31 07:11:23 MetaFG_0] (main.py 265): INFO Train: [62/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3044 (0.3064)	loss 1.6334 (1.3346)	grad_norm 31.5894 (29.8263)	mem 4879MB
[2022-05-31 07:11:27 MetaFG_0] (main.py 265): INFO Train: [62/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2960 (0.3064)	loss 1.5739 (1.3350)	grad_norm 31.6813 (29.7918)	mem 4879MB
[2022-05-31 07:11:30 MetaFG_0] (main.py 265): INFO Train: [62/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2997 (0.3064)	loss 1.6238 (1.3351)	grad_norm 33.1095 (29.7803)	mem 4879MB
[2022-05-31 07:11:33 MetaFG_0] (main.py 265): INFO Train: [62/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.3041 (0.3064)	loss 1.4271 (1.3353)	grad_norm 24.1188 (29.7834)	mem 4879MB
[2022-05-31 07:11:36 MetaFG_0] (main.py 265): INFO Train: [62/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.3012 (0.3064)	loss 1.3391 (1.3349)	grad_norm 34.0926 (29.8189)	mem 4879MB
[2022-05-31 07:11:39 MetaFG_0] (main.py 265): INFO Train: [62/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2983 (0.3064)	loss 1.5141 (1.3353)	grad_norm 23.2499 (29.7962)	mem 4879MB
[2022-05-31 07:11:42 MetaFG_0] (main.py 265): INFO Train: [62/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2959 (0.3064)	loss 1.3272 (1.3356)	grad_norm 25.5508 (29.7746)	mem 4879MB
[2022-05-31 07:11:45 MetaFG_0] (main.py 265): INFO Train: [62/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2934 (0.3064)	loss 1.3015 (1.3360)	grad_norm 22.9632 (29.7646)	mem 4879MB
[2022-05-31 07:11:48 MetaFG_0] (main.py 265): INFO Train: [62/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2925 (0.3064)	loss 1.1950 (1.3361)	grad_norm 24.6661 (29.7264)	mem 4879MB
[2022-05-31 07:11:51 MetaFG_0] (main.py 265): INFO Train: [62/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2987 (0.3064)	loss 1.4002 (1.3363)	grad_norm 29.8770 (29.7342)	mem 4879MB
[2022-05-31 07:11:54 MetaFG_0] (main.py 265): INFO Train: [62/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2996 (0.3064)	loss 1.5311 (1.3367)	grad_norm 28.7169 (29.7540)	mem 4879MB
[2022-05-31 07:11:57 MetaFG_0] (main.py 265): INFO Train: [62/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2995 (0.3064)	loss 1.3947 (1.3369)	grad_norm 34.9624 (29.7636)	mem 4879MB
[2022-05-31 07:12:00 MetaFG_0] (main.py 265): INFO Train: [62/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2994 (0.3064)	loss 1.3303 (1.3377)	grad_norm 23.5307 (29.7474)	mem 4879MB
[2022-05-31 07:12:03 MetaFG_0] (main.py 265): INFO Train: [62/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2932 (0.3064)	loss 1.6400 (1.3380)	grad_norm 34.1586 (29.7335)	mem 4879MB
[2022-05-31 07:12:06 MetaFG_0] (main.py 265): INFO Train: [62/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2932 (0.3064)	loss 1.5473 (1.3381)	grad_norm 20.4557 (29.7766)	mem 4879MB
[2022-05-31 07:12:09 MetaFG_0] (main.py 265): INFO Train: [62/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2946 (0.3063)	loss 1.7566 (1.3378)	grad_norm 27.0967 (29.8002)	mem 4879MB
[2022-05-31 07:12:10 MetaFG_0] (main.py 272): INFO EPOCH 62 training takes 0:07:58
[2022-05-31 07:12:10 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_62.pth saving......
[2022-05-31 07:12:11 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_62.pth saved !!!
[2022-05-31 07:12:11 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:12:12 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:12:12 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:12:13 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.695 (0.695)	Loss 0.5047 (0.5047)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 07:12:14 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.152)	Loss 1.0708 (0.5443)	Acc@1 71.875 (89.489)	Acc@5 96.875 (99.148)	Mem 4879MB
[2022-05-31 07:12:15 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.103 (0.124)	Loss 0.7737 (0.5754)	Acc@1 75.000 (87.649)	Acc@5 96.875 (98.958)	Mem 4879MB
[2022-05-31 07:12:16 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.097 (0.116)	Loss 1.0734 (0.6171)	Acc@1 65.625 (86.694)	Acc@5 100.000 (98.992)	Mem 4879MB
[2022-05-31 07:12:17 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.111)	Loss 0.7180 (0.6285)	Acc@1 81.250 (86.128)	Acc@5 100.000 (99.162)	Mem 4879MB
[2022-05-31 07:12:18 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.108)	Loss 0.7310 (0.6148)	Acc@1 81.250 (86.520)	Acc@5 100.000 (99.203)	Mem 4879MB
[2022-05-31 07:12:19 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.105)	Loss 0.6054 (0.6219)	Acc@1 90.625 (86.475)	Acc@5 93.750 (99.129)	Mem 4879MB
[2022-05-31 07:12:20 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.099 (0.104)	Loss 0.5997 (0.6191)	Acc@1 90.625 (86.620)	Acc@5 96.875 (99.032)	Mem 4879MB
[2022-05-31 07:12:21 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.099 (0.103)	Loss 0.7052 (0.6224)	Acc@1 78.125 (86.613)	Acc@5 96.875 (98.881)	Mem 4879MB
[2022-05-31 07:12:21 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.102)	Loss 0.6136 (0.6159)	Acc@1 87.500 (86.882)	Acc@5 100.000 (98.867)	Mem 4879MB
[2022-05-31 07:12:22 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.095 (0.101)	Loss 0.5855 (0.6257)	Acc@1 87.500 (86.355)	Acc@5 96.875 (98.762)	Mem 4879MB
[2022-05-31 07:12:23 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.103 (0.100)	Loss 0.6107 (0.6215)	Acc@1 87.500 (86.430)	Acc@5 100.000 (98.846)	Mem 4879MB
[2022-05-31 07:12:24 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 0.5888 (0.6195)	Acc@1 87.500 (86.493)	Acc@5 100.000 (98.812)	Mem 4879MB
[2022-05-31 07:12:25 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.088 (0.100)	Loss 0.5381 (0.6243)	Acc@1 93.750 (86.427)	Acc@5 100.000 (98.783)	Mem 4879MB
[2022-05-31 07:12:26 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.099)	Loss 0.8749 (0.6211)	Acc@1 84.375 (86.503)	Acc@5 96.875 (98.825)	Mem 4879MB
[2022-05-31 07:12:27 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.101 (0.099)	Loss 0.3187 (0.6214)	Acc@1 100.000 (86.548)	Acc@5 100.000 (98.738)	Mem 4879MB
[2022-05-31 07:12:28 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.099)	Loss 0.6212 (0.6172)	Acc@1 84.375 (86.724)	Acc@5 96.875 (98.738)	Mem 4879MB
[2022-05-31 07:12:29 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.099)	Loss 0.5969 (0.6201)	Acc@1 87.500 (86.678)	Acc@5 96.875 (98.702)	Mem 4879MB
[2022-05-31 07:12:30 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.098)	Loss 0.5073 (0.6180)	Acc@1 87.500 (86.671)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 07:12:31 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.098)	Loss 0.6542 (0.6177)	Acc@1 84.375 (86.616)	Acc@5 96.875 (98.658)	Mem 4879MB
[2022-05-31 07:12:32 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.095 (0.098)	Loss 0.4785 (0.6165)	Acc@1 93.750 (86.692)	Acc@5 100.000 (98.663)	Mem 4879MB
[2022-05-31 07:12:33 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.098)	Loss 0.6186 (0.6196)	Acc@1 78.125 (86.582)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 07:12:34 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.098)	Loss 0.5146 (0.6160)	Acc@1 87.500 (86.722)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 07:12:35 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.109 (0.098)	Loss 0.3600 (0.6170)	Acc@1 96.875 (86.729)	Acc@5 100.000 (98.715)	Mem 4879MB
[2022-05-31 07:12:36 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.6127 (0.6152)	Acc@1 87.500 (86.787)	Acc@5 100.000 (98.742)	Mem 4879MB
[2022-05-31 07:12:37 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.9459 (0.6163)	Acc@1 81.250 (86.678)	Acc@5 100.000 (98.780)	Mem 4879MB
[2022-05-31 07:12:38 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 0.6342 (0.6166)	Acc@1 84.375 (86.686)	Acc@5 100.000 (98.743)	Mem 4879MB
[2022-05-31 07:12:39 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.105 (0.097)	Loss 0.5050 (0.6176)	Acc@1 93.750 (86.635)	Acc@5 96.875 (98.732)	Mem 4879MB
[2022-05-31 07:12:39 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.4690 (0.6180)	Acc@1 96.875 (86.644)	Acc@5 100.000 (98.710)	Mem 4879MB
[2022-05-31 07:12:40 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 0.5182 (0.6178)	Acc@1 90.625 (86.673)	Acc@5 100.000 (98.711)	Mem 4879MB
[2022-05-31 07:12:41 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.097)	Loss 0.7340 (0.6201)	Acc@1 78.125 (86.597)	Acc@5 100.000 (98.681)	Mem 4879MB
[2022-05-31 07:12:42 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.4909 (0.6175)	Acc@1 93.750 (86.696)	Acc@5 96.875 (98.714)	Mem 4879MB
[2022-05-31 07:12:43 MetaFG_0] (main.py 330): INFO  * Acc@1 86.690 Acc@5 98.710
[2022-05-31 07:12:43 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 86.7%
[2022-05-31 07:12:43 MetaFG_0] (main.py 171): INFO Max accuracy: 86.84%
[2022-05-31 07:12:44 MetaFG_0] (main.py 265): INFO Train: [63/300][0/1562]	eta 0:28:58 lr 0.000006	time 1.1129 (1.1129)	loss 1.6452 (1.6452)	grad_norm 16.0583 (16.0583)	mem 4879MB
[2022-05-31 07:12:47 MetaFG_0] (main.py 265): INFO Train: [63/300][10/1562]	eta 0:09:55 lr 0.000006	time 0.2933 (0.3838)	loss 0.8102 (1.3675)	grad_norm 29.5701 (32.5418)	mem 4879MB
[2022-05-31 07:12:50 MetaFG_0] (main.py 265): INFO Train: [63/300][20/1562]	eta 0:08:53 lr 0.000006	time 0.2986 (0.3460)	loss 1.3789 (1.3093)	grad_norm 34.3759 (30.5037)	mem 4879MB
[2022-05-31 07:12:53 MetaFG_0] (main.py 265): INFO Train: [63/300][30/1562]	eta 0:08:28 lr 0.000006	time 0.2994 (0.3321)	loss 1.6259 (1.3652)	grad_norm 45.8677 (32.5978)	mem 4879MB
[2022-05-31 07:12:56 MetaFG_0] (main.py 265): INFO Train: [63/300][40/1562]	eta 0:08:15 lr 0.000006	time 0.2923 (0.3254)	loss 1.4450 (1.3828)	grad_norm 28.1149 (32.1358)	mem 4879MB
[2022-05-31 07:12:59 MetaFG_0] (main.py 265): INFO Train: [63/300][50/1562]	eta 0:08:06 lr 0.000006	time 0.2953 (0.3215)	loss 0.8419 (1.3766)	grad_norm 24.3941 (32.8034)	mem 4879MB
[2022-05-31 07:13:02 MetaFG_0] (main.py 265): INFO Train: [63/300][60/1562]	eta 0:07:58 lr 0.000006	time 0.3018 (0.3189)	loss 1.3062 (1.3757)	grad_norm 25.2229 (33.4961)	mem 4879MB
[2022-05-31 07:13:05 MetaFG_0] (main.py 265): INFO Train: [63/300][70/1562]	eta 0:07:52 lr 0.000006	time 0.2995 (0.3168)	loss 1.6998 (1.3693)	grad_norm 31.8906 (33.2748)	mem 4879MB
[2022-05-31 07:13:08 MetaFG_0] (main.py 265): INFO Train: [63/300][80/1562]	eta 0:07:47 lr 0.000006	time 0.2995 (0.3156)	loss 1.2881 (1.3715)	grad_norm 28.2575 (33.0567)	mem 4879MB
[2022-05-31 07:13:11 MetaFG_0] (main.py 265): INFO Train: [63/300][90/1562]	eta 0:07:42 lr 0.000006	time 0.2921 (0.3142)	loss 1.0604 (1.3685)	grad_norm 22.2544 (32.6309)	mem 4879MB
[2022-05-31 07:13:14 MetaFG_0] (main.py 265): INFO Train: [63/300][100/1562]	eta 0:07:38 lr 0.000006	time 0.2999 (0.3134)	loss 1.5236 (1.3771)	grad_norm 42.0837 (33.0957)	mem 4879MB
[2022-05-31 07:13:17 MetaFG_0] (main.py 265): INFO Train: [63/300][110/1562]	eta 0:07:33 lr 0.000006	time 0.2927 (0.3126)	loss 1.4764 (1.3702)	grad_norm 28.8421 (33.1662)	mem 4879MB
[2022-05-31 07:13:20 MetaFG_0] (main.py 265): INFO Train: [63/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2979 (0.3120)	loss 1.6960 (1.3737)	grad_norm 32.6494 (33.2903)	mem 4879MB
[2022-05-31 07:13:23 MetaFG_0] (main.py 265): INFO Train: [63/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2951 (0.3114)	loss 1.3633 (1.3762)	grad_norm 15.3431 (33.5358)	mem 4879MB
[2022-05-31 07:13:26 MetaFG_0] (main.py 265): INFO Train: [63/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.2939 (0.3111)	loss 1.1686 (1.3795)	grad_norm 30.9224 (33.5629)	mem 4879MB
[2022-05-31 07:13:29 MetaFG_0] (main.py 265): INFO Train: [63/300][150/1562]	eta 0:07:18 lr 0.000006	time 0.2930 (0.3108)	loss 1.5429 (1.3813)	grad_norm 47.7817 (33.5017)	mem 4879MB
[2022-05-31 07:13:33 MetaFG_0] (main.py 265): INFO Train: [63/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.2929 (0.3105)	loss 1.4260 (1.3862)	grad_norm 29.9849 (33.5919)	mem 4879MB
[2022-05-31 07:13:36 MetaFG_0] (main.py 265): INFO Train: [63/300][170/1562]	eta 0:07:11 lr 0.000006	time 0.2922 (0.3101)	loss 1.4643 (1.3796)	grad_norm 25.4086 (33.2790)	mem 4879MB
[2022-05-31 07:13:39 MetaFG_0] (main.py 265): INFO Train: [63/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2983 (0.3099)	loss 1.4328 (1.3715)	grad_norm 20.8404 (32.9839)	mem 4879MB
[2022-05-31 07:13:42 MetaFG_0] (main.py 265): INFO Train: [63/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2988 (0.3098)	loss 0.9438 (1.3688)	grad_norm 26.1749 (32.8661)	mem 4879MB
[2022-05-31 07:13:45 MetaFG_0] (main.py 265): INFO Train: [63/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2928 (0.3095)	loss 1.5790 (1.3673)	grad_norm 17.8380 (32.5619)	mem 4879MB
[2022-05-31 07:13:48 MetaFG_0] (main.py 265): INFO Train: [63/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2929 (0.3093)	loss 1.5706 (1.3689)	grad_norm 33.1939 (32.3097)	mem 4879MB
[2022-05-31 07:13:51 MetaFG_0] (main.py 265): INFO Train: [63/300][220/1562]	eta 0:06:54 lr 0.000006	time 0.2940 (0.3092)	loss 1.5415 (1.3691)	grad_norm 29.1169 (32.1617)	mem 4879MB
[2022-05-31 07:13:54 MetaFG_0] (main.py 265): INFO Train: [63/300][230/1562]	eta 0:06:51 lr 0.000006	time 0.2924 (0.3089)	loss 1.3345 (1.3753)	grad_norm 22.2466 (32.1015)	mem 4879MB
[2022-05-31 07:13:57 MetaFG_0] (main.py 265): INFO Train: [63/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2933 (0.3088)	loss 1.0724 (1.3730)	grad_norm 19.2633 (31.9940)	mem 4879MB
[2022-05-31 07:14:00 MetaFG_0] (main.py 265): INFO Train: [63/300][250/1562]	eta 0:06:44 lr 0.000006	time 0.2987 (0.3087)	loss 1.4645 (1.3691)	grad_norm 18.1794 (31.7353)	mem 4879MB
[2022-05-31 07:14:03 MetaFG_0] (main.py 265): INFO Train: [63/300][260/1562]	eta 0:06:41 lr 0.000006	time 0.2997 (0.3086)	loss 1.3602 (1.3667)	grad_norm 28.8171 (31.8720)	mem 4879MB
[2022-05-31 07:14:06 MetaFG_0] (main.py 265): INFO Train: [63/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2941 (0.3085)	loss 0.8982 (1.3649)	grad_norm 19.8223 (31.7852)	mem 4879MB
[2022-05-31 07:14:09 MetaFG_0] (main.py 265): INFO Train: [63/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.3001 (0.3084)	loss 1.0840 (1.3657)	grad_norm 34.4039 (31.7970)	mem 4879MB
[2022-05-31 07:14:12 MetaFG_0] (main.py 265): INFO Train: [63/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2940 (0.3083)	loss 1.5449 (1.3667)	grad_norm 42.4930 (31.7751)	mem 4879MB
[2022-05-31 07:14:15 MetaFG_0] (main.py 265): INFO Train: [63/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2932 (0.3082)	loss 1.6750 (1.3674)	grad_norm 41.2281 (31.7325)	mem 4879MB
[2022-05-31 07:14:18 MetaFG_0] (main.py 265): INFO Train: [63/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2992 (0.3082)	loss 1.6253 (1.3635)	grad_norm 44.6264 (31.7571)	mem 4879MB
[2022-05-31 07:14:21 MetaFG_0] (main.py 265): INFO Train: [63/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2935 (0.3081)	loss 1.3694 (1.3632)	grad_norm 40.2934 (31.7602)	mem 4879MB
[2022-05-31 07:14:24 MetaFG_0] (main.py 265): INFO Train: [63/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2945 (0.3080)	loss 1.0943 (1.3599)	grad_norm 27.4466 (31.8106)	mem 4879MB
[2022-05-31 07:14:28 MetaFG_0] (main.py 265): INFO Train: [63/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2934 (0.3079)	loss 1.4335 (1.3601)	grad_norm 33.5061 (31.8032)	mem 4879MB
[2022-05-31 07:14:31 MetaFG_0] (main.py 265): INFO Train: [63/300][350/1562]	eta 0:06:13 lr 0.000006	time 0.2986 (0.3078)	loss 1.4551 (1.3634)	grad_norm 58.3561 (31.8050)	mem 4879MB
[2022-05-31 07:14:34 MetaFG_0] (main.py 265): INFO Train: [63/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2991 (0.3077)	loss 1.4809 (1.3628)	grad_norm 44.6836 (31.9288)	mem 4879MB
[2022-05-31 07:14:37 MetaFG_0] (main.py 265): INFO Train: [63/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.3006 (0.3077)	loss 1.2852 (1.3603)	grad_norm 29.1586 (31.8380)	mem 4879MB
[2022-05-31 07:14:40 MetaFG_0] (main.py 265): INFO Train: [63/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2993 (0.3076)	loss 1.2604 (1.3616)	grad_norm 21.8826 (31.6903)	mem 4879MB
[2022-05-31 07:14:43 MetaFG_0] (main.py 265): INFO Train: [63/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.3011 (0.3076)	loss 1.5671 (1.3612)	grad_norm 32.8412 (31.7585)	mem 4879MB
[2022-05-31 07:14:46 MetaFG_0] (main.py 265): INFO Train: [63/300][400/1562]	eta 0:05:57 lr 0.000006	time 0.2990 (0.3076)	loss 1.4255 (1.3623)	grad_norm 48.1667 (31.6311)	mem 4879MB
[2022-05-31 07:14:49 MetaFG_0] (main.py 265): INFO Train: [63/300][410/1562]	eta 0:05:54 lr 0.000006	time 0.2922 (0.3075)	loss 1.5361 (1.3602)	grad_norm 27.2322 (31.5861)	mem 4879MB
[2022-05-31 07:14:52 MetaFG_0] (main.py 265): INFO Train: [63/300][420/1562]	eta 0:05:51 lr 0.000006	time 0.2990 (0.3074)	loss 1.4301 (1.3584)	grad_norm 28.8231 (31.6214)	mem 4879MB
[2022-05-31 07:14:55 MetaFG_0] (main.py 265): INFO Train: [63/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2978 (0.3074)	loss 1.3777 (1.3588)	grad_norm 37.5550 (31.5021)	mem 4879MB
[2022-05-31 07:14:58 MetaFG_0] (main.py 265): INFO Train: [63/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2996 (0.3073)	loss 1.2687 (1.3584)	grad_norm 26.5716 (31.3721)	mem 4879MB
[2022-05-31 07:15:01 MetaFG_0] (main.py 265): INFO Train: [63/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.3024 (0.3073)	loss 1.3725 (1.3585)	grad_norm 56.0813 (31.3301)	mem 4879MB
[2022-05-31 07:15:04 MetaFG_0] (main.py 265): INFO Train: [63/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2929 (0.3072)	loss 1.2509 (1.3585)	grad_norm 24.5496 (31.2338)	mem 4879MB
[2022-05-31 07:15:07 MetaFG_0] (main.py 265): INFO Train: [63/300][470/1562]	eta 0:05:35 lr 0.000006	time 0.2984 (0.3072)	loss 0.9405 (1.3556)	grad_norm 19.1669 (31.2080)	mem 4879MB
[2022-05-31 07:15:10 MetaFG_0] (main.py 265): INFO Train: [63/300][480/1562]	eta 0:05:32 lr 0.000006	time 0.3002 (0.3071)	loss 1.0240 (1.3554)	grad_norm 29.6860 (31.0967)	mem 4879MB
[2022-05-31 07:15:13 MetaFG_0] (main.py 265): INFO Train: [63/300][490/1562]	eta 0:05:29 lr 0.000006	time 0.2933 (0.3071)	loss 0.8960 (1.3545)	grad_norm 34.0866 (30.9983)	mem 4879MB
[2022-05-31 07:15:16 MetaFG_0] (main.py 265): INFO Train: [63/300][500/1562]	eta 0:05:26 lr 0.000006	time 0.3005 (0.3070)	loss 1.3223 (1.3540)	grad_norm 32.7122 (30.9562)	mem 4879MB
[2022-05-31 07:15:19 MetaFG_0] (main.py 265): INFO Train: [63/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2941 (0.3070)	loss 1.0768 (1.3531)	grad_norm 19.0198 (30.9392)	mem 4879MB
[2022-05-31 07:15:22 MetaFG_0] (main.py 265): INFO Train: [63/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2983 (0.3069)	loss 1.4564 (1.3528)	grad_norm 22.5803 (30.8958)	mem 4879MB
[2022-05-31 07:15:25 MetaFG_0] (main.py 265): INFO Train: [63/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2944 (0.3069)	loss 1.4275 (1.3508)	grad_norm 18.8078 (30.8262)	mem 4879MB
[2022-05-31 07:15:29 MetaFG_0] (main.py 265): INFO Train: [63/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2932 (0.3069)	loss 1.2691 (1.3509)	grad_norm 29.0949 (30.8080)	mem 4879MB
[2022-05-31 07:15:32 MetaFG_0] (main.py 265): INFO Train: [63/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2963 (0.3068)	loss 1.1302 (1.3516)	grad_norm 21.6458 (30.7302)	mem 4879MB
[2022-05-31 07:15:35 MetaFG_0] (main.py 265): INFO Train: [63/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2979 (0.3068)	loss 1.3229 (1.3511)	grad_norm 17.3349 (30.7083)	mem 4879MB
[2022-05-31 07:15:38 MetaFG_0] (main.py 265): INFO Train: [63/300][570/1562]	eta 0:05:04 lr 0.000006	time 0.2978 (0.3068)	loss 1.2038 (1.3504)	grad_norm 35.4540 (30.7270)	mem 4879MB
[2022-05-31 07:15:41 MetaFG_0] (main.py 265): INFO Train: [63/300][580/1562]	eta 0:05:01 lr 0.000006	time 0.3084 (0.3068)	loss 1.3708 (1.3506)	grad_norm 34.6542 (30.6304)	mem 4879MB
[2022-05-31 07:15:44 MetaFG_0] (main.py 265): INFO Train: [63/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.3064 (0.3068)	loss 1.4156 (1.3509)	grad_norm 31.3515 (30.6667)	mem 4879MB
[2022-05-31 07:15:47 MetaFG_0] (main.py 265): INFO Train: [63/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2958 (0.3068)	loss 1.2294 (1.3514)	grad_norm 22.8944 (30.6933)	mem 4879MB
[2022-05-31 07:15:50 MetaFG_0] (main.py 265): INFO Train: [63/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2917 (0.3068)	loss 0.9306 (1.3495)	grad_norm 18.2135 (30.6638)	mem 4879MB
[2022-05-31 07:15:53 MetaFG_0] (main.py 265): INFO Train: [63/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.3005 (0.3067)	loss 1.5442 (1.3508)	grad_norm 30.1413 (30.7046)	mem 4879MB
[2022-05-31 07:15:56 MetaFG_0] (main.py 265): INFO Train: [63/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2938 (0.3067)	loss 1.3727 (1.3507)	grad_norm 20.8589 (30.6803)	mem 4879MB
[2022-05-31 07:15:59 MetaFG_0] (main.py 265): INFO Train: [63/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.3003 (0.3067)	loss 1.3071 (1.3504)	grad_norm 20.1812 (30.7409)	mem 4879MB
[2022-05-31 07:16:02 MetaFG_0] (main.py 265): INFO Train: [63/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2954 (0.3067)	loss 1.1400 (1.3496)	grad_norm 48.9272 (30.7894)	mem 4879MB
[2022-05-31 07:16:05 MetaFG_0] (main.py 265): INFO Train: [63/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2928 (0.3066)	loss 1.2505 (1.3489)	grad_norm 16.6042 (30.8415)	mem 4879MB
[2022-05-31 07:16:08 MetaFG_0] (main.py 265): INFO Train: [63/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2936 (0.3066)	loss 1.1922 (1.3488)	grad_norm 25.5829 (30.8565)	mem 4879MB
[2022-05-31 07:16:12 MetaFG_0] (main.py 265): INFO Train: [63/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.3475 (0.3070)	loss 1.4348 (1.3463)	grad_norm 16.4202 (30.8047)	mem 4879MB
[2022-05-31 07:16:15 MetaFG_0] (main.py 265): INFO Train: [63/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.2988 (0.3070)	loss 1.4854 (1.3490)	grad_norm 28.9011 (30.8264)	mem 4879MB
[2022-05-31 07:16:18 MetaFG_0] (main.py 265): INFO Train: [63/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.3037 (0.3070)	loss 1.4514 (1.3483)	grad_norm 33.1648 (30.7686)	mem 4879MB
[2022-05-31 07:16:21 MetaFG_0] (main.py 265): INFO Train: [63/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2933 (0.3070)	loss 1.0060 (1.3469)	grad_norm 34.3314 (30.6929)	mem 4879MB
[2022-05-31 07:16:24 MetaFG_0] (main.py 265): INFO Train: [63/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2977 (0.3070)	loss 1.1177 (1.3476)	grad_norm 34.0787 (30.8390)	mem 4879MB
[2022-05-31 07:16:27 MetaFG_0] (main.py 265): INFO Train: [63/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2934 (0.3070)	loss 1.4651 (1.3480)	grad_norm 31.3703 (30.8236)	mem 4879MB
[2022-05-31 07:16:30 MetaFG_0] (main.py 265): INFO Train: [63/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2986 (0.3070)	loss 1.0252 (1.3475)	grad_norm 28.3293 (30.8361)	mem 4879MB
[2022-05-31 07:16:33 MetaFG_0] (main.py 265): INFO Train: [63/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2961 (0.3069)	loss 1.3404 (1.3472)	grad_norm 35.7584 (30.7874)	mem 4879MB
[2022-05-31 07:16:36 MetaFG_0] (main.py 265): INFO Train: [63/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.2943 (0.3069)	loss 1.6462 (1.3472)	grad_norm 20.3134 (30.7409)	mem 4879MB
[2022-05-31 07:16:39 MetaFG_0] (main.py 265): INFO Train: [63/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.3000 (0.3069)	loss 1.5325 (1.3468)	grad_norm 21.3815 (30.7572)	mem 4879MB
[2022-05-31 07:16:42 MetaFG_0] (main.py 265): INFO Train: [63/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2943 (0.3069)	loss 1.4679 (1.3480)	grad_norm 25.6461 (30.7528)	mem 4879MB
[2022-05-31 07:16:45 MetaFG_0] (main.py 265): INFO Train: [63/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2981 (0.3069)	loss 1.4851 (1.3487)	grad_norm 25.7801 (30.7264)	mem 4879MB
[2022-05-31 07:16:48 MetaFG_0] (main.py 265): INFO Train: [63/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.2931 (0.3069)	loss 1.7332 (1.3476)	grad_norm 70.7005 (30.8225)	mem 4879MB
[2022-05-31 07:16:51 MetaFG_0] (main.py 265): INFO Train: [63/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2948 (0.3068)	loss 1.4382 (1.3483)	grad_norm 23.8908 (30.7785)	mem 4879MB
[2022-05-31 07:16:54 MetaFG_0] (main.py 265): INFO Train: [63/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2925 (0.3068)	loss 1.2366 (1.3476)	grad_norm 27.1505 (30.7622)	mem 4879MB
[2022-05-31 07:16:57 MetaFG_0] (main.py 265): INFO Train: [63/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2931 (0.3068)	loss 1.7731 (1.3474)	grad_norm 30.2750 (30.7545)	mem 4879MB
[2022-05-31 07:17:01 MetaFG_0] (main.py 265): INFO Train: [63/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2929 (0.3068)	loss 1.2672 (1.3484)	grad_norm 29.8989 (30.7617)	mem 4879MB
[2022-05-31 07:17:04 MetaFG_0] (main.py 265): INFO Train: [63/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2942 (0.3068)	loss 1.3615 (1.3493)	grad_norm 28.4751 (30.6968)	mem 4879MB
[2022-05-31 07:17:07 MetaFG_0] (main.py 265): INFO Train: [63/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.3011 (0.3067)	loss 0.9535 (1.3502)	grad_norm 31.8676 (30.6985)	mem 4879MB
[2022-05-31 07:17:10 MetaFG_0] (main.py 265): INFO Train: [63/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3007 (0.3067)	loss 1.1493 (1.3492)	grad_norm 43.9616 (30.7466)	mem 4879MB
[2022-05-31 07:17:13 MetaFG_0] (main.py 265): INFO Train: [63/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.2997 (0.3067)	loss 1.3971 (1.3499)	grad_norm 17.2756 (30.7723)	mem 4879MB
[2022-05-31 07:17:16 MetaFG_0] (main.py 265): INFO Train: [63/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.2995 (0.3067)	loss 1.5174 (1.3502)	grad_norm 37.3774 (30.7577)	mem 4879MB
[2022-05-31 07:17:19 MetaFG_0] (main.py 265): INFO Train: [63/300][900/1562]	eta 0:03:23 lr 0.000006	time 0.2930 (0.3067)	loss 1.3883 (1.3501)	grad_norm 30.2140 (30.6754)	mem 4879MB
[2022-05-31 07:17:22 MetaFG_0] (main.py 265): INFO Train: [63/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2981 (0.3067)	loss 1.4303 (1.3500)	grad_norm 27.1397 (30.6642)	mem 4879MB
[2022-05-31 07:17:25 MetaFG_0] (main.py 265): INFO Train: [63/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2941 (0.3066)	loss 0.9363 (1.3504)	grad_norm 39.6881 (30.6866)	mem 4879MB
[2022-05-31 07:17:28 MetaFG_0] (main.py 265): INFO Train: [63/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2992 (0.3066)	loss 1.3446 (1.3486)	grad_norm 23.1448 (30.6806)	mem 4879MB
[2022-05-31 07:17:31 MetaFG_0] (main.py 265): INFO Train: [63/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2939 (0.3066)	loss 1.4100 (1.3479)	grad_norm 21.0757 (30.6305)	mem 4879MB
[2022-05-31 07:17:34 MetaFG_0] (main.py 265): INFO Train: [63/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.3005 (0.3066)	loss 1.6545 (1.3481)	grad_norm 44.2083 (30.6830)	mem 4879MB
[2022-05-31 07:17:37 MetaFG_0] (main.py 265): INFO Train: [63/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.3002 (0.3066)	loss 1.0354 (1.3478)	grad_norm 50.9745 (30.6805)	mem 4879MB
[2022-05-31 07:17:40 MetaFG_0] (main.py 265): INFO Train: [63/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2916 (0.3066)	loss 1.4416 (1.3498)	grad_norm 77.9557 (30.7541)	mem 4879MB
[2022-05-31 07:17:43 MetaFG_0] (main.py 265): INFO Train: [63/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.2924 (0.3066)	loss 1.4476 (1.3504)	grad_norm 36.1784 (30.7491)	mem 4879MB
[2022-05-31 07:17:46 MetaFG_0] (main.py 265): INFO Train: [63/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2945 (0.3065)	loss 1.5114 (1.3495)	grad_norm 25.6547 (30.7637)	mem 4879MB
[2022-05-31 07:17:49 MetaFG_0] (main.py 265): INFO Train: [63/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2996 (0.3065)	loss 1.5596 (1.3496)	grad_norm 38.7383 (30.7551)	mem 4879MB
[2022-05-31 07:17:52 MetaFG_0] (main.py 265): INFO Train: [63/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2928 (0.3065)	loss 1.7095 (1.3495)	grad_norm 37.2618 (30.7628)	mem 4879MB
[2022-05-31 07:17:55 MetaFG_0] (main.py 265): INFO Train: [63/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2950 (0.3065)	loss 1.2966 (1.3507)	grad_norm 49.2661 (30.7917)	mem 4879MB
[2022-05-31 07:17:59 MetaFG_0] (main.py 265): INFO Train: [63/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.3010 (0.3065)	loss 1.4642 (1.3507)	grad_norm 18.1675 (30.7606)	mem 4879MB
[2022-05-31 07:18:02 MetaFG_0] (main.py 265): INFO Train: [63/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.3005 (0.3065)	loss 1.3978 (1.3514)	grad_norm 22.0905 (30.8049)	mem 4879MB
[2022-05-31 07:18:05 MetaFG_0] (main.py 265): INFO Train: [63/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2950 (0.3065)	loss 1.7381 (1.3525)	grad_norm 31.1756 (30.7953)	mem 4879MB
[2022-05-31 07:18:08 MetaFG_0] (main.py 265): INFO Train: [63/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2926 (0.3065)	loss 1.5706 (1.3530)	grad_norm 27.0864 (30.7718)	mem 4879MB
[2022-05-31 07:18:11 MetaFG_0] (main.py 265): INFO Train: [63/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2926 (0.3065)	loss 1.4502 (1.3525)	grad_norm 34.7391 (30.7609)	mem 4879MB
[2022-05-31 07:18:14 MetaFG_0] (main.py 265): INFO Train: [63/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.3008 (0.3065)	loss 1.5458 (1.3512)	grad_norm 18.5507 (30.7432)	mem 4879MB
[2022-05-31 07:18:17 MetaFG_0] (main.py 265): INFO Train: [63/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2993 (0.3065)	loss 1.4783 (1.3515)	grad_norm 62.4017 (30.7409)	mem 4879MB
[2022-05-31 07:18:20 MetaFG_0] (main.py 265): INFO Train: [63/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2928 (0.3065)	loss 0.9701 (1.3517)	grad_norm 79.3116 (30.7499)	mem 4879MB
[2022-05-31 07:18:23 MetaFG_0] (main.py 265): INFO Train: [63/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2932 (0.3065)	loss 1.2119 (1.3528)	grad_norm 22.1381 (30.7159)	mem 4879MB
[2022-05-31 07:18:26 MetaFG_0] (main.py 265): INFO Train: [63/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2932 (0.3065)	loss 1.4375 (1.3525)	grad_norm 26.7749 (30.7012)	mem 4879MB
[2022-05-31 07:18:29 MetaFG_0] (main.py 265): INFO Train: [63/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2940 (0.3065)	loss 0.9547 (1.3515)	grad_norm 15.7234 (30.6749)	mem 4879MB
[2022-05-31 07:18:32 MetaFG_0] (main.py 265): INFO Train: [63/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2937 (0.3065)	loss 1.4825 (1.3524)	grad_norm 28.4683 (30.6769)	mem 4879MB
[2022-05-31 07:18:35 MetaFG_0] (main.py 265): INFO Train: [63/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2933 (0.3065)	loss 1.0602 (1.3521)	grad_norm 25.1644 (30.7077)	mem 4879MB
[2022-05-31 07:18:38 MetaFG_0] (main.py 265): INFO Train: [63/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2938 (0.3065)	loss 1.5336 (1.3518)	grad_norm 30.4815 (30.7315)	mem 4879MB
[2022-05-31 07:18:41 MetaFG_0] (main.py 265): INFO Train: [63/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2918 (0.3064)	loss 1.2099 (1.3518)	grad_norm 34.5409 (30.7592)	mem 4879MB
[2022-05-31 07:18:44 MetaFG_0] (main.py 265): INFO Train: [63/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.2939 (0.3064)	loss 1.5156 (1.3525)	grad_norm 22.9297 (30.7250)	mem 4879MB
[2022-05-31 07:18:47 MetaFG_0] (main.py 265): INFO Train: [63/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2939 (0.3064)	loss 1.0215 (1.3509)	grad_norm 38.7585 (30.7249)	mem 4879MB
[2022-05-31 07:18:50 MetaFG_0] (main.py 265): INFO Train: [63/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2999 (0.3064)	loss 0.8266 (1.3505)	grad_norm 60.1507 (30.7437)	mem 4879MB
[2022-05-31 07:18:54 MetaFG_0] (main.py 265): INFO Train: [63/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2998 (0.3064)	loss 1.2328 (1.3509)	grad_norm 53.3038 (30.7714)	mem 4879MB
[2022-05-31 07:18:57 MetaFG_0] (main.py 265): INFO Train: [63/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.3001 (0.3064)	loss 1.5708 (1.3501)	grad_norm 13.1878 (30.8601)	mem 4879MB
[2022-05-31 07:19:00 MetaFG_0] (main.py 265): INFO Train: [63/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2978 (0.3064)	loss 1.4527 (1.3504)	grad_norm 25.6213 (30.8103)	mem 4879MB
[2022-05-31 07:19:03 MetaFG_0] (main.py 265): INFO Train: [63/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2922 (0.3064)	loss 1.4451 (1.3503)	grad_norm 25.9975 (30.7832)	mem 4879MB
[2022-05-31 07:19:06 MetaFG_0] (main.py 265): INFO Train: [63/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2926 (0.3064)	loss 1.1751 (1.3502)	grad_norm 38.3796 (30.7918)	mem 4879MB
[2022-05-31 07:19:09 MetaFG_0] (main.py 265): INFO Train: [63/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2927 (0.3064)	loss 1.5706 (1.3508)	grad_norm 22.7658 (30.7749)	mem 4879MB
[2022-05-31 07:19:12 MetaFG_0] (main.py 265): INFO Train: [63/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2991 (0.3064)	loss 1.7802 (1.3515)	grad_norm 39.7496 (30.7427)	mem 4879MB
[2022-05-31 07:19:15 MetaFG_0] (main.py 265): INFO Train: [63/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2933 (0.3064)	loss 1.4065 (1.3503)	grad_norm 20.5208 (30.7310)	mem 4879MB
[2022-05-31 07:19:18 MetaFG_0] (main.py 265): INFO Train: [63/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2939 (0.3064)	loss 1.4019 (1.3500)	grad_norm 28.6916 (30.7232)	mem 4879MB
[2022-05-31 07:19:21 MetaFG_0] (main.py 265): INFO Train: [63/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2995 (0.3064)	loss 0.9485 (1.3493)	grad_norm 33.5877 (30.6877)	mem 4879MB
[2022-05-31 07:19:24 MetaFG_0] (main.py 265): INFO Train: [63/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2996 (0.3064)	loss 1.3855 (1.3485)	grad_norm 21.9957 (30.6886)	mem 4879MB
[2022-05-31 07:19:27 MetaFG_0] (main.py 265): INFO Train: [63/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2917 (0.3063)	loss 1.4843 (1.3486)	grad_norm 26.2097 (30.7645)	mem 4879MB
[2022-05-31 07:19:30 MetaFG_0] (main.py 265): INFO Train: [63/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2941 (0.3063)	loss 1.4752 (1.3495)	grad_norm 26.1665 (30.7581)	mem 4879MB
[2022-05-31 07:19:33 MetaFG_0] (main.py 265): INFO Train: [63/300][1340/1562]	eta 0:01:08 lr 0.000006	time 0.2996 (0.3063)	loss 1.4225 (1.3488)	grad_norm 21.1162 (30.7308)	mem 4879MB
[2022-05-31 07:19:36 MetaFG_0] (main.py 265): INFO Train: [63/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2936 (0.3063)	loss 1.1089 (1.3479)	grad_norm 50.2595 (30.7459)	mem 4879MB
[2022-05-31 07:19:39 MetaFG_0] (main.py 265): INFO Train: [63/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2984 (0.3063)	loss 1.1944 (1.3473)	grad_norm 39.6828 (30.7653)	mem 4879MB
[2022-05-31 07:19:42 MetaFG_0] (main.py 265): INFO Train: [63/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2944 (0.3063)	loss 1.5403 (1.3467)	grad_norm 32.3201 (30.7812)	mem 4879MB
[2022-05-31 07:19:46 MetaFG_0] (main.py 265): INFO Train: [63/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2935 (0.3063)	loss 1.5725 (1.3470)	grad_norm 21.9737 (30.7747)	mem 4879MB
[2022-05-31 07:19:49 MetaFG_0] (main.py 265): INFO Train: [63/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2950 (0.3063)	loss 1.4967 (1.3472)	grad_norm 25.8584 (30.7887)	mem 4879MB
[2022-05-31 07:19:52 MetaFG_0] (main.py 265): INFO Train: [63/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2958 (0.3063)	loss 1.7078 (1.3468)	grad_norm 25.5898 (30.7883)	mem 4879MB
[2022-05-31 07:19:55 MetaFG_0] (main.py 265): INFO Train: [63/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2999 (0.3063)	loss 0.9279 (1.3471)	grad_norm 28.9345 (30.7811)	mem 4879MB
[2022-05-31 07:19:58 MetaFG_0] (main.py 265): INFO Train: [63/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2933 (0.3063)	loss 1.6483 (1.3478)	grad_norm 26.1426 (30.7534)	mem 4879MB
[2022-05-31 07:20:01 MetaFG_0] (main.py 265): INFO Train: [63/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2996 (0.3063)	loss 1.4335 (1.3478)	grad_norm 24.5356 (30.7883)	mem 4879MB
[2022-05-31 07:20:04 MetaFG_0] (main.py 265): INFO Train: [63/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2924 (0.3063)	loss 0.7584 (1.3473)	grad_norm 29.6354 (30.7999)	mem 4879MB
[2022-05-31 07:20:07 MetaFG_0] (main.py 265): INFO Train: [63/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2987 (0.3063)	loss 1.5478 (1.3472)	grad_norm 22.0483 (30.8098)	mem 4879MB
[2022-05-31 07:20:10 MetaFG_0] (main.py 265): INFO Train: [63/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2923 (0.3063)	loss 1.4413 (1.3472)	grad_norm 24.6457 (30.7973)	mem 4879MB
[2022-05-31 07:20:13 MetaFG_0] (main.py 265): INFO Train: [63/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2992 (0.3062)	loss 1.2914 (1.3474)	grad_norm 50.2781 (30.7609)	mem 4879MB
[2022-05-31 07:20:16 MetaFG_0] (main.py 265): INFO Train: [63/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2923 (0.3062)	loss 1.5731 (1.3474)	grad_norm 32.5930 (30.7597)	mem 4879MB
[2022-05-31 07:20:19 MetaFG_0] (main.py 265): INFO Train: [63/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2949 (0.3062)	loss 1.3712 (1.3472)	grad_norm 13.7032 (30.7661)	mem 4879MB
[2022-05-31 07:20:22 MetaFG_0] (main.py 265): INFO Train: [63/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2980 (0.3062)	loss 1.5647 (1.3475)	grad_norm 19.9969 (30.7517)	mem 4879MB
[2022-05-31 07:20:25 MetaFG_0] (main.py 265): INFO Train: [63/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.3014 (0.3062)	loss 1.4783 (1.3473)	grad_norm 23.7069 (30.7425)	mem 4879MB
[2022-05-31 07:20:28 MetaFG_0] (main.py 265): INFO Train: [63/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2921 (0.3062)	loss 1.4942 (1.3468)	grad_norm 16.7686 (30.7301)	mem 4879MB
[2022-05-31 07:20:31 MetaFG_0] (main.py 265): INFO Train: [63/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2992 (0.3062)	loss 1.4174 (1.3465)	grad_norm 28.2463 (30.7187)	mem 4879MB
[2022-05-31 07:20:34 MetaFG_0] (main.py 265): INFO Train: [63/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2954 (0.3062)	loss 1.2316 (1.3467)	grad_norm 40.0385 (30.7104)	mem 4879MB
[2022-05-31 07:20:37 MetaFG_0] (main.py 265): INFO Train: [63/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.3003 (0.3062)	loss 1.2693 (1.3472)	grad_norm 19.5174 (30.6954)	mem 4879MB
[2022-05-31 07:20:40 MetaFG_0] (main.py 265): INFO Train: [63/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2919 (0.3061)	loss 1.0944 (1.3468)	grad_norm 14.9293 (30.6809)	mem 4879MB
[2022-05-31 07:20:41 MetaFG_0] (main.py 272): INFO EPOCH 63 training takes 0:07:58
[2022-05-31 07:20:41 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_63.pth saving......
[2022-05-31 07:20:42 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_63.pth saved !!!
[2022-05-31 07:20:42 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:20:43 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:20:43 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:20:44 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.725 (0.725)	Loss 0.6333 (0.6333)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 07:20:45 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.089 (0.162)	Loss 0.8801 (0.5934)	Acc@1 81.250 (88.636)	Acc@5 96.875 (99.148)	Mem 4879MB
[2022-05-31 07:20:46 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.131)	Loss 0.7025 (0.5834)	Acc@1 81.250 (88.244)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 07:20:47 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.104 (0.119)	Loss 0.7484 (0.5846)	Acc@1 90.625 (88.710)	Acc@5 96.875 (98.387)	Mem 4879MB
[2022-05-31 07:20:48 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.093 (0.112)	Loss 0.5627 (0.5829)	Acc@1 87.500 (88.491)	Acc@5 96.875 (98.552)	Mem 4879MB
[2022-05-31 07:20:49 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.090 (0.109)	Loss 1.2644 (0.5893)	Acc@1 71.875 (88.542)	Acc@5 84.375 (98.346)	Mem 4879MB
[2022-05-31 07:20:50 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.089 (0.106)	Loss 0.7982 (0.6085)	Acc@1 78.125 (87.961)	Acc@5 100.000 (98.463)	Mem 4879MB
[2022-05-31 07:20:51 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.104)	Loss 0.7729 (0.6106)	Acc@1 81.250 (87.896)	Acc@5 100.000 (98.504)	Mem 4879MB
[2022-05-31 07:20:51 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.103)	Loss 0.5592 (0.6026)	Acc@1 87.500 (87.963)	Acc@5 100.000 (98.650)	Mem 4879MB
[2022-05-31 07:20:52 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.102)	Loss 0.3799 (0.6052)	Acc@1 93.750 (87.603)	Acc@5 100.000 (98.764)	Mem 4879MB
[2022-05-31 07:20:53 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.097 (0.101)	Loss 0.5581 (0.6123)	Acc@1 84.375 (87.283)	Acc@5 100.000 (98.762)	Mem 4879MB
[2022-05-31 07:20:54 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 0.6182 (0.6138)	Acc@1 84.375 (87.190)	Acc@5 100.000 (98.733)	Mem 4879MB
[2022-05-31 07:20:55 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.093 (0.100)	Loss 0.5582 (0.6144)	Acc@1 87.500 (87.242)	Acc@5 100.000 (98.735)	Mem 4879MB
[2022-05-31 07:20:56 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.091 (0.099)	Loss 0.7647 (0.6091)	Acc@1 81.250 (87.309)	Acc@5 100.000 (98.760)	Mem 4879MB
[2022-05-31 07:20:57 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.092 (0.099)	Loss 0.5197 (0.6053)	Acc@1 90.625 (87.411)	Acc@5 100.000 (98.781)	Mem 4879MB
[2022-05-31 07:20:58 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.102 (0.099)	Loss 0.8163 (0.6073)	Acc@1 78.125 (87.376)	Acc@5 100.000 (98.800)	Mem 4879MB
[2022-05-31 07:20:59 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.099)	Loss 0.5945 (0.6085)	Acc@1 93.750 (87.384)	Acc@5 96.875 (98.738)	Mem 4879MB
[2022-05-31 07:21:00 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.099 (0.098)	Loss 0.6321 (0.6103)	Acc@1 87.500 (87.317)	Acc@5 96.875 (98.739)	Mem 4879MB
[2022-05-31 07:21:01 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.104 (0.098)	Loss 0.6665 (0.6051)	Acc@1 78.125 (87.396)	Acc@5 100.000 (98.757)	Mem 4879MB
[2022-05-31 07:21:02 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.103 (0.098)	Loss 0.6687 (0.6100)	Acc@1 87.500 (87.287)	Acc@5 100.000 (98.691)	Mem 4879MB
[2022-05-31 07:21:03 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.096 (0.097)	Loss 0.5610 (0.6080)	Acc@1 90.625 (87.329)	Acc@5 100.000 (98.694)	Mem 4879MB
[2022-05-31 07:21:04 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.6501 (0.6137)	Acc@1 90.625 (87.219)	Acc@5 96.875 (98.637)	Mem 4879MB
[2022-05-31 07:21:05 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.110 (0.097)	Loss 0.5080 (0.6120)	Acc@1 90.625 (87.245)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 07:21:06 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.097)	Loss 0.4501 (0.6149)	Acc@1 93.750 (87.162)	Acc@5 100.000 (98.607)	Mem 4879MB
[2022-05-31 07:21:07 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.097 (0.097)	Loss 0.8311 (0.6131)	Acc@1 75.000 (87.202)	Acc@5 96.875 (98.626)	Mem 4879MB
[2022-05-31 07:21:07 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.097)	Loss 0.5584 (0.6140)	Acc@1 87.500 (87.139)	Acc@5 100.000 (98.643)	Mem 4879MB
[2022-05-31 07:21:08 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.4441 (0.6103)	Acc@1 93.750 (87.261)	Acc@5 100.000 (98.683)	Mem 4879MB
[2022-05-31 07:21:09 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.100 (0.097)	Loss 0.4757 (0.6074)	Acc@1 93.750 (87.339)	Acc@5 100.000 (98.708)	Mem 4879MB
[2022-05-31 07:21:10 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 0.6820 (0.6090)	Acc@1 84.375 (87.211)	Acc@5 100.000 (98.732)	Mem 4879MB
[2022-05-31 07:21:11 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.088 (0.096)	Loss 0.7725 (0.6092)	Acc@1 81.250 (87.199)	Acc@5 93.750 (98.722)	Mem 4879MB
[2022-05-31 07:21:12 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.090 (0.096)	Loss 0.6950 (0.6112)	Acc@1 81.250 (87.064)	Acc@5 100.000 (98.702)	Mem 4879MB
[2022-05-31 07:21:13 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.5195 (0.6120)	Acc@1 90.625 (87.078)	Acc@5 96.875 (98.684)	Mem 4879MB
[2022-05-31 07:21:13 MetaFG_0] (main.py 330): INFO  * Acc@1 87.100 Acc@5 98.680
[2022-05-31 07:21:13 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.1%
[2022-05-31 07:21:13 MetaFG_0] (main.py 171): INFO Max accuracy: 87.10%
[2022-05-31 07:21:14 MetaFG_0] (main.py 265): INFO Train: [64/300][0/1562]	eta 0:26:57 lr 0.000006	time 1.0352 (1.0352)	loss 1.4256 (1.4256)	grad_norm 36.3892 (36.3892)	mem 4879MB
[2022-05-31 07:21:17 MetaFG_0] (main.py 265): INFO Train: [64/300][10/1562]	eta 0:09:46 lr 0.000006	time 0.2934 (0.3776)	loss 1.7572 (1.4007)	grad_norm 36.3420 (28.1879)	mem 4879MB
[2022-05-31 07:21:21 MetaFG_0] (main.py 265): INFO Train: [64/300][20/1562]	eta 0:08:49 lr 0.000006	time 0.2974 (0.3432)	loss 1.3410 (1.3950)	grad_norm 26.9730 (27.7837)	mem 4879MB
[2022-05-31 07:21:24 MetaFG_0] (main.py 265): INFO Train: [64/300][30/1562]	eta 0:08:27 lr 0.000006	time 0.3005 (0.3311)	loss 1.6166 (1.3947)	grad_norm 23.6156 (28.9613)	mem 4879MB
[2022-05-31 07:21:27 MetaFG_0] (main.py 265): INFO Train: [64/300][40/1562]	eta 0:08:14 lr 0.000006	time 0.2995 (0.3249)	loss 1.4697 (1.3726)	grad_norm 19.9130 (28.7693)	mem 4879MB
[2022-05-31 07:21:30 MetaFG_0] (main.py 265): INFO Train: [64/300][50/1562]	eta 0:08:06 lr 0.000006	time 0.3009 (0.3215)	loss 1.0914 (1.3492)	grad_norm 22.4694 (28.7652)	mem 4879MB
[2022-05-31 07:21:33 MetaFG_0] (main.py 265): INFO Train: [64/300][60/1562]	eta 0:07:59 lr 0.000006	time 0.2930 (0.3191)	loss 1.6212 (1.3465)	grad_norm 22.2590 (28.7583)	mem 4879MB
[2022-05-31 07:21:36 MetaFG_0] (main.py 265): INFO Train: [64/300][70/1562]	eta 0:07:53 lr 0.000006	time 0.2923 (0.3170)	loss 1.5223 (1.3384)	grad_norm 29.8620 (28.6786)	mem 4879MB
[2022-05-31 07:21:39 MetaFG_0] (main.py 265): INFO Train: [64/300][80/1562]	eta 0:07:47 lr 0.000006	time 0.2992 (0.3158)	loss 0.8620 (1.3352)	grad_norm 30.5021 (28.6756)	mem 4879MB
[2022-05-31 07:21:42 MetaFG_0] (main.py 265): INFO Train: [64/300][90/1562]	eta 0:07:42 lr 0.000006	time 0.2926 (0.3145)	loss 1.0459 (1.3298)	grad_norm 42.0731 (28.7584)	mem 4879MB
[2022-05-31 07:21:45 MetaFG_0] (main.py 265): INFO Train: [64/300][100/1562]	eta 0:07:38 lr 0.000006	time 0.2986 (0.3138)	loss 1.4839 (1.3257)	grad_norm 16.7274 (28.2979)	mem 4879MB
[2022-05-31 07:21:48 MetaFG_0] (main.py 265): INFO Train: [64/300][110/1562]	eta 0:07:34 lr 0.000006	time 0.2924 (0.3130)	loss 1.3532 (1.3214)	grad_norm 23.2062 (27.9482)	mem 4879MB
[2022-05-31 07:21:51 MetaFG_0] (main.py 265): INFO Train: [64/300][120/1562]	eta 0:07:30 lr 0.000006	time 0.2988 (0.3124)	loss 1.1664 (1.3122)	grad_norm 24.8341 (28.2670)	mem 4879MB
[2022-05-31 07:21:54 MetaFG_0] (main.py 265): INFO Train: [64/300][130/1562]	eta 0:07:26 lr 0.000006	time 0.2917 (0.3119)	loss 0.8222 (1.3061)	grad_norm 26.9312 (28.3443)	mem 4879MB
[2022-05-31 07:21:57 MetaFG_0] (main.py 265): INFO Train: [64/300][140/1562]	eta 0:07:22 lr 0.000006	time 0.2920 (0.3114)	loss 1.4377 (1.3106)	grad_norm 28.9289 (28.2822)	mem 4879MB
[2022-05-31 07:22:00 MetaFG_0] (main.py 265): INFO Train: [64/300][150/1562]	eta 0:07:19 lr 0.000006	time 0.2926 (0.3110)	loss 1.2415 (1.3143)	grad_norm 23.7532 (28.3050)	mem 4879MB
[2022-05-31 07:22:03 MetaFG_0] (main.py 265): INFO Train: [64/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.2935 (0.3107)	loss 0.9961 (1.3060)	grad_norm 21.6698 (28.3594)	mem 4879MB
[2022-05-31 07:22:06 MetaFG_0] (main.py 265): INFO Train: [64/300][170/1562]	eta 0:07:12 lr 0.000006	time 0.2961 (0.3104)	loss 1.0805 (1.3128)	grad_norm 25.7850 (28.4784)	mem 4879MB
[2022-05-31 07:22:09 MetaFG_0] (main.py 265): INFO Train: [64/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2926 (0.3102)	loss 0.9394 (1.3017)	grad_norm 37.7836 (28.4790)	mem 4879MB
[2022-05-31 07:22:13 MetaFG_0] (main.py 265): INFO Train: [64/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2938 (0.3100)	loss 1.3500 (1.3105)	grad_norm 28.9762 (28.5678)	mem 4879MB
[2022-05-31 07:22:16 MetaFG_0] (main.py 265): INFO Train: [64/300][200/1562]	eta 0:07:01 lr 0.000006	time 0.2995 (0.3097)	loss 1.6994 (1.3162)	grad_norm 40.8393 (28.8660)	mem 4879MB
[2022-05-31 07:22:19 MetaFG_0] (main.py 265): INFO Train: [64/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2936 (0.3095)	loss 1.0582 (1.3219)	grad_norm 41.3570 (28.9105)	mem 4879MB
[2022-05-31 07:22:22 MetaFG_0] (main.py 265): INFO Train: [64/300][220/1562]	eta 0:06:57 lr 0.000006	time 0.2996 (0.3109)	loss 0.9008 (1.3226)	grad_norm 17.1394 (28.8926)	mem 4879MB
[2022-05-31 07:22:25 MetaFG_0] (main.py 265): INFO Train: [64/300][230/1562]	eta 0:06:53 lr 0.000006	time 0.2924 (0.3107)	loss 1.4321 (1.3244)	grad_norm 36.4530 (28.9948)	mem 4879MB
[2022-05-31 07:22:28 MetaFG_0] (main.py 265): INFO Train: [64/300][240/1562]	eta 0:06:50 lr 0.000006	time 0.2940 (0.3104)	loss 1.3538 (1.3316)	grad_norm 47.0150 (29.7946)	mem 4879MB
[2022-05-31 07:22:31 MetaFG_0] (main.py 265): INFO Train: [64/300][250/1562]	eta 0:06:47 lr 0.000006	time 0.2934 (0.3102)	loss 1.0493 (1.3324)	grad_norm 16.0163 (29.7503)	mem 4879MB
[2022-05-31 07:22:34 MetaFG_0] (main.py 265): INFO Train: [64/300][260/1562]	eta 0:06:43 lr 0.000006	time 0.2938 (0.3101)	loss 1.0427 (1.3315)	grad_norm 23.7151 (29.7363)	mem 4879MB
[2022-05-31 07:22:37 MetaFG_0] (main.py 265): INFO Train: [64/300][270/1562]	eta 0:06:40 lr 0.000006	time 0.2988 (0.3100)	loss 1.1278 (1.3302)	grad_norm 19.7611 (29.6405)	mem 4879MB
[2022-05-31 07:22:40 MetaFG_0] (main.py 265): INFO Train: [64/300][280/1562]	eta 0:06:37 lr 0.000006	time 0.2993 (0.3098)	loss 1.6445 (1.3351)	grad_norm 50.4392 (29.7751)	mem 4879MB
[2022-05-31 07:22:43 MetaFG_0] (main.py 265): INFO Train: [64/300][290/1562]	eta 0:06:33 lr 0.000006	time 0.2941 (0.3097)	loss 1.4976 (1.3358)	grad_norm 23.2403 (29.7670)	mem 4879MB
[2022-05-31 07:22:46 MetaFG_0] (main.py 265): INFO Train: [64/300][300/1562]	eta 0:06:30 lr 0.000006	time 0.3008 (0.3096)	loss 1.3847 (1.3339)	grad_norm 23.7533 (29.6758)	mem 4879MB
[2022-05-31 07:22:50 MetaFG_0] (main.py 265): INFO Train: [64/300][310/1562]	eta 0:06:27 lr 0.000006	time 0.2987 (0.3095)	loss 1.5299 (1.3362)	grad_norm 34.5781 (29.6879)	mem 4879MB
[2022-05-31 07:22:53 MetaFG_0] (main.py 265): INFO Train: [64/300][320/1562]	eta 0:06:24 lr 0.000006	time 0.2953 (0.3093)	loss 1.3723 (1.3384)	grad_norm 19.6618 (29.7382)	mem 4879MB
[2022-05-31 07:22:56 MetaFG_0] (main.py 265): INFO Train: [64/300][330/1562]	eta 0:06:20 lr 0.000006	time 0.2933 (0.3091)	loss 1.1116 (1.3348)	grad_norm 42.6153 (29.8430)	mem 4879MB
[2022-05-31 07:22:59 MetaFG_0] (main.py 265): INFO Train: [64/300][340/1562]	eta 0:06:17 lr 0.000006	time 0.2931 (0.3090)	loss 1.5172 (1.3356)	grad_norm 21.1750 (29.8618)	mem 4879MB
[2022-05-31 07:23:02 MetaFG_0] (main.py 265): INFO Train: [64/300][350/1562]	eta 0:06:14 lr 0.000006	time 0.2986 (0.3089)	loss 1.4314 (1.3416)	grad_norm 27.4199 (29.9791)	mem 4879MB
[2022-05-31 07:23:05 MetaFG_0] (main.py 265): INFO Train: [64/300][360/1562]	eta 0:06:11 lr 0.000006	time 0.3004 (0.3088)	loss 1.2147 (1.3423)	grad_norm 39.8710 (29.9576)	mem 4879MB
[2022-05-31 07:23:08 MetaFG_0] (main.py 265): INFO Train: [64/300][370/1562]	eta 0:06:08 lr 0.000006	time 0.3016 (0.3088)	loss 1.6156 (1.3427)	grad_norm 38.4382 (29.9813)	mem 4879MB
[2022-05-31 07:23:11 MetaFG_0] (main.py 265): INFO Train: [64/300][380/1562]	eta 0:06:04 lr 0.000006	time 0.2926 (0.3087)	loss 0.9418 (1.3420)	grad_norm 26.0874 (30.1151)	mem 4879MB
[2022-05-31 07:23:14 MetaFG_0] (main.py 265): INFO Train: [64/300][390/1562]	eta 0:06:01 lr 0.000006	time 0.2989 (0.3087)	loss 1.3472 (1.3422)	grad_norm 31.2809 (30.0616)	mem 4879MB
[2022-05-31 07:23:17 MetaFG_0] (main.py 265): INFO Train: [64/300][400/1562]	eta 0:05:58 lr 0.000006	time 0.2945 (0.3086)	loss 1.6045 (1.3419)	grad_norm 43.1374 (30.2888)	mem 4879MB
[2022-05-31 07:23:20 MetaFG_0] (main.py 265): INFO Train: [64/300][410/1562]	eta 0:05:55 lr 0.000006	time 0.2937 (0.3085)	loss 1.0234 (1.3435)	grad_norm 28.6463 (30.2403)	mem 4879MB
[2022-05-31 07:23:23 MetaFG_0] (main.py 265): INFO Train: [64/300][420/1562]	eta 0:05:52 lr 0.000006	time 0.2944 (0.3085)	loss 1.2569 (1.3408)	grad_norm 35.8902 (30.2892)	mem 4879MB
[2022-05-31 07:23:26 MetaFG_0] (main.py 265): INFO Train: [64/300][430/1562]	eta 0:05:49 lr 0.000006	time 0.2959 (0.3084)	loss 1.1982 (1.3400)	grad_norm 22.8489 (30.1809)	mem 4879MB
[2022-05-31 07:23:29 MetaFG_0] (main.py 265): INFO Train: [64/300][440/1562]	eta 0:05:46 lr 0.000006	time 0.3020 (0.3084)	loss 1.6578 (1.3415)	grad_norm 28.2824 (30.1852)	mem 4879MB
[2022-05-31 07:23:32 MetaFG_0] (main.py 265): INFO Train: [64/300][450/1562]	eta 0:05:42 lr 0.000006	time 0.2934 (0.3083)	loss 1.4450 (1.3395)	grad_norm 23.3784 (30.2531)	mem 4879MB
[2022-05-31 07:23:35 MetaFG_0] (main.py 265): INFO Train: [64/300][460/1562]	eta 0:05:39 lr 0.000006	time 0.2991 (0.3083)	loss 1.6932 (1.3380)	grad_norm 34.1026 (30.2623)	mem 4879MB
[2022-05-31 07:23:38 MetaFG_0] (main.py 265): INFO Train: [64/300][470/1562]	eta 0:05:36 lr 0.000006	time 0.3016 (0.3082)	loss 1.2687 (1.3381)	grad_norm 25.9477 (30.2285)	mem 4879MB
[2022-05-31 07:23:42 MetaFG_0] (main.py 265): INFO Train: [64/300][480/1562]	eta 0:05:33 lr 0.000006	time 0.2985 (0.3082)	loss 1.2565 (1.3359)	grad_norm 36.8987 (30.3335)	mem 4879MB
[2022-05-31 07:23:45 MetaFG_0] (main.py 265): INFO Train: [64/300][490/1562]	eta 0:05:30 lr 0.000006	time 0.2937 (0.3081)	loss 1.7072 (1.3368)	grad_norm 22.9815 (30.2472)	mem 4879MB
[2022-05-31 07:23:48 MetaFG_0] (main.py 265): INFO Train: [64/300][500/1562]	eta 0:05:27 lr 0.000006	time 0.2941 (0.3081)	loss 1.2168 (1.3329)	grad_norm 38.4650 (30.2502)	mem 4879MB
[2022-05-31 07:23:51 MetaFG_0] (main.py 265): INFO Train: [64/300][510/1562]	eta 0:05:24 lr 0.000006	time 0.2933 (0.3080)	loss 1.3041 (1.3328)	grad_norm 25.1512 (30.3190)	mem 4879MB
[2022-05-31 07:23:54 MetaFG_0] (main.py 265): INFO Train: [64/300][520/1562]	eta 0:05:20 lr 0.000006	time 0.2942 (0.3079)	loss 1.3921 (1.3309)	grad_norm 24.1750 (30.1998)	mem 4879MB
[2022-05-31 07:23:57 MetaFG_0] (main.py 265): INFO Train: [64/300][530/1562]	eta 0:05:17 lr 0.000006	time 0.2926 (0.3079)	loss 1.0624 (1.3314)	grad_norm 27.7319 (30.0755)	mem 4879MB
[2022-05-31 07:24:00 MetaFG_0] (main.py 265): INFO Train: [64/300][540/1562]	eta 0:05:14 lr 0.000006	time 0.2988 (0.3078)	loss 1.0143 (1.3311)	grad_norm 70.1348 (30.1452)	mem 4879MB
[2022-05-31 07:24:03 MetaFG_0] (main.py 265): INFO Train: [64/300][550/1562]	eta 0:05:11 lr 0.000006	time 0.3000 (0.3077)	loss 1.5435 (1.3321)	grad_norm 30.8217 (30.1023)	mem 4879MB
[2022-05-31 07:24:06 MetaFG_0] (main.py 265): INFO Train: [64/300][560/1562]	eta 0:05:08 lr 0.000006	time 0.2934 (0.3077)	loss 1.5962 (1.3335)	grad_norm 32.3797 (30.1795)	mem 4879MB
[2022-05-31 07:24:09 MetaFG_0] (main.py 265): INFO Train: [64/300][570/1562]	eta 0:05:05 lr 0.000006	time 0.3015 (0.3076)	loss 1.0961 (1.3343)	grad_norm 42.9391 (30.2342)	mem 4879MB
[2022-05-31 07:24:12 MetaFG_0] (main.py 265): INFO Train: [64/300][580/1562]	eta 0:05:02 lr 0.000006	time 0.2991 (0.3076)	loss 1.3775 (1.3322)	grad_norm 72.6053 (30.2814)	mem 4879MB
[2022-05-31 07:24:15 MetaFG_0] (main.py 265): INFO Train: [64/300][590/1562]	eta 0:04:58 lr 0.000006	time 0.2940 (0.3076)	loss 1.3668 (1.3328)	grad_norm 30.5984 (30.3973)	mem 4879MB
[2022-05-31 07:24:18 MetaFG_0] (main.py 265): INFO Train: [64/300][600/1562]	eta 0:04:55 lr 0.000006	time 0.2995 (0.3076)	loss 1.3582 (1.3316)	grad_norm 28.3605 (30.4111)	mem 4879MB
[2022-05-31 07:24:21 MetaFG_0] (main.py 265): INFO Train: [64/300][610/1562]	eta 0:04:52 lr 0.000006	time 0.2986 (0.3075)	loss 1.5449 (1.3333)	grad_norm 16.0503 (30.3223)	mem 4879MB
[2022-05-31 07:24:24 MetaFG_0] (main.py 265): INFO Train: [64/300][620/1562]	eta 0:04:49 lr 0.000006	time 0.2929 (0.3075)	loss 1.7428 (1.3339)	grad_norm 32.5424 (30.3241)	mem 4879MB
[2022-05-31 07:24:27 MetaFG_0] (main.py 265): INFO Train: [64/300][630/1562]	eta 0:04:46 lr 0.000006	time 0.2981 (0.3075)	loss 1.5781 (1.3347)	grad_norm 26.6750 (30.3658)	mem 4879MB
[2022-05-31 07:24:30 MetaFG_0] (main.py 265): INFO Train: [64/300][640/1562]	eta 0:04:43 lr 0.000006	time 0.2935 (0.3074)	loss 1.4910 (1.3355)	grad_norm 36.9564 (30.4036)	mem 4879MB
[2022-05-31 07:24:33 MetaFG_0] (main.py 265): INFO Train: [64/300][650/1562]	eta 0:04:40 lr 0.000006	time 0.2926 (0.3074)	loss 1.4071 (1.3348)	grad_norm 31.7935 (30.4595)	mem 4879MB
[2022-05-31 07:24:36 MetaFG_0] (main.py 265): INFO Train: [64/300][660/1562]	eta 0:04:37 lr 0.000006	time 0.3038 (0.3073)	loss 1.0965 (1.3349)	grad_norm 32.8239 (30.4224)	mem 4879MB
[2022-05-31 07:24:40 MetaFG_0] (main.py 265): INFO Train: [64/300][670/1562]	eta 0:04:34 lr 0.000006	time 0.2938 (0.3073)	loss 1.4030 (1.3354)	grad_norm 25.0841 (30.3783)	mem 4879MB
[2022-05-31 07:24:43 MetaFG_0] (main.py 265): INFO Train: [64/300][680/1562]	eta 0:04:30 lr 0.000006	time 0.2933 (0.3073)	loss 1.4006 (1.3369)	grad_norm 37.7891 (30.4227)	mem 4879MB
[2022-05-31 07:24:46 MetaFG_0] (main.py 265): INFO Train: [64/300][690/1562]	eta 0:04:27 lr 0.000006	time 0.3003 (0.3072)	loss 1.4338 (1.3356)	grad_norm 27.0636 (30.3554)	mem 4879MB
[2022-05-31 07:24:49 MetaFG_0] (main.py 265): INFO Train: [64/300][700/1562]	eta 0:04:24 lr 0.000006	time 0.3004 (0.3072)	loss 1.5177 (1.3364)	grad_norm 30.7773 (30.3723)	mem 4879MB
[2022-05-31 07:24:52 MetaFG_0] (main.py 265): INFO Train: [64/300][710/1562]	eta 0:04:21 lr 0.000006	time 0.2918 (0.3072)	loss 1.0482 (1.3353)	grad_norm 27.6968 (30.3630)	mem 4879MB
[2022-05-31 07:24:55 MetaFG_0] (main.py 265): INFO Train: [64/300][720/1562]	eta 0:04:18 lr 0.000006	time 0.2997 (0.3071)	loss 0.9742 (1.3332)	grad_norm 38.4912 (30.3931)	mem 4879MB
[2022-05-31 07:24:58 MetaFG_0] (main.py 265): INFO Train: [64/300][730/1562]	eta 0:04:15 lr 0.000006	time 0.2992 (0.3071)	loss 1.4423 (1.3347)	grad_norm 46.0152 (30.4389)	mem 4879MB
[2022-05-31 07:25:01 MetaFG_0] (main.py 265): INFO Train: [64/300][740/1562]	eta 0:04:12 lr 0.000006	time 0.2987 (0.3071)	loss 1.6337 (1.3329)	grad_norm 27.4517 (30.4567)	mem 4879MB
[2022-05-31 07:25:04 MetaFG_0] (main.py 265): INFO Train: [64/300][750/1562]	eta 0:04:09 lr 0.000006	time 0.2989 (0.3071)	loss 1.5698 (1.3339)	grad_norm 24.6472 (30.4177)	mem 4879MB
[2022-05-31 07:25:07 MetaFG_0] (main.py 265): INFO Train: [64/300][760/1562]	eta 0:04:06 lr 0.000006	time 0.2936 (0.3071)	loss 1.4166 (1.3337)	grad_norm 20.9717 (30.3293)	mem 4879MB
[2022-05-31 07:25:10 MetaFG_0] (main.py 265): INFO Train: [64/300][770/1562]	eta 0:04:03 lr 0.000006	time 0.2991 (0.3071)	loss 1.4480 (1.3337)	grad_norm 24.1631 (30.3184)	mem 4879MB
[2022-05-31 07:25:13 MetaFG_0] (main.py 265): INFO Train: [64/300][780/1562]	eta 0:04:00 lr 0.000006	time 0.3016 (0.3071)	loss 1.2148 (1.3338)	grad_norm 27.3079 (30.3120)	mem 4879MB
[2022-05-31 07:25:16 MetaFG_0] (main.py 265): INFO Train: [64/300][790/1562]	eta 0:03:57 lr 0.000006	time 0.2933 (0.3070)	loss 0.6898 (1.3327)	grad_norm 20.2043 (30.3553)	mem 4879MB
[2022-05-31 07:25:19 MetaFG_0] (main.py 265): INFO Train: [64/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.3002 (0.3070)	loss 1.1906 (1.3331)	grad_norm 26.5149 (30.3505)	mem 4879MB
[2022-05-31 07:25:22 MetaFG_0] (main.py 265): INFO Train: [64/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.3065 (0.3070)	loss 1.3845 (1.3344)	grad_norm 42.5085 (30.3254)	mem 4879MB
[2022-05-31 07:25:25 MetaFG_0] (main.py 265): INFO Train: [64/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2990 (0.3070)	loss 1.5764 (1.3350)	grad_norm 24.4867 (30.3536)	mem 4879MB
[2022-05-31 07:25:28 MetaFG_0] (main.py 265): INFO Train: [64/300][830/1562]	eta 0:03:44 lr 0.000006	time 0.2957 (0.3069)	loss 1.4059 (1.3347)	grad_norm 39.1363 (30.3664)	mem 4879MB
[2022-05-31 07:25:31 MetaFG_0] (main.py 265): INFO Train: [64/300][840/1562]	eta 0:03:41 lr 0.000006	time 0.2999 (0.3069)	loss 1.1803 (1.3339)	grad_norm 21.9485 (30.3678)	mem 4879MB
[2022-05-31 07:25:34 MetaFG_0] (main.py 265): INFO Train: [64/300][850/1562]	eta 0:03:38 lr 0.000006	time 0.2923 (0.3069)	loss 1.4533 (1.3334)	grad_norm 38.1000 (30.3504)	mem 4879MB
[2022-05-31 07:25:38 MetaFG_0] (main.py 265): INFO Train: [64/300][860/1562]	eta 0:03:35 lr 0.000006	time 0.3000 (0.3069)	loss 1.1420 (1.3343)	grad_norm 18.0648 (30.2968)	mem 4879MB
[2022-05-31 07:25:41 MetaFG_0] (main.py 265): INFO Train: [64/300][870/1562]	eta 0:03:32 lr 0.000006	time 0.3008 (0.3069)	loss 1.4398 (1.3336)	grad_norm 17.5437 (30.2676)	mem 4879MB
[2022-05-31 07:25:44 MetaFG_0] (main.py 265): INFO Train: [64/300][880/1562]	eta 0:03:29 lr 0.000006	time 0.3006 (0.3069)	loss 1.2800 (1.3344)	grad_norm 31.1440 (30.2190)	mem 4879MB
[2022-05-31 07:25:47 MetaFG_0] (main.py 265): INFO Train: [64/300][890/1562]	eta 0:03:26 lr 0.000006	time 0.2996 (0.3069)	loss 1.7658 (1.3349)	grad_norm 32.6702 (30.2566)	mem 4879MB
[2022-05-31 07:25:50 MetaFG_0] (main.py 265): INFO Train: [64/300][900/1562]	eta 0:03:23 lr 0.000006	time 0.3007 (0.3069)	loss 1.5591 (1.3342)	grad_norm 34.4713 (30.2538)	mem 4879MB
[2022-05-31 07:25:53 MetaFG_0] (main.py 265): INFO Train: [64/300][910/1562]	eta 0:03:20 lr 0.000006	time 0.2927 (0.3068)	loss 1.4149 (1.3348)	grad_norm 33.9216 (30.2626)	mem 4879MB
[2022-05-31 07:25:56 MetaFG_0] (main.py 265): INFO Train: [64/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.3003 (0.3069)	loss 1.3656 (1.3336)	grad_norm 33.9737 (30.2729)	mem 4879MB
[2022-05-31 07:25:59 MetaFG_0] (main.py 265): INFO Train: [64/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.3000 (0.3069)	loss 1.5355 (1.3334)	grad_norm 28.9571 (30.2482)	mem 4879MB
[2022-05-31 07:26:02 MetaFG_0] (main.py 265): INFO Train: [64/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2977 (0.3068)	loss 0.8776 (1.3317)	grad_norm 29.7116 (30.2638)	mem 4879MB
[2022-05-31 07:26:05 MetaFG_0] (main.py 265): INFO Train: [64/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2923 (0.3068)	loss 1.5562 (1.3314)	grad_norm 20.4158 (30.3235)	mem 4879MB
[2022-05-31 07:26:08 MetaFG_0] (main.py 265): INFO Train: [64/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2924 (0.3068)	loss 1.3612 (1.3305)	grad_norm 39.4941 (30.2823)	mem 4879MB
[2022-05-31 07:26:11 MetaFG_0] (main.py 265): INFO Train: [64/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2995 (0.3068)	loss 1.5934 (1.3310)	grad_norm 26.5445 (30.2672)	mem 4879MB
[2022-05-31 07:26:14 MetaFG_0] (main.py 265): INFO Train: [64/300][980/1562]	eta 0:02:58 lr 0.000006	time 0.3006 (0.3068)	loss 1.3148 (1.3302)	grad_norm 18.3744 (30.2426)	mem 4879MB
[2022-05-31 07:26:17 MetaFG_0] (main.py 265): INFO Train: [64/300][990/1562]	eta 0:02:55 lr 0.000006	time 0.2960 (0.3068)	loss 1.5830 (1.3305)	grad_norm 26.1780 (30.1890)	mem 4879MB
[2022-05-31 07:26:20 MetaFG_0] (main.py 265): INFO Train: [64/300][1000/1562]	eta 0:02:52 lr 0.000006	time 0.2953 (0.3068)	loss 1.6383 (1.3307)	grad_norm 28.8263 (30.1755)	mem 4879MB
[2022-05-31 07:26:23 MetaFG_0] (main.py 265): INFO Train: [64/300][1010/1562]	eta 0:02:49 lr 0.000006	time 0.2933 (0.3068)	loss 1.5822 (1.3306)	grad_norm 30.7179 (30.1593)	mem 4879MB
[2022-05-31 07:26:27 MetaFG_0] (main.py 265): INFO Train: [64/300][1020/1562]	eta 0:02:46 lr 0.000006	time 0.2989 (0.3068)	loss 1.5793 (1.3310)	grad_norm 28.2941 (30.1259)	mem 4879MB
[2022-05-31 07:26:30 MetaFG_0] (main.py 265): INFO Train: [64/300][1030/1562]	eta 0:02:43 lr 0.000006	time 0.2929 (0.3068)	loss 1.5336 (1.3319)	grad_norm 20.1160 (30.1026)	mem 4879MB
[2022-05-31 07:26:33 MetaFG_0] (main.py 265): INFO Train: [64/300][1040/1562]	eta 0:02:40 lr 0.000006	time 0.2926 (0.3068)	loss 1.5357 (1.3319)	grad_norm 22.3014 (30.1427)	mem 4879MB
[2022-05-31 07:26:36 MetaFG_0] (main.py 265): INFO Train: [64/300][1050/1562]	eta 0:02:37 lr 0.000006	time 0.2993 (0.3068)	loss 1.0734 (1.3330)	grad_norm 29.1515 (30.1599)	mem 4879MB
[2022-05-31 07:26:39 MetaFG_0] (main.py 265): INFO Train: [64/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2939 (0.3068)	loss 1.2921 (1.3320)	grad_norm 25.7500 (30.1954)	mem 4879MB
[2022-05-31 07:26:42 MetaFG_0] (main.py 265): INFO Train: [64/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2958 (0.3068)	loss 1.4090 (1.3324)	grad_norm 32.5880 (30.1772)	mem 4879MB
[2022-05-31 07:26:45 MetaFG_0] (main.py 265): INFO Train: [64/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2920 (0.3067)	loss 1.3931 (1.3319)	grad_norm 25.1583 (30.2044)	mem 4879MB
[2022-05-31 07:26:48 MetaFG_0] (main.py 265): INFO Train: [64/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2940 (0.3067)	loss 1.4640 (1.3319)	grad_norm 23.1583 (30.2631)	mem 4879MB
[2022-05-31 07:26:51 MetaFG_0] (main.py 265): INFO Train: [64/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2929 (0.3067)	loss 1.5004 (1.3329)	grad_norm 21.1998 (30.2373)	mem 4879MB
[2022-05-31 07:26:54 MetaFG_0] (main.py 265): INFO Train: [64/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2934 (0.3067)	loss 1.0724 (1.3325)	grad_norm 17.5897 (30.2449)	mem 4879MB
[2022-05-31 07:26:57 MetaFG_0] (main.py 265): INFO Train: [64/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2950 (0.3066)	loss 1.5944 (1.3319)	grad_norm 26.0752 (30.2825)	mem 4879MB
[2022-05-31 07:27:00 MetaFG_0] (main.py 265): INFO Train: [64/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2994 (0.3066)	loss 1.5460 (1.3323)	grad_norm 28.7091 (30.2983)	mem 4879MB
[2022-05-31 07:27:03 MetaFG_0] (main.py 265): INFO Train: [64/300][1140/1562]	eta 0:02:09 lr 0.000006	time 0.2983 (0.3066)	loss 1.3146 (1.3328)	grad_norm 20.4684 (30.2853)	mem 4879MB
[2022-05-31 07:27:06 MetaFG_0] (main.py 265): INFO Train: [64/300][1150/1562]	eta 0:02:06 lr 0.000006	time 0.2988 (0.3066)	loss 1.1419 (1.3334)	grad_norm 34.2843 (30.2727)	mem 4879MB
[2022-05-31 07:27:09 MetaFG_0] (main.py 265): INFO Train: [64/300][1160/1562]	eta 0:02:03 lr 0.000006	time 0.2924 (0.3066)	loss 1.5557 (1.3316)	grad_norm 25.2480 (30.2764)	mem 4879MB
[2022-05-31 07:27:12 MetaFG_0] (main.py 265): INFO Train: [64/300][1170/1562]	eta 0:02:00 lr 0.000006	time 0.2979 (0.3065)	loss 1.4716 (1.3318)	grad_norm 43.7387 (30.2923)	mem 4879MB
[2022-05-31 07:27:15 MetaFG_0] (main.py 265): INFO Train: [64/300][1180/1562]	eta 0:01:57 lr 0.000006	time 0.3005 (0.3065)	loss 1.2213 (1.3321)	grad_norm 45.1997 (30.2680)	mem 4879MB
[2022-05-31 07:27:18 MetaFG_0] (main.py 265): INFO Train: [64/300][1190/1562]	eta 0:01:54 lr 0.000006	time 0.2936 (0.3065)	loss 1.4004 (1.3330)	grad_norm 18.1080 (30.2560)	mem 4879MB
[2022-05-31 07:27:21 MetaFG_0] (main.py 265): INFO Train: [64/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2940 (0.3065)	loss 1.4279 (1.3337)	grad_norm 31.0092 (30.2476)	mem 4879MB
[2022-05-31 07:27:24 MetaFG_0] (main.py 265): INFO Train: [64/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2923 (0.3065)	loss 1.4239 (1.3334)	grad_norm 37.1350 (30.2724)	mem 4879MB
[2022-05-31 07:27:28 MetaFG_0] (main.py 265): INFO Train: [64/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2990 (0.3065)	loss 1.0652 (1.3322)	grad_norm 24.7300 (30.3598)	mem 4879MB
[2022-05-31 07:27:31 MetaFG_0] (main.py 265): INFO Train: [64/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2927 (0.3065)	loss 1.1259 (1.3308)	grad_norm 23.3726 (30.3763)	mem 4879MB
[2022-05-31 07:27:34 MetaFG_0] (main.py 265): INFO Train: [64/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.3006 (0.3064)	loss 1.7011 (1.3309)	grad_norm 36.5656 (30.3549)	mem 4879MB
[2022-05-31 07:27:37 MetaFG_0] (main.py 265): INFO Train: [64/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2924 (0.3064)	loss 1.2483 (1.3315)	grad_norm 25.4380 (30.3517)	mem 4879MB
[2022-05-31 07:27:40 MetaFG_0] (main.py 265): INFO Train: [64/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.3011 (0.3064)	loss 1.4031 (1.3314)	grad_norm 21.0415 (30.3357)	mem 4879MB
[2022-05-31 07:27:43 MetaFG_0] (main.py 265): INFO Train: [64/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2993 (0.3064)	loss 1.5509 (1.3315)	grad_norm 42.6128 (30.3403)	mem 4879MB
[2022-05-31 07:27:46 MetaFG_0] (main.py 265): INFO Train: [64/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2934 (0.3064)	loss 1.4177 (1.3323)	grad_norm 19.6333 (30.3278)	mem 4879MB
[2022-05-31 07:27:49 MetaFG_0] (main.py 265): INFO Train: [64/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2936 (0.3064)	loss 0.9136 (1.3320)	grad_norm 56.6294 (30.3263)	mem 4879MB
[2022-05-31 07:27:52 MetaFG_0] (main.py 265): INFO Train: [64/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2992 (0.3063)	loss 1.2988 (1.3323)	grad_norm 46.8224 (inf)	mem 4879MB
[2022-05-31 07:27:55 MetaFG_0] (main.py 265): INFO Train: [64/300][1310/1562]	eta 0:01:17 lr 0.000006	time 0.2926 (0.3063)	loss 1.2643 (1.3323)	grad_norm 42.2036 (inf)	mem 4879MB
[2022-05-31 07:27:58 MetaFG_0] (main.py 265): INFO Train: [64/300][1320/1562]	eta 0:01:14 lr 0.000006	time 0.2937 (0.3063)	loss 1.4855 (1.3325)	grad_norm 17.4383 (inf)	mem 4879MB
[2022-05-31 07:28:01 MetaFG_0] (main.py 265): INFO Train: [64/300][1330/1562]	eta 0:01:11 lr 0.000006	time 0.2924 (0.3063)	loss 0.9855 (1.3330)	grad_norm 38.7836 (inf)	mem 4879MB
[2022-05-31 07:28:04 MetaFG_0] (main.py 265): INFO Train: [64/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2929 (0.3063)	loss 1.7275 (1.3343)	grad_norm 40.5018 (inf)	mem 4879MB
[2022-05-31 07:28:07 MetaFG_0] (main.py 265): INFO Train: [64/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2930 (0.3062)	loss 1.0008 (1.3349)	grad_norm 29.4692 (inf)	mem 4879MB
[2022-05-31 07:28:10 MetaFG_0] (main.py 265): INFO Train: [64/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2938 (0.3062)	loss 1.1164 (1.3356)	grad_norm 21.1647 (inf)	mem 4879MB
[2022-05-31 07:28:13 MetaFG_0] (main.py 265): INFO Train: [64/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2993 (0.3062)	loss 1.5452 (1.3354)	grad_norm 42.2252 (inf)	mem 4879MB
[2022-05-31 07:28:16 MetaFG_0] (main.py 265): INFO Train: [64/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2996 (0.3062)	loss 1.5530 (1.3352)	grad_norm 23.7209 (inf)	mem 4879MB
[2022-05-31 07:28:19 MetaFG_0] (main.py 265): INFO Train: [64/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2925 (0.3062)	loss 0.9184 (1.3346)	grad_norm 31.4265 (inf)	mem 4879MB
[2022-05-31 07:28:22 MetaFG_0] (main.py 265): INFO Train: [64/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2925 (0.3062)	loss 1.3075 (1.3342)	grad_norm 29.2761 (inf)	mem 4879MB
[2022-05-31 07:28:25 MetaFG_0] (main.py 265): INFO Train: [64/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.3196 (0.3062)	loss 1.6034 (1.3346)	grad_norm 18.4945 (inf)	mem 4879MB
[2022-05-31 07:28:29 MetaFG_0] (main.py 265): INFO Train: [64/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.3000 (0.3063)	loss 1.1340 (1.3343)	grad_norm 16.6026 (inf)	mem 4879MB
[2022-05-31 07:28:32 MetaFG_0] (main.py 265): INFO Train: [64/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2977 (0.3063)	loss 1.6512 (1.3340)	grad_norm 68.2922 (inf)	mem 4879MB
[2022-05-31 07:28:35 MetaFG_0] (main.py 265): INFO Train: [64/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2923 (0.3063)	loss 1.4218 (1.3349)	grad_norm 20.5342 (inf)	mem 4879MB
[2022-05-31 07:28:38 MetaFG_0] (main.py 265): INFO Train: [64/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2929 (0.3063)	loss 1.7823 (1.3351)	grad_norm 27.7317 (inf)	mem 4879MB
[2022-05-31 07:28:41 MetaFG_0] (main.py 265): INFO Train: [64/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2929 (0.3063)	loss 1.2850 (1.3354)	grad_norm 15.9777 (inf)	mem 4879MB
[2022-05-31 07:28:44 MetaFG_0] (main.py 265): INFO Train: [64/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2929 (0.3063)	loss 1.0476 (1.3345)	grad_norm 44.4947 (inf)	mem 4879MB
[2022-05-31 07:28:47 MetaFG_0] (main.py 265): INFO Train: [64/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2914 (0.3063)	loss 1.6003 (1.3341)	grad_norm 21.2958 (inf)	mem 4879MB
[2022-05-31 07:28:50 MetaFG_0] (main.py 265): INFO Train: [64/300][1490/1562]	eta 0:00:22 lr 0.000006	time 0.2942 (0.3062)	loss 1.5987 (1.3347)	grad_norm 22.8797 (inf)	mem 4879MB
[2022-05-31 07:28:53 MetaFG_0] (main.py 265): INFO Train: [64/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.3001 (0.3062)	loss 1.3600 (1.3344)	grad_norm 25.0181 (inf)	mem 4879MB
[2022-05-31 07:28:56 MetaFG_0] (main.py 265): INFO Train: [64/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2929 (0.3062)	loss 1.3703 (1.3350)	grad_norm 40.8785 (inf)	mem 4879MB
[2022-05-31 07:28:59 MetaFG_0] (main.py 265): INFO Train: [64/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2920 (0.3062)	loss 1.4085 (1.3353)	grad_norm 36.5428 (inf)	mem 4879MB
[2022-05-31 07:29:02 MetaFG_0] (main.py 265): INFO Train: [64/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2935 (0.3062)	loss 1.6013 (1.3362)	grad_norm 38.3497 (inf)	mem 4879MB
[2022-05-31 07:29:05 MetaFG_0] (main.py 265): INFO Train: [64/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2926 (0.3062)	loss 1.1882 (1.3362)	grad_norm 19.2642 (inf)	mem 4879MB
[2022-05-31 07:29:08 MetaFG_0] (main.py 265): INFO Train: [64/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2978 (0.3061)	loss 0.7735 (1.3366)	grad_norm 34.1285 (inf)	mem 4879MB
[2022-05-31 07:29:11 MetaFG_0] (main.py 265): INFO Train: [64/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2941 (0.3061)	loss 1.3368 (1.3362)	grad_norm 23.1702 (inf)	mem 4879MB
[2022-05-31 07:29:12 MetaFG_0] (main.py 272): INFO EPOCH 64 training takes 0:07:58
[2022-05-31 07:29:12 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_64.pth saving......
[2022-05-31 07:29:12 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_64.pth saved !!!
[2022-05-31 07:29:12 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:29:14 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:29:14 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:29:14 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.688 (0.688)	Loss 0.7330 (0.7330)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 07:29:15 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.095 (0.150)	Loss 0.5627 (0.6362)	Acc@1 90.625 (85.511)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 07:29:16 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.087 (0.124)	Loss 0.5992 (0.6220)	Acc@1 84.375 (85.714)	Acc@5 100.000 (98.363)	Mem 4879MB
[2022-05-31 07:29:17 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.088 (0.114)	Loss 0.9725 (0.6595)	Acc@1 81.250 (85.181)	Acc@5 96.875 (98.286)	Mem 4879MB
[2022-05-31 07:29:18 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.110)	Loss 0.7307 (0.6543)	Acc@1 81.250 (85.213)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 07:29:19 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.089 (0.107)	Loss 0.5207 (0.6474)	Acc@1 87.500 (85.784)	Acc@5 100.000 (98.591)	Mem 4879MB
[2022-05-31 07:29:20 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.108 (0.105)	Loss 0.6774 (0.6371)	Acc@1 90.625 (86.168)	Acc@5 96.875 (98.668)	Mem 4879MB
[2022-05-31 07:29:21 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.094 (0.103)	Loss 0.5115 (0.6271)	Acc@1 90.625 (86.400)	Acc@5 96.875 (98.636)	Mem 4879MB
[2022-05-31 07:29:22 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.102)	Loss 0.5157 (0.6150)	Acc@1 93.750 (86.998)	Acc@5 100.000 (98.765)	Mem 4879MB
[2022-05-31 07:29:23 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.093 (0.101)	Loss 0.5414 (0.6099)	Acc@1 87.500 (86.985)	Acc@5 100.000 (98.832)	Mem 4879MB
[2022-05-31 07:29:24 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.5535 (0.6080)	Acc@1 93.750 (87.129)	Acc@5 100.000 (98.793)	Mem 4879MB
[2022-05-31 07:29:25 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.100)	Loss 1.0215 (0.6121)	Acc@1 71.875 (86.965)	Acc@5 93.750 (98.677)	Mem 4879MB
[2022-05-31 07:29:26 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.099)	Loss 0.3344 (0.6176)	Acc@1 100.000 (86.803)	Acc@5 100.000 (98.657)	Mem 4879MB
[2022-05-31 07:29:27 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.3288 (0.6125)	Acc@1 100.000 (86.927)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 07:29:28 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.098)	Loss 0.7787 (0.6174)	Acc@1 84.375 (86.791)	Acc@5 96.875 (98.626)	Mem 4879MB
[2022-05-31 07:29:28 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.092 (0.098)	Loss 0.4412 (0.6121)	Acc@1 90.625 (86.962)	Acc@5 96.875 (98.675)	Mem 4879MB
[2022-05-31 07:29:29 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.4762 (0.6149)	Acc@1 93.750 (86.879)	Acc@5 100.000 (98.641)	Mem 4879MB
[2022-05-31 07:29:30 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.6211 (0.6153)	Acc@1 87.500 (86.860)	Acc@5 96.875 (98.666)	Mem 4879MB
[2022-05-31 07:29:31 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.4965 (0.6161)	Acc@1 87.500 (86.827)	Acc@5 100.000 (98.705)	Mem 4879MB
[2022-05-31 07:29:32 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.095 (0.097)	Loss 0.5167 (0.6174)	Acc@1 93.750 (86.862)	Acc@5 100.000 (98.707)	Mem 4879MB
[2022-05-31 07:29:33 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.092 (0.097)	Loss 0.6492 (0.6178)	Acc@1 87.500 (86.940)	Acc@5 96.875 (98.663)	Mem 4879MB
[2022-05-31 07:29:34 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 0.5633 (0.6167)	Acc@1 87.500 (87.026)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 07:29:35 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.6162 (0.6169)	Acc@1 87.500 (86.991)	Acc@5 96.875 (98.643)	Mem 4879MB
[2022-05-31 07:29:36 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 0.6736 (0.6218)	Acc@1 87.500 (86.878)	Acc@5 96.875 (98.620)	Mem 4879MB
[2022-05-31 07:29:37 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.097 (0.096)	Loss 0.5089 (0.6197)	Acc@1 87.500 (86.955)	Acc@5 100.000 (98.638)	Mem 4879MB
[2022-05-31 07:29:38 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 0.4566 (0.6191)	Acc@1 93.750 (86.977)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 07:29:39 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 0.7951 (0.6194)	Acc@1 87.500 (87.009)	Acc@5 93.750 (98.635)	Mem 4879MB
[2022-05-31 07:29:40 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.4486 (0.6180)	Acc@1 93.750 (87.027)	Acc@5 100.000 (98.651)	Mem 4879MB
[2022-05-31 07:29:41 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.097 (0.096)	Loss 0.6986 (0.6163)	Acc@1 78.125 (87.066)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 07:29:42 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.090 (0.096)	Loss 0.7304 (0.6128)	Acc@1 78.125 (87.156)	Acc@5 96.875 (98.690)	Mem 4879MB
[2022-05-31 07:29:43 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.097 (0.096)	Loss 0.4153 (0.6113)	Acc@1 90.625 (87.230)	Acc@5 100.000 (98.681)	Mem 4879MB
[2022-05-31 07:29:44 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7587 (0.6114)	Acc@1 87.500 (87.259)	Acc@5 96.875 (98.694)	Mem 4879MB
[2022-05-31 07:29:44 MetaFG_0] (main.py 330): INFO  * Acc@1 87.280 Acc@5 98.700
[2022-05-31 07:29:44 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.3%
[2022-05-31 07:29:44 MetaFG_0] (main.py 171): INFO Max accuracy: 87.28%
[2022-05-31 07:29:45 MetaFG_0] (main.py 265): INFO Train: [65/300][0/1562]	eta 0:27:12 lr 0.000006	time 1.0449 (1.0449)	loss 1.4210 (1.4210)	grad_norm 18.7021 (18.7021)	mem 4879MB
[2022-05-31 07:29:48 MetaFG_0] (main.py 265): INFO Train: [65/300][10/1562]	eta 0:09:43 lr 0.000006	time 0.2918 (0.3761)	loss 1.1143 (1.3515)	grad_norm 47.0228 (29.4768)	mem 4879MB
[2022-05-31 07:29:51 MetaFG_0] (main.py 265): INFO Train: [65/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.3000 (0.3429)	loss 0.9639 (1.3450)	grad_norm 30.6339 (29.9192)	mem 4879MB
[2022-05-31 07:29:54 MetaFG_0] (main.py 265): INFO Train: [65/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2929 (0.3300)	loss 1.1218 (1.3400)	grad_norm 38.0316 (30.3668)	mem 4879MB
[2022-05-31 07:29:57 MetaFG_0] (main.py 265): INFO Train: [65/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2990 (0.3238)	loss 1.6191 (1.3517)	grad_norm 65.4008 (30.8278)	mem 4879MB
[2022-05-31 07:30:00 MetaFG_0] (main.py 265): INFO Train: [65/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2985 (0.3200)	loss 0.8390 (1.3317)	grad_norm 47.3598 (30.9143)	mem 4879MB
[2022-05-31 07:30:03 MetaFG_0] (main.py 265): INFO Train: [65/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2917 (0.3171)	loss 0.9767 (1.3298)	grad_norm 25.5935 (30.6685)	mem 4879MB
[2022-05-31 07:30:06 MetaFG_0] (main.py 265): INFO Train: [65/300][70/1562]	eta 0:07:50 lr 0.000006	time 0.2980 (0.3153)	loss 1.6479 (1.3399)	grad_norm 27.0146 (30.8054)	mem 4879MB
[2022-05-31 07:30:09 MetaFG_0] (main.py 265): INFO Train: [65/300][80/1562]	eta 0:07:45 lr 0.000006	time 0.3000 (0.3140)	loss 0.8938 (1.3310)	grad_norm 20.8998 (30.6564)	mem 4879MB
[2022-05-31 07:30:12 MetaFG_0] (main.py 265): INFO Train: [65/300][90/1562]	eta 0:07:40 lr 0.000006	time 0.3002 (0.3129)	loss 1.1901 (1.3276)	grad_norm 12.9902 (30.0027)	mem 4879MB
[2022-05-31 07:30:15 MetaFG_0] (main.py 265): INFO Train: [65/300][100/1562]	eta 0:07:35 lr 0.000006	time 0.2932 (0.3119)	loss 1.3141 (1.3302)	grad_norm 31.7701 (29.7656)	mem 4879MB
[2022-05-31 07:30:18 MetaFG_0] (main.py 265): INFO Train: [65/300][110/1562]	eta 0:07:31 lr 0.000006	time 0.2990 (0.3112)	loss 1.1536 (1.3300)	grad_norm 29.0539 (29.3358)	mem 4879MB
[2022-05-31 07:30:21 MetaFG_0] (main.py 265): INFO Train: [65/300][120/1562]	eta 0:07:27 lr 0.000006	time 0.2919 (0.3106)	loss 1.3648 (1.3248)	grad_norm 35.0546 (28.8381)	mem 4879MB
[2022-05-31 07:30:24 MetaFG_0] (main.py 265): INFO Train: [65/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2998 (0.3102)	loss 1.4632 (1.3306)	grad_norm 28.0712 (28.7431)	mem 4879MB
[2022-05-31 07:30:27 MetaFG_0] (main.py 265): INFO Train: [65/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.3015 (0.3100)	loss 1.4816 (1.3236)	grad_norm 11.0626 (28.2255)	mem 4879MB
[2022-05-31 07:30:31 MetaFG_0] (main.py 265): INFO Train: [65/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.2926 (0.3096)	loss 1.4262 (1.3256)	grad_norm 26.1682 (28.1911)	mem 4879MB
[2022-05-31 07:30:34 MetaFG_0] (main.py 265): INFO Train: [65/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.3000 (0.3093)	loss 1.1231 (1.3290)	grad_norm 46.3336 (28.4058)	mem 4879MB
[2022-05-31 07:30:37 MetaFG_0] (main.py 265): INFO Train: [65/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2989 (0.3092)	loss 1.4783 (1.3300)	grad_norm 24.5141 (28.1533)	mem 4879MB
[2022-05-31 07:30:40 MetaFG_0] (main.py 265): INFO Train: [65/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2918 (0.3090)	loss 1.5195 (1.3292)	grad_norm 32.7141 (28.2726)	mem 4879MB
[2022-05-31 07:30:43 MetaFG_0] (main.py 265): INFO Train: [65/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2925 (0.3087)	loss 1.4461 (1.3315)	grad_norm 18.6049 (28.1609)	mem 4879MB
[2022-05-31 07:30:46 MetaFG_0] (main.py 265): INFO Train: [65/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2928 (0.3085)	loss 1.0244 (1.3362)	grad_norm 31.7618 (28.2430)	mem 4879MB
[2022-05-31 07:30:49 MetaFG_0] (main.py 265): INFO Train: [65/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.2947 (0.3083)	loss 1.1899 (1.3334)	grad_norm 20.5484 (28.3164)	mem 4879MB
[2022-05-31 07:30:52 MetaFG_0] (main.py 265): INFO Train: [65/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2986 (0.3083)	loss 1.4283 (1.3375)	grad_norm 28.7820 (28.4018)	mem 4879MB
[2022-05-31 07:30:55 MetaFG_0] (main.py 265): INFO Train: [65/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2997 (0.3081)	loss 1.6298 (1.3388)	grad_norm 23.2408 (28.5221)	mem 4879MB
[2022-05-31 07:30:58 MetaFG_0] (main.py 265): INFO Train: [65/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2926 (0.3081)	loss 1.4141 (1.3411)	grad_norm 35.4008 (28.5620)	mem 4879MB
[2022-05-31 07:31:01 MetaFG_0] (main.py 265): INFO Train: [65/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2926 (0.3079)	loss 1.5831 (1.3435)	grad_norm 18.0877 (28.5570)	mem 4879MB
[2022-05-31 07:31:04 MetaFG_0] (main.py 265): INFO Train: [65/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2927 (0.3078)	loss 1.6124 (1.3440)	grad_norm 28.3002 (28.7341)	mem 4879MB
[2022-05-31 07:31:07 MetaFG_0] (main.py 265): INFO Train: [65/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2920 (0.3077)	loss 1.6096 (1.3476)	grad_norm 30.7191 (28.5789)	mem 4879MB
[2022-05-31 07:31:10 MetaFG_0] (main.py 265): INFO Train: [65/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.3001 (0.3076)	loss 1.5824 (1.3420)	grad_norm 19.1882 (28.6481)	mem 4879MB
[2022-05-31 07:31:13 MetaFG_0] (main.py 265): INFO Train: [65/300][290/1562]	eta 0:06:31 lr 0.000006	time 0.2933 (0.3075)	loss 1.5867 (1.3424)	grad_norm 27.7393 (28.9049)	mem 4879MB
[2022-05-31 07:31:16 MetaFG_0] (main.py 265): INFO Train: [65/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.2985 (0.3074)	loss 1.6435 (1.3429)	grad_norm 30.8417 (28.8978)	mem 4879MB
[2022-05-31 07:31:19 MetaFG_0] (main.py 265): INFO Train: [65/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2944 (0.3073)	loss 1.0992 (1.3381)	grad_norm 26.5520 (28.9349)	mem 4879MB
[2022-05-31 07:31:22 MetaFG_0] (main.py 265): INFO Train: [65/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.2933 (0.3072)	loss 1.6268 (1.3378)	grad_norm 30.8373 (28.9153)	mem 4879MB
[2022-05-31 07:31:25 MetaFG_0] (main.py 265): INFO Train: [65/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2932 (0.3072)	loss 1.2817 (1.3348)	grad_norm 18.4491 (28.7949)	mem 4879MB
[2022-05-31 07:31:28 MetaFG_0] (main.py 265): INFO Train: [65/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.2923 (0.3071)	loss 1.3941 (1.3353)	grad_norm 19.6981 (28.8315)	mem 4879MB
[2022-05-31 07:31:32 MetaFG_0] (main.py 265): INFO Train: [65/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.3008 (0.3070)	loss 1.4345 (1.3366)	grad_norm 23.6879 (28.8909)	mem 4879MB
[2022-05-31 07:31:35 MetaFG_0] (main.py 265): INFO Train: [65/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2973 (0.3070)	loss 0.8145 (1.3368)	grad_norm 16.4190 (28.9078)	mem 4879MB
[2022-05-31 07:31:38 MetaFG_0] (main.py 265): INFO Train: [65/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2987 (0.3069)	loss 1.5405 (1.3366)	grad_norm 22.1693 (28.8518)	mem 4879MB
[2022-05-31 07:31:41 MetaFG_0] (main.py 265): INFO Train: [65/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2942 (0.3069)	loss 1.3650 (1.3366)	grad_norm 29.1264 (29.0051)	mem 4879MB
[2022-05-31 07:31:44 MetaFG_0] (main.py 265): INFO Train: [65/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2985 (0.3068)	loss 1.5066 (1.3360)	grad_norm 25.2568 (28.9571)	mem 4879MB
[2022-05-31 07:31:47 MetaFG_0] (main.py 265): INFO Train: [65/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2984 (0.3069)	loss 1.2575 (1.3378)	grad_norm 34.4848 (28.9028)	mem 4879MB
[2022-05-31 07:31:50 MetaFG_0] (main.py 265): INFO Train: [65/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2917 (0.3068)	loss 1.4034 (1.3389)	grad_norm 20.1429 (28.9793)	mem 4879MB
[2022-05-31 07:31:53 MetaFG_0] (main.py 265): INFO Train: [65/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2986 (0.3067)	loss 1.0987 (1.3386)	grad_norm 25.4093 (29.0613)	mem 4879MB
[2022-05-31 07:31:56 MetaFG_0] (main.py 265): INFO Train: [65/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2990 (0.3066)	loss 1.5369 (1.3384)	grad_norm 43.3559 (29.0557)	mem 4879MB
[2022-05-31 07:31:59 MetaFG_0] (main.py 265): INFO Train: [65/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2980 (0.3066)	loss 1.5362 (1.3405)	grad_norm 23.4047 (29.2859)	mem 4879MB
[2022-05-31 07:32:02 MetaFG_0] (main.py 265): INFO Train: [65/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2928 (0.3066)	loss 1.2587 (1.3401)	grad_norm 19.1657 (29.4315)	mem 4879MB
[2022-05-31 07:32:05 MetaFG_0] (main.py 265): INFO Train: [65/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.2939 (0.3065)	loss 1.3127 (1.3410)	grad_norm 35.4465 (29.5469)	mem 4879MB
[2022-05-31 07:32:08 MetaFG_0] (main.py 265): INFO Train: [65/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2919 (0.3065)	loss 1.4198 (1.3412)	grad_norm 33.8789 (29.6012)	mem 4879MB
[2022-05-31 07:32:11 MetaFG_0] (main.py 265): INFO Train: [65/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2935 (0.3064)	loss 1.5901 (1.3429)	grad_norm 24.9482 (29.5281)	mem 4879MB
[2022-05-31 07:32:14 MetaFG_0] (main.py 265): INFO Train: [65/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2989 (0.3063)	loss 1.3142 (1.3413)	grad_norm 48.3675 (29.6909)	mem 4879MB
[2022-05-31 07:32:17 MetaFG_0] (main.py 265): INFO Train: [65/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2932 (0.3063)	loss 1.4078 (1.3408)	grad_norm 18.5192 (29.7368)	mem 4879MB
[2022-05-31 07:32:20 MetaFG_0] (main.py 265): INFO Train: [65/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2928 (0.3062)	loss 1.4211 (1.3399)	grad_norm 23.7563 (29.6921)	mem 4879MB
[2022-05-31 07:32:23 MetaFG_0] (main.py 265): INFO Train: [65/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2919 (0.3062)	loss 1.5327 (1.3398)	grad_norm 33.2286 (29.6608)	mem 4879MB
[2022-05-31 07:32:26 MetaFG_0] (main.py 265): INFO Train: [65/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2940 (0.3061)	loss 1.2486 (1.3403)	grad_norm 22.2017 (29.7049)	mem 4879MB
[2022-05-31 07:32:29 MetaFG_0] (main.py 265): INFO Train: [65/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2933 (0.3061)	loss 1.0472 (1.3389)	grad_norm 26.4497 (29.8016)	mem 4879MB
[2022-05-31 07:32:32 MetaFG_0] (main.py 265): INFO Train: [65/300][550/1562]	eta 0:05:09 lr 0.000006	time 0.2925 (0.3061)	loss 1.4537 (1.3391)	grad_norm 34.9411 (29.8217)	mem 4879MB
[2022-05-31 07:32:35 MetaFG_0] (main.py 265): INFO Train: [65/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2934 (0.3061)	loss 1.5899 (1.3398)	grad_norm 40.3725 (29.8564)	mem 4879MB
[2022-05-31 07:32:39 MetaFG_0] (main.py 265): INFO Train: [65/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2958 (0.3060)	loss 1.6140 (1.3378)	grad_norm 22.2505 (29.9074)	mem 4879MB
[2022-05-31 07:32:42 MetaFG_0] (main.py 265): INFO Train: [65/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2926 (0.3060)	loss 1.0576 (1.3361)	grad_norm 51.8397 (29.9408)	mem 4879MB
[2022-05-31 07:32:45 MetaFG_0] (main.py 265): INFO Train: [65/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2992 (0.3060)	loss 1.2357 (1.3359)	grad_norm 32.6981 (29.9991)	mem 4879MB
[2022-05-31 07:32:48 MetaFG_0] (main.py 265): INFO Train: [65/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2997 (0.3060)	loss 1.2325 (1.3367)	grad_norm 25.4546 (30.0033)	mem 4879MB
[2022-05-31 07:32:51 MetaFG_0] (main.py 265): INFO Train: [65/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2994 (0.3060)	loss 1.3319 (1.3382)	grad_norm 34.0683 (29.9953)	mem 4879MB
[2022-05-31 07:32:54 MetaFG_0] (main.py 265): INFO Train: [65/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2938 (0.3060)	loss 1.4800 (1.3393)	grad_norm 25.2275 (29.9834)	mem 4879MB
[2022-05-31 07:32:57 MetaFG_0] (main.py 265): INFO Train: [65/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2923 (0.3060)	loss 1.1564 (1.3386)	grad_norm 19.8293 (30.0402)	mem 4879MB
[2022-05-31 07:33:00 MetaFG_0] (main.py 265): INFO Train: [65/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2928 (0.3059)	loss 1.3336 (1.3391)	grad_norm 25.8287 (29.9867)	mem 4879MB
[2022-05-31 07:33:03 MetaFG_0] (main.py 265): INFO Train: [65/300][650/1562]	eta 0:04:38 lr 0.000006	time 0.2934 (0.3059)	loss 1.0647 (1.3395)	grad_norm 43.3309 (29.9937)	mem 4879MB
[2022-05-31 07:33:06 MetaFG_0] (main.py 265): INFO Train: [65/300][660/1562]	eta 0:04:35 lr 0.000006	time 0.2987 (0.3059)	loss 1.4737 (1.3394)	grad_norm 26.0093 (29.9627)	mem 4879MB
[2022-05-31 07:33:09 MetaFG_0] (main.py 265): INFO Train: [65/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2985 (0.3059)	loss 1.2721 (1.3392)	grad_norm 34.8343 (29.9140)	mem 4879MB
[2022-05-31 07:33:12 MetaFG_0] (main.py 265): INFO Train: [65/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2986 (0.3059)	loss 0.9304 (1.3375)	grad_norm 50.9476 (29.9897)	mem 4879MB
[2022-05-31 07:33:15 MetaFG_0] (main.py 265): INFO Train: [65/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.3004 (0.3059)	loss 1.0909 (1.3383)	grad_norm 33.8949 (29.9788)	mem 4879MB
[2022-05-31 07:33:18 MetaFG_0] (main.py 265): INFO Train: [65/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2978 (0.3059)	loss 1.5528 (1.3389)	grad_norm 27.7436 (30.0682)	mem 4879MB
[2022-05-31 07:33:21 MetaFG_0] (main.py 265): INFO Train: [65/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2935 (0.3059)	loss 1.0226 (1.3376)	grad_norm 21.7392 (30.0281)	mem 4879MB
[2022-05-31 07:33:24 MetaFG_0] (main.py 265): INFO Train: [65/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.3008 (0.3058)	loss 1.5395 (1.3373)	grad_norm 19.2594 (30.0472)	mem 4879MB
[2022-05-31 07:33:27 MetaFG_0] (main.py 265): INFO Train: [65/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2948 (0.3058)	loss 1.5332 (1.3387)	grad_norm 23.7987 (30.0296)	mem 4879MB
[2022-05-31 07:33:30 MetaFG_0] (main.py 265): INFO Train: [65/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2927 (0.3058)	loss 1.0353 (1.3379)	grad_norm 35.8131 (29.9761)	mem 4879MB
[2022-05-31 07:33:33 MetaFG_0] (main.py 265): INFO Train: [65/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2933 (0.3057)	loss 1.6126 (1.3389)	grad_norm 24.8346 (29.9783)	mem 4879MB
[2022-05-31 07:33:36 MetaFG_0] (main.py 265): INFO Train: [65/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2937 (0.3057)	loss 1.3967 (1.3387)	grad_norm 19.7104 (30.0097)	mem 4879MB
[2022-05-31 07:33:39 MetaFG_0] (main.py 265): INFO Train: [65/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2949 (0.3057)	loss 1.3644 (1.3402)	grad_norm 33.8093 (29.9820)	mem 4879MB
[2022-05-31 07:33:43 MetaFG_0] (main.py 265): INFO Train: [65/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2927 (0.3057)	loss 1.3116 (1.3396)	grad_norm 34.1973 (30.0271)	mem 4879MB
[2022-05-31 07:33:46 MetaFG_0] (main.py 265): INFO Train: [65/300][790/1562]	eta 0:03:55 lr 0.000006	time 0.2979 (0.3057)	loss 1.2733 (1.3401)	grad_norm 44.2643 (29.9819)	mem 4879MB
[2022-05-31 07:33:49 MetaFG_0] (main.py 265): INFO Train: [65/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2946 (0.3056)	loss 1.2915 (1.3396)	grad_norm 15.2449 (30.0556)	mem 4879MB
[2022-05-31 07:33:52 MetaFG_0] (main.py 265): INFO Train: [65/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2917 (0.3056)	loss 1.5300 (1.3403)	grad_norm 30.6418 (30.0671)	mem 4879MB
[2022-05-31 07:33:55 MetaFG_0] (main.py 265): INFO Train: [65/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.2917 (0.3056)	loss 0.7995 (1.3403)	grad_norm 13.7724 (30.0783)	mem 4879MB
[2022-05-31 07:33:58 MetaFG_0] (main.py 265): INFO Train: [65/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2938 (0.3056)	loss 1.1239 (1.3396)	grad_norm 25.5139 (30.1905)	mem 4879MB
[2022-05-31 07:34:01 MetaFG_0] (main.py 265): INFO Train: [65/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2932 (0.3055)	loss 1.4846 (1.3397)	grad_norm 21.6001 (30.1535)	mem 4879MB
[2022-05-31 07:34:04 MetaFG_0] (main.py 265): INFO Train: [65/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2989 (0.3055)	loss 1.5730 (1.3398)	grad_norm 24.7793 (30.1162)	mem 4879MB
[2022-05-31 07:34:07 MetaFG_0] (main.py 265): INFO Train: [65/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2985 (0.3055)	loss 1.2975 (1.3389)	grad_norm 41.6020 (30.0858)	mem 4879MB
[2022-05-31 07:34:10 MetaFG_0] (main.py 265): INFO Train: [65/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2924 (0.3055)	loss 1.1386 (1.3372)	grad_norm 29.3221 (30.1429)	mem 4879MB
[2022-05-31 07:34:13 MetaFG_0] (main.py 265): INFO Train: [65/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2942 (0.3054)	loss 1.5154 (1.3365)	grad_norm 39.2056 (30.1991)	mem 4879MB
[2022-05-31 07:34:16 MetaFG_0] (main.py 265): INFO Train: [65/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2998 (0.3054)	loss 1.3825 (1.3364)	grad_norm 58.1661 (30.2928)	mem 4879MB
[2022-05-31 07:34:19 MetaFG_0] (main.py 265): INFO Train: [65/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2990 (0.3054)	loss 1.2852 (1.3362)	grad_norm 40.0180 (30.2811)	mem 4879MB
[2022-05-31 07:34:22 MetaFG_0] (main.py 265): INFO Train: [65/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2924 (0.3054)	loss 1.1001 (1.3341)	grad_norm 17.1661 (30.2255)	mem 4879MB
[2022-05-31 07:34:25 MetaFG_0] (main.py 265): INFO Train: [65/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2979 (0.3054)	loss 1.1498 (1.3337)	grad_norm 21.0909 (30.2554)	mem 4879MB
[2022-05-31 07:34:28 MetaFG_0] (main.py 265): INFO Train: [65/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2926 (0.3054)	loss 1.4741 (1.3333)	grad_norm 27.6763 (30.2270)	mem 4879MB
[2022-05-31 07:34:31 MetaFG_0] (main.py 265): INFO Train: [65/300][940/1562]	eta 0:03:09 lr 0.000006	time 0.2987 (0.3054)	loss 1.1737 (1.3330)	grad_norm 26.4781 (30.3450)	mem 4879MB
[2022-05-31 07:34:34 MetaFG_0] (main.py 265): INFO Train: [65/300][950/1562]	eta 0:03:06 lr 0.000006	time 0.2939 (0.3053)	loss 1.4422 (1.3333)	grad_norm 25.1057 (30.3257)	mem 4879MB
[2022-05-31 07:34:37 MetaFG_0] (main.py 265): INFO Train: [65/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2925 (0.3053)	loss 1.2724 (1.3335)	grad_norm 25.0823 (30.3016)	mem 4879MB
[2022-05-31 07:34:40 MetaFG_0] (main.py 265): INFO Train: [65/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2987 (0.3053)	loss 1.4809 (1.3331)	grad_norm 28.8888 (30.2290)	mem 4879MB
[2022-05-31 07:34:43 MetaFG_0] (main.py 265): INFO Train: [65/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3325 (0.3054)	loss 1.5591 (1.3344)	grad_norm 26.7773 (30.2309)	mem 4879MB
[2022-05-31 07:34:47 MetaFG_0] (main.py 265): INFO Train: [65/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2994 (0.3055)	loss 1.7286 (1.3349)	grad_norm 134.3244 (30.3686)	mem 4879MB
[2022-05-31 07:34:50 MetaFG_0] (main.py 265): INFO Train: [65/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2935 (0.3055)	loss 1.4348 (1.3357)	grad_norm 29.3838 (30.3517)	mem 4879MB
[2022-05-31 07:34:53 MetaFG_0] (main.py 265): INFO Train: [65/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2933 (0.3055)	loss 1.0822 (1.3356)	grad_norm 26.3653 (30.3780)	mem 4879MB
[2022-05-31 07:34:56 MetaFG_0] (main.py 265): INFO Train: [65/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2989 (0.3055)	loss 1.4837 (1.3359)	grad_norm 20.8309 (30.3550)	mem 4879MB
[2022-05-31 07:34:59 MetaFG_0] (main.py 265): INFO Train: [65/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2929 (0.3054)	loss 1.5085 (1.3358)	grad_norm 27.1850 (30.3593)	mem 4879MB
[2022-05-31 07:35:02 MetaFG_0] (main.py 265): INFO Train: [65/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2977 (0.3054)	loss 1.0728 (1.3355)	grad_norm 32.6780 (30.3710)	mem 4879MB
[2022-05-31 07:35:05 MetaFG_0] (main.py 265): INFO Train: [65/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2941 (0.3054)	loss 1.6306 (1.3362)	grad_norm 21.7965 (30.3319)	mem 4879MB
[2022-05-31 07:35:08 MetaFG_0] (main.py 265): INFO Train: [65/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2927 (0.3054)	loss 1.4316 (1.3368)	grad_norm 28.8180 (30.3106)	mem 4879MB
[2022-05-31 07:35:11 MetaFG_0] (main.py 265): INFO Train: [65/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.2920 (0.3054)	loss 1.6095 (1.3360)	grad_norm 27.7883 (30.3259)	mem 4879MB
[2022-05-31 07:35:14 MetaFG_0] (main.py 265): INFO Train: [65/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2990 (0.3054)	loss 1.3515 (1.3364)	grad_norm 40.6661 (30.3034)	mem 4879MB
[2022-05-31 07:35:17 MetaFG_0] (main.py 265): INFO Train: [65/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2999 (0.3054)	loss 1.5898 (1.3373)	grad_norm 30.9582 (30.2996)	mem 4879MB
[2022-05-31 07:35:20 MetaFG_0] (main.py 265): INFO Train: [65/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2925 (0.3054)	loss 1.3829 (1.3376)	grad_norm 37.0487 (30.2698)	mem 4879MB
[2022-05-31 07:35:23 MetaFG_0] (main.py 265): INFO Train: [65/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2994 (0.3054)	loss 1.4959 (1.3376)	grad_norm 28.5493 (30.3119)	mem 4879MB
[2022-05-31 07:35:26 MetaFG_0] (main.py 265): INFO Train: [65/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2949 (0.3054)	loss 1.2128 (1.3371)	grad_norm 34.2862 (30.3604)	mem 4879MB
[2022-05-31 07:35:29 MetaFG_0] (main.py 265): INFO Train: [65/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2929 (0.3054)	loss 1.1020 (1.3368)	grad_norm 17.7948 (30.3554)	mem 4879MB
[2022-05-31 07:35:32 MetaFG_0] (main.py 265): INFO Train: [65/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2938 (0.3054)	loss 1.3389 (1.3361)	grad_norm 23.2764 (30.3632)	mem 4879MB
[2022-05-31 07:35:35 MetaFG_0] (main.py 265): INFO Train: [65/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2928 (0.3054)	loss 1.3822 (1.3365)	grad_norm 18.5349 (30.3160)	mem 4879MB
[2022-05-31 07:35:38 MetaFG_0] (main.py 265): INFO Train: [65/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2921 (0.3054)	loss 1.4031 (1.3367)	grad_norm 15.3062 (30.2933)	mem 4879MB
[2022-05-31 07:35:41 MetaFG_0] (main.py 265): INFO Train: [65/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.3017 (0.3054)	loss 1.4276 (1.3362)	grad_norm 39.4347 (30.2849)	mem 4879MB
[2022-05-31 07:35:44 MetaFG_0] (main.py 265): INFO Train: [65/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2924 (0.3054)	loss 1.4553 (1.3365)	grad_norm 21.3520 (30.2939)	mem 4879MB
[2022-05-31 07:35:47 MetaFG_0] (main.py 265): INFO Train: [65/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2994 (0.3054)	loss 1.5025 (1.3370)	grad_norm 27.0469 (30.2685)	mem 4879MB
[2022-05-31 07:35:51 MetaFG_0] (main.py 265): INFO Train: [65/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2996 (0.3054)	loss 0.7137 (1.3372)	grad_norm 24.1647 (30.3022)	mem 4879MB
[2022-05-31 07:35:54 MetaFG_0] (main.py 265): INFO Train: [65/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2926 (0.3053)	loss 1.2215 (1.3372)	grad_norm 13.7708 (30.2617)	mem 4879MB
[2022-05-31 07:35:57 MetaFG_0] (main.py 265): INFO Train: [65/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2928 (0.3053)	loss 0.9235 (1.3371)	grad_norm 67.9760 (30.2893)	mem 4879MB
[2022-05-31 07:36:00 MetaFG_0] (main.py 265): INFO Train: [65/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2936 (0.3053)	loss 1.4541 (1.3375)	grad_norm 24.1508 (30.2912)	mem 4879MB
[2022-05-31 07:36:03 MetaFG_0] (main.py 265): INFO Train: [65/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2991 (0.3053)	loss 1.4691 (1.3382)	grad_norm 38.3903 (30.2979)	mem 4879MB
[2022-05-31 07:36:06 MetaFG_0] (main.py 265): INFO Train: [65/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.3033 (0.3053)	loss 1.6283 (1.3387)	grad_norm 48.0052 (30.3279)	mem 4879MB
[2022-05-31 07:36:09 MetaFG_0] (main.py 265): INFO Train: [65/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2926 (0.3053)	loss 1.0561 (1.3383)	grad_norm 20.0838 (30.3241)	mem 4879MB
[2022-05-31 07:36:12 MetaFG_0] (main.py 265): INFO Train: [65/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2917 (0.3053)	loss 1.4357 (1.3379)	grad_norm 31.3557 (30.3293)	mem 4879MB
[2022-05-31 07:36:15 MetaFG_0] (main.py 265): INFO Train: [65/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2983 (0.3053)	loss 0.8143 (1.3377)	grad_norm 22.5625 (30.3347)	mem 4879MB
[2022-05-31 07:36:18 MetaFG_0] (main.py 265): INFO Train: [65/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.3029 (0.3053)	loss 1.4519 (1.3388)	grad_norm 35.3976 (30.3596)	mem 4879MB
[2022-05-31 07:36:21 MetaFG_0] (main.py 265): INFO Train: [65/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2984 (0.3053)	loss 1.5071 (1.3386)	grad_norm 26.9029 (30.3455)	mem 4879MB
[2022-05-31 07:36:24 MetaFG_0] (main.py 265): INFO Train: [65/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2963 (0.3053)	loss 1.7153 (1.3392)	grad_norm 32.1324 (30.3245)	mem 4879MB
[2022-05-31 07:36:27 MetaFG_0] (main.py 265): INFO Train: [65/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2972 (0.3052)	loss 0.9920 (1.3398)	grad_norm 28.6671 (30.3497)	mem 4879MB
[2022-05-31 07:36:30 MetaFG_0] (main.py 265): INFO Train: [65/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2937 (0.3052)	loss 1.6147 (1.3390)	grad_norm 27.2749 (30.3350)	mem 4879MB
[2022-05-31 07:36:33 MetaFG_0] (main.py 265): INFO Train: [65/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2950 (0.3052)	loss 1.5571 (1.3396)	grad_norm 19.7894 (30.2945)	mem 4879MB
[2022-05-31 07:36:36 MetaFG_0] (main.py 265): INFO Train: [65/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2995 (0.3052)	loss 1.3573 (1.3397)	grad_norm 27.5149 (30.3007)	mem 4879MB
[2022-05-31 07:36:39 MetaFG_0] (main.py 265): INFO Train: [65/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2993 (0.3052)	loss 1.3365 (1.3397)	grad_norm 38.1385 (30.3181)	mem 4879MB
[2022-05-31 07:36:42 MetaFG_0] (main.py 265): INFO Train: [65/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2926 (0.3052)	loss 1.3922 (1.3397)	grad_norm 15.3168 (30.3385)	mem 4879MB
[2022-05-31 07:36:45 MetaFG_0] (main.py 265): INFO Train: [65/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2932 (0.3052)	loss 1.4007 (1.3390)	grad_norm 28.2592 (30.3506)	mem 4879MB
[2022-05-31 07:36:48 MetaFG_0] (main.py 265): INFO Train: [65/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2926 (0.3052)	loss 1.3945 (1.3389)	grad_norm 32.8158 (30.3491)	mem 4879MB
[2022-05-31 07:36:51 MetaFG_0] (main.py 265): INFO Train: [65/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.3001 (0.3052)	loss 1.2427 (1.3389)	grad_norm 41.2442 (30.3518)	mem 4879MB
[2022-05-31 07:36:54 MetaFG_0] (main.py 265): INFO Train: [65/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2943 (0.3052)	loss 1.5707 (1.3395)	grad_norm 23.0209 (30.3469)	mem 4879MB
[2022-05-31 07:36:58 MetaFG_0] (main.py 265): INFO Train: [65/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2994 (0.3052)	loss 1.2853 (1.3394)	grad_norm 21.5093 (30.3538)	mem 4879MB
[2022-05-31 07:37:01 MetaFG_0] (main.py 265): INFO Train: [65/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2976 (0.3052)	loss 0.9562 (1.3389)	grad_norm 27.5623 (30.3270)	mem 4879MB
[2022-05-31 07:37:04 MetaFG_0] (main.py 265): INFO Train: [65/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2929 (0.3052)	loss 1.3932 (1.3385)	grad_norm 27.4104 (30.3144)	mem 4879MB
[2022-05-31 07:37:07 MetaFG_0] (main.py 265): INFO Train: [65/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2924 (0.3052)	loss 1.5572 (1.3394)	grad_norm 36.0031 (30.3195)	mem 4879MB
[2022-05-31 07:37:10 MetaFG_0] (main.py 265): INFO Train: [65/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.3003 (0.3052)	loss 1.4955 (1.3394)	grad_norm 23.0579 (30.3357)	mem 4879MB
[2022-05-31 07:37:13 MetaFG_0] (main.py 265): INFO Train: [65/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2920 (0.3052)	loss 1.4633 (1.3396)	grad_norm 38.6534 (30.3320)	mem 4879MB
[2022-05-31 07:37:16 MetaFG_0] (main.py 265): INFO Train: [65/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2940 (0.3052)	loss 1.3323 (1.3390)	grad_norm 20.5710 (30.3319)	mem 4879MB
[2022-05-31 07:37:19 MetaFG_0] (main.py 265): INFO Train: [65/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2986 (0.3052)	loss 1.2503 (1.3390)	grad_norm 35.9881 (30.3361)	mem 4879MB
[2022-05-31 07:37:22 MetaFG_0] (main.py 265): INFO Train: [65/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2980 (0.3052)	loss 1.2482 (1.3394)	grad_norm 24.2993 (30.3055)	mem 4879MB
[2022-05-31 07:37:25 MetaFG_0] (main.py 265): INFO Train: [65/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2937 (0.3051)	loss 1.3055 (1.3396)	grad_norm 22.2128 (30.2739)	mem 4879MB
[2022-05-31 07:37:28 MetaFG_0] (main.py 265): INFO Train: [65/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2937 (0.3052)	loss 1.3773 (1.3390)	grad_norm 15.6734 (30.2631)	mem 4879MB
[2022-05-31 07:37:31 MetaFG_0] (main.py 265): INFO Train: [65/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2936 (0.3051)	loss 1.4243 (1.3382)	grad_norm 27.2106 (30.2376)	mem 4879MB
[2022-05-31 07:37:34 MetaFG_0] (main.py 265): INFO Train: [65/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2922 (0.3051)	loss 1.1890 (1.3388)	grad_norm 24.1970 (30.2269)	mem 4879MB
[2022-05-31 07:37:37 MetaFG_0] (main.py 265): INFO Train: [65/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2939 (0.3051)	loss 1.2897 (1.3386)	grad_norm 21.0912 (30.2065)	mem 4879MB
[2022-05-31 07:37:40 MetaFG_0] (main.py 265): INFO Train: [65/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2913 (0.3051)	loss 1.2104 (1.3383)	grad_norm 32.0881 (30.1844)	mem 4879MB
[2022-05-31 07:37:40 MetaFG_0] (main.py 272): INFO EPOCH 65 training takes 0:07:56
[2022-05-31 07:37:40 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_65.pth saving......
[2022-05-31 07:37:41 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_65.pth saved !!!
[2022-05-31 07:37:41 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:37:43 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:37:43 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:37:43 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.652 (0.652)	Loss 0.5539 (0.5539)	Acc@1 87.500 (87.500)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 07:37:44 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.094 (0.147)	Loss 0.6168 (0.6620)	Acc@1 87.500 (82.386)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 07:37:45 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.122)	Loss 0.4555 (0.6256)	Acc@1 90.625 (84.226)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 07:37:46 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.093 (0.112)	Loss 0.7720 (0.6113)	Acc@1 78.125 (85.685)	Acc@5 100.000 (98.286)	Mem 4879MB
[2022-05-31 07:37:47 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.097 (0.108)	Loss 0.5627 (0.5878)	Acc@1 84.375 (86.204)	Acc@5 100.000 (98.476)	Mem 4879MB
[2022-05-31 07:37:48 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.106)	Loss 0.4231 (0.5875)	Acc@1 93.750 (86.275)	Acc@5 100.000 (98.468)	Mem 4879MB
[2022-05-31 07:37:49 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.104)	Loss 0.5144 (0.5943)	Acc@1 87.500 (86.219)	Acc@5 100.000 (98.412)	Mem 4879MB
[2022-05-31 07:37:50 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.102)	Loss 0.7009 (0.5849)	Acc@1 87.500 (86.576)	Acc@5 96.875 (98.504)	Mem 4879MB
[2022-05-31 07:37:51 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.101)	Loss 0.7253 (0.6044)	Acc@1 81.250 (86.073)	Acc@5 93.750 (98.418)	Mem 4879MB
[2022-05-31 07:37:52 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.092 (0.100)	Loss 0.6739 (0.6007)	Acc@1 78.125 (86.092)	Acc@5 100.000 (98.386)	Mem 4879MB
[2022-05-31 07:37:53 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.3914 (0.5911)	Acc@1 96.875 (86.479)	Acc@5 100.000 (98.484)	Mem 4879MB
[2022-05-31 07:37:54 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.099)	Loss 0.7494 (0.5930)	Acc@1 78.125 (86.430)	Acc@5 100.000 (98.480)	Mem 4879MB
[2022-05-31 07:37:55 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.099)	Loss 0.6052 (0.5866)	Acc@1 87.500 (86.777)	Acc@5 100.000 (98.450)	Mem 4879MB
[2022-05-31 07:37:56 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.098)	Loss 0.2674 (0.5826)	Acc@1 100.000 (86.904)	Acc@5 100.000 (98.473)	Mem 4879MB
[2022-05-31 07:37:57 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.5498 (0.5910)	Acc@1 81.250 (86.658)	Acc@5 100.000 (98.404)	Mem 4879MB
[2022-05-31 07:37:58 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.102 (0.098)	Loss 0.6203 (0.5868)	Acc@1 87.500 (86.858)	Acc@5 100.000 (98.427)	Mem 4879MB
[2022-05-31 07:37:58 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.5567 (0.5848)	Acc@1 90.625 (86.840)	Acc@5 100.000 (98.486)	Mem 4879MB
[2022-05-31 07:37:59 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.097)	Loss 0.4287 (0.5810)	Acc@1 93.750 (87.043)	Acc@5 96.875 (98.501)	Mem 4879MB
[2022-05-31 07:38:00 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.093 (0.097)	Loss 0.5044 (0.5807)	Acc@1 90.625 (86.948)	Acc@5 100.000 (98.532)	Mem 4879MB
[2022-05-31 07:38:01 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 0.5348 (0.5775)	Acc@1 90.625 (87.026)	Acc@5 96.875 (98.593)	Mem 4879MB
[2022-05-31 07:38:02 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 0.6910 (0.5775)	Acc@1 84.375 (86.971)	Acc@5 96.875 (98.601)	Mem 4879MB
[2022-05-31 07:38:03 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.096)	Loss 0.4311 (0.5722)	Acc@1 93.750 (87.248)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 07:38:04 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.096)	Loss 0.7630 (0.5726)	Acc@1 81.250 (87.146)	Acc@5 100.000 (98.643)	Mem 4879MB
[2022-05-31 07:38:05 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.097 (0.096)	Loss 0.8933 (0.5745)	Acc@1 84.375 (87.162)	Acc@5 90.625 (98.607)	Mem 4879MB
[2022-05-31 07:38:06 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.096)	Loss 0.5275 (0.5746)	Acc@1 90.625 (87.150)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 07:38:07 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.096)	Loss 0.4545 (0.5771)	Acc@1 93.750 (87.126)	Acc@5 96.875 (98.543)	Mem 4879MB
[2022-05-31 07:38:08 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.096)	Loss 0.6079 (0.5762)	Acc@1 81.250 (87.117)	Acc@5 100.000 (98.587)	Mem 4879MB
[2022-05-31 07:38:09 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.096)	Loss 0.8092 (0.5758)	Acc@1 81.250 (87.143)	Acc@5 100.000 (98.616)	Mem 4879MB
[2022-05-31 07:38:10 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.5660 (0.5779)	Acc@1 81.250 (87.022)	Acc@5 100.000 (98.610)	Mem 4879MB
[2022-05-31 07:38:11 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.3333 (0.5784)	Acc@1 93.750 (87.006)	Acc@5 100.000 (98.615)	Mem 4879MB
[2022-05-31 07:38:12 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.092 (0.096)	Loss 0.2828 (0.5768)	Acc@1 100.000 (87.095)	Acc@5 100.000 (98.630)	Mem 4879MB
[2022-05-31 07:38:13 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6937 (0.5761)	Acc@1 84.375 (87.138)	Acc@5 96.875 (98.643)	Mem 4879MB
[2022-05-31 07:38:13 MetaFG_0] (main.py 330): INFO  * Acc@1 87.150 Acc@5 98.630
[2022-05-31 07:38:13 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.2%
[2022-05-31 07:38:13 MetaFG_0] (main.py 171): INFO Max accuracy: 87.28%
[2022-05-31 07:38:14 MetaFG_0] (main.py 265): INFO Train: [66/300][0/1562]	eta 0:28:05 lr 0.000006	time 1.0789 (1.0789)	loss 1.6177 (1.6177)	grad_norm 22.3965 (22.3965)	mem 4879MB
[2022-05-31 07:38:17 MetaFG_0] (main.py 265): INFO Train: [66/300][10/1562]	eta 0:09:49 lr 0.000006	time 0.2924 (0.3796)	loss 1.3745 (1.2741)	grad_norm 33.9717 (27.0704)	mem 4879MB
[2022-05-31 07:38:20 MetaFG_0] (main.py 265): INFO Train: [66/300][20/1562]	eta 0:08:48 lr 0.000006	time 0.2924 (0.3426)	loss 1.3518 (1.3123)	grad_norm 24.4029 (27.7441)	mem 4879MB
[2022-05-31 07:38:23 MetaFG_0] (main.py 265): INFO Train: [66/300][30/1562]	eta 0:08:25 lr 0.000006	time 0.2937 (0.3301)	loss 1.6019 (1.3286)	grad_norm 25.9287 (28.1215)	mem 4879MB
[2022-05-31 07:38:26 MetaFG_0] (main.py 265): INFO Train: [66/300][40/1562]	eta 0:08:13 lr 0.000006	time 0.2981 (0.3245)	loss 1.2714 (1.3507)	grad_norm 44.1501 (29.1640)	mem 4879MB
[2022-05-31 07:38:29 MetaFG_0] (main.py 265): INFO Train: [66/300][50/1562]	eta 0:08:04 lr 0.000006	time 0.2993 (0.3204)	loss 1.2197 (1.3312)	grad_norm 28.3870 (29.6311)	mem 4879MB
[2022-05-31 07:38:32 MetaFG_0] (main.py 265): INFO Train: [66/300][60/1562]	eta 0:07:57 lr 0.000006	time 0.2936 (0.3180)	loss 1.5069 (1.3262)	grad_norm 28.1122 (30.3188)	mem 4879MB
[2022-05-31 07:38:35 MetaFG_0] (main.py 265): INFO Train: [66/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2977 (0.3160)	loss 1.5619 (1.3453)	grad_norm 24.3233 (29.6660)	mem 4879MB
[2022-05-31 07:38:38 MetaFG_0] (main.py 265): INFO Train: [66/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2919 (0.3145)	loss 0.9086 (1.3318)	grad_norm 37.3994 (30.4583)	mem 4879MB
[2022-05-31 07:38:41 MetaFG_0] (main.py 265): INFO Train: [66/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2926 (0.3136)	loss 1.6381 (1.3294)	grad_norm 22.1834 (30.0142)	mem 4879MB
[2022-05-31 07:38:44 MetaFG_0] (main.py 265): INFO Train: [66/300][100/1562]	eta 0:07:36 lr 0.000006	time 0.2921 (0.3125)	loss 1.4644 (1.3345)	grad_norm 41.0906 (29.3686)	mem 4879MB
[2022-05-31 07:38:47 MetaFG_0] (main.py 265): INFO Train: [66/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2926 (0.3117)	loss 1.1255 (1.3327)	grad_norm 42.4711 (28.8907)	mem 4879MB
[2022-05-31 07:38:50 MetaFG_0] (main.py 265): INFO Train: [66/300][120/1562]	eta 0:07:28 lr 0.000006	time 0.2988 (0.3111)	loss 1.5358 (1.3366)	grad_norm 26.4486 (28.4352)	mem 4879MB
[2022-05-31 07:38:53 MetaFG_0] (main.py 265): INFO Train: [66/300][130/1562]	eta 0:07:24 lr 0.000006	time 0.2928 (0.3105)	loss 1.3689 (1.3382)	grad_norm 24.9242 (28.5930)	mem 4879MB
[2022-05-31 07:38:56 MetaFG_0] (main.py 265): INFO Train: [66/300][140/1562]	eta 0:07:20 lr 0.000006	time 0.2953 (0.3101)	loss 1.4242 (1.3436)	grad_norm 27.7905 (28.7704)	mem 4879MB
[2022-05-31 07:39:00 MetaFG_0] (main.py 265): INFO Train: [66/300][150/1562]	eta 0:07:17 lr 0.000006	time 0.3000 (0.3097)	loss 1.1601 (1.3454)	grad_norm 41.5044 (28.7301)	mem 4879MB
[2022-05-31 07:39:03 MetaFG_0] (main.py 265): INFO Train: [66/300][160/1562]	eta 0:07:13 lr 0.000006	time 0.2935 (0.3095)	loss 1.0120 (1.3313)	grad_norm 37.0576 (28.7461)	mem 4879MB
[2022-05-31 07:39:06 MetaFG_0] (main.py 265): INFO Train: [66/300][170/1562]	eta 0:07:10 lr 0.000006	time 0.2940 (0.3093)	loss 1.1704 (1.3314)	grad_norm 57.3616 (29.7198)	mem 4879MB
[2022-05-31 07:39:09 MetaFG_0] (main.py 265): INFO Train: [66/300][180/1562]	eta 0:07:07 lr 0.000006	time 0.2927 (0.3091)	loss 1.7184 (1.3344)	grad_norm 30.0836 (29.3893)	mem 4879MB
[2022-05-31 07:39:12 MetaFG_0] (main.py 265): INFO Train: [66/300][190/1562]	eta 0:07:03 lr 0.000006	time 0.2979 (0.3088)	loss 1.7361 (1.3378)	grad_norm 21.7991 (29.7814)	mem 4879MB
[2022-05-31 07:39:15 MetaFG_0] (main.py 265): INFO Train: [66/300][200/1562]	eta 0:07:00 lr 0.000006	time 0.2924 (0.3086)	loss 1.0192 (1.3370)	grad_norm 38.4021 (29.9662)	mem 4879MB
[2022-05-31 07:39:18 MetaFG_0] (main.py 265): INFO Train: [66/300][210/1562]	eta 0:06:56 lr 0.000006	time 0.3001 (0.3084)	loss 1.1954 (1.3378)	grad_norm 27.2670 (30.2055)	mem 4879MB
[2022-05-31 07:39:21 MetaFG_0] (main.py 265): INFO Train: [66/300][220/1562]	eta 0:06:53 lr 0.000006	time 0.2974 (0.3082)	loss 1.4486 (1.3369)	grad_norm 32.8912 (30.3992)	mem 4879MB
[2022-05-31 07:39:24 MetaFG_0] (main.py 265): INFO Train: [66/300][230/1562]	eta 0:06:50 lr 0.000006	time 0.2981 (0.3081)	loss 1.4802 (1.3362)	grad_norm 20.8670 (30.6298)	mem 4879MB
[2022-05-31 07:39:27 MetaFG_0] (main.py 265): INFO Train: [66/300][240/1562]	eta 0:06:47 lr 0.000006	time 0.2928 (0.3080)	loss 1.4862 (1.3398)	grad_norm 20.4562 (30.4681)	mem 4879MB
[2022-05-31 07:39:30 MetaFG_0] (main.py 265): INFO Train: [66/300][250/1562]	eta 0:06:43 lr 0.000006	time 0.2975 (0.3078)	loss 1.4727 (1.3408)	grad_norm 26.2346 (30.3860)	mem 4879MB
[2022-05-31 07:39:33 MetaFG_0] (main.py 265): INFO Train: [66/300][260/1562]	eta 0:06:40 lr 0.000006	time 0.2929 (0.3077)	loss 0.9781 (1.3390)	grad_norm 26.6660 (30.5194)	mem 4879MB
[2022-05-31 07:39:36 MetaFG_0] (main.py 265): INFO Train: [66/300][270/1562]	eta 0:06:37 lr 0.000006	time 0.2946 (0.3075)	loss 1.4723 (1.3385)	grad_norm 31.7659 (30.4422)	mem 4879MB
[2022-05-31 07:39:39 MetaFG_0] (main.py 265): INFO Train: [66/300][280/1562]	eta 0:06:34 lr 0.000006	time 0.2984 (0.3074)	loss 1.2475 (1.3429)	grad_norm 46.5637 (30.4756)	mem 4879MB
[2022-05-31 07:39:42 MetaFG_0] (main.py 265): INFO Train: [66/300][290/1562]	eta 0:06:30 lr 0.000006	time 0.2921 (0.3072)	loss 1.0906 (1.3415)	grad_norm 35.3418 (30.4391)	mem 4879MB
[2022-05-31 07:39:45 MetaFG_0] (main.py 265): INFO Train: [66/300][300/1562]	eta 0:06:27 lr 0.000006	time 0.3091 (0.3072)	loss 1.7251 (1.3446)	grad_norm 49.8787 (30.5139)	mem 4879MB
[2022-05-31 07:39:48 MetaFG_0] (main.py 265): INFO Train: [66/300][310/1562]	eta 0:06:24 lr 0.000006	time 0.2925 (0.3071)	loss 1.5819 (1.3455)	grad_norm 59.6923 (30.4356)	mem 4879MB
[2022-05-31 07:39:51 MetaFG_0] (main.py 265): INFO Train: [66/300][320/1562]	eta 0:06:21 lr 0.000006	time 0.3013 (0.3070)	loss 1.4019 (1.3468)	grad_norm 42.3718 (30.4068)	mem 4879MB
[2022-05-31 07:39:54 MetaFG_0] (main.py 265): INFO Train: [66/300][330/1562]	eta 0:06:18 lr 0.000006	time 0.2941 (0.3069)	loss 1.0832 (1.3485)	grad_norm 31.9014 (30.4604)	mem 4879MB
[2022-05-31 07:39:57 MetaFG_0] (main.py 265): INFO Train: [66/300][340/1562]	eta 0:06:15 lr 0.000006	time 0.3009 (0.3069)	loss 1.7032 (1.3520)	grad_norm 38.8201 (30.6307)	mem 4879MB
[2022-05-31 07:40:00 MetaFG_0] (main.py 265): INFO Train: [66/300][350/1562]	eta 0:06:11 lr 0.000006	time 0.2999 (0.3068)	loss 1.6530 (1.3483)	grad_norm 48.7664 (30.7113)	mem 4879MB
[2022-05-31 07:40:03 MetaFG_0] (main.py 265): INFO Train: [66/300][360/1562]	eta 0:06:08 lr 0.000006	time 0.2919 (0.3067)	loss 1.3280 (1.3438)	grad_norm 37.8835 (30.7778)	mem 4879MB
[2022-05-31 07:40:07 MetaFG_0] (main.py 265): INFO Train: [66/300][370/1562]	eta 0:06:05 lr 0.000006	time 0.2932 (0.3067)	loss 1.4602 (1.3428)	grad_norm 39.3864 (30.6781)	mem 4879MB
[2022-05-31 07:40:10 MetaFG_0] (main.py 265): INFO Train: [66/300][380/1562]	eta 0:06:02 lr 0.000006	time 0.2940 (0.3066)	loss 1.3089 (1.3452)	grad_norm 22.2790 (30.6604)	mem 4879MB
[2022-05-31 07:40:13 MetaFG_0] (main.py 265): INFO Train: [66/300][390/1562]	eta 0:05:59 lr 0.000006	time 0.2922 (0.3065)	loss 1.2951 (1.3447)	grad_norm 20.4603 (30.6269)	mem 4879MB
[2022-05-31 07:40:16 MetaFG_0] (main.py 265): INFO Train: [66/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2957 (0.3065)	loss 1.4804 (1.3442)	grad_norm 24.6993 (30.6923)	mem 4879MB
[2022-05-31 07:40:19 MetaFG_0] (main.py 265): INFO Train: [66/300][410/1562]	eta 0:05:52 lr 0.000006	time 0.2923 (0.3064)	loss 1.2745 (1.3454)	grad_norm 23.2858 (30.8810)	mem 4879MB
[2022-05-31 07:40:22 MetaFG_0] (main.py 265): INFO Train: [66/300][420/1562]	eta 0:05:49 lr 0.000006	time 0.2923 (0.3063)	loss 1.4472 (1.3466)	grad_norm 32.2468 (31.0748)	mem 4879MB
[2022-05-31 07:40:25 MetaFG_0] (main.py 265): INFO Train: [66/300][430/1562]	eta 0:05:46 lr 0.000006	time 0.2923 (0.3063)	loss 1.4022 (1.3468)	grad_norm 31.7188 (30.9408)	mem 4879MB
[2022-05-31 07:40:28 MetaFG_0] (main.py 265): INFO Train: [66/300][440/1562]	eta 0:05:43 lr 0.000006	time 0.3002 (0.3062)	loss 1.2699 (1.3457)	grad_norm 21.7882 (30.9147)	mem 4879MB
[2022-05-31 07:40:31 MetaFG_0] (main.py 265): INFO Train: [66/300][450/1562]	eta 0:05:40 lr 0.000006	time 0.2921 (0.3062)	loss 1.3787 (1.3473)	grad_norm 37.8892 (31.2049)	mem 4879MB
[2022-05-31 07:40:34 MetaFG_0] (main.py 265): INFO Train: [66/300][460/1562]	eta 0:05:37 lr 0.000006	time 0.3000 (0.3062)	loss 1.4147 (1.3432)	grad_norm 31.2292 (31.1296)	mem 4879MB
[2022-05-31 07:40:37 MetaFG_0] (main.py 265): INFO Train: [66/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2927 (0.3061)	loss 1.2759 (1.3448)	grad_norm 45.7722 (31.0513)	mem 4879MB
[2022-05-31 07:40:40 MetaFG_0] (main.py 265): INFO Train: [66/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.2932 (0.3061)	loss 1.3584 (1.3463)	grad_norm 25.8246 (30.9841)	mem 4879MB
[2022-05-31 07:40:43 MetaFG_0] (main.py 265): INFO Train: [66/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2925 (0.3061)	loss 0.9971 (1.3445)	grad_norm 44.4085 (31.1241)	mem 4879MB
[2022-05-31 07:40:46 MetaFG_0] (main.py 265): INFO Train: [66/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2995 (0.3060)	loss 0.8952 (1.3442)	grad_norm 39.5445 (31.1398)	mem 4879MB
[2022-05-31 07:40:49 MetaFG_0] (main.py 265): INFO Train: [66/300][510/1562]	eta 0:05:21 lr 0.000006	time 0.2928 (0.3060)	loss 1.4036 (1.3429)	grad_norm 17.3061 (31.0322)	mem 4879MB
[2022-05-31 07:40:52 MetaFG_0] (main.py 265): INFO Train: [66/300][520/1562]	eta 0:05:18 lr 0.000006	time 0.2985 (0.3060)	loss 1.0783 (1.3413)	grad_norm 17.7300 (30.9447)	mem 4879MB
[2022-05-31 07:40:55 MetaFG_0] (main.py 265): INFO Train: [66/300][530/1562]	eta 0:05:15 lr 0.000006	time 0.2932 (0.3060)	loss 1.0112 (1.3416)	grad_norm 30.8757 (31.0056)	mem 4879MB
[2022-05-31 07:40:58 MetaFG_0] (main.py 265): INFO Train: [66/300][540/1562]	eta 0:05:12 lr 0.000006	time 0.2990 (0.3060)	loss 1.4509 (1.3398)	grad_norm 27.2984 (30.9858)	mem 4879MB
[2022-05-31 07:41:02 MetaFG_0] (main.py 265): INFO Train: [66/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.3178 (0.3064)	loss 1.4689 (1.3402)	grad_norm 36.5855 (31.0184)	mem 4879MB
[2022-05-31 07:41:05 MetaFG_0] (main.py 265): INFO Train: [66/300][560/1562]	eta 0:05:06 lr 0.000006	time 0.2999 (0.3064)	loss 1.3983 (1.3401)	grad_norm 29.0401 (31.0950)	mem 4879MB
[2022-05-31 07:41:08 MetaFG_0] (main.py 265): INFO Train: [66/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2973 (0.3063)	loss 1.5598 (1.3396)	grad_norm 23.8114 (31.0392)	mem 4879MB
[2022-05-31 07:41:11 MetaFG_0] (main.py 265): INFO Train: [66/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.2925 (0.3063)	loss 1.4688 (1.3407)	grad_norm 21.0779 (30.9443)	mem 4879MB
[2022-05-31 07:41:14 MetaFG_0] (main.py 265): INFO Train: [66/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2996 (0.3063)	loss 1.4804 (1.3412)	grad_norm 43.3107 (30.9768)	mem 4879MB
[2022-05-31 07:41:17 MetaFG_0] (main.py 265): INFO Train: [66/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2920 (0.3062)	loss 1.5167 (1.3412)	grad_norm 50.9209 (31.0030)	mem 4879MB
[2022-05-31 07:41:20 MetaFG_0] (main.py 265): INFO Train: [66/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2920 (0.3062)	loss 1.5892 (1.3415)	grad_norm 18.3621 (30.9948)	mem 4879MB
[2022-05-31 07:41:23 MetaFG_0] (main.py 265): INFO Train: [66/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2994 (0.3062)	loss 1.3158 (1.3406)	grad_norm 22.1144 (30.9587)	mem 4879MB
[2022-05-31 07:41:26 MetaFG_0] (main.py 265): INFO Train: [66/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2971 (0.3061)	loss 1.1145 (1.3401)	grad_norm 23.8616 (30.9013)	mem 4879MB
[2022-05-31 07:41:29 MetaFG_0] (main.py 265): INFO Train: [66/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2941 (0.3061)	loss 1.0882 (1.3401)	grad_norm 46.3851 (30.9053)	mem 4879MB
[2022-05-31 07:41:32 MetaFG_0] (main.py 265): INFO Train: [66/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2934 (0.3060)	loss 1.3379 (1.3394)	grad_norm 27.0015 (30.9492)	mem 4879MB
[2022-05-31 07:41:35 MetaFG_0] (main.py 265): INFO Train: [66/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2924 (0.3060)	loss 1.0494 (1.3378)	grad_norm 49.9977 (30.9865)	mem 4879MB
[2022-05-31 07:41:38 MetaFG_0] (main.py 265): INFO Train: [66/300][670/1562]	eta 0:04:32 lr 0.000006	time 0.2929 (0.3060)	loss 0.8089 (1.3373)	grad_norm 18.5728 (30.9364)	mem 4879MB
[2022-05-31 07:41:41 MetaFG_0] (main.py 265): INFO Train: [66/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2984 (0.3060)	loss 1.3429 (1.3360)	grad_norm 33.4344 (30.9512)	mem 4879MB
[2022-05-31 07:41:44 MetaFG_0] (main.py 265): INFO Train: [66/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.2977 (0.3060)	loss 1.6543 (1.3363)	grad_norm 64.3107 (30.9472)	mem 4879MB
[2022-05-31 07:41:47 MetaFG_0] (main.py 265): INFO Train: [66/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.2948 (0.3060)	loss 1.7262 (1.3372)	grad_norm 24.2304 (30.8974)	mem 4879MB
[2022-05-31 07:41:50 MetaFG_0] (main.py 265): INFO Train: [66/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2918 (0.3059)	loss 1.1937 (1.3361)	grad_norm 55.6515 (30.9483)	mem 4879MB
[2022-05-31 07:41:53 MetaFG_0] (main.py 265): INFO Train: [66/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2941 (0.3059)	loss 1.5334 (1.3370)	grad_norm 37.7642 (30.9326)	mem 4879MB
[2022-05-31 07:41:56 MetaFG_0] (main.py 265): INFO Train: [66/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2929 (0.3059)	loss 1.1080 (1.3370)	grad_norm 27.1865 (31.0211)	mem 4879MB
[2022-05-31 07:41:59 MetaFG_0] (main.py 265): INFO Train: [66/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2930 (0.3058)	loss 1.3687 (1.3362)	grad_norm 27.0580 (30.9258)	mem 4879MB
[2022-05-31 07:42:02 MetaFG_0] (main.py 265): INFO Train: [66/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.2921 (0.3058)	loss 1.4407 (1.3353)	grad_norm 19.4050 (30.9498)	mem 4879MB
[2022-05-31 07:42:05 MetaFG_0] (main.py 265): INFO Train: [66/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2937 (0.3058)	loss 1.4693 (1.3349)	grad_norm 22.6429 (30.9419)	mem 4879MB
[2022-05-31 07:42:09 MetaFG_0] (main.py 265): INFO Train: [66/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.2924 (0.3058)	loss 1.2468 (1.3349)	grad_norm 24.0098 (30.9554)	mem 4879MB
[2022-05-31 07:42:12 MetaFG_0] (main.py 265): INFO Train: [66/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2988 (0.3058)	loss 1.3052 (1.3344)	grad_norm 26.9231 (30.9774)	mem 4879MB
[2022-05-31 07:42:15 MetaFG_0] (main.py 265): INFO Train: [66/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.2992 (0.3057)	loss 1.6318 (1.3359)	grad_norm 41.5917 (30.9866)	mem 4879MB
[2022-05-31 07:42:18 MetaFG_0] (main.py 265): INFO Train: [66/300][800/1562]	eta 0:03:52 lr 0.000006	time 0.2978 (0.3057)	loss 1.3862 (1.3378)	grad_norm 20.7827 (30.9966)	mem 4879MB
[2022-05-31 07:42:21 MetaFG_0] (main.py 265): INFO Train: [66/300][810/1562]	eta 0:03:49 lr 0.000006	time 0.2995 (0.3057)	loss 1.4235 (1.3368)	grad_norm 26.4835 (31.0219)	mem 4879MB
[2022-05-31 07:42:24 MetaFG_0] (main.py 265): INFO Train: [66/300][820/1562]	eta 0:03:46 lr 0.000006	time 0.3016 (0.3057)	loss 1.5140 (1.3373)	grad_norm 17.6856 (30.9643)	mem 4879MB
[2022-05-31 07:42:27 MetaFG_0] (main.py 265): INFO Train: [66/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.3001 (0.3057)	loss 1.4950 (1.3382)	grad_norm 22.4490 (30.9214)	mem 4879MB
[2022-05-31 07:42:30 MetaFG_0] (main.py 265): INFO Train: [66/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2927 (0.3057)	loss 1.5652 (1.3392)	grad_norm 36.4618 (30.9350)	mem 4879MB
[2022-05-31 07:42:33 MetaFG_0] (main.py 265): INFO Train: [66/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2938 (0.3057)	loss 1.4912 (1.3389)	grad_norm 30.3231 (30.9005)	mem 4879MB
[2022-05-31 07:42:36 MetaFG_0] (main.py 265): INFO Train: [66/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2986 (0.3057)	loss 1.5991 (1.3396)	grad_norm 19.9636 (30.9000)	mem 4879MB
[2022-05-31 07:42:39 MetaFG_0] (main.py 265): INFO Train: [66/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.2928 (0.3057)	loss 0.9975 (1.3394)	grad_norm 32.5117 (30.8786)	mem 4879MB
[2022-05-31 07:42:42 MetaFG_0] (main.py 265): INFO Train: [66/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2940 (0.3057)	loss 1.7035 (1.3398)	grad_norm 34.6512 (30.8765)	mem 4879MB
[2022-05-31 07:42:45 MetaFG_0] (main.py 265): INFO Train: [66/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2927 (0.3057)	loss 1.1153 (1.3393)	grad_norm 26.9160 (30.8565)	mem 4879MB
[2022-05-31 07:42:48 MetaFG_0] (main.py 265): INFO Train: [66/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2919 (0.3057)	loss 1.1379 (1.3385)	grad_norm 24.2395 (30.8259)	mem 4879MB
[2022-05-31 07:42:51 MetaFG_0] (main.py 265): INFO Train: [66/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2938 (0.3057)	loss 1.5518 (1.3380)	grad_norm 17.7486 (30.7874)	mem 4879MB
[2022-05-31 07:42:54 MetaFG_0] (main.py 265): INFO Train: [66/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2979 (0.3057)	loss 0.8441 (1.3376)	grad_norm 38.1704 (30.8137)	mem 4879MB
[2022-05-31 07:42:57 MetaFG_0] (main.py 265): INFO Train: [66/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2997 (0.3057)	loss 1.5071 (1.3375)	grad_norm 26.1227 (30.7967)	mem 4879MB
[2022-05-31 07:43:00 MetaFG_0] (main.py 265): INFO Train: [66/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2927 (0.3057)	loss 1.7131 (1.3384)	grad_norm 30.0378 (30.7717)	mem 4879MB
[2022-05-31 07:43:03 MetaFG_0] (main.py 265): INFO Train: [66/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2918 (0.3056)	loss 1.6404 (1.3373)	grad_norm 29.9443 (30.7519)	mem 4879MB
[2022-05-31 07:43:06 MetaFG_0] (main.py 265): INFO Train: [66/300][960/1562]	eta 0:03:03 lr 0.000006	time 0.2926 (0.3056)	loss 1.6159 (1.3378)	grad_norm 44.4016 (30.7276)	mem 4879MB
[2022-05-31 07:43:10 MetaFG_0] (main.py 265): INFO Train: [66/300][970/1562]	eta 0:03:00 lr 0.000006	time 0.2992 (0.3056)	loss 0.7278 (1.3377)	grad_norm 27.0976 (30.7212)	mem 4879MB
[2022-05-31 07:43:13 MetaFG_0] (main.py 265): INFO Train: [66/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3015 (0.3056)	loss 1.1477 (1.3376)	grad_norm 34.6804 (30.6948)	mem 4879MB
[2022-05-31 07:43:16 MetaFG_0] (main.py 265): INFO Train: [66/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2929 (0.3056)	loss 1.0565 (1.3368)	grad_norm 30.5952 (30.6685)	mem 4879MB
[2022-05-31 07:43:19 MetaFG_0] (main.py 265): INFO Train: [66/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2936 (0.3056)	loss 1.4476 (1.3367)	grad_norm 17.0426 (30.6371)	mem 4879MB
[2022-05-31 07:43:22 MetaFG_0] (main.py 265): INFO Train: [66/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2991 (0.3056)	loss 1.6477 (1.3362)	grad_norm 29.3914 (30.6088)	mem 4879MB
[2022-05-31 07:43:25 MetaFG_0] (main.py 265): INFO Train: [66/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2927 (0.3055)	loss 1.3747 (1.3365)	grad_norm 59.5924 (30.6142)	mem 4879MB
[2022-05-31 07:43:28 MetaFG_0] (main.py 265): INFO Train: [66/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2929 (0.3055)	loss 1.3760 (1.3354)	grad_norm 22.0048 (30.6024)	mem 4879MB
[2022-05-31 07:43:31 MetaFG_0] (main.py 265): INFO Train: [66/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2937 (0.3056)	loss 1.0320 (1.3364)	grad_norm 25.5521 (30.5755)	mem 4879MB
[2022-05-31 07:43:34 MetaFG_0] (main.py 265): INFO Train: [66/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2983 (0.3055)	loss 1.3733 (1.3349)	grad_norm 20.8611 (30.5607)	mem 4879MB
[2022-05-31 07:43:37 MetaFG_0] (main.py 265): INFO Train: [66/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2945 (0.3055)	loss 1.2012 (1.3344)	grad_norm 18.6467 (30.5827)	mem 4879MB
[2022-05-31 07:43:40 MetaFG_0] (main.py 265): INFO Train: [66/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.3010 (0.3055)	loss 1.2559 (1.3338)	grad_norm 34.6091 (30.5687)	mem 4879MB
[2022-05-31 07:43:43 MetaFG_0] (main.py 265): INFO Train: [66/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2937 (0.3055)	loss 1.6363 (1.3342)	grad_norm 31.8567 (30.5545)	mem 4879MB
[2022-05-31 07:43:46 MetaFG_0] (main.py 265): INFO Train: [66/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2991 (0.3055)	loss 1.3796 (1.3347)	grad_norm 41.4994 (30.5531)	mem 4879MB
[2022-05-31 07:43:49 MetaFG_0] (main.py 265): INFO Train: [66/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2982 (0.3055)	loss 1.2059 (1.3345)	grad_norm 23.3050 (30.5059)	mem 4879MB
[2022-05-31 07:43:52 MetaFG_0] (main.py 265): INFO Train: [66/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2977 (0.3055)	loss 1.6905 (1.3345)	grad_norm 58.4177 (30.5031)	mem 4879MB
[2022-05-31 07:43:55 MetaFG_0] (main.py 265): INFO Train: [66/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2926 (0.3055)	loss 1.4352 (1.3338)	grad_norm 34.8034 (30.5259)	mem 4879MB
[2022-05-31 07:43:58 MetaFG_0] (main.py 265): INFO Train: [66/300][1130/1562]	eta 0:02:11 lr 0.000006	time 0.2936 (0.3055)	loss 1.5903 (1.3342)	grad_norm 21.6971 (30.5144)	mem 4879MB
[2022-05-31 07:44:01 MetaFG_0] (main.py 265): INFO Train: [66/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2921 (0.3054)	loss 1.0559 (1.3338)	grad_norm 29.2832 (30.5161)	mem 4879MB
[2022-05-31 07:44:04 MetaFG_0] (main.py 265): INFO Train: [66/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2926 (0.3054)	loss 1.4555 (1.3344)	grad_norm 39.9659 (30.5438)	mem 4879MB
[2022-05-31 07:44:07 MetaFG_0] (main.py 265): INFO Train: [66/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2935 (0.3054)	loss 1.1987 (1.3342)	grad_norm 38.9012 (30.5974)	mem 4879MB
[2022-05-31 07:44:10 MetaFG_0] (main.py 265): INFO Train: [66/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2932 (0.3055)	loss 1.3959 (1.3344)	grad_norm 21.0601 (30.5936)	mem 4879MB
[2022-05-31 07:44:13 MetaFG_0] (main.py 265): INFO Train: [66/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.3003 (0.3054)	loss 1.6512 (1.3344)	grad_norm 19.1851 (30.5070)	mem 4879MB
[2022-05-31 07:44:17 MetaFG_0] (main.py 265): INFO Train: [66/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2934 (0.3054)	loss 1.3807 (1.3335)	grad_norm 30.5109 (30.4737)	mem 4879MB
[2022-05-31 07:44:20 MetaFG_0] (main.py 265): INFO Train: [66/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2918 (0.3054)	loss 1.0989 (1.3322)	grad_norm 21.6430 (30.4537)	mem 4879MB
[2022-05-31 07:44:23 MetaFG_0] (main.py 265): INFO Train: [66/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2930 (0.3054)	loss 1.3695 (1.3322)	grad_norm 18.6123 (30.4647)	mem 4879MB
[2022-05-31 07:44:26 MetaFG_0] (main.py 265): INFO Train: [66/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2996 (0.3054)	loss 1.3668 (1.3326)	grad_norm 23.4947 (30.4336)	mem 4879MB
[2022-05-31 07:44:29 MetaFG_0] (main.py 265): INFO Train: [66/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2936 (0.3054)	loss 1.4991 (1.3326)	grad_norm 19.4979 (30.3726)	mem 4879MB
[2022-05-31 07:44:32 MetaFG_0] (main.py 265): INFO Train: [66/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2926 (0.3054)	loss 1.5283 (1.3318)	grad_norm 22.0271 (30.3380)	mem 4879MB
[2022-05-31 07:44:35 MetaFG_0] (main.py 265): INFO Train: [66/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2931 (0.3054)	loss 1.6761 (1.3320)	grad_norm 22.7863 (30.3269)	mem 4879MB
[2022-05-31 07:44:38 MetaFG_0] (main.py 265): INFO Train: [66/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2919 (0.3053)	loss 1.0475 (1.3318)	grad_norm 22.7023 (30.2864)	mem 4879MB
[2022-05-31 07:44:41 MetaFG_0] (main.py 265): INFO Train: [66/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2944 (0.3053)	loss 1.3663 (1.3309)	grad_norm 18.7158 (30.2603)	mem 4879MB
[2022-05-31 07:44:44 MetaFG_0] (main.py 265): INFO Train: [66/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2921 (0.3053)	loss 1.4559 (1.3308)	grad_norm 21.6904 (30.2500)	mem 4879MB
[2022-05-31 07:44:47 MetaFG_0] (main.py 265): INFO Train: [66/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2925 (0.3053)	loss 1.1099 (1.3312)	grad_norm 15.1327 (30.2088)	mem 4879MB
[2022-05-31 07:44:50 MetaFG_0] (main.py 265): INFO Train: [66/300][1300/1562]	eta 0:01:19 lr 0.000006	time 0.2920 (0.3053)	loss 0.9771 (1.3312)	grad_norm 35.2884 (30.1918)	mem 4879MB
[2022-05-31 07:44:53 MetaFG_0] (main.py 265): INFO Train: [66/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2922 (0.3053)	loss 1.2100 (1.3327)	grad_norm 24.6585 (30.1800)	mem 4879MB
[2022-05-31 07:44:56 MetaFG_0] (main.py 265): INFO Train: [66/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2924 (0.3053)	loss 1.5297 (1.3320)	grad_norm 30.4318 (30.1468)	mem 4879MB
[2022-05-31 07:44:59 MetaFG_0] (main.py 265): INFO Train: [66/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2934 (0.3053)	loss 0.9681 (1.3311)	grad_norm 33.9395 (30.1482)	mem 4879MB
[2022-05-31 07:45:02 MetaFG_0] (main.py 265): INFO Train: [66/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2930 (0.3053)	loss 1.5442 (1.3314)	grad_norm 22.3669 (30.1385)	mem 4879MB
[2022-05-31 07:45:05 MetaFG_0] (main.py 265): INFO Train: [66/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2989 (0.3053)	loss 1.0917 (1.3317)	grad_norm 26.0582 (30.1208)	mem 4879MB
[2022-05-31 07:45:08 MetaFG_0] (main.py 265): INFO Train: [66/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2925 (0.3053)	loss 1.1553 (1.3320)	grad_norm 19.6724 (30.1050)	mem 4879MB
[2022-05-31 07:45:11 MetaFG_0] (main.py 265): INFO Train: [66/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2935 (0.3053)	loss 1.4667 (1.3316)	grad_norm 21.7803 (30.0839)	mem 4879MB
[2022-05-31 07:45:14 MetaFG_0] (main.py 265): INFO Train: [66/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2937 (0.3053)	loss 1.0104 (1.3316)	grad_norm 42.1641 (30.0593)	mem 4879MB
[2022-05-31 07:45:17 MetaFG_0] (main.py 265): INFO Train: [66/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2938 (0.3053)	loss 1.5829 (1.3321)	grad_norm 26.4910 (30.0681)	mem 4879MB
[2022-05-31 07:45:20 MetaFG_0] (main.py 265): INFO Train: [66/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2936 (0.3053)	loss 1.5665 (1.3321)	grad_norm 23.8972 (30.0570)	mem 4879MB
[2022-05-31 07:45:23 MetaFG_0] (main.py 265): INFO Train: [66/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2951 (0.3053)	loss 1.2922 (1.3324)	grad_norm 31.9419 (30.0946)	mem 4879MB
[2022-05-31 07:45:27 MetaFG_0] (main.py 265): INFO Train: [66/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2927 (0.3052)	loss 1.0517 (1.3321)	grad_norm 37.2743 (30.0723)	mem 4879MB
[2022-05-31 07:45:30 MetaFG_0] (main.py 265): INFO Train: [66/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2987 (0.3052)	loss 1.4607 (1.3316)	grad_norm 25.4422 (30.0567)	mem 4879MB
[2022-05-31 07:45:33 MetaFG_0] (main.py 265): INFO Train: [66/300][1440/1562]	eta 0:00:37 lr 0.000006	time 0.2925 (0.3052)	loss 1.2942 (1.3314)	grad_norm 28.9588 (30.0458)	mem 4879MB
[2022-05-31 07:45:36 MetaFG_0] (main.py 265): INFO Train: [66/300][1450/1562]	eta 0:00:34 lr 0.000006	time 0.2986 (0.3052)	loss 1.3649 (1.3317)	grad_norm 38.5614 (30.0643)	mem 4879MB
[2022-05-31 07:45:39 MetaFG_0] (main.py 265): INFO Train: [66/300][1460/1562]	eta 0:00:31 lr 0.000006	time 0.2924 (0.3052)	loss 1.1016 (1.3318)	grad_norm 37.6379 (30.0772)	mem 4879MB
[2022-05-31 07:45:42 MetaFG_0] (main.py 265): INFO Train: [66/300][1470/1562]	eta 0:00:28 lr 0.000006	time 0.2931 (0.3052)	loss 1.4414 (1.3312)	grad_norm 30.8193 (30.0800)	mem 4879MB
[2022-05-31 07:45:45 MetaFG_0] (main.py 265): INFO Train: [66/300][1480/1562]	eta 0:00:25 lr 0.000006	time 0.2986 (0.3052)	loss 1.4629 (1.3312)	grad_norm 19.5706 (30.0762)	mem 4879MB
[2022-05-31 07:45:48 MetaFG_0] (main.py 265): INFO Train: [66/300][1490/1562]	eta 0:00:21 lr 0.000006	time 0.2930 (0.3052)	loss 1.4653 (1.3312)	grad_norm 53.6763 (30.0729)	mem 4879MB
[2022-05-31 07:45:51 MetaFG_0] (main.py 265): INFO Train: [66/300][1500/1562]	eta 0:00:18 lr 0.000006	time 0.2929 (0.3052)	loss 1.0690 (1.3307)	grad_norm 33.5273 (30.0696)	mem 4879MB
[2022-05-31 07:45:54 MetaFG_0] (main.py 265): INFO Train: [66/300][1510/1562]	eta 0:00:15 lr 0.000006	time 0.2914 (0.3052)	loss 1.4641 (1.3308)	grad_norm 13.9458 (30.0539)	mem 4879MB
[2022-05-31 07:45:57 MetaFG_0] (main.py 265): INFO Train: [66/300][1520/1562]	eta 0:00:12 lr 0.000006	time 0.2919 (0.3052)	loss 1.5158 (1.3309)	grad_norm 28.3287 (30.0448)	mem 4879MB
[2022-05-31 07:46:00 MetaFG_0] (main.py 265): INFO Train: [66/300][1530/1562]	eta 0:00:09 lr 0.000006	time 0.2945 (0.3052)	loss 1.0893 (1.3308)	grad_norm 12.7139 (30.0186)	mem 4879MB
[2022-05-31 07:46:03 MetaFG_0] (main.py 265): INFO Train: [66/300][1540/1562]	eta 0:00:06 lr 0.000006	time 0.2929 (0.3052)	loss 1.4298 (1.3308)	grad_norm 44.3008 (30.0354)	mem 4879MB
[2022-05-31 07:46:06 MetaFG_0] (main.py 265): INFO Train: [66/300][1550/1562]	eta 0:00:03 lr 0.000006	time 0.2943 (0.3052)	loss 1.1523 (1.3303)	grad_norm 31.6847 (30.0377)	mem 4879MB
[2022-05-31 07:46:09 MetaFG_0] (main.py 265): INFO Train: [66/300][1560/1562]	eta 0:00:00 lr 0.000006	time 0.2925 (0.3051)	loss 1.6762 (1.3299)	grad_norm 23.5566 (30.0327)	mem 4879MB
[2022-05-31 07:46:10 MetaFG_0] (main.py 272): INFO EPOCH 66 training takes 0:07:56
[2022-05-31 07:46:10 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_66.pth saving......
[2022-05-31 07:46:10 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_66.pth saved !!!
[2022-05-31 07:46:10 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:46:12 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:46:12 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:46:13 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.738 (0.738)	Loss 0.5468 (0.5468)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 07:46:13 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.153)	Loss 0.4380 (0.5709)	Acc@1 93.750 (88.920)	Acc@5 100.000 (98.580)	Mem 4879MB
[2022-05-31 07:46:14 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.125)	Loss 0.8965 (0.6058)	Acc@1 68.750 (87.202)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 07:46:15 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.094 (0.115)	Loss 0.5718 (0.6193)	Acc@1 90.625 (86.290)	Acc@5 96.875 (98.488)	Mem 4879MB
[2022-05-31 07:46:16 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.110)	Loss 0.7669 (0.6072)	Acc@1 78.125 (86.509)	Acc@5 100.000 (98.552)	Mem 4879MB
[2022-05-31 07:46:17 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.088 (0.107)	Loss 0.6057 (0.5988)	Acc@1 87.500 (86.949)	Acc@5 100.000 (98.529)	Mem 4879MB
[2022-05-31 07:46:18 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.105)	Loss 0.5967 (0.5900)	Acc@1 87.500 (87.449)	Acc@5 100.000 (98.514)	Mem 4879MB
[2022-05-31 07:46:19 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.095 (0.103)	Loss 0.7192 (0.6038)	Acc@1 81.250 (87.104)	Acc@5 96.875 (98.327)	Mem 4879MB
[2022-05-31 07:46:20 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.088 (0.102)	Loss 0.3898 (0.6042)	Acc@1 93.750 (86.728)	Acc@5 100.000 (98.418)	Mem 4879MB
[2022-05-31 07:46:21 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.092 (0.101)	Loss 0.3576 (0.5963)	Acc@1 100.000 (87.260)	Acc@5 100.000 (98.489)	Mem 4879MB
[2022-05-31 07:46:22 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.7596 (0.5923)	Acc@1 81.250 (87.376)	Acc@5 100.000 (98.484)	Mem 4879MB
[2022-05-31 07:46:23 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.092 (0.100)	Loss 0.6573 (0.5972)	Acc@1 90.625 (87.416)	Acc@5 96.875 (98.452)	Mem 4879MB
[2022-05-31 07:46:24 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.095 (0.099)	Loss 0.4615 (0.6007)	Acc@1 90.625 (87.345)	Acc@5 100.000 (98.502)	Mem 4879MB
[2022-05-31 07:46:25 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.091 (0.098)	Loss 0.4386 (0.6019)	Acc@1 90.625 (87.238)	Acc@5 100.000 (98.473)	Mem 4879MB
[2022-05-31 07:46:26 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.093 (0.098)	Loss 0.5040 (0.5950)	Acc@1 90.625 (87.456)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 07:46:27 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.4698 (0.5978)	Acc@1 87.500 (87.376)	Acc@5 100.000 (98.469)	Mem 4879MB
[2022-05-31 07:46:28 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.098)	Loss 0.5312 (0.5973)	Acc@1 90.625 (87.403)	Acc@5 100.000 (98.467)	Mem 4879MB
[2022-05-31 07:46:28 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.098)	Loss 0.5184 (0.5946)	Acc@1 93.750 (87.573)	Acc@5 100.000 (98.501)	Mem 4879MB
[2022-05-31 07:46:29 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.099 (0.097)	Loss 0.2936 (0.5968)	Acc@1 100.000 (87.500)	Acc@5 100.000 (98.515)	Mem 4879MB
[2022-05-31 07:46:30 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 0.5188 (0.5990)	Acc@1 93.750 (87.418)	Acc@5 100.000 (98.560)	Mem 4879MB
[2022-05-31 07:46:31 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.088 (0.097)	Loss 0.5221 (0.6001)	Acc@1 90.625 (87.360)	Acc@5 100.000 (98.570)	Mem 4879MB
[2022-05-31 07:46:32 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 0.7551 (0.6034)	Acc@1 87.500 (87.233)	Acc@5 96.875 (98.563)	Mem 4879MB
[2022-05-31 07:46:33 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.099 (0.097)	Loss 0.7113 (0.6066)	Acc@1 78.125 (87.048)	Acc@5 96.875 (98.515)	Mem 4879MB
[2022-05-31 07:46:34 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.100 (0.097)	Loss 0.4457 (0.6038)	Acc@1 96.875 (87.148)	Acc@5 100.000 (98.539)	Mem 4879MB
[2022-05-31 07:46:35 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.090 (0.096)	Loss 0.7104 (0.6025)	Acc@1 87.500 (87.202)	Acc@5 93.750 (98.561)	Mem 4879MB
[2022-05-31 07:46:36 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.096)	Loss 0.4963 (0.6007)	Acc@1 96.875 (87.239)	Acc@5 100.000 (98.606)	Mem 4879MB
[2022-05-31 07:46:37 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.089 (0.096)	Loss 0.4638 (0.5993)	Acc@1 90.625 (87.261)	Acc@5 100.000 (98.647)	Mem 4879MB
[2022-05-31 07:46:38 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.096)	Loss 0.5521 (0.5970)	Acc@1 87.500 (87.304)	Acc@5 100.000 (98.685)	Mem 4879MB
[2022-05-31 07:46:39 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.101 (0.096)	Loss 0.6525 (0.5956)	Acc@1 81.250 (87.322)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 07:46:40 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.095 (0.096)	Loss 0.6498 (0.5977)	Acc@1 84.375 (87.199)	Acc@5 100.000 (98.701)	Mem 4879MB
[2022-05-31 07:46:41 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.091 (0.096)	Loss 0.3411 (0.5984)	Acc@1 96.875 (87.189)	Acc@5 100.000 (98.681)	Mem 4879MB
[2022-05-31 07:46:42 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.7782 (0.6002)	Acc@1 75.000 (87.108)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 07:46:42 MetaFG_0] (main.py 330): INFO  * Acc@1 87.130 Acc@5 98.680
[2022-05-31 07:46:42 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.1%
[2022-05-31 07:46:42 MetaFG_0] (main.py 171): INFO Max accuracy: 87.28%
[2022-05-31 07:46:43 MetaFG_0] (main.py 265): INFO Train: [67/300][0/1562]	eta 0:27:59 lr 0.000006	time 1.0752 (1.0752)	loss 1.6937 (1.6937)	grad_norm 22.7661 (22.7661)	mem 4879MB
[2022-05-31 07:46:46 MetaFG_0] (main.py 265): INFO Train: [67/300][10/1562]	eta 0:09:44 lr 0.000006	time 0.2940 (0.3764)	loss 1.5748 (1.3046)	grad_norm 17.3089 (32.3623)	mem 4879MB
[2022-05-31 07:46:49 MetaFG_0] (main.py 265): INFO Train: [67/300][20/1562]	eta 0:08:46 lr 0.000006	time 0.2915 (0.3411)	loss 1.3466 (1.2928)	grad_norm 36.5434 (30.2316)	mem 4879MB
[2022-05-31 07:46:52 MetaFG_0] (main.py 265): INFO Train: [67/300][30/1562]	eta 0:08:24 lr 0.000006	time 0.2930 (0.3294)	loss 1.3144 (1.2989)	grad_norm 45.2268 (31.0473)	mem 4879MB
[2022-05-31 07:46:55 MetaFG_0] (main.py 265): INFO Train: [67/300][40/1562]	eta 0:08:12 lr 0.000006	time 0.2983 (0.3237)	loss 1.0149 (1.2908)	grad_norm 27.4972 (30.8418)	mem 4879MB
[2022-05-31 07:46:58 MetaFG_0] (main.py 265): INFO Train: [67/300][50/1562]	eta 0:08:03 lr 0.000006	time 0.2931 (0.3199)	loss 1.6885 (1.3284)	grad_norm 39.8345 (30.4437)	mem 4879MB
[2022-05-31 07:47:01 MetaFG_0] (main.py 265): INFO Train: [67/300][60/1562]	eta 0:07:56 lr 0.000006	time 0.2986 (0.3175)	loss 1.5054 (1.3314)	grad_norm 34.5925 (29.8085)	mem 4879MB
[2022-05-31 07:47:04 MetaFG_0] (main.py 265): INFO Train: [67/300][70/1562]	eta 0:07:51 lr 0.000006	time 0.2922 (0.3160)	loss 1.2674 (1.3373)	grad_norm 52.0993 (31.0520)	mem 4879MB
[2022-05-31 07:47:07 MetaFG_0] (main.py 265): INFO Train: [67/300][80/1562]	eta 0:07:46 lr 0.000006	time 0.2995 (0.3147)	loss 1.4181 (1.3400)	grad_norm 22.3557 (31.3241)	mem 4879MB
[2022-05-31 07:47:10 MetaFG_0] (main.py 265): INFO Train: [67/300][90/1562]	eta 0:07:41 lr 0.000006	time 0.2926 (0.3138)	loss 1.3584 (1.3439)	grad_norm 23.0579 (31.4254)	mem 4879MB
[2022-05-31 07:47:13 MetaFG_0] (main.py 265): INFO Train: [67/300][100/1562]	eta 0:07:37 lr 0.000006	time 0.2928 (0.3127)	loss 0.7853 (1.3328)	grad_norm 32.6949 (31.1940)	mem 4879MB
[2022-05-31 07:47:16 MetaFG_0] (main.py 265): INFO Train: [67/300][110/1562]	eta 0:07:32 lr 0.000006	time 0.2937 (0.3120)	loss 1.2509 (1.3293)	grad_norm 23.4061 (30.5323)	mem 4879MB
[2022-05-31 07:47:20 MetaFG_0] (main.py 265): INFO Train: [67/300][120/1562]	eta 0:07:29 lr 0.000006	time 0.2948 (0.3115)	loss 1.2715 (1.3334)	grad_norm 40.0086 (30.8064)	mem 4879MB
[2022-05-31 07:47:23 MetaFG_0] (main.py 265): INFO Train: [67/300][130/1562]	eta 0:07:25 lr 0.000006	time 0.2927 (0.3110)	loss 1.5148 (1.3403)	grad_norm 26.1897 (30.9518)	mem 4879MB
[2022-05-31 07:47:26 MetaFG_0] (main.py 265): INFO Train: [67/300][140/1562]	eta 0:07:23 lr 0.000006	time 0.3006 (0.3119)	loss 1.0626 (1.3386)	grad_norm 22.3171 (30.7988)	mem 4879MB
[2022-05-31 07:47:29 MetaFG_0] (main.py 265): INFO Train: [67/300][150/1562]	eta 0:07:19 lr 0.000006	time 0.2943 (0.3113)	loss 1.5372 (1.3462)	grad_norm 27.3496 (30.6767)	mem 4879MB
[2022-05-31 07:47:32 MetaFG_0] (main.py 265): INFO Train: [67/300][160/1562]	eta 0:07:15 lr 0.000006	time 0.2925 (0.3109)	loss 1.0349 (1.3425)	grad_norm 25.5406 (30.1865)	mem 4879MB
[2022-05-31 07:47:35 MetaFG_0] (main.py 265): INFO Train: [67/300][170/1562]	eta 0:07:12 lr 0.000006	time 0.2944 (0.3106)	loss 1.3740 (1.3442)	grad_norm 14.6030 (30.2749)	mem 4879MB
[2022-05-31 07:47:38 MetaFG_0] (main.py 265): INFO Train: [67/300][180/1562]	eta 0:07:08 lr 0.000006	time 0.2927 (0.3104)	loss 1.0575 (1.3440)	grad_norm 23.0003 (30.2533)	mem 4879MB
[2022-05-31 07:47:41 MetaFG_0] (main.py 265): INFO Train: [67/300][190/1562]	eta 0:07:05 lr 0.000006	time 0.2937 (0.3101)	loss 1.2671 (1.3368)	grad_norm 30.2895 (30.6800)	mem 4879MB
[2022-05-31 07:47:44 MetaFG_0] (main.py 265): INFO Train: [67/300][200/1562]	eta 0:07:02 lr 0.000006	time 0.2995 (0.3100)	loss 0.9120 (1.3363)	grad_norm 24.3344 (30.3558)	mem 4879MB
[2022-05-31 07:47:47 MetaFG_0] (main.py 265): INFO Train: [67/300][210/1562]	eta 0:06:58 lr 0.000006	time 0.2976 (0.3097)	loss 0.9438 (1.3285)	grad_norm 86.9743 (30.7072)	mem 4879MB
[2022-05-31 07:47:50 MetaFG_0] (main.py 265): INFO Train: [67/300][220/1562]	eta 0:06:55 lr 0.000006	time 0.2939 (0.3095)	loss 1.2114 (1.3264)	grad_norm 21.1973 (30.6348)	mem 4879MB
[2022-05-31 07:47:53 MetaFG_0] (main.py 265): INFO Train: [67/300][230/1562]	eta 0:06:52 lr 0.000006	time 0.2925 (0.3093)	loss 1.3677 (1.3253)	grad_norm 21.7805 (30.7017)	mem 4879MB
[2022-05-31 07:47:56 MetaFG_0] (main.py 265): INFO Train: [67/300][240/1562]	eta 0:06:48 lr 0.000006	time 0.2925 (0.3091)	loss 1.2239 (1.3255)	grad_norm 28.6579 (30.6137)	mem 4879MB
[2022-05-31 07:47:59 MetaFG_0] (main.py 265): INFO Train: [67/300][250/1562]	eta 0:06:45 lr 0.000006	time 0.3002 (0.3090)	loss 1.5791 (1.3297)	grad_norm 17.2906 (30.3576)	mem 4879MB
[2022-05-31 07:48:02 MetaFG_0] (main.py 265): INFO Train: [67/300][260/1562]	eta 0:06:42 lr 0.000006	time 0.2921 (0.3088)	loss 1.0853 (1.3256)	grad_norm 35.1640 (30.3713)	mem 4879MB
[2022-05-31 07:48:05 MetaFG_0] (main.py 265): INFO Train: [67/300][270/1562]	eta 0:06:38 lr 0.000006	time 0.2941 (0.3086)	loss 1.6384 (1.3264)	grad_norm 31.7392 (30.3263)	mem 4879MB
[2022-05-31 07:48:09 MetaFG_0] (main.py 265): INFO Train: [67/300][280/1562]	eta 0:06:35 lr 0.000006	time 0.3019 (0.3085)	loss 0.8541 (1.3266)	grad_norm 47.0060 (30.4546)	mem 4879MB
[2022-05-31 07:48:12 MetaFG_0] (main.py 265): INFO Train: [67/300][290/1562]	eta 0:06:32 lr 0.000006	time 0.2940 (0.3083)	loss 1.1106 (1.3275)	grad_norm 30.9248 (nan)	mem 4879MB
[2022-05-31 07:48:15 MetaFG_0] (main.py 265): INFO Train: [67/300][300/1562]	eta 0:06:28 lr 0.000006	time 0.2933 (0.3082)	loss 1.4300 (1.3264)	grad_norm 28.4223 (nan)	mem 4879MB
[2022-05-31 07:48:18 MetaFG_0] (main.py 265): INFO Train: [67/300][310/1562]	eta 0:06:25 lr 0.000006	time 0.2993 (0.3080)	loss 0.7386 (1.3253)	grad_norm 28.7363 (nan)	mem 4879MB
[2022-05-31 07:48:21 MetaFG_0] (main.py 265): INFO Train: [67/300][320/1562]	eta 0:06:22 lr 0.000006	time 0.2984 (0.3079)	loss 1.3465 (1.3256)	grad_norm 21.3447 (nan)	mem 4879MB
[2022-05-31 07:48:24 MetaFG_0] (main.py 265): INFO Train: [67/300][330/1562]	eta 0:06:19 lr 0.000006	time 0.2989 (0.3078)	loss 1.0897 (1.3280)	grad_norm 23.5016 (nan)	mem 4879MB
[2022-05-31 07:48:27 MetaFG_0] (main.py 265): INFO Train: [67/300][340/1562]	eta 0:06:16 lr 0.000006	time 0.2994 (0.3078)	loss 1.5180 (1.3284)	grad_norm 53.4702 (nan)	mem 4879MB
[2022-05-31 07:48:30 MetaFG_0] (main.py 265): INFO Train: [67/300][350/1562]	eta 0:06:12 lr 0.000006	time 0.2917 (0.3077)	loss 0.9715 (1.3299)	grad_norm 40.8864 (nan)	mem 4879MB
[2022-05-31 07:48:33 MetaFG_0] (main.py 265): INFO Train: [67/300][360/1562]	eta 0:06:09 lr 0.000006	time 0.2919 (0.3075)	loss 1.5284 (1.3287)	grad_norm 36.2177 (nan)	mem 4879MB
[2022-05-31 07:48:36 MetaFG_0] (main.py 265): INFO Train: [67/300][370/1562]	eta 0:06:06 lr 0.000006	time 0.2926 (0.3074)	loss 1.1622 (1.3287)	grad_norm 30.5205 (nan)	mem 4879MB
[2022-05-31 07:48:39 MetaFG_0] (main.py 265): INFO Train: [67/300][380/1562]	eta 0:06:03 lr 0.000006	time 0.2976 (0.3073)	loss 0.9210 (1.3274)	grad_norm 20.4773 (nan)	mem 4879MB
[2022-05-31 07:48:42 MetaFG_0] (main.py 265): INFO Train: [67/300][390/1562]	eta 0:06:00 lr 0.000006	time 0.2947 (0.3073)	loss 1.6318 (1.3303)	grad_norm 37.0201 (nan)	mem 4879MB
[2022-05-31 07:48:45 MetaFG_0] (main.py 265): INFO Train: [67/300][400/1562]	eta 0:05:56 lr 0.000006	time 0.2943 (0.3072)	loss 1.1854 (1.3285)	grad_norm 23.9372 (nan)	mem 4879MB
[2022-05-31 07:48:48 MetaFG_0] (main.py 265): INFO Train: [67/300][410/1562]	eta 0:05:53 lr 0.000006	time 0.2986 (0.3071)	loss 1.3835 (1.3288)	grad_norm 33.9067 (nan)	mem 4879MB
[2022-05-31 07:48:51 MetaFG_0] (main.py 265): INFO Train: [67/300][420/1562]	eta 0:05:50 lr 0.000006	time 0.2993 (0.3070)	loss 1.5693 (1.3286)	grad_norm 20.6043 (nan)	mem 4879MB
[2022-05-31 07:48:54 MetaFG_0] (main.py 265): INFO Train: [67/300][430/1562]	eta 0:05:47 lr 0.000006	time 0.2950 (0.3070)	loss 1.2350 (1.3260)	grad_norm 32.0577 (nan)	mem 4879MB
[2022-05-31 07:48:57 MetaFG_0] (main.py 265): INFO Train: [67/300][440/1562]	eta 0:05:44 lr 0.000006	time 0.2918 (0.3069)	loss 1.0179 (1.3247)	grad_norm 25.3048 (nan)	mem 4879MB
[2022-05-31 07:49:00 MetaFG_0] (main.py 265): INFO Train: [67/300][450/1562]	eta 0:05:41 lr 0.000006	time 0.2917 (0.3068)	loss 1.4824 (1.3267)	grad_norm 18.6765 (nan)	mem 4879MB
[2022-05-31 07:49:03 MetaFG_0] (main.py 265): INFO Train: [67/300][460/1562]	eta 0:05:38 lr 0.000006	time 0.2943 (0.3068)	loss 1.5087 (1.3269)	grad_norm 41.9850 (nan)	mem 4879MB
[2022-05-31 07:49:06 MetaFG_0] (main.py 265): INFO Train: [67/300][470/1562]	eta 0:05:34 lr 0.000006	time 0.2922 (0.3067)	loss 1.4233 (1.3280)	grad_norm 35.6423 (nan)	mem 4879MB
[2022-05-31 07:49:09 MetaFG_0] (main.py 265): INFO Train: [67/300][480/1562]	eta 0:05:31 lr 0.000006	time 0.3016 (0.3067)	loss 1.2233 (1.3275)	grad_norm 33.5271 (nan)	mem 4879MB
[2022-05-31 07:49:12 MetaFG_0] (main.py 265): INFO Train: [67/300][490/1562]	eta 0:05:28 lr 0.000006	time 0.2924 (0.3066)	loss 1.4674 (1.3285)	grad_norm 25.2616 (nan)	mem 4879MB
[2022-05-31 07:49:15 MetaFG_0] (main.py 265): INFO Train: [67/300][500/1562]	eta 0:05:25 lr 0.000006	time 0.2932 (0.3066)	loss 1.4291 (1.3275)	grad_norm 29.7944 (nan)	mem 4879MB
[2022-05-31 07:49:18 MetaFG_0] (main.py 265): INFO Train: [67/300][510/1562]	eta 0:05:22 lr 0.000006	time 0.2924 (0.3065)	loss 0.8423 (1.3271)	grad_norm 27.5565 (nan)	mem 4879MB
[2022-05-31 07:49:22 MetaFG_0] (main.py 265): INFO Train: [67/300][520/1562]	eta 0:05:19 lr 0.000006	time 0.2983 (0.3065)	loss 1.5450 (1.3288)	grad_norm 28.0239 (nan)	mem 4879MB
[2022-05-31 07:49:25 MetaFG_0] (main.py 265): INFO Train: [67/300][530/1562]	eta 0:05:16 lr 0.000006	time 0.2920 (0.3064)	loss 1.1077 (1.3283)	grad_norm 23.6980 (nan)	mem 4879MB
[2022-05-31 07:49:28 MetaFG_0] (main.py 265): INFO Train: [67/300][540/1562]	eta 0:05:13 lr 0.000006	time 0.2987 (0.3064)	loss 1.0949 (1.3269)	grad_norm 22.2010 (nan)	mem 4879MB
[2022-05-31 07:49:31 MetaFG_0] (main.py 265): INFO Train: [67/300][550/1562]	eta 0:05:10 lr 0.000006	time 0.2930 (0.3064)	loss 1.5544 (1.3267)	grad_norm 37.7044 (nan)	mem 4879MB
[2022-05-31 07:49:34 MetaFG_0] (main.py 265): INFO Train: [67/300][560/1562]	eta 0:05:07 lr 0.000006	time 0.2988 (0.3064)	loss 1.4893 (1.3285)	grad_norm 23.8140 (nan)	mem 4879MB
[2022-05-31 07:49:37 MetaFG_0] (main.py 265): INFO Train: [67/300][570/1562]	eta 0:05:03 lr 0.000006	time 0.2941 (0.3064)	loss 1.1713 (1.3282)	grad_norm 13.7065 (nan)	mem 4879MB
[2022-05-31 07:49:40 MetaFG_0] (main.py 265): INFO Train: [67/300][580/1562]	eta 0:05:00 lr 0.000006	time 0.3001 (0.3064)	loss 1.0770 (1.3281)	grad_norm 25.8666 (nan)	mem 4879MB
[2022-05-31 07:49:43 MetaFG_0] (main.py 265): INFO Train: [67/300][590/1562]	eta 0:04:57 lr 0.000006	time 0.2936 (0.3064)	loss 1.6375 (1.3291)	grad_norm 33.9683 (nan)	mem 4879MB
[2022-05-31 07:49:46 MetaFG_0] (main.py 265): INFO Train: [67/300][600/1562]	eta 0:04:54 lr 0.000006	time 0.2928 (0.3063)	loss 1.3327 (1.3280)	grad_norm 29.7294 (nan)	mem 4879MB
[2022-05-31 07:49:49 MetaFG_0] (main.py 265): INFO Train: [67/300][610/1562]	eta 0:04:51 lr 0.000006	time 0.2993 (0.3063)	loss 1.5077 (1.3283)	grad_norm 18.1687 (nan)	mem 4879MB
[2022-05-31 07:49:52 MetaFG_0] (main.py 265): INFO Train: [67/300][620/1562]	eta 0:04:48 lr 0.000006	time 0.2978 (0.3063)	loss 1.5168 (1.3284)	grad_norm 28.2343 (nan)	mem 4879MB
[2022-05-31 07:49:55 MetaFG_0] (main.py 265): INFO Train: [67/300][630/1562]	eta 0:04:45 lr 0.000006	time 0.2928 (0.3062)	loss 1.5382 (1.3286)	grad_norm 18.5997 (nan)	mem 4879MB
[2022-05-31 07:49:58 MetaFG_0] (main.py 265): INFO Train: [67/300][640/1562]	eta 0:04:42 lr 0.000006	time 0.2921 (0.3062)	loss 1.7262 (1.3275)	grad_norm 34.9142 (nan)	mem 4879MB
[2022-05-31 07:50:01 MetaFG_0] (main.py 265): INFO Train: [67/300][650/1562]	eta 0:04:39 lr 0.000006	time 0.2981 (0.3062)	loss 1.4734 (1.3275)	grad_norm 25.1850 (nan)	mem 4879MB
[2022-05-31 07:50:04 MetaFG_0] (main.py 265): INFO Train: [67/300][660/1562]	eta 0:04:36 lr 0.000006	time 0.2959 (0.3062)	loss 0.9342 (1.3258)	grad_norm 28.4230 (nan)	mem 4879MB
[2022-05-31 07:50:07 MetaFG_0] (main.py 265): INFO Train: [67/300][670/1562]	eta 0:04:33 lr 0.000006	time 0.2998 (0.3061)	loss 1.3868 (1.3270)	grad_norm 40.2707 (nan)	mem 4879MB
[2022-05-31 07:50:10 MetaFG_0] (main.py 265): INFO Train: [67/300][680/1562]	eta 0:04:29 lr 0.000006	time 0.2929 (0.3061)	loss 1.0425 (1.3260)	grad_norm 22.6991 (nan)	mem 4879MB
[2022-05-31 07:50:13 MetaFG_0] (main.py 265): INFO Train: [67/300][690/1562]	eta 0:04:26 lr 0.000006	time 0.3042 (0.3061)	loss 1.5331 (1.3263)	grad_norm 27.6612 (nan)	mem 4879MB
[2022-05-31 07:50:16 MetaFG_0] (main.py 265): INFO Train: [67/300][700/1562]	eta 0:04:23 lr 0.000006	time 0.3014 (0.3061)	loss 1.3494 (1.3260)	grad_norm 18.2370 (nan)	mem 4879MB
[2022-05-31 07:50:19 MetaFG_0] (main.py 265): INFO Train: [67/300][710/1562]	eta 0:04:20 lr 0.000006	time 0.2996 (0.3061)	loss 0.9565 (1.3249)	grad_norm 38.5143 (nan)	mem 4879MB
[2022-05-31 07:50:23 MetaFG_0] (main.py 265): INFO Train: [67/300][720/1562]	eta 0:04:17 lr 0.000006	time 0.2930 (0.3061)	loss 1.4667 (1.3258)	grad_norm 15.5434 (nan)	mem 4879MB
[2022-05-31 07:50:26 MetaFG_0] (main.py 265): INFO Train: [67/300][730/1562]	eta 0:04:14 lr 0.000006	time 0.2989 (0.3061)	loss 0.6773 (1.3255)	grad_norm 20.4057 (nan)	mem 4879MB
[2022-05-31 07:50:29 MetaFG_0] (main.py 265): INFO Train: [67/300][740/1562]	eta 0:04:11 lr 0.000006	time 0.2992 (0.3061)	loss 1.4460 (1.3266)	grad_norm 31.1336 (nan)	mem 4879MB
[2022-05-31 07:50:32 MetaFG_0] (main.py 265): INFO Train: [67/300][750/1562]	eta 0:04:08 lr 0.000006	time 0.3010 (0.3061)	loss 0.8993 (1.3263)	grad_norm 30.9802 (nan)	mem 4879MB
[2022-05-31 07:50:35 MetaFG_0] (main.py 265): INFO Train: [67/300][760/1562]	eta 0:04:05 lr 0.000006	time 0.2997 (0.3060)	loss 1.3154 (1.3269)	grad_norm 27.0495 (nan)	mem 4879MB
[2022-05-31 07:50:38 MetaFG_0] (main.py 265): INFO Train: [67/300][770/1562]	eta 0:04:02 lr 0.000006	time 0.3004 (0.3060)	loss 1.2818 (1.3263)	grad_norm 34.9501 (nan)	mem 4879MB
[2022-05-31 07:50:41 MetaFG_0] (main.py 265): INFO Train: [67/300][780/1562]	eta 0:03:59 lr 0.000006	time 0.2930 (0.3060)	loss 0.9548 (1.3267)	grad_norm 37.5184 (nan)	mem 4879MB
[2022-05-31 07:50:44 MetaFG_0] (main.py 265): INFO Train: [67/300][790/1562]	eta 0:03:56 lr 0.000006	time 0.3004 (0.3060)	loss 1.7304 (1.3286)	grad_norm 31.6044 (nan)	mem 4879MB
[2022-05-31 07:50:47 MetaFG_0] (main.py 265): INFO Train: [67/300][800/1562]	eta 0:03:53 lr 0.000006	time 0.3017 (0.3060)	loss 1.5013 (1.3278)	grad_norm 19.7338 (nan)	mem 4879MB
[2022-05-31 07:50:50 MetaFG_0] (main.py 265): INFO Train: [67/300][810/1562]	eta 0:03:50 lr 0.000006	time 0.2986 (0.3060)	loss 1.3630 (1.3283)	grad_norm 23.5237 (nan)	mem 4879MB
[2022-05-31 07:50:53 MetaFG_0] (main.py 265): INFO Train: [67/300][820/1562]	eta 0:03:47 lr 0.000006	time 0.2924 (0.3059)	loss 1.2742 (1.3282)	grad_norm 28.3542 (nan)	mem 4879MB
[2022-05-31 07:50:56 MetaFG_0] (main.py 265): INFO Train: [67/300][830/1562]	eta 0:03:43 lr 0.000006	time 0.2933 (0.3059)	loss 0.9172 (1.3286)	grad_norm 32.6768 (nan)	mem 4879MB
[2022-05-31 07:50:59 MetaFG_0] (main.py 265): INFO Train: [67/300][840/1562]	eta 0:03:40 lr 0.000006	time 0.2984 (0.3059)	loss 0.9481 (1.3277)	grad_norm 40.9377 (nan)	mem 4879MB
[2022-05-31 07:51:02 MetaFG_0] (main.py 265): INFO Train: [67/300][850/1562]	eta 0:03:37 lr 0.000006	time 0.2936 (0.3059)	loss 1.3247 (1.3286)	grad_norm 23.6964 (nan)	mem 4879MB
[2022-05-31 07:51:05 MetaFG_0] (main.py 265): INFO Train: [67/300][860/1562]	eta 0:03:34 lr 0.000006	time 0.2949 (0.3059)	loss 1.7307 (1.3290)	grad_norm 24.1386 (nan)	mem 4879MB
[2022-05-31 07:51:08 MetaFG_0] (main.py 265): INFO Train: [67/300][870/1562]	eta 0:03:31 lr 0.000006	time 0.3030 (0.3059)	loss 1.4395 (1.3297)	grad_norm 40.2864 (nan)	mem 4879MB
[2022-05-31 07:51:11 MetaFG_0] (main.py 265): INFO Train: [67/300][880/1562]	eta 0:03:28 lr 0.000006	time 0.2927 (0.3059)	loss 1.3939 (1.3302)	grad_norm 29.9538 (nan)	mem 4879MB
[2022-05-31 07:51:14 MetaFG_0] (main.py 265): INFO Train: [67/300][890/1562]	eta 0:03:25 lr 0.000006	time 0.2930 (0.3059)	loss 1.5361 (1.3309)	grad_norm 38.3402 (nan)	mem 4879MB
[2022-05-31 07:51:17 MetaFG_0] (main.py 265): INFO Train: [67/300][900/1562]	eta 0:03:22 lr 0.000006	time 0.2925 (0.3059)	loss 1.1541 (1.3322)	grad_norm 34.2346 (nan)	mem 4879MB
[2022-05-31 07:51:20 MetaFG_0] (main.py 265): INFO Train: [67/300][910/1562]	eta 0:03:19 lr 0.000006	time 0.2933 (0.3059)	loss 1.4502 (1.3326)	grad_norm 27.3422 (nan)	mem 4879MB
[2022-05-31 07:51:24 MetaFG_0] (main.py 265): INFO Train: [67/300][920/1562]	eta 0:03:16 lr 0.000006	time 0.2933 (0.3059)	loss 1.5351 (1.3317)	grad_norm 34.5052 (nan)	mem 4879MB
[2022-05-31 07:51:27 MetaFG_0] (main.py 265): INFO Train: [67/300][930/1562]	eta 0:03:13 lr 0.000006	time 0.2998 (0.3059)	loss 1.3871 (1.3314)	grad_norm 31.1472 (nan)	mem 4879MB
[2022-05-31 07:51:30 MetaFG_0] (main.py 265): INFO Train: [67/300][940/1562]	eta 0:03:10 lr 0.000006	time 0.2990 (0.3059)	loss 1.0966 (1.3309)	grad_norm 20.0626 (nan)	mem 4879MB
[2022-05-31 07:51:33 MetaFG_0] (main.py 265): INFO Train: [67/300][950/1562]	eta 0:03:07 lr 0.000006	time 0.2963 (0.3058)	loss 1.2350 (1.3308)	grad_norm 34.7436 (nan)	mem 4879MB
[2022-05-31 07:51:36 MetaFG_0] (main.py 265): INFO Train: [67/300][960/1562]	eta 0:03:04 lr 0.000006	time 0.2981 (0.3059)	loss 1.3411 (1.3308)	grad_norm 17.4071 (nan)	mem 4879MB
[2022-05-31 07:51:39 MetaFG_0] (main.py 265): INFO Train: [67/300][970/1562]	eta 0:03:01 lr 0.000006	time 0.2976 (0.3059)	loss 1.0607 (1.3311)	grad_norm 34.1859 (nan)	mem 4879MB
[2022-05-31 07:51:42 MetaFG_0] (main.py 265): INFO Train: [67/300][980/1562]	eta 0:02:57 lr 0.000006	time 0.3022 (0.3058)	loss 1.3305 (1.3308)	grad_norm 26.5143 (nan)	mem 4879MB
[2022-05-31 07:51:45 MetaFG_0] (main.py 265): INFO Train: [67/300][990/1562]	eta 0:02:54 lr 0.000006	time 0.2915 (0.3058)	loss 1.4298 (1.3309)	grad_norm 25.7980 (nan)	mem 4879MB
[2022-05-31 07:51:48 MetaFG_0] (main.py 265): INFO Train: [67/300][1000/1562]	eta 0:02:51 lr 0.000006	time 0.2939 (0.3058)	loss 1.2493 (1.3299)	grad_norm 29.2884 (nan)	mem 4879MB
[2022-05-31 07:51:51 MetaFG_0] (main.py 265): INFO Train: [67/300][1010/1562]	eta 0:02:48 lr 0.000006	time 0.2987 (0.3058)	loss 1.1814 (1.3309)	grad_norm 41.7760 (nan)	mem 4879MB
[2022-05-31 07:51:54 MetaFG_0] (main.py 265): INFO Train: [67/300][1020/1562]	eta 0:02:45 lr 0.000006	time 0.2948 (0.3057)	loss 1.3153 (1.3307)	grad_norm 48.2526 (nan)	mem 4879MB
[2022-05-31 07:51:57 MetaFG_0] (main.py 265): INFO Train: [67/300][1030/1562]	eta 0:02:42 lr 0.000006	time 0.2921 (0.3057)	loss 1.5927 (1.3311)	grad_norm 32.1036 (nan)	mem 4879MB
[2022-05-31 07:52:00 MetaFG_0] (main.py 265): INFO Train: [67/300][1040/1562]	eta 0:02:39 lr 0.000006	time 0.2925 (0.3057)	loss 1.0839 (1.3309)	grad_norm 28.9698 (nan)	mem 4879MB
[2022-05-31 07:52:03 MetaFG_0] (main.py 265): INFO Train: [67/300][1050/1562]	eta 0:02:36 lr 0.000006	time 0.2919 (0.3057)	loss 1.5704 (1.3310)	grad_norm 41.2385 (nan)	mem 4879MB
[2022-05-31 07:52:06 MetaFG_0] (main.py 265): INFO Train: [67/300][1060/1562]	eta 0:02:33 lr 0.000006	time 0.2934 (0.3057)	loss 0.7726 (1.3302)	grad_norm 11.8943 (nan)	mem 4879MB
[2022-05-31 07:52:09 MetaFG_0] (main.py 265): INFO Train: [67/300][1070/1562]	eta 0:02:30 lr 0.000006	time 0.3006 (0.3057)	loss 1.3296 (1.3304)	grad_norm 33.8049 (nan)	mem 4879MB
[2022-05-31 07:52:12 MetaFG_0] (main.py 265): INFO Train: [67/300][1080/1562]	eta 0:02:27 lr 0.000006	time 0.2947 (0.3057)	loss 1.4922 (1.3297)	grad_norm 35.1518 (nan)	mem 4879MB
[2022-05-31 07:52:15 MetaFG_0] (main.py 265): INFO Train: [67/300][1090/1562]	eta 0:02:24 lr 0.000006	time 0.2930 (0.3057)	loss 1.5225 (1.3309)	grad_norm 16.9596 (nan)	mem 4879MB
[2022-05-31 07:52:18 MetaFG_0] (main.py 265): INFO Train: [67/300][1100/1562]	eta 0:02:21 lr 0.000006	time 0.2987 (0.3057)	loss 1.5005 (1.3319)	grad_norm 20.2715 (nan)	mem 4879MB
[2022-05-31 07:52:21 MetaFG_0] (main.py 265): INFO Train: [67/300][1110/1562]	eta 0:02:18 lr 0.000006	time 0.2931 (0.3056)	loss 1.6351 (1.3320)	grad_norm 24.2691 (nan)	mem 4879MB
[2022-05-31 07:52:24 MetaFG_0] (main.py 265): INFO Train: [67/300][1120/1562]	eta 0:02:15 lr 0.000006	time 0.2925 (0.3056)	loss 1.4888 (1.3322)	grad_norm 25.0094 (nan)	mem 4879MB
[2022-05-31 07:52:27 MetaFG_0] (main.py 265): INFO Train: [67/300][1130/1562]	eta 0:02:12 lr 0.000006	time 0.2932 (0.3056)	loss 0.9944 (1.3320)	grad_norm 37.9818 (nan)	mem 4879MB
[2022-05-31 07:52:31 MetaFG_0] (main.py 265): INFO Train: [67/300][1140/1562]	eta 0:02:08 lr 0.000006	time 0.2925 (0.3056)	loss 1.4875 (1.3322)	grad_norm 54.9481 (nan)	mem 4879MB
[2022-05-31 07:52:34 MetaFG_0] (main.py 265): INFO Train: [67/300][1150/1562]	eta 0:02:05 lr 0.000006	time 0.2933 (0.3056)	loss 1.1214 (1.3320)	grad_norm 26.4486 (nan)	mem 4879MB
[2022-05-31 07:52:37 MetaFG_0] (main.py 265): INFO Train: [67/300][1160/1562]	eta 0:02:02 lr 0.000006	time 0.2984 (0.3056)	loss 1.4955 (1.3324)	grad_norm 36.8215 (nan)	mem 4879MB
[2022-05-31 07:52:40 MetaFG_0] (main.py 265): INFO Train: [67/300][1170/1562]	eta 0:01:59 lr 0.000006	time 0.2953 (0.3056)	loss 1.5625 (1.3327)	grad_norm 93.1458 (nan)	mem 4879MB
[2022-05-31 07:52:43 MetaFG_0] (main.py 265): INFO Train: [67/300][1180/1562]	eta 0:01:56 lr 0.000006	time 0.2927 (0.3056)	loss 1.3421 (1.3324)	grad_norm 31.1339 (nan)	mem 4879MB
[2022-05-31 07:52:46 MetaFG_0] (main.py 265): INFO Train: [67/300][1190/1562]	eta 0:01:53 lr 0.000006	time 0.2937 (0.3056)	loss 1.1326 (1.3325)	grad_norm 33.1274 (nan)	mem 4879MB
[2022-05-31 07:52:49 MetaFG_0] (main.py 265): INFO Train: [67/300][1200/1562]	eta 0:01:50 lr 0.000006	time 0.2921 (0.3056)	loss 1.7321 (1.3314)	grad_norm 39.9476 (nan)	mem 4879MB
[2022-05-31 07:52:52 MetaFG_0] (main.py 265): INFO Train: [67/300][1210/1562]	eta 0:01:47 lr 0.000006	time 0.2927 (0.3056)	loss 1.0264 (1.3318)	grad_norm 24.1160 (nan)	mem 4879MB
[2022-05-31 07:52:55 MetaFG_0] (main.py 265): INFO Train: [67/300][1220/1562]	eta 0:01:44 lr 0.000006	time 0.2986 (0.3056)	loss 1.5885 (1.3321)	grad_norm 36.6493 (nan)	mem 4879MB
[2022-05-31 07:52:58 MetaFG_0] (main.py 265): INFO Train: [67/300][1230/1562]	eta 0:01:41 lr 0.000006	time 0.2925 (0.3055)	loss 1.6219 (1.3324)	grad_norm 27.9831 (nan)	mem 4879MB
[2022-05-31 07:53:01 MetaFG_0] (main.py 265): INFO Train: [67/300][1240/1562]	eta 0:01:38 lr 0.000006	time 0.2986 (0.3056)	loss 1.1744 (1.3326)	grad_norm 28.3890 (nan)	mem 4879MB
[2022-05-31 07:53:04 MetaFG_0] (main.py 265): INFO Train: [67/300][1250/1562]	eta 0:01:35 lr 0.000006	time 0.2926 (0.3055)	loss 1.5454 (1.3328)	grad_norm 33.0422 (nan)	mem 4879MB
[2022-05-31 07:53:07 MetaFG_0] (main.py 265): INFO Train: [67/300][1260/1562]	eta 0:01:32 lr 0.000006	time 0.2930 (0.3055)	loss 1.2651 (1.3331)	grad_norm 25.5566 (nan)	mem 4879MB
[2022-05-31 07:53:10 MetaFG_0] (main.py 265): INFO Train: [67/300][1270/1562]	eta 0:01:29 lr 0.000006	time 0.2927 (0.3055)	loss 1.6477 (1.3333)	grad_norm 38.5480 (nan)	mem 4879MB
[2022-05-31 07:53:13 MetaFG_0] (main.py 265): INFO Train: [67/300][1280/1562]	eta 0:01:26 lr 0.000006	time 0.2926 (0.3055)	loss 1.4087 (1.3331)	grad_norm 58.0774 (nan)	mem 4879MB
[2022-05-31 07:53:16 MetaFG_0] (main.py 265): INFO Train: [67/300][1290/1562]	eta 0:01:23 lr 0.000006	time 0.2939 (0.3055)	loss 1.4686 (1.3330)	grad_norm 33.4265 (nan)	mem 4879MB
[2022-05-31 07:53:19 MetaFG_0] (main.py 265): INFO Train: [67/300][1300/1562]	eta 0:01:20 lr 0.000006	time 0.2928 (0.3055)	loss 1.0769 (1.3333)	grad_norm 34.3627 (nan)	mem 4879MB
[2022-05-31 07:53:22 MetaFG_0] (main.py 265): INFO Train: [67/300][1310/1562]	eta 0:01:16 lr 0.000006	time 0.2938 (0.3055)	loss 1.3480 (1.3337)	grad_norm 31.0516 (nan)	mem 4879MB
[2022-05-31 07:53:25 MetaFG_0] (main.py 265): INFO Train: [67/300][1320/1562]	eta 0:01:13 lr 0.000006	time 0.2931 (0.3055)	loss 1.3436 (1.3336)	grad_norm 26.5501 (nan)	mem 4879MB
[2022-05-31 07:53:28 MetaFG_0] (main.py 265): INFO Train: [67/300][1330/1562]	eta 0:01:10 lr 0.000006	time 0.2938 (0.3055)	loss 1.5965 (1.3340)	grad_norm 33.2567 (nan)	mem 4879MB
[2022-05-31 07:53:32 MetaFG_0] (main.py 265): INFO Train: [67/300][1340/1562]	eta 0:01:07 lr 0.000006	time 0.2986 (0.3057)	loss 1.4091 (1.3339)	grad_norm 31.7450 (nan)	mem 4879MB
[2022-05-31 07:53:35 MetaFG_0] (main.py 265): INFO Train: [67/300][1350/1562]	eta 0:01:04 lr 0.000006	time 0.2976 (0.3057)	loss 1.3471 (1.3347)	grad_norm 30.1701 (nan)	mem 4879MB
[2022-05-31 07:53:38 MetaFG_0] (main.py 265): INFO Train: [67/300][1360/1562]	eta 0:01:01 lr 0.000006	time 0.2931 (0.3057)	loss 1.2787 (1.3345)	grad_norm 27.8105 (nan)	mem 4879MB
[2022-05-31 07:53:41 MetaFG_0] (main.py 265): INFO Train: [67/300][1370/1562]	eta 0:00:58 lr 0.000006	time 0.2938 (0.3057)	loss 1.3547 (1.3347)	grad_norm 23.9343 (nan)	mem 4879MB
[2022-05-31 07:53:44 MetaFG_0] (main.py 265): INFO Train: [67/300][1380/1562]	eta 0:00:55 lr 0.000006	time 0.2995 (0.3057)	loss 1.5537 (1.3341)	grad_norm 27.0046 (nan)	mem 4879MB
[2022-05-31 07:53:47 MetaFG_0] (main.py 265): INFO Train: [67/300][1390/1562]	eta 0:00:52 lr 0.000006	time 0.2933 (0.3056)	loss 1.6189 (1.3344)	grad_norm 32.4842 (nan)	mem 4879MB
[2022-05-31 07:53:50 MetaFG_0] (main.py 265): INFO Train: [67/300][1400/1562]	eta 0:00:49 lr 0.000006	time 0.2933 (0.3056)	loss 1.5707 (1.3339)	grad_norm 26.4938 (nan)	mem 4879MB
[2022-05-31 07:53:53 MetaFG_0] (main.py 265): INFO Train: [67/300][1410/1562]	eta 0:00:46 lr 0.000006	time 0.2994 (0.3056)	loss 1.4570 (1.3341)	grad_norm 33.7722 (nan)	mem 4879MB
[2022-05-31 07:53:56 MetaFG_0] (main.py 265): INFO Train: [67/300][1420/1562]	eta 0:00:43 lr 0.000006	time 0.2924 (0.3056)	loss 1.3744 (1.3340)	grad_norm 27.8132 (nan)	mem 4879MB
[2022-05-31 07:53:59 MetaFG_0] (main.py 265): INFO Train: [67/300][1430/1562]	eta 0:00:40 lr 0.000006	time 0.2990 (0.3056)	loss 0.9596 (1.3343)	grad_norm 31.0266 (nan)	mem 4879MB
[2022-05-31 07:54:02 MetaFG_0] (main.py 265): INFO Train: [67/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2923 (0.3056)	loss 1.2619 (1.3342)	grad_norm 26.2984 (nan)	mem 4879MB
[2022-05-31 07:54:05 MetaFG_0] (main.py 265): INFO Train: [67/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2991 (0.3056)	loss 1.7369 (1.3351)	grad_norm 25.8879 (nan)	mem 4879MB
[2022-05-31 07:54:08 MetaFG_0] (main.py 265): INFO Train: [67/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2929 (0.3056)	loss 1.2737 (1.3353)	grad_norm 30.3788 (nan)	mem 4879MB
[2022-05-31 07:54:11 MetaFG_0] (main.py 265): INFO Train: [67/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2930 (0.3056)	loss 1.7682 (1.3362)	grad_norm 47.2279 (nan)	mem 4879MB
[2022-05-31 07:54:14 MetaFG_0] (main.py 265): INFO Train: [67/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2986 (0.3056)	loss 1.3550 (1.3357)	grad_norm 46.6850 (nan)	mem 4879MB
[2022-05-31 07:54:17 MetaFG_0] (main.py 265): INFO Train: [67/300][1490/1562]	eta 0:00:22 lr 0.000005	time 0.2945 (0.3056)	loss 1.3356 (1.3350)	grad_norm 30.4801 (nan)	mem 4879MB
[2022-05-31 07:54:21 MetaFG_0] (main.py 265): INFO Train: [67/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2964 (0.3056)	loss 1.5478 (1.3351)	grad_norm 22.2288 (nan)	mem 4879MB
[2022-05-31 07:54:24 MetaFG_0] (main.py 265): INFO Train: [67/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2947 (0.3056)	loss 0.9302 (1.3348)	grad_norm 38.5001 (nan)	mem 4879MB
[2022-05-31 07:54:27 MetaFG_0] (main.py 265): INFO Train: [67/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2930 (0.3056)	loss 0.7921 (1.3347)	grad_norm 22.5446 (nan)	mem 4879MB
[2022-05-31 07:54:30 MetaFG_0] (main.py 265): INFO Train: [67/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2931 (0.3056)	loss 0.9380 (1.3352)	grad_norm 36.8158 (nan)	mem 4879MB
[2022-05-31 07:54:33 MetaFG_0] (main.py 265): INFO Train: [67/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2921 (0.3056)	loss 1.2276 (1.3352)	grad_norm 26.3928 (nan)	mem 4879MB
[2022-05-31 07:54:36 MetaFG_0] (main.py 265): INFO Train: [67/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2978 (0.3056)	loss 0.9241 (1.3351)	grad_norm 51.3865 (nan)	mem 4879MB
[2022-05-31 07:54:39 MetaFG_0] (main.py 265): INFO Train: [67/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2930 (0.3055)	loss 1.3107 (1.3350)	grad_norm 32.2979 (nan)	mem 4879MB
[2022-05-31 07:54:39 MetaFG_0] (main.py 272): INFO EPOCH 67 training takes 0:07:57
[2022-05-31 07:54:39 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_67.pth saving......
[2022-05-31 07:54:40 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_67.pth saved !!!
[2022-05-31 07:54:40 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 07:54:41 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 07:54:41 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 07:54:42 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.709 (0.709)	Loss 0.5199 (0.5199)	Acc@1 93.750 (93.750)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 07:54:43 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.154)	Loss 0.5432 (0.5342)	Acc@1 87.500 (90.057)	Acc@5 100.000 (99.148)	Mem 4879MB
[2022-05-31 07:54:44 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.088 (0.126)	Loss 0.7798 (0.5828)	Acc@1 81.250 (88.542)	Acc@5 93.750 (98.661)	Mem 4879MB
[2022-05-31 07:54:45 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.090 (0.115)	Loss 0.5510 (0.5886)	Acc@1 87.500 (88.306)	Acc@5 100.000 (98.690)	Mem 4879MB
[2022-05-31 07:54:46 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.107 (0.110)	Loss 0.5017 (0.5804)	Acc@1 93.750 (88.338)	Acc@5 100.000 (98.628)	Mem 4879MB
[2022-05-31 07:54:47 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.107)	Loss 0.6051 (0.5935)	Acc@1 84.375 (87.684)	Acc@5 100.000 (98.652)	Mem 4879MB
[2022-05-31 07:54:48 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.104)	Loss 0.5132 (0.5887)	Acc@1 84.375 (87.500)	Acc@5 100.000 (98.668)	Mem 4879MB
[2022-05-31 07:54:49 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.087 (0.103)	Loss 0.6407 (0.5996)	Acc@1 84.375 (87.280)	Acc@5 100.000 (98.548)	Mem 4879MB
[2022-05-31 07:54:50 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.094 (0.102)	Loss 0.5010 (0.6058)	Acc@1 90.625 (87.114)	Acc@5 100.000 (98.611)	Mem 4879MB
[2022-05-31 07:54:51 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.097 (0.101)	Loss 0.4617 (0.6055)	Acc@1 90.625 (87.088)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 07:54:51 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.101)	Loss 0.4897 (0.6058)	Acc@1 93.750 (86.881)	Acc@5 100.000 (98.700)	Mem 4879MB
[2022-05-31 07:54:52 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.095 (0.100)	Loss 0.6430 (0.6056)	Acc@1 87.500 (86.937)	Acc@5 96.875 (98.733)	Mem 4879MB
[2022-05-31 07:54:53 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.5767 (0.6066)	Acc@1 93.750 (87.009)	Acc@5 100.000 (98.631)	Mem 4879MB
[2022-05-31 07:54:54 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.093 (0.099)	Loss 0.4476 (0.6041)	Acc@1 93.750 (87.094)	Acc@5 100.000 (98.712)	Mem 4879MB
[2022-05-31 07:54:55 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.7770 (0.6098)	Acc@1 84.375 (86.835)	Acc@5 96.875 (98.692)	Mem 4879MB
[2022-05-31 07:54:56 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 0.3980 (0.6128)	Acc@1 96.875 (86.693)	Acc@5 100.000 (98.655)	Mem 4879MB
[2022-05-31 07:54:57 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.098)	Loss 0.5102 (0.6059)	Acc@1 87.500 (86.918)	Acc@5 100.000 (98.738)	Mem 4879MB
[2022-05-31 07:54:58 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.6868 (0.6073)	Acc@1 81.250 (86.915)	Acc@5 100.000 (98.757)	Mem 4879MB
[2022-05-31 07:54:59 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.096 (0.098)	Loss 0.3270 (0.6048)	Acc@1 96.875 (86.999)	Acc@5 100.000 (98.757)	Mem 4879MB
[2022-05-31 07:55:00 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.098)	Loss 0.4639 (0.6052)	Acc@1 90.625 (87.042)	Acc@5 100.000 (98.740)	Mem 4879MB
[2022-05-31 07:55:01 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.088 (0.097)	Loss 0.4433 (0.6050)	Acc@1 87.500 (87.065)	Acc@5 100.000 (98.710)	Mem 4879MB
[2022-05-31 07:55:02 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.097)	Loss 0.5640 (0.6079)	Acc@1 84.375 (86.967)	Acc@5 100.000 (98.667)	Mem 4879MB
[2022-05-31 07:55:03 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.099 (0.097)	Loss 0.4937 (0.6069)	Acc@1 90.625 (86.949)	Acc@5 96.875 (98.699)	Mem 4879MB
[2022-05-31 07:55:04 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 0.6368 (0.6059)	Acc@1 84.375 (86.905)	Acc@5 100.000 (98.715)	Mem 4879MB
[2022-05-31 07:55:05 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.097 (0.097)	Loss 0.5832 (0.6068)	Acc@1 84.375 (86.955)	Acc@5 96.875 (98.677)	Mem 4879MB
[2022-05-31 07:55:06 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.087 (0.097)	Loss 0.3981 (0.6060)	Acc@1 96.875 (87.014)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 07:55:07 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.087 (0.097)	Loss 0.9564 (0.6089)	Acc@1 78.125 (86.949)	Acc@5 93.750 (98.671)	Mem 4879MB
[2022-05-31 07:55:08 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.095 (0.097)	Loss 0.4919 (0.6061)	Acc@1 96.875 (87.027)	Acc@5 96.875 (98.685)	Mem 4879MB
[2022-05-31 07:55:08 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.096)	Loss 0.6178 (0.6063)	Acc@1 90.625 (87.022)	Acc@5 96.875 (98.677)	Mem 4879MB
[2022-05-31 07:55:09 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.100 (0.096)	Loss 0.8141 (0.6049)	Acc@1 78.125 (87.006)	Acc@5 100.000 (98.711)	Mem 4879MB
[2022-05-31 07:55:10 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.089 (0.096)	Loss 0.4563 (0.6044)	Acc@1 93.750 (87.022)	Acc@5 100.000 (98.733)	Mem 4879MB
[2022-05-31 07:55:11 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.086 (0.096)	Loss 0.4528 (0.6028)	Acc@1 93.750 (87.068)	Acc@5 100.000 (98.724)	Mem 4879MB
[2022-05-31 07:55:11 MetaFG_0] (main.py 330): INFO  * Acc@1 87.110 Acc@5 98.730
[2022-05-31 07:55:11 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.1%
[2022-05-31 07:55:11 MetaFG_0] (main.py 171): INFO Max accuracy: 87.28%
[2022-05-31 07:55:13 MetaFG_0] (main.py 265): INFO Train: [68/300][0/1562]	eta 0:26:54 lr 0.000005	time 1.0338 (1.0338)	loss 1.3043 (1.3043)	grad_norm 34.4074 (34.4074)	mem 4879MB
[2022-05-31 07:55:16 MetaFG_0] (main.py 265): INFO Train: [68/300][10/1562]	eta 0:09:47 lr 0.000005	time 0.2942 (0.3783)	loss 1.4508 (1.3879)	grad_norm 28.2296 (28.7162)	mem 4879MB
[2022-05-31 07:55:19 MetaFG_0] (main.py 265): INFO Train: [68/300][20/1562]	eta 0:08:50 lr 0.000005	time 0.2994 (0.3438)	loss 1.4028 (1.3628)	grad_norm 29.4016 (28.4090)	mem 4879MB
[2022-05-31 07:55:22 MetaFG_0] (main.py 265): INFO Train: [68/300][30/1562]	eta 0:08:27 lr 0.000005	time 0.2990 (0.3315)	loss 1.5310 (1.3918)	grad_norm 41.5470 (30.0762)	mem 4879MB
[2022-05-31 07:55:25 MetaFG_0] (main.py 265): INFO Train: [68/300][40/1562]	eta 0:08:15 lr 0.000005	time 0.2944 (0.3253)	loss 1.3582 (1.4117)	grad_norm 47.5083 (30.1601)	mem 4879MB
[2022-05-31 07:55:28 MetaFG_0] (main.py 265): INFO Train: [68/300][50/1562]	eta 0:08:06 lr 0.000005	time 0.2987 (0.3215)	loss 1.7343 (1.3979)	grad_norm 37.6764 (30.4037)	mem 4879MB
[2022-05-31 07:55:31 MetaFG_0] (main.py 265): INFO Train: [68/300][60/1562]	eta 0:07:59 lr 0.000005	time 0.2963 (0.3190)	loss 1.4506 (1.3780)	grad_norm 22.3392 (29.6574)	mem 4879MB
[2022-05-31 07:55:34 MetaFG_0] (main.py 265): INFO Train: [68/300][70/1562]	eta 0:07:52 lr 0.000005	time 0.2938 (0.3166)	loss 1.0179 (1.3498)	grad_norm 35.4987 (30.0691)	mem 4879MB
[2022-05-31 07:55:37 MetaFG_0] (main.py 265): INFO Train: [68/300][80/1562]	eta 0:07:46 lr 0.000005	time 0.2915 (0.3150)	loss 1.6806 (1.3538)	grad_norm 33.9438 (30.1682)	mem 4879MB
[2022-05-31 07:55:40 MetaFG_0] (main.py 265): INFO Train: [68/300][90/1562]	eta 0:07:42 lr 0.000005	time 0.2987 (0.3140)	loss 1.2958 (1.3402)	grad_norm 26.6584 (29.9227)	mem 4879MB
[2022-05-31 07:55:43 MetaFG_0] (main.py 265): INFO Train: [68/300][100/1562]	eta 0:07:37 lr 0.000005	time 0.3013 (0.3131)	loss 1.1571 (1.3339)	grad_norm 29.8077 (29.9676)	mem 4879MB
[2022-05-31 07:55:46 MetaFG_0] (main.py 265): INFO Train: [68/300][110/1562]	eta 0:07:33 lr 0.000005	time 0.2926 (0.3124)	loss 1.6488 (1.3414)	grad_norm 34.1958 (29.6334)	mem 4879MB
[2022-05-31 07:55:49 MetaFG_0] (main.py 265): INFO Train: [68/300][120/1562]	eta 0:07:29 lr 0.000005	time 0.2990 (0.3120)	loss 1.3394 (1.3344)	grad_norm 27.5840 (29.6921)	mem 4879MB
[2022-05-31 07:55:52 MetaFG_0] (main.py 265): INFO Train: [68/300][130/1562]	eta 0:07:25 lr 0.000005	time 0.2944 (0.3114)	loss 1.4273 (1.3312)	grad_norm 33.2146 (30.0944)	mem 4879MB
[2022-05-31 07:55:55 MetaFG_0] (main.py 265): INFO Train: [68/300][140/1562]	eta 0:07:22 lr 0.000005	time 0.2939 (0.3110)	loss 1.5271 (1.3343)	grad_norm 29.0816 (29.7612)	mem 4879MB
[2022-05-31 07:55:58 MetaFG_0] (main.py 265): INFO Train: [68/300][150/1562]	eta 0:07:18 lr 0.000005	time 0.2938 (0.3106)	loss 1.1979 (1.3372)	grad_norm 26.5073 (29.3678)	mem 4879MB
[2022-05-31 07:56:01 MetaFG_0] (main.py 265): INFO Train: [68/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.2926 (0.3101)	loss 1.4590 (1.3391)	grad_norm 22.6594 (29.0944)	mem 4879MB
[2022-05-31 07:56:04 MetaFG_0] (main.py 265): INFO Train: [68/300][170/1562]	eta 0:07:11 lr 0.000005	time 0.2921 (0.3098)	loss 0.9532 (1.3416)	grad_norm 22.0486 (29.7596)	mem 4879MB
[2022-05-31 07:56:08 MetaFG_0] (main.py 265): INFO Train: [68/300][180/1562]	eta 0:07:07 lr 0.000005	time 0.3053 (0.3095)	loss 1.4318 (1.3443)	grad_norm 24.0906 (29.9224)	mem 4879MB
[2022-05-31 07:56:11 MetaFG_0] (main.py 265): INFO Train: [68/300][190/1562]	eta 0:07:04 lr 0.000005	time 0.2936 (0.3092)	loss 1.5534 (1.3451)	grad_norm 36.6531 (29.7970)	mem 4879MB
[2022-05-31 07:56:14 MetaFG_0] (main.py 265): INFO Train: [68/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2921 (0.3090)	loss 0.9871 (1.3413)	grad_norm 23.6402 (29.7193)	mem 4879MB
[2022-05-31 07:56:17 MetaFG_0] (main.py 265): INFO Train: [68/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2988 (0.3088)	loss 1.3139 (1.3374)	grad_norm 25.8056 (29.5903)	mem 4879MB
[2022-05-31 07:56:20 MetaFG_0] (main.py 265): INFO Train: [68/300][220/1562]	eta 0:06:54 lr 0.000005	time 0.2927 (0.3086)	loss 1.4291 (1.3348)	grad_norm 15.0894 (29.4060)	mem 4879MB
[2022-05-31 07:56:23 MetaFG_0] (main.py 265): INFO Train: [68/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2926 (0.3084)	loss 1.3075 (1.3356)	grad_norm 33.8102 (29.6319)	mem 4879MB
[2022-05-31 07:56:26 MetaFG_0] (main.py 265): INFO Train: [68/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2925 (0.3082)	loss 1.4989 (1.3363)	grad_norm 37.4648 (29.6258)	mem 4879MB
[2022-05-31 07:56:29 MetaFG_0] (main.py 265): INFO Train: [68/300][250/1562]	eta 0:06:44 lr 0.000005	time 0.2940 (0.3080)	loss 0.9837 (1.3352)	grad_norm 17.9664 (29.4938)	mem 4879MB
[2022-05-31 07:56:32 MetaFG_0] (main.py 265): INFO Train: [68/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.3000 (0.3079)	loss 1.2391 (1.3296)	grad_norm 22.6612 (29.5298)	mem 4879MB
[2022-05-31 07:56:35 MetaFG_0] (main.py 265): INFO Train: [68/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2925 (0.3077)	loss 1.1829 (1.3309)	grad_norm 57.6062 (29.6334)	mem 4879MB
[2022-05-31 07:56:38 MetaFG_0] (main.py 265): INFO Train: [68/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2920 (0.3075)	loss 1.3769 (1.3284)	grad_norm 38.0541 (29.5561)	mem 4879MB
[2022-05-31 07:56:41 MetaFG_0] (main.py 265): INFO Train: [68/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2934 (0.3074)	loss 1.5854 (1.3325)	grad_norm 25.0323 (29.6317)	mem 4879MB
[2022-05-31 07:56:44 MetaFG_0] (main.py 265): INFO Train: [68/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2935 (0.3074)	loss 1.3183 (1.3317)	grad_norm 48.9471 (29.6411)	mem 4879MB
[2022-05-31 07:56:47 MetaFG_0] (main.py 265): INFO Train: [68/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2943 (0.3073)	loss 1.6712 (1.3314)	grad_norm 27.3249 (29.6009)	mem 4879MB
[2022-05-31 07:56:50 MetaFG_0] (main.py 265): INFO Train: [68/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2942 (0.3073)	loss 1.4995 (1.3329)	grad_norm 27.2841 (29.5454)	mem 4879MB
[2022-05-31 07:56:53 MetaFG_0] (main.py 265): INFO Train: [68/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2987 (0.3072)	loss 1.4505 (1.3300)	grad_norm 22.1901 (29.4054)	mem 4879MB
[2022-05-31 07:56:56 MetaFG_0] (main.py 265): INFO Train: [68/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2922 (0.3071)	loss 1.5291 (1.3308)	grad_norm 61.4305 (29.3132)	mem 4879MB
[2022-05-31 07:56:59 MetaFG_0] (main.py 265): INFO Train: [68/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2993 (0.3071)	loss 1.4018 (1.3299)	grad_norm 23.5069 (29.3325)	mem 4879MB
[2022-05-31 07:57:02 MetaFG_0] (main.py 265): INFO Train: [68/300][360/1562]	eta 0:06:09 lr 0.000005	time 0.2985 (0.3070)	loss 1.0605 (1.3294)	grad_norm 21.3761 (29.3429)	mem 4879MB
[2022-05-31 07:57:05 MetaFG_0] (main.py 265): INFO Train: [68/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2935 (0.3069)	loss 1.5626 (1.3305)	grad_norm 24.7355 (29.2308)	mem 4879MB
[2022-05-31 07:57:08 MetaFG_0] (main.py 265): INFO Train: [68/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2927 (0.3069)	loss 1.4767 (1.3298)	grad_norm 15.9420 (29.1389)	mem 4879MB
[2022-05-31 07:57:11 MetaFG_0] (main.py 265): INFO Train: [68/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2919 (0.3068)	loss 1.5174 (1.3304)	grad_norm 24.3412 (29.1127)	mem 4879MB
[2022-05-31 07:57:14 MetaFG_0] (main.py 265): INFO Train: [68/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2989 (0.3067)	loss 1.0761 (1.3308)	grad_norm 25.0916 (29.0937)	mem 4879MB
[2022-05-31 07:57:18 MetaFG_0] (main.py 265): INFO Train: [68/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.3015 (0.3067)	loss 1.2712 (1.3312)	grad_norm 17.1225 (29.1613)	mem 4879MB
[2022-05-31 07:57:21 MetaFG_0] (main.py 265): INFO Train: [68/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2921 (0.3067)	loss 1.4576 (1.3332)	grad_norm 26.2364 (29.2293)	mem 4879MB
[2022-05-31 07:57:24 MetaFG_0] (main.py 265): INFO Train: [68/300][430/1562]	eta 0:05:47 lr 0.000005	time 0.3000 (0.3067)	loss 1.4266 (1.3316)	grad_norm 26.0188 (29.2227)	mem 4879MB
[2022-05-31 07:57:27 MetaFG_0] (main.py 265): INFO Train: [68/300][440/1562]	eta 0:05:44 lr 0.000005	time 0.2991 (0.3067)	loss 1.3427 (1.3313)	grad_norm 19.9644 (29.2194)	mem 4879MB
[2022-05-31 07:57:30 MetaFG_0] (main.py 265): INFO Train: [68/300][450/1562]	eta 0:05:41 lr 0.000005	time 0.2997 (0.3067)	loss 1.5625 (1.3329)	grad_norm 43.1932 (29.4566)	mem 4879MB
[2022-05-31 07:57:33 MetaFG_0] (main.py 265): INFO Train: [68/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2997 (0.3067)	loss 1.2751 (1.3343)	grad_norm 24.4920 (29.4383)	mem 4879MB
[2022-05-31 07:57:36 MetaFG_0] (main.py 265): INFO Train: [68/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2987 (0.3066)	loss 1.6189 (1.3339)	grad_norm 22.8062 (29.4107)	mem 4879MB
[2022-05-31 07:57:39 MetaFG_0] (main.py 265): INFO Train: [68/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2930 (0.3066)	loss 1.2041 (1.3325)	grad_norm 27.8938 (29.4557)	mem 4879MB
[2022-05-31 07:57:42 MetaFG_0] (main.py 265): INFO Train: [68/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2940 (0.3065)	loss 1.4539 (1.3328)	grad_norm 31.2909 (30.0075)	mem 4879MB
[2022-05-31 07:57:45 MetaFG_0] (main.py 265): INFO Train: [68/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.3001 (0.3065)	loss 1.6995 (1.3341)	grad_norm 19.6147 (29.9493)	mem 4879MB
[2022-05-31 07:57:48 MetaFG_0] (main.py 265): INFO Train: [68/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2925 (0.3064)	loss 1.0754 (1.3336)	grad_norm 20.3378 (30.0365)	mem 4879MB
[2022-05-31 07:57:51 MetaFG_0] (main.py 265): INFO Train: [68/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.2927 (0.3063)	loss 1.3335 (1.3340)	grad_norm 25.8646 (30.0881)	mem 4879MB
[2022-05-31 07:57:54 MetaFG_0] (main.py 265): INFO Train: [68/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2917 (0.3063)	loss 1.6118 (1.3345)	grad_norm 25.1656 (30.0449)	mem 4879MB
[2022-05-31 07:57:57 MetaFG_0] (main.py 265): INFO Train: [68/300][540/1562]	eta 0:05:13 lr 0.000005	time 0.2931 (0.3063)	loss 1.3596 (1.3338)	grad_norm 21.1319 (29.9309)	mem 4879MB
[2022-05-31 07:58:00 MetaFG_0] (main.py 265): INFO Train: [68/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2927 (0.3063)	loss 1.1087 (1.3331)	grad_norm 30.5725 (29.9320)	mem 4879MB
[2022-05-31 07:58:03 MetaFG_0] (main.py 265): INFO Train: [68/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.3006 (0.3063)	loss 1.2696 (1.3320)	grad_norm 25.4179 (29.8746)	mem 4879MB
[2022-05-31 07:58:06 MetaFG_0] (main.py 265): INFO Train: [68/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2942 (0.3063)	loss 1.5877 (1.3319)	grad_norm 35.3626 (29.8226)	mem 4879MB
[2022-05-31 07:58:09 MetaFG_0] (main.py 265): INFO Train: [68/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2944 (0.3063)	loss 1.4284 (1.3307)	grad_norm 22.4350 (29.8022)	mem 4879MB
[2022-05-31 07:58:12 MetaFG_0] (main.py 265): INFO Train: [68/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2929 (0.3062)	loss 0.9875 (1.3289)	grad_norm 43.8517 (29.7221)	mem 4879MB
[2022-05-31 07:58:16 MetaFG_0] (main.py 265): INFO Train: [68/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2994 (0.3062)	loss 0.9640 (1.3279)	grad_norm 43.1958 (29.6855)	mem 4879MB
[2022-05-31 07:58:19 MetaFG_0] (main.py 265): INFO Train: [68/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2927 (0.3062)	loss 0.8091 (1.3268)	grad_norm 35.7696 (29.6766)	mem 4879MB
[2022-05-31 07:58:22 MetaFG_0] (main.py 265): INFO Train: [68/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2976 (0.3062)	loss 1.5971 (1.3286)	grad_norm 34.0036 (29.7389)	mem 4879MB
[2022-05-31 07:58:25 MetaFG_0] (main.py 265): INFO Train: [68/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2933 (0.3062)	loss 1.4309 (1.3282)	grad_norm 27.8526 (29.6952)	mem 4879MB
[2022-05-31 07:58:28 MetaFG_0] (main.py 265): INFO Train: [68/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2983 (0.3062)	loss 1.6492 (1.3286)	grad_norm 19.0050 (29.6629)	mem 4879MB
[2022-05-31 07:58:31 MetaFG_0] (main.py 265): INFO Train: [68/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.2935 (0.3062)	loss 1.2738 (1.3276)	grad_norm 27.3863 (29.7372)	mem 4879MB
[2022-05-31 07:58:34 MetaFG_0] (main.py 265): INFO Train: [68/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2982 (0.3061)	loss 1.4483 (1.3276)	grad_norm 28.4074 (29.7188)	mem 4879MB
[2022-05-31 07:58:37 MetaFG_0] (main.py 265): INFO Train: [68/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2930 (0.3061)	loss 1.1281 (1.3262)	grad_norm 36.5839 (29.7018)	mem 4879MB
[2022-05-31 07:58:40 MetaFG_0] (main.py 265): INFO Train: [68/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2926 (0.3061)	loss 1.5794 (1.3269)	grad_norm 36.3866 (29.7053)	mem 4879MB
[2022-05-31 07:58:43 MetaFG_0] (main.py 265): INFO Train: [68/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2931 (0.3061)	loss 1.6519 (1.3291)	grad_norm 36.0998 (29.6372)	mem 4879MB
[2022-05-31 07:58:46 MetaFG_0] (main.py 265): INFO Train: [68/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2988 (0.3061)	loss 1.5268 (1.3312)	grad_norm 22.2922 (29.6893)	mem 4879MB
[2022-05-31 07:58:49 MetaFG_0] (main.py 265): INFO Train: [68/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2941 (0.3060)	loss 1.1725 (1.3315)	grad_norm 25.1407 (29.7056)	mem 4879MB
[2022-05-31 07:58:52 MetaFG_0] (main.py 265): INFO Train: [68/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2988 (0.3060)	loss 1.5099 (1.3309)	grad_norm 26.6486 (29.7375)	mem 4879MB
[2022-05-31 07:58:55 MetaFG_0] (main.py 265): INFO Train: [68/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2923 (0.3061)	loss 1.2309 (1.3307)	grad_norm 20.8659 (29.7397)	mem 4879MB
[2022-05-31 07:58:58 MetaFG_0] (main.py 265): INFO Train: [68/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2935 (0.3060)	loss 1.4571 (1.3325)	grad_norm 39.7188 (29.8147)	mem 4879MB
[2022-05-31 07:59:01 MetaFG_0] (main.py 265): INFO Train: [68/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.3026 (0.3060)	loss 1.3516 (1.3321)	grad_norm 31.1757 (29.7855)	mem 4879MB
[2022-05-31 07:59:04 MetaFG_0] (main.py 265): INFO Train: [68/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2944 (0.3060)	loss 1.3710 (1.3330)	grad_norm 17.9613 (29.7631)	mem 4879MB
[2022-05-31 07:59:07 MetaFG_0] (main.py 265): INFO Train: [68/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2993 (0.3060)	loss 1.4788 (1.3322)	grad_norm 22.9123 (29.7424)	mem 4879MB
[2022-05-31 07:59:10 MetaFG_0] (main.py 265): INFO Train: [68/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.3010 (0.3060)	loss 1.3557 (1.3311)	grad_norm 20.2559 (29.6861)	mem 4879MB
[2022-05-31 07:59:13 MetaFG_0] (main.py 265): INFO Train: [68/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.2981 (0.3059)	loss 1.3553 (1.3306)	grad_norm 33.9150 (29.7007)	mem 4879MB
[2022-05-31 07:59:17 MetaFG_0] (main.py 265): INFO Train: [68/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2999 (0.3060)	loss 1.4882 (1.3305)	grad_norm 24.3307 (29.7154)	mem 4879MB
[2022-05-31 07:59:20 MetaFG_0] (main.py 265): INFO Train: [68/300][810/1562]	eta 0:03:50 lr 0.000005	time 0.2943 (0.3059)	loss 1.3058 (1.3316)	grad_norm 23.4363 (29.7344)	mem 4879MB
[2022-05-31 07:59:23 MetaFG_0] (main.py 265): INFO Train: [68/300][820/1562]	eta 0:03:47 lr 0.000005	time 0.2937 (0.3059)	loss 1.5344 (1.3307)	grad_norm 25.9100 (29.7516)	mem 4879MB
[2022-05-31 07:59:26 MetaFG_0] (main.py 265): INFO Train: [68/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2925 (0.3060)	loss 1.1331 (1.3297)	grad_norm 53.8096 (29.7998)	mem 4879MB
[2022-05-31 07:59:29 MetaFG_0] (main.py 265): INFO Train: [68/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2997 (0.3059)	loss 0.7342 (1.3295)	grad_norm 14.2117 (29.7550)	mem 4879MB
[2022-05-31 07:59:32 MetaFG_0] (main.py 265): INFO Train: [68/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2932 (0.3059)	loss 1.2246 (1.3291)	grad_norm 38.5468 (29.7053)	mem 4879MB
[2022-05-31 07:59:35 MetaFG_0] (main.py 265): INFO Train: [68/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.3009 (0.3059)	loss 1.3091 (1.3288)	grad_norm 39.2277 (29.7045)	mem 4879MB
[2022-05-31 07:59:38 MetaFG_0] (main.py 265): INFO Train: [68/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2990 (0.3059)	loss 1.5689 (1.3293)	grad_norm 36.9003 (29.7213)	mem 4879MB
[2022-05-31 07:59:41 MetaFG_0] (main.py 265): INFO Train: [68/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2943 (0.3058)	loss 1.2543 (1.3300)	grad_norm 21.0432 (29.7081)	mem 4879MB
[2022-05-31 07:59:44 MetaFG_0] (main.py 265): INFO Train: [68/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2938 (0.3058)	loss 1.4372 (1.3289)	grad_norm 25.4619 (29.6907)	mem 4879MB
[2022-05-31 07:59:47 MetaFG_0] (main.py 265): INFO Train: [68/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2947 (0.3058)	loss 1.1676 (1.3284)	grad_norm 44.3685 (29.7214)	mem 4879MB
[2022-05-31 07:59:50 MetaFG_0] (main.py 265): INFO Train: [68/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2918 (0.3058)	loss 1.3880 (1.3286)	grad_norm 21.2860 (29.7018)	mem 4879MB
[2022-05-31 07:59:53 MetaFG_0] (main.py 265): INFO Train: [68/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2995 (0.3058)	loss 1.1752 (1.3276)	grad_norm 42.1066 (29.7359)	mem 4879MB
[2022-05-31 07:59:56 MetaFG_0] (main.py 265): INFO Train: [68/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2952 (0.3058)	loss 1.4249 (1.3270)	grad_norm 22.0268 (29.7680)	mem 4879MB
[2022-05-31 07:59:59 MetaFG_0] (main.py 265): INFO Train: [68/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.2989 (0.3058)	loss 1.5101 (1.3267)	grad_norm 38.3802 (29.7616)	mem 4879MB
[2022-05-31 08:00:02 MetaFG_0] (main.py 265): INFO Train: [68/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2945 (0.3058)	loss 1.4582 (1.3272)	grad_norm 24.8264 (29.7327)	mem 4879MB
[2022-05-31 08:00:05 MetaFG_0] (main.py 265): INFO Train: [68/300][960/1562]	eta 0:03:04 lr 0.000005	time 0.3297 (0.3059)	loss 0.9534 (1.3266)	grad_norm 35.8894 (29.7468)	mem 4879MB
[2022-05-31 08:00:09 MetaFG_0] (main.py 265): INFO Train: [68/300][970/1562]	eta 0:03:01 lr 0.000005	time 0.2943 (0.3060)	loss 1.5030 (1.3259)	grad_norm 19.1936 (29.7829)	mem 4879MB
[2022-05-31 08:00:12 MetaFG_0] (main.py 265): INFO Train: [68/300][980/1562]	eta 0:02:58 lr 0.000005	time 0.2935 (0.3060)	loss 1.4260 (1.3253)	grad_norm 46.0774 (29.7963)	mem 4879MB
[2022-05-31 08:00:15 MetaFG_0] (main.py 265): INFO Train: [68/300][990/1562]	eta 0:02:55 lr 0.000005	time 0.3018 (0.3060)	loss 0.9613 (1.3242)	grad_norm 28.1963 (29.7982)	mem 4879MB
[2022-05-31 08:00:18 MetaFG_0] (main.py 265): INFO Train: [68/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2932 (0.3060)	loss 1.2875 (1.3241)	grad_norm 31.7575 (29.7847)	mem 4879MB
[2022-05-31 08:00:21 MetaFG_0] (main.py 265): INFO Train: [68/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2942 (0.3059)	loss 1.3904 (1.3248)	grad_norm 18.5703 (29.7472)	mem 4879MB
[2022-05-31 08:00:24 MetaFG_0] (main.py 265): INFO Train: [68/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2939 (0.3059)	loss 0.9185 (1.3256)	grad_norm 46.7271 (29.7220)	mem 4879MB
[2022-05-31 08:00:27 MetaFG_0] (main.py 265): INFO Train: [68/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2927 (0.3059)	loss 1.3345 (1.3265)	grad_norm 46.8853 (29.7060)	mem 4879MB
[2022-05-31 08:00:30 MetaFG_0] (main.py 265): INFO Train: [68/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2924 (0.3059)	loss 1.4038 (1.3269)	grad_norm 21.3746 (29.6724)	mem 4879MB
[2022-05-31 08:00:33 MetaFG_0] (main.py 265): INFO Train: [68/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2952 (0.3059)	loss 1.3617 (1.3264)	grad_norm 14.9164 (29.6324)	mem 4879MB
[2022-05-31 08:00:36 MetaFG_0] (main.py 265): INFO Train: [68/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2918 (0.3059)	loss 0.8254 (1.3254)	grad_norm 26.4409 (29.6230)	mem 4879MB
[2022-05-31 08:00:39 MetaFG_0] (main.py 265): INFO Train: [68/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2928 (0.3059)	loss 1.6890 (1.3259)	grad_norm 34.8568 (29.6041)	mem 4879MB
[2022-05-31 08:00:42 MetaFG_0] (main.py 265): INFO Train: [68/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2988 (0.3059)	loss 1.4220 (1.3248)	grad_norm 30.5530 (29.6204)	mem 4879MB
[2022-05-31 08:00:45 MetaFG_0] (main.py 265): INFO Train: [68/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2991 (0.3059)	loss 1.5497 (1.3253)	grad_norm 23.8344 (29.5997)	mem 4879MB
[2022-05-31 08:00:48 MetaFG_0] (main.py 265): INFO Train: [68/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2930 (0.3058)	loss 1.4490 (1.3260)	grad_norm 26.2998 (29.6687)	mem 4879MB
[2022-05-31 08:00:51 MetaFG_0] (main.py 265): INFO Train: [68/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2935 (0.3058)	loss 0.7725 (1.3262)	grad_norm 18.7485 (29.6567)	mem 4879MB
[2022-05-31 08:00:54 MetaFG_0] (main.py 265): INFO Train: [68/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2924 (0.3058)	loss 1.6811 (1.3261)	grad_norm 30.5857 (29.6307)	mem 4879MB
[2022-05-31 08:00:57 MetaFG_0] (main.py 265): INFO Train: [68/300][1130/1562]	eta 0:02:12 lr 0.000005	time 0.2993 (0.3058)	loss 1.2669 (1.3255)	grad_norm 35.0241 (29.6829)	mem 4879MB
[2022-05-31 08:01:00 MetaFG_0] (main.py 265): INFO Train: [68/300][1140/1562]	eta 0:02:09 lr 0.000005	time 0.3017 (0.3058)	loss 1.5067 (1.3258)	grad_norm 34.3499 (29.6715)	mem 4879MB
[2022-05-31 08:01:03 MetaFG_0] (main.py 265): INFO Train: [68/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2928 (0.3058)	loss 1.1443 (1.3251)	grad_norm 20.8159 (29.6361)	mem 4879MB
[2022-05-31 08:01:07 MetaFG_0] (main.py 265): INFO Train: [68/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2992 (0.3058)	loss 1.3454 (1.3254)	grad_norm 41.2632 (29.6863)	mem 4879MB
[2022-05-31 08:01:10 MetaFG_0] (main.py 265): INFO Train: [68/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2942 (0.3058)	loss 1.5317 (1.3257)	grad_norm 34.1967 (29.6905)	mem 4879MB
[2022-05-31 08:01:13 MetaFG_0] (main.py 265): INFO Train: [68/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2934 (0.3058)	loss 1.1079 (1.3259)	grad_norm 27.2451 (29.6938)	mem 4879MB
[2022-05-31 08:01:16 MetaFG_0] (main.py 265): INFO Train: [68/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2954 (0.3058)	loss 1.4742 (1.3255)	grad_norm 31.8827 (29.6896)	mem 4879MB
[2022-05-31 08:01:19 MetaFG_0] (main.py 265): INFO Train: [68/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2989 (0.3058)	loss 1.2297 (1.3250)	grad_norm 29.7557 (29.6602)	mem 4879MB
[2022-05-31 08:01:22 MetaFG_0] (main.py 265): INFO Train: [68/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2982 (0.3058)	loss 1.0921 (1.3247)	grad_norm 23.3465 (29.6538)	mem 4879MB
[2022-05-31 08:01:25 MetaFG_0] (main.py 265): INFO Train: [68/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.3009 (0.3058)	loss 1.4094 (1.3249)	grad_norm 26.5003 (29.6536)	mem 4879MB
[2022-05-31 08:01:28 MetaFG_0] (main.py 265): INFO Train: [68/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2980 (0.3058)	loss 1.6522 (1.3254)	grad_norm 30.2241 (29.6498)	mem 4879MB
[2022-05-31 08:01:31 MetaFG_0] (main.py 265): INFO Train: [68/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.3005 (0.3058)	loss 1.5091 (1.3260)	grad_norm 39.7724 (29.6606)	mem 4879MB
[2022-05-31 08:01:34 MetaFG_0] (main.py 265): INFO Train: [68/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2992 (0.3058)	loss 1.3623 (1.3264)	grad_norm 15.9822 (29.6321)	mem 4879MB
[2022-05-31 08:01:37 MetaFG_0] (main.py 265): INFO Train: [68/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2924 (0.3058)	loss 1.7402 (1.3265)	grad_norm 24.6715 (29.6312)	mem 4879MB
[2022-05-31 08:01:40 MetaFG_0] (main.py 265): INFO Train: [68/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2919 (0.3058)	loss 0.9303 (1.3265)	grad_norm 45.4932 (29.6629)	mem 4879MB
[2022-05-31 08:01:43 MetaFG_0] (main.py 265): INFO Train: [68/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2935 (0.3058)	loss 1.0020 (1.3255)	grad_norm 21.2721 (29.6582)	mem 4879MB
[2022-05-31 08:01:46 MetaFG_0] (main.py 265): INFO Train: [68/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2919 (0.3058)	loss 1.5880 (1.3256)	grad_norm 24.2841 (29.6455)	mem 4879MB
[2022-05-31 08:01:49 MetaFG_0] (main.py 265): INFO Train: [68/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2993 (0.3057)	loss 1.7084 (1.3263)	grad_norm 36.4088 (29.6458)	mem 4879MB
[2022-05-31 08:01:52 MetaFG_0] (main.py 265): INFO Train: [68/300][1310/1562]	eta 0:01:17 lr 0.000005	time 0.3009 (0.3057)	loss 1.2882 (1.3266)	grad_norm 15.5969 (29.6300)	mem 4879MB
[2022-05-31 08:01:55 MetaFG_0] (main.py 265): INFO Train: [68/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2931 (0.3057)	loss 1.1427 (1.3264)	grad_norm 24.0614 (29.6025)	mem 4879MB
[2022-05-31 08:01:58 MetaFG_0] (main.py 265): INFO Train: [68/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2968 (0.3057)	loss 1.8038 (1.3269)	grad_norm 30.1817 (29.6251)	mem 4879MB
[2022-05-31 08:02:01 MetaFG_0] (main.py 265): INFO Train: [68/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2921 (0.3057)	loss 1.1779 (1.3274)	grad_norm 45.4786 (29.6494)	mem 4879MB
[2022-05-31 08:02:05 MetaFG_0] (main.py 265): INFO Train: [68/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2992 (0.3057)	loss 1.5316 (1.3265)	grad_norm 28.6395 (29.6194)	mem 4879MB
[2022-05-31 08:02:08 MetaFG_0] (main.py 265): INFO Train: [68/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2946 (0.3057)	loss 1.4470 (1.3269)	grad_norm 23.4601 (29.6469)	mem 4879MB
[2022-05-31 08:02:11 MetaFG_0] (main.py 265): INFO Train: [68/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2936 (0.3057)	loss 1.3809 (1.3272)	grad_norm 33.0182 (29.6362)	mem 4879MB
[2022-05-31 08:02:14 MetaFG_0] (main.py 265): INFO Train: [68/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.3012 (0.3057)	loss 1.2865 (1.3275)	grad_norm 38.3565 (29.6220)	mem 4879MB
[2022-05-31 08:02:17 MetaFG_0] (main.py 265): INFO Train: [68/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.3004 (0.3057)	loss 0.7786 (1.3274)	grad_norm 33.4470 (29.6204)	mem 4879MB
[2022-05-31 08:02:20 MetaFG_0] (main.py 265): INFO Train: [68/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2990 (0.3057)	loss 1.5826 (1.3281)	grad_norm 61.5979 (29.6336)	mem 4879MB
[2022-05-31 08:02:23 MetaFG_0] (main.py 265): INFO Train: [68/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.3000 (0.3057)	loss 1.6182 (1.3282)	grad_norm 32.4338 (29.6869)	mem 4879MB
[2022-05-31 08:02:26 MetaFG_0] (main.py 265): INFO Train: [68/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2945 (0.3057)	loss 1.0130 (1.3286)	grad_norm 39.2262 (29.6766)	mem 4879MB
[2022-05-31 08:02:29 MetaFG_0] (main.py 265): INFO Train: [68/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2921 (0.3057)	loss 1.2279 (1.3277)	grad_norm 23.6278 (29.6595)	mem 4879MB
[2022-05-31 08:02:32 MetaFG_0] (main.py 265): INFO Train: [68/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2938 (0.3057)	loss 1.4913 (1.3279)	grad_norm 18.1156 (29.6542)	mem 4879MB
[2022-05-31 08:02:35 MetaFG_0] (main.py 265): INFO Train: [68/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.3010 (0.3057)	loss 1.4499 (1.3281)	grad_norm 24.5916 (29.6536)	mem 4879MB
[2022-05-31 08:02:38 MetaFG_0] (main.py 265): INFO Train: [68/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2919 (0.3057)	loss 1.4378 (1.3284)	grad_norm 39.4170 (29.6546)	mem 4879MB
[2022-05-31 08:02:41 MetaFG_0] (main.py 265): INFO Train: [68/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2937 (0.3057)	loss 1.5769 (1.3286)	grad_norm 52.7478 (29.6549)	mem 4879MB
[2022-05-31 08:02:44 MetaFG_0] (main.py 265): INFO Train: [68/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2926 (0.3057)	loss 1.3324 (1.3290)	grad_norm 25.9132 (29.6621)	mem 4879MB
[2022-05-31 08:02:47 MetaFG_0] (main.py 265): INFO Train: [68/300][1490/1562]	eta 0:00:22 lr 0.000005	time 0.2925 (0.3057)	loss 1.4870 (1.3287)	grad_norm 25.9814 (29.6608)	mem 4879MB
[2022-05-31 08:02:50 MetaFG_0] (main.py 265): INFO Train: [68/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2932 (0.3057)	loss 1.4427 (1.3287)	grad_norm 43.5532 (29.6754)	mem 4879MB
[2022-05-31 08:02:53 MetaFG_0] (main.py 265): INFO Train: [68/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2947 (0.3057)	loss 1.2987 (1.3292)	grad_norm 27.8204 (29.6410)	mem 4879MB
[2022-05-31 08:02:56 MetaFG_0] (main.py 265): INFO Train: [68/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2939 (0.3056)	loss 0.9905 (1.3283)	grad_norm 17.0856 (29.6414)	mem 4879MB
[2022-05-31 08:02:59 MetaFG_0] (main.py 265): INFO Train: [68/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2952 (0.3056)	loss 1.3784 (1.3283)	grad_norm 25.6838 (29.6267)	mem 4879MB
[2022-05-31 08:03:02 MetaFG_0] (main.py 265): INFO Train: [68/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2948 (0.3056)	loss 1.6972 (1.3288)	grad_norm 25.5138 (29.6161)	mem 4879MB
[2022-05-31 08:03:06 MetaFG_0] (main.py 265): INFO Train: [68/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2998 (0.3056)	loss 1.6532 (1.3295)	grad_norm 32.4830 (29.6030)	mem 4879MB
[2022-05-31 08:03:09 MetaFG_0] (main.py 265): INFO Train: [68/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2924 (0.3056)	loss 1.1521 (1.3289)	grad_norm 29.5033 (29.5890)	mem 4879MB
[2022-05-31 08:03:09 MetaFG_0] (main.py 272): INFO EPOCH 68 training takes 0:07:57
[2022-05-31 08:03:09 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_68.pth saving......
[2022-05-31 08:03:10 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_68.pth saved !!!
[2022-05-31 08:03:10 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:03:11 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:03:11 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:03:12 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.725 (0.725)	Loss 0.5095 (0.5095)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 08:03:13 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.153)	Loss 0.4294 (0.5936)	Acc@1 93.750 (88.636)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 08:03:14 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.099 (0.125)	Loss 0.4429 (0.5720)	Acc@1 90.625 (89.286)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 08:03:15 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.116)	Loss 0.5081 (0.5590)	Acc@1 93.750 (88.911)	Acc@5 100.000 (98.790)	Mem 4879MB
[2022-05-31 08:03:16 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.110)	Loss 0.4116 (0.5731)	Acc@1 93.750 (88.872)	Acc@5 100.000 (98.704)	Mem 4879MB
[2022-05-31 08:03:17 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.107)	Loss 0.5826 (0.5794)	Acc@1 87.500 (88.480)	Acc@5 100.000 (98.713)	Mem 4879MB
[2022-05-31 08:03:18 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.090 (0.105)	Loss 0.4743 (0.5745)	Acc@1 90.625 (88.525)	Acc@5 100.000 (98.873)	Mem 4879MB
[2022-05-31 08:03:19 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.103 (0.104)	Loss 0.6222 (0.5865)	Acc@1 87.500 (88.204)	Acc@5 96.875 (98.900)	Mem 4879MB
[2022-05-31 08:03:20 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.087 (0.103)	Loss 0.5858 (0.5930)	Acc@1 84.375 (87.886)	Acc@5 96.875 (98.727)	Mem 4879MB
[2022-05-31 08:03:21 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.092 (0.102)	Loss 0.5844 (0.6017)	Acc@1 84.375 (87.603)	Acc@5 100.000 (98.626)	Mem 4879MB
[2022-05-31 08:03:22 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.092 (0.101)	Loss 0.5660 (0.6015)	Acc@1 87.500 (87.438)	Acc@5 100.000 (98.700)	Mem 4879MB
[2022-05-31 08:03:23 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.092 (0.101)	Loss 0.7654 (0.6055)	Acc@1 84.375 (87.331)	Acc@5 93.750 (98.564)	Mem 4879MB
[2022-05-31 08:03:24 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.096 (0.100)	Loss 0.5031 (0.6041)	Acc@1 84.375 (87.190)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 08:03:25 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.096 (0.100)	Loss 0.7821 (0.6025)	Acc@1 84.375 (87.214)	Acc@5 96.875 (98.640)	Mem 4879MB
[2022-05-31 08:03:25 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.099)	Loss 0.7608 (0.5992)	Acc@1 81.250 (87.301)	Acc@5 96.875 (98.604)	Mem 4879MB
[2022-05-31 08:03:26 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.099)	Loss 0.6779 (0.5973)	Acc@1 87.500 (87.252)	Acc@5 96.875 (98.655)	Mem 4879MB
[2022-05-31 08:03:27 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.104 (0.099)	Loss 0.4482 (0.5993)	Acc@1 87.500 (87.112)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 08:03:28 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.100 (0.099)	Loss 0.3453 (0.5996)	Acc@1 96.875 (87.098)	Acc@5 100.000 (98.648)	Mem 4879MB
[2022-05-31 08:03:29 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.102 (0.098)	Loss 0.6439 (0.5974)	Acc@1 81.250 (87.137)	Acc@5 96.875 (98.671)	Mem 4879MB
[2022-05-31 08:03:30 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.098)	Loss 0.4885 (0.6018)	Acc@1 93.750 (86.993)	Acc@5 100.000 (98.642)	Mem 4879MB
[2022-05-31 08:03:31 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.088 (0.098)	Loss 0.6166 (0.6040)	Acc@1 87.500 (86.956)	Acc@5 96.875 (98.647)	Mem 4879MB
[2022-05-31 08:03:32 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.098)	Loss 0.4405 (0.6016)	Acc@1 93.750 (87.026)	Acc@5 100.000 (98.667)	Mem 4879MB
[2022-05-31 08:03:33 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.097)	Loss 0.7187 (0.6005)	Acc@1 81.250 (87.076)	Acc@5 100.000 (98.671)	Mem 4879MB
[2022-05-31 08:03:34 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 0.7070 (0.6022)	Acc@1 84.375 (86.959)	Acc@5 93.750 (98.647)	Mem 4879MB
[2022-05-31 08:03:35 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.088 (0.097)	Loss 0.5895 (0.6000)	Acc@1 84.375 (87.007)	Acc@5 100.000 (98.651)	Mem 4879MB
[2022-05-31 08:03:36 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.090 (0.097)	Loss 0.7489 (0.6018)	Acc@1 84.375 (86.977)	Acc@5 96.875 (98.655)	Mem 4879MB
[2022-05-31 08:03:37 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 0.4578 (0.5995)	Acc@1 90.625 (87.021)	Acc@5 100.000 (98.647)	Mem 4879MB
[2022-05-31 08:03:38 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 0.7561 (0.5969)	Acc@1 81.250 (87.143)	Acc@5 100.000 (98.662)	Mem 4879MB
[2022-05-31 08:03:39 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.6087 (0.5946)	Acc@1 87.500 (87.222)	Acc@5 100.000 (98.677)	Mem 4879MB
[2022-05-31 08:03:40 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.6598 (0.5941)	Acc@1 90.625 (87.296)	Acc@5 100.000 (98.658)	Mem 4879MB
[2022-05-31 08:03:41 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.096)	Loss 0.3550 (0.5931)	Acc@1 93.750 (87.355)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 08:03:41 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.6993 (0.5922)	Acc@1 84.375 (87.389)	Acc@5 96.875 (98.674)	Mem 4879MB
[2022-05-31 08:03:42 MetaFG_0] (main.py 330): INFO  * Acc@1 87.380 Acc@5 98.670
[2022-05-31 08:03:42 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.4%
[2022-05-31 08:03:42 MetaFG_0] (main.py 171): INFO Max accuracy: 87.38%
[2022-05-31 08:03:43 MetaFG_0] (main.py 265): INFO Train: [69/300][0/1562]	eta 0:29:16 lr 0.000005	time 1.1244 (1.1244)	loss 1.4267 (1.4267)	grad_norm 33.4280 (33.4280)	mem 4879MB
[2022-05-31 08:03:46 MetaFG_0] (main.py 265): INFO Train: [69/300][10/1562]	eta 0:09:50 lr 0.000005	time 0.2995 (0.3803)	loss 1.5542 (1.4393)	grad_norm 21.8831 (27.4931)	mem 4879MB
[2022-05-31 08:03:49 MetaFG_0] (main.py 265): INFO Train: [69/300][20/1562]	eta 0:08:50 lr 0.000005	time 0.2927 (0.3442)	loss 1.5974 (1.3960)	grad_norm 28.3511 (27.7993)	mem 4879MB
[2022-05-31 08:03:52 MetaFG_0] (main.py 265): INFO Train: [69/300][30/1562]	eta 0:08:27 lr 0.000005	time 0.2933 (0.3313)	loss 1.6989 (1.3972)	grad_norm 24.6384 (28.1706)	mem 4879MB
[2022-05-31 08:03:55 MetaFG_0] (main.py 265): INFO Train: [69/300][40/1562]	eta 0:08:14 lr 0.000005	time 0.2980 (0.3247)	loss 1.7012 (1.3848)	grad_norm 45.5769 (28.1159)	mem 4879MB
[2022-05-31 08:03:58 MetaFG_0] (main.py 265): INFO Train: [69/300][50/1562]	eta 0:08:05 lr 0.000005	time 0.2939 (0.3212)	loss 1.5312 (1.3938)	grad_norm 20.6646 (27.5872)	mem 4879MB
[2022-05-31 08:04:01 MetaFG_0] (main.py 265): INFO Train: [69/300][60/1562]	eta 0:07:58 lr 0.000005	time 0.2921 (0.3183)	loss 0.8490 (1.3924)	grad_norm 26.4298 (27.4415)	mem 4879MB
[2022-05-31 08:04:04 MetaFG_0] (main.py 265): INFO Train: [69/300][70/1562]	eta 0:07:52 lr 0.000005	time 0.2927 (0.3164)	loss 1.5742 (1.4099)	grad_norm 19.7689 (27.2700)	mem 4879MB
[2022-05-31 08:04:07 MetaFG_0] (main.py 265): INFO Train: [69/300][80/1562]	eta 0:07:46 lr 0.000005	time 0.2939 (0.3150)	loss 1.3744 (1.4056)	grad_norm 25.2864 (27.5037)	mem 4879MB
[2022-05-31 08:04:10 MetaFG_0] (main.py 265): INFO Train: [69/300][90/1562]	eta 0:07:41 lr 0.000005	time 0.2977 (0.3138)	loss 1.3608 (1.4117)	grad_norm 18.1197 (27.0089)	mem 4879MB
[2022-05-31 08:04:13 MetaFG_0] (main.py 265): INFO Train: [69/300][100/1562]	eta 0:07:37 lr 0.000005	time 0.2937 (0.3129)	loss 0.8293 (1.4041)	grad_norm 16.4560 (27.1232)	mem 4879MB
[2022-05-31 08:04:16 MetaFG_0] (main.py 265): INFO Train: [69/300][110/1562]	eta 0:07:33 lr 0.000005	time 0.2983 (0.3122)	loss 1.1100 (1.4050)	grad_norm 20.3759 (26.9914)	mem 4879MB
[2022-05-31 08:04:19 MetaFG_0] (main.py 265): INFO Train: [69/300][120/1562]	eta 0:07:29 lr 0.000005	time 0.2993 (0.3115)	loss 0.8978 (1.3901)	grad_norm 57.4102 (27.1767)	mem 4879MB
[2022-05-31 08:04:22 MetaFG_0] (main.py 265): INFO Train: [69/300][130/1562]	eta 0:07:25 lr 0.000005	time 0.2930 (0.3110)	loss 1.4837 (1.3860)	grad_norm 23.5389 (27.1673)	mem 4879MB
[2022-05-31 08:04:25 MetaFG_0] (main.py 265): INFO Train: [69/300][140/1562]	eta 0:07:21 lr 0.000005	time 0.2981 (0.3105)	loss 1.4006 (1.3791)	grad_norm 38.1169 (27.1947)	mem 4879MB
[2022-05-31 08:04:29 MetaFG_0] (main.py 265): INFO Train: [69/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2924 (0.3102)	loss 1.2017 (1.3765)	grad_norm 30.0560 (27.3044)	mem 4879MB
[2022-05-31 08:04:32 MetaFG_0] (main.py 265): INFO Train: [69/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.3022 (0.3098)	loss 1.6738 (1.3780)	grad_norm 86.5451 (27.7256)	mem 4879MB
[2022-05-31 08:04:35 MetaFG_0] (main.py 265): INFO Train: [69/300][170/1562]	eta 0:07:10 lr 0.000005	time 0.3004 (0.3096)	loss 1.5776 (1.3689)	grad_norm 33.7367 (28.1949)	mem 4879MB
[2022-05-31 08:04:38 MetaFG_0] (main.py 265): INFO Train: [69/300][180/1562]	eta 0:07:07 lr 0.000005	time 0.2984 (0.3093)	loss 1.4375 (1.3661)	grad_norm 20.9654 (28.2624)	mem 4879MB
[2022-05-31 08:04:41 MetaFG_0] (main.py 265): INFO Train: [69/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2941 (0.3090)	loss 1.4520 (1.3635)	grad_norm 26.6327 (inf)	mem 4879MB
[2022-05-31 08:04:44 MetaFG_0] (main.py 265): INFO Train: [69/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2991 (0.3088)	loss 1.6230 (1.3646)	grad_norm 28.7353 (inf)	mem 4879MB
[2022-05-31 08:04:47 MetaFG_0] (main.py 265): INFO Train: [69/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.3040 (0.3086)	loss 1.3263 (1.3648)	grad_norm 20.7393 (inf)	mem 4879MB
[2022-05-31 08:04:50 MetaFG_0] (main.py 265): INFO Train: [69/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2960 (0.3083)	loss 1.2980 (1.3618)	grad_norm 16.0211 (inf)	mem 4879MB
[2022-05-31 08:04:53 MetaFG_0] (main.py 265): INFO Train: [69/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.3001 (0.3082)	loss 1.4364 (1.3568)	grad_norm 33.8715 (inf)	mem 4879MB
[2022-05-31 08:04:56 MetaFG_0] (main.py 265): INFO Train: [69/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2940 (0.3080)	loss 1.1947 (1.3557)	grad_norm 42.6619 (inf)	mem 4879MB
[2022-05-31 08:04:59 MetaFG_0] (main.py 265): INFO Train: [69/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2931 (0.3078)	loss 1.5613 (1.3552)	grad_norm 21.0449 (inf)	mem 4879MB
[2022-05-31 08:05:02 MetaFG_0] (main.py 265): INFO Train: [69/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2922 (0.3078)	loss 1.3300 (1.3544)	grad_norm 24.5090 (inf)	mem 4879MB
[2022-05-31 08:05:05 MetaFG_0] (main.py 265): INFO Train: [69/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2924 (0.3077)	loss 1.2387 (1.3501)	grad_norm 54.7258 (inf)	mem 4879MB
[2022-05-31 08:05:08 MetaFG_0] (main.py 265): INFO Train: [69/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2940 (0.3076)	loss 1.4490 (1.3495)	grad_norm 27.0895 (inf)	mem 4879MB
[2022-05-31 08:05:11 MetaFG_0] (main.py 265): INFO Train: [69/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2923 (0.3075)	loss 1.1821 (1.3484)	grad_norm 25.9873 (inf)	mem 4879MB
[2022-05-31 08:05:14 MetaFG_0] (main.py 265): INFO Train: [69/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2929 (0.3074)	loss 1.1211 (1.3458)	grad_norm 27.0615 (inf)	mem 4879MB
[2022-05-31 08:05:17 MetaFG_0] (main.py 265): INFO Train: [69/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2933 (0.3072)	loss 1.4979 (1.3448)	grad_norm 23.2473 (inf)	mem 4879MB
[2022-05-31 08:05:20 MetaFG_0] (main.py 265): INFO Train: [69/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.3037 (0.3072)	loss 1.4758 (1.3460)	grad_norm 24.0626 (inf)	mem 4879MB
[2022-05-31 08:05:23 MetaFG_0] (main.py 265): INFO Train: [69/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2943 (0.3071)	loss 1.5978 (1.3467)	grad_norm 20.4251 (inf)	mem 4879MB
[2022-05-31 08:05:26 MetaFG_0] (main.py 265): INFO Train: [69/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2989 (0.3070)	loss 1.0589 (1.3456)	grad_norm 75.5576 (inf)	mem 4879MB
[2022-05-31 08:05:29 MetaFG_0] (main.py 265): INFO Train: [69/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2954 (0.3069)	loss 1.6044 (1.3443)	grad_norm 31.1594 (inf)	mem 4879MB
[2022-05-31 08:05:32 MetaFG_0] (main.py 265): INFO Train: [69/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2927 (0.3069)	loss 1.2916 (1.3419)	grad_norm 14.2796 (inf)	mem 4879MB
[2022-05-31 08:05:36 MetaFG_0] (main.py 265): INFO Train: [69/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2925 (0.3069)	loss 1.4210 (1.3397)	grad_norm 29.0999 (inf)	mem 4879MB
[2022-05-31 08:05:39 MetaFG_0] (main.py 265): INFO Train: [69/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2936 (0.3068)	loss 1.4280 (1.3417)	grad_norm 40.5634 (inf)	mem 4879MB
[2022-05-31 08:05:42 MetaFG_0] (main.py 265): INFO Train: [69/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2993 (0.3067)	loss 1.4032 (1.3401)	grad_norm 34.7736 (inf)	mem 4879MB
[2022-05-31 08:05:45 MetaFG_0] (main.py 265): INFO Train: [69/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2988 (0.3067)	loss 1.3523 (1.3375)	grad_norm 21.9194 (inf)	mem 4879MB
[2022-05-31 08:05:48 MetaFG_0] (main.py 265): INFO Train: [69/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2940 (0.3066)	loss 1.7290 (1.3362)	grad_norm 48.1421 (inf)	mem 4879MB
[2022-05-31 08:05:51 MetaFG_0] (main.py 265): INFO Train: [69/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2991 (0.3065)	loss 0.7725 (1.3372)	grad_norm 28.8452 (inf)	mem 4879MB
[2022-05-31 08:05:54 MetaFG_0] (main.py 265): INFO Train: [69/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2928 (0.3064)	loss 1.4543 (1.3383)	grad_norm 23.9556 (inf)	mem 4879MB
[2022-05-31 08:05:57 MetaFG_0] (main.py 265): INFO Train: [69/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2945 (0.3064)	loss 1.4638 (1.3391)	grad_norm 19.3222 (inf)	mem 4879MB
[2022-05-31 08:06:00 MetaFG_0] (main.py 265): INFO Train: [69/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2984 (0.3064)	loss 1.5613 (1.3418)	grad_norm 18.0452 (inf)	mem 4879MB
[2022-05-31 08:06:03 MetaFG_0] (main.py 265): INFO Train: [69/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2982 (0.3063)	loss 1.0082 (1.3405)	grad_norm 26.7928 (inf)	mem 4879MB
[2022-05-31 08:06:06 MetaFG_0] (main.py 265): INFO Train: [69/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2998 (0.3063)	loss 1.5248 (1.3391)	grad_norm 32.8974 (inf)	mem 4879MB
[2022-05-31 08:06:09 MetaFG_0] (main.py 265): INFO Train: [69/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.3006 (0.3062)	loss 1.4584 (1.3385)	grad_norm 23.5709 (inf)	mem 4879MB
[2022-05-31 08:06:12 MetaFG_0] (main.py 265): INFO Train: [69/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2954 (0.3062)	loss 1.1601 (1.3385)	grad_norm 40.8340 (inf)	mem 4879MB
[2022-05-31 08:06:15 MetaFG_0] (main.py 265): INFO Train: [69/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2914 (0.3062)	loss 1.1089 (1.3378)	grad_norm 37.9678 (inf)	mem 4879MB
[2022-05-31 08:06:18 MetaFG_0] (main.py 265): INFO Train: [69/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2925 (0.3061)	loss 1.2143 (1.3369)	grad_norm 30.7554 (inf)	mem 4879MB
[2022-05-31 08:06:21 MetaFG_0] (main.py 265): INFO Train: [69/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.2933 (0.3061)	loss 1.2369 (1.3364)	grad_norm 37.7751 (inf)	mem 4879MB
[2022-05-31 08:06:24 MetaFG_0] (main.py 265): INFO Train: [69/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2986 (0.3061)	loss 1.3145 (1.3382)	grad_norm 39.6280 (inf)	mem 4879MB
[2022-05-31 08:06:27 MetaFG_0] (main.py 265): INFO Train: [69/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2927 (0.3061)	loss 1.5553 (1.3395)	grad_norm 45.7525 (inf)	mem 4879MB
[2022-05-31 08:06:30 MetaFG_0] (main.py 265): INFO Train: [69/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2936 (0.3060)	loss 1.4984 (1.3393)	grad_norm 29.2003 (inf)	mem 4879MB
[2022-05-31 08:06:33 MetaFG_0] (main.py 265): INFO Train: [69/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.3383 (0.3061)	loss 1.4638 (1.3393)	grad_norm 25.4423 (inf)	mem 4879MB
[2022-05-31 08:06:37 MetaFG_0] (main.py 265): INFO Train: [69/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2943 (0.3063)	loss 1.1761 (1.3369)	grad_norm 31.5140 (inf)	mem 4879MB
[2022-05-31 08:06:40 MetaFG_0] (main.py 265): INFO Train: [69/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2927 (0.3063)	loss 0.9244 (1.3381)	grad_norm 21.2748 (inf)	mem 4879MB
[2022-05-31 08:06:43 MetaFG_0] (main.py 265): INFO Train: [69/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2923 (0.3063)	loss 1.7533 (1.3371)	grad_norm 27.4887 (inf)	mem 4879MB
[2022-05-31 08:06:46 MetaFG_0] (main.py 265): INFO Train: [69/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2937 (0.3063)	loss 1.2449 (1.3378)	grad_norm 25.3171 (inf)	mem 4879MB
[2022-05-31 08:06:49 MetaFG_0] (main.py 265): INFO Train: [69/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2942 (0.3063)	loss 1.5430 (1.3390)	grad_norm 18.4871 (inf)	mem 4879MB
[2022-05-31 08:06:52 MetaFG_0] (main.py 265): INFO Train: [69/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2997 (0.3063)	loss 1.4249 (1.3402)	grad_norm 39.4502 (inf)	mem 4879MB
[2022-05-31 08:06:55 MetaFG_0] (main.py 265): INFO Train: [69/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2923 (0.3063)	loss 1.1682 (1.3417)	grad_norm 24.8919 (inf)	mem 4879MB
[2022-05-31 08:06:58 MetaFG_0] (main.py 265): INFO Train: [69/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2941 (0.3062)	loss 1.4516 (1.3407)	grad_norm 27.3393 (inf)	mem 4879MB
[2022-05-31 08:07:01 MetaFG_0] (main.py 265): INFO Train: [69/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.2922 (0.3062)	loss 1.4607 (1.3402)	grad_norm 19.0304 (inf)	mem 4879MB
[2022-05-31 08:07:04 MetaFG_0] (main.py 265): INFO Train: [69/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2989 (0.3062)	loss 1.4415 (1.3405)	grad_norm 38.2211 (inf)	mem 4879MB
[2022-05-31 08:07:07 MetaFG_0] (main.py 265): INFO Train: [69/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2924 (0.3061)	loss 0.9687 (1.3404)	grad_norm 22.0232 (inf)	mem 4879MB
[2022-05-31 08:07:10 MetaFG_0] (main.py 265): INFO Train: [69/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2937 (0.3061)	loss 1.5482 (1.3408)	grad_norm 30.3967 (inf)	mem 4879MB
[2022-05-31 08:07:13 MetaFG_0] (main.py 265): INFO Train: [69/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2926 (0.3061)	loss 1.0699 (1.3404)	grad_norm 29.2177 (inf)	mem 4879MB
[2022-05-31 08:07:16 MetaFG_0] (main.py 265): INFO Train: [69/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.3001 (0.3061)	loss 1.2911 (1.3389)	grad_norm 28.1356 (inf)	mem 4879MB
[2022-05-31 08:07:19 MetaFG_0] (main.py 265): INFO Train: [69/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2923 (0.3061)	loss 0.8249 (1.3384)	grad_norm 23.8980 (inf)	mem 4879MB
[2022-05-31 08:07:22 MetaFG_0] (main.py 265): INFO Train: [69/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2990 (0.3061)	loss 1.3762 (1.3396)	grad_norm 30.3675 (inf)	mem 4879MB
[2022-05-31 08:07:25 MetaFG_0] (main.py 265): INFO Train: [69/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.3003 (0.3061)	loss 1.5267 (1.3398)	grad_norm 19.8104 (inf)	mem 4879MB
[2022-05-31 08:07:28 MetaFG_0] (main.py 265): INFO Train: [69/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2946 (0.3061)	loss 1.2026 (1.3396)	grad_norm 27.3301 (inf)	mem 4879MB
[2022-05-31 08:07:32 MetaFG_0] (main.py 265): INFO Train: [69/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2948 (0.3061)	loss 1.4720 (1.3393)	grad_norm 25.2265 (inf)	mem 4879MB
[2022-05-31 08:07:35 MetaFG_0] (main.py 265): INFO Train: [69/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2940 (0.3061)	loss 1.2936 (1.3378)	grad_norm 25.7139 (inf)	mem 4879MB
[2022-05-31 08:07:38 MetaFG_0] (main.py 265): INFO Train: [69/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2942 (0.3061)	loss 0.7576 (1.3381)	grad_norm 58.2001 (inf)	mem 4879MB
[2022-05-31 08:07:41 MetaFG_0] (main.py 265): INFO Train: [69/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.3008 (0.3061)	loss 1.2405 (1.3382)	grad_norm 40.0882 (inf)	mem 4879MB
[2022-05-31 08:07:44 MetaFG_0] (main.py 265): INFO Train: [69/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.3020 (0.3060)	loss 1.0263 (1.3370)	grad_norm 28.8488 (inf)	mem 4879MB
[2022-05-31 08:07:47 MetaFG_0] (main.py 265): INFO Train: [69/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2982 (0.3060)	loss 0.8050 (1.3345)	grad_norm 28.4352 (inf)	mem 4879MB
[2022-05-31 08:07:50 MetaFG_0] (main.py 265): INFO Train: [69/300][810/1562]	eta 0:03:50 lr 0.000005	time 0.2978 (0.3060)	loss 1.3846 (1.3333)	grad_norm 24.7217 (inf)	mem 4879MB
[2022-05-31 08:07:53 MetaFG_0] (main.py 265): INFO Train: [69/300][820/1562]	eta 0:03:47 lr 0.000005	time 0.3001 (0.3060)	loss 1.2010 (1.3322)	grad_norm 38.1367 (inf)	mem 4879MB
[2022-05-31 08:07:56 MetaFG_0] (main.py 265): INFO Train: [69/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2989 (0.3060)	loss 1.6181 (1.3322)	grad_norm 28.0162 (inf)	mem 4879MB
[2022-05-31 08:07:59 MetaFG_0] (main.py 265): INFO Train: [69/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2941 (0.3060)	loss 1.0368 (1.3315)	grad_norm 29.5827 (inf)	mem 4879MB
[2022-05-31 08:08:02 MetaFG_0] (main.py 265): INFO Train: [69/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2941 (0.3060)	loss 1.3145 (1.3326)	grad_norm 21.4354 (inf)	mem 4879MB
[2022-05-31 08:08:05 MetaFG_0] (main.py 265): INFO Train: [69/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2977 (0.3060)	loss 1.2010 (1.3328)	grad_norm 35.0345 (inf)	mem 4879MB
[2022-05-31 08:08:08 MetaFG_0] (main.py 265): INFO Train: [69/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2982 (0.3060)	loss 1.4901 (1.3342)	grad_norm 35.9092 (inf)	mem 4879MB
[2022-05-31 08:08:11 MetaFG_0] (main.py 265): INFO Train: [69/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2929 (0.3060)	loss 1.1946 (1.3332)	grad_norm 40.1239 (inf)	mem 4879MB
[2022-05-31 08:08:14 MetaFG_0] (main.py 265): INFO Train: [69/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2921 (0.3060)	loss 1.4461 (1.3334)	grad_norm 26.9340 (inf)	mem 4879MB
[2022-05-31 08:08:17 MetaFG_0] (main.py 265): INFO Train: [69/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2932 (0.3060)	loss 0.9648 (1.3326)	grad_norm 38.8016 (inf)	mem 4879MB
[2022-05-31 08:08:20 MetaFG_0] (main.py 265): INFO Train: [69/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2929 (0.3060)	loss 1.2589 (1.3339)	grad_norm 20.2775 (inf)	mem 4879MB
[2022-05-31 08:08:23 MetaFG_0] (main.py 265): INFO Train: [69/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.3007 (0.3060)	loss 1.3600 (1.3333)	grad_norm 27.3178 (inf)	mem 4879MB
[2022-05-31 08:08:27 MetaFG_0] (main.py 265): INFO Train: [69/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2988 (0.3060)	loss 1.4144 (1.3332)	grad_norm 29.8301 (inf)	mem 4879MB
[2022-05-31 08:08:30 MetaFG_0] (main.py 265): INFO Train: [69/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.2927 (0.3059)	loss 1.1804 (1.3343)	grad_norm 33.7402 (inf)	mem 4879MB
[2022-05-31 08:08:33 MetaFG_0] (main.py 265): INFO Train: [69/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2936 (0.3059)	loss 1.3967 (1.3340)	grad_norm 17.6591 (inf)	mem 4879MB
[2022-05-31 08:08:36 MetaFG_0] (main.py 265): INFO Train: [69/300][960/1562]	eta 0:03:04 lr 0.000005	time 0.2964 (0.3059)	loss 1.2486 (1.3347)	grad_norm 23.5381 (inf)	mem 4879MB
[2022-05-31 08:08:39 MetaFG_0] (main.py 265): INFO Train: [69/300][970/1562]	eta 0:03:01 lr 0.000005	time 0.2928 (0.3059)	loss 1.0051 (1.3335)	grad_norm 60.2614 (inf)	mem 4879MB
[2022-05-31 08:08:42 MetaFG_0] (main.py 265): INFO Train: [69/300][980/1562]	eta 0:02:58 lr 0.000005	time 0.2993 (0.3059)	loss 1.4819 (1.3321)	grad_norm 24.7868 (inf)	mem 4879MB
[2022-05-31 08:08:45 MetaFG_0] (main.py 265): INFO Train: [69/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2925 (0.3059)	loss 1.3158 (1.3305)	grad_norm 15.9802 (inf)	mem 4879MB
[2022-05-31 08:08:48 MetaFG_0] (main.py 265): INFO Train: [69/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2999 (0.3059)	loss 1.1842 (1.3308)	grad_norm 29.2623 (inf)	mem 4879MB
[2022-05-31 08:08:51 MetaFG_0] (main.py 265): INFO Train: [69/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2985 (0.3059)	loss 1.3098 (1.3300)	grad_norm 39.1950 (inf)	mem 4879MB
[2022-05-31 08:08:54 MetaFG_0] (main.py 265): INFO Train: [69/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.3001 (0.3059)	loss 1.4325 (1.3301)	grad_norm 21.0373 (inf)	mem 4879MB
[2022-05-31 08:08:57 MetaFG_0] (main.py 265): INFO Train: [69/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.3000 (0.3058)	loss 1.4966 (1.3305)	grad_norm 17.6600 (inf)	mem 4879MB
[2022-05-31 08:09:00 MetaFG_0] (main.py 265): INFO Train: [69/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2933 (0.3058)	loss 1.2975 (1.3313)	grad_norm 30.9150 (inf)	mem 4879MB
[2022-05-31 08:09:03 MetaFG_0] (main.py 265): INFO Train: [69/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2929 (0.3058)	loss 0.8689 (1.3309)	grad_norm 36.6170 (inf)	mem 4879MB
[2022-05-31 08:09:06 MetaFG_0] (main.py 265): INFO Train: [69/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2941 (0.3058)	loss 1.4235 (1.3311)	grad_norm 35.0581 (inf)	mem 4879MB
[2022-05-31 08:09:09 MetaFG_0] (main.py 265): INFO Train: [69/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2919 (0.3058)	loss 1.5051 (1.3309)	grad_norm 25.9220 (inf)	mem 4879MB
[2022-05-31 08:09:12 MetaFG_0] (main.py 265): INFO Train: [69/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2996 (0.3058)	loss 1.3611 (1.3303)	grad_norm 31.2404 (inf)	mem 4879MB
[2022-05-31 08:09:15 MetaFG_0] (main.py 265): INFO Train: [69/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.3026 (0.3058)	loss 1.6938 (1.3304)	grad_norm 43.3205 (inf)	mem 4879MB
[2022-05-31 08:09:18 MetaFG_0] (main.py 265): INFO Train: [69/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2996 (0.3058)	loss 1.5091 (1.3305)	grad_norm 22.3132 (inf)	mem 4879MB
[2022-05-31 08:09:21 MetaFG_0] (main.py 265): INFO Train: [69/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2999 (0.3058)	loss 1.0117 (1.3302)	grad_norm 29.5723 (inf)	mem 4879MB
[2022-05-31 08:09:24 MetaFG_0] (main.py 265): INFO Train: [69/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2931 (0.3058)	loss 1.3265 (1.3304)	grad_norm 82.4241 (inf)	mem 4879MB
[2022-05-31 08:09:28 MetaFG_0] (main.py 265): INFO Train: [69/300][1130/1562]	eta 0:02:12 lr 0.000005	time 0.2936 (0.3058)	loss 1.2219 (1.3311)	grad_norm 27.7642 (inf)	mem 4879MB
[2022-05-31 08:09:31 MetaFG_0] (main.py 265): INFO Train: [69/300][1140/1562]	eta 0:02:09 lr 0.000005	time 0.2950 (0.3058)	loss 0.8543 (1.3316)	grad_norm 44.0875 (inf)	mem 4879MB
[2022-05-31 08:09:34 MetaFG_0] (main.py 265): INFO Train: [69/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2944 (0.3058)	loss 1.4135 (1.3314)	grad_norm 30.9002 (inf)	mem 4879MB
[2022-05-31 08:09:37 MetaFG_0] (main.py 265): INFO Train: [69/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2927 (0.3058)	loss 0.8952 (1.3318)	grad_norm 23.6978 (inf)	mem 4879MB
[2022-05-31 08:09:40 MetaFG_0] (main.py 265): INFO Train: [69/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2952 (0.3058)	loss 1.6273 (1.3315)	grad_norm 12.8254 (inf)	mem 4879MB
[2022-05-31 08:09:43 MetaFG_0] (main.py 265): INFO Train: [69/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2989 (0.3058)	loss 1.6109 (1.3298)	grad_norm 24.0981 (inf)	mem 4879MB
[2022-05-31 08:09:46 MetaFG_0] (main.py 265): INFO Train: [69/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.3033 (0.3058)	loss 1.3915 (1.3294)	grad_norm 29.6806 (inf)	mem 4879MB
[2022-05-31 08:09:49 MetaFG_0] (main.py 265): INFO Train: [69/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2990 (0.3057)	loss 0.9408 (1.3284)	grad_norm 48.4227 (inf)	mem 4879MB
[2022-05-31 08:09:52 MetaFG_0] (main.py 265): INFO Train: [69/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2919 (0.3057)	loss 1.4194 (1.3278)	grad_norm 29.1303 (inf)	mem 4879MB
[2022-05-31 08:09:55 MetaFG_0] (main.py 265): INFO Train: [69/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2965 (0.3057)	loss 1.5836 (1.3283)	grad_norm 29.6614 (inf)	mem 4879MB
[2022-05-31 08:09:58 MetaFG_0] (main.py 265): INFO Train: [69/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.3010 (0.3057)	loss 0.8283 (1.3279)	grad_norm 45.4150 (inf)	mem 4879MB
[2022-05-31 08:10:01 MetaFG_0] (main.py 265): INFO Train: [69/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2929 (0.3057)	loss 1.1453 (1.3271)	grad_norm 46.9151 (inf)	mem 4879MB
[2022-05-31 08:10:04 MetaFG_0] (main.py 265): INFO Train: [69/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2928 (0.3057)	loss 1.1961 (1.3263)	grad_norm 20.7294 (inf)	mem 4879MB
[2022-05-31 08:10:07 MetaFG_0] (main.py 265): INFO Train: [69/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2933 (0.3057)	loss 1.4833 (1.3268)	grad_norm 48.9102 (inf)	mem 4879MB
[2022-05-31 08:10:10 MetaFG_0] (main.py 265): INFO Train: [69/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2928 (0.3057)	loss 1.2282 (1.3264)	grad_norm 32.1744 (inf)	mem 4879MB
[2022-05-31 08:10:13 MetaFG_0] (main.py 265): INFO Train: [69/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2927 (0.3057)	loss 1.3679 (1.3271)	grad_norm 21.7201 (inf)	mem 4879MB
[2022-05-31 08:10:16 MetaFG_0] (main.py 265): INFO Train: [69/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2996 (0.3056)	loss 1.4583 (1.3277)	grad_norm 29.2463 (inf)	mem 4879MB
[2022-05-31 08:10:19 MetaFG_0] (main.py 265): INFO Train: [69/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2932 (0.3056)	loss 1.1994 (1.3274)	grad_norm 18.9920 (inf)	mem 4879MB
[2022-05-31 08:10:22 MetaFG_0] (main.py 265): INFO Train: [69/300][1310/1562]	eta 0:01:17 lr 0.000005	time 0.2953 (0.3056)	loss 1.3927 (1.3276)	grad_norm 20.0143 (inf)	mem 4879MB
[2022-05-31 08:10:25 MetaFG_0] (main.py 265): INFO Train: [69/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2929 (0.3056)	loss 0.9931 (1.3261)	grad_norm 29.3890 (inf)	mem 4879MB
[2022-05-31 08:10:28 MetaFG_0] (main.py 265): INFO Train: [69/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2988 (0.3056)	loss 1.3125 (1.3266)	grad_norm 26.0139 (inf)	mem 4879MB
[2022-05-31 08:10:31 MetaFG_0] (main.py 265): INFO Train: [69/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2943 (0.3056)	loss 1.2936 (1.3270)	grad_norm 23.9152 (inf)	mem 4879MB
[2022-05-31 08:10:35 MetaFG_0] (main.py 265): INFO Train: [69/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2988 (0.3056)	loss 1.2455 (1.3278)	grad_norm 21.7555 (inf)	mem 4879MB
[2022-05-31 08:10:38 MetaFG_0] (main.py 265): INFO Train: [69/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2984 (0.3056)	loss 1.3281 (1.3273)	grad_norm 34.1511 (inf)	mem 4879MB
[2022-05-31 08:10:41 MetaFG_0] (main.py 265): INFO Train: [69/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2936 (0.3056)	loss 1.2846 (1.3267)	grad_norm 48.4955 (inf)	mem 4879MB
[2022-05-31 08:10:44 MetaFG_0] (main.py 265): INFO Train: [69/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2920 (0.3056)	loss 1.3996 (1.3271)	grad_norm 15.2953 (inf)	mem 4879MB
[2022-05-31 08:10:47 MetaFG_0] (main.py 265): INFO Train: [69/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2921 (0.3056)	loss 0.9775 (1.3266)	grad_norm 24.0256 (inf)	mem 4879MB
[2022-05-31 08:10:50 MetaFG_0] (main.py 265): INFO Train: [69/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2986 (0.3056)	loss 1.3296 (1.3272)	grad_norm 24.4098 (inf)	mem 4879MB
[2022-05-31 08:10:53 MetaFG_0] (main.py 265): INFO Train: [69/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2937 (0.3056)	loss 1.6615 (1.3272)	grad_norm 27.8977 (inf)	mem 4879MB
[2022-05-31 08:10:56 MetaFG_0] (main.py 265): INFO Train: [69/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2983 (0.3056)	loss 1.3084 (1.3272)	grad_norm 25.9685 (inf)	mem 4879MB
[2022-05-31 08:10:59 MetaFG_0] (main.py 265): INFO Train: [69/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2931 (0.3055)	loss 1.3191 (1.3276)	grad_norm 19.2834 (inf)	mem 4879MB
[2022-05-31 08:11:02 MetaFG_0] (main.py 265): INFO Train: [69/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2919 (0.3055)	loss 1.1998 (1.3281)	grad_norm 23.9399 (inf)	mem 4879MB
[2022-05-31 08:11:05 MetaFG_0] (main.py 265): INFO Train: [69/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2926 (0.3055)	loss 1.6754 (1.3284)	grad_norm 23.2718 (inf)	mem 4879MB
[2022-05-31 08:11:08 MetaFG_0] (main.py 265): INFO Train: [69/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2920 (0.3055)	loss 1.4403 (1.3273)	grad_norm 32.9962 (inf)	mem 4879MB
[2022-05-31 08:11:11 MetaFG_0] (main.py 265): INFO Train: [69/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2926 (0.3055)	loss 1.3784 (1.3270)	grad_norm 21.6992 (inf)	mem 4879MB
[2022-05-31 08:11:14 MetaFG_0] (main.py 265): INFO Train: [69/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2923 (0.3055)	loss 0.8405 (1.3264)	grad_norm 40.4101 (inf)	mem 4879MB
[2022-05-31 08:11:17 MetaFG_0] (main.py 265): INFO Train: [69/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2950 (0.3055)	loss 1.3905 (1.3267)	grad_norm 24.0568 (inf)	mem 4879MB
[2022-05-31 08:11:20 MetaFG_0] (main.py 265): INFO Train: [69/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2939 (0.3055)	loss 1.5891 (1.3259)	grad_norm 15.2260 (inf)	mem 4879MB
[2022-05-31 08:11:23 MetaFG_0] (main.py 265): INFO Train: [69/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2928 (0.3055)	loss 0.8229 (1.3258)	grad_norm 30.4379 (inf)	mem 4879MB
[2022-05-31 08:11:26 MetaFG_0] (main.py 265): INFO Train: [69/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2928 (0.3055)	loss 1.4253 (1.3257)	grad_norm 18.1461 (inf)	mem 4879MB
[2022-05-31 08:11:29 MetaFG_0] (main.py 265): INFO Train: [69/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2986 (0.3055)	loss 1.5444 (1.3263)	grad_norm 26.4112 (inf)	mem 4879MB
[2022-05-31 08:11:32 MetaFG_0] (main.py 265): INFO Train: [69/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2922 (0.3055)	loss 1.3642 (1.3271)	grad_norm 17.4307 (inf)	mem 4879MB
[2022-05-31 08:11:35 MetaFG_0] (main.py 265): INFO Train: [69/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2987 (0.3055)	loss 1.3315 (1.3269)	grad_norm 36.8898 (inf)	mem 4879MB
[2022-05-31 08:11:38 MetaFG_0] (main.py 265): INFO Train: [69/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2927 (0.3054)	loss 1.0942 (1.3268)	grad_norm 47.3243 (inf)	mem 4879MB
[2022-05-31 08:11:39 MetaFG_0] (main.py 272): INFO EPOCH 69 training takes 0:07:57
[2022-05-31 08:11:39 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_69.pth saving......
[2022-05-31 08:11:40 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_69.pth saved !!!
[2022-05-31 08:11:40 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:11:41 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:11:41 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:11:42 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.695 (0.695)	Loss 0.6523 (0.6523)	Acc@1 90.625 (90.625)	Acc@5 96.875 (96.875)	Mem 4879MB
[2022-05-31 08:11:43 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.095 (0.154)	Loss 0.5926 (0.5790)	Acc@1 84.375 (89.205)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 08:11:44 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.126)	Loss 0.5813 (0.5668)	Acc@1 78.125 (88.988)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 08:11:45 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.116)	Loss 0.4196 (0.5677)	Acc@1 96.875 (89.214)	Acc@5 100.000 (98.891)	Mem 4879MB
[2022-05-31 08:11:46 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.110)	Loss 0.6386 (0.5841)	Acc@1 87.500 (88.415)	Acc@5 100.000 (99.009)	Mem 4879MB
[2022-05-31 08:11:47 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.107)	Loss 0.4355 (0.5818)	Acc@1 93.750 (88.603)	Acc@5 100.000 (98.897)	Mem 4879MB
[2022-05-31 08:11:48 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.096 (0.106)	Loss 0.6619 (0.5926)	Acc@1 84.375 (88.166)	Acc@5 96.875 (98.770)	Mem 4879MB
[2022-05-31 08:11:49 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.104)	Loss 0.6063 (0.5885)	Acc@1 87.500 (88.380)	Acc@5 100.000 (98.768)	Mem 4879MB
[2022-05-31 08:11:49 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.103)	Loss 0.7112 (0.5844)	Acc@1 81.250 (88.387)	Acc@5 96.875 (98.804)	Mem 4879MB
[2022-05-31 08:11:50 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.102)	Loss 0.4740 (0.5902)	Acc@1 87.500 (88.049)	Acc@5 100.000 (98.764)	Mem 4879MB
[2022-05-31 08:11:51 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.101)	Loss 0.5566 (0.5979)	Acc@1 90.625 (87.748)	Acc@5 96.875 (98.700)	Mem 4879MB
[2022-05-31 08:11:52 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 0.7812 (0.6089)	Acc@1 81.250 (87.331)	Acc@5 100.000 (98.620)	Mem 4879MB
[2022-05-31 08:11:53 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.097 (0.100)	Loss 0.5715 (0.6080)	Acc@1 87.500 (87.371)	Acc@5 100.000 (98.657)	Mem 4879MB
[2022-05-31 08:11:54 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.093 (0.099)	Loss 0.6433 (0.6085)	Acc@1 87.500 (87.381)	Acc@5 96.875 (98.569)	Mem 4879MB
[2022-05-31 08:11:55 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.099)	Loss 0.7983 (0.6082)	Acc@1 78.125 (87.323)	Acc@5 100.000 (98.582)	Mem 4879MB
[2022-05-31 08:11:56 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.098)	Loss 0.6264 (0.6060)	Acc@1 87.500 (87.293)	Acc@5 100.000 (98.634)	Mem 4879MB
[2022-05-31 08:11:57 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.092 (0.098)	Loss 0.8716 (0.6067)	Acc@1 75.000 (87.209)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 08:11:58 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.4663 (0.6063)	Acc@1 93.750 (87.299)	Acc@5 100.000 (98.684)	Mem 4879MB
[2022-05-31 08:11:59 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.098 (0.097)	Loss 0.5132 (0.6029)	Acc@1 90.625 (87.414)	Acc@5 100.000 (98.722)	Mem 4879MB
[2022-05-31 08:12:00 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.097)	Loss 0.5814 (0.6005)	Acc@1 84.375 (87.402)	Acc@5 100.000 (98.789)	Mem 4879MB
[2022-05-31 08:12:01 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.5429 (0.5996)	Acc@1 87.500 (87.484)	Acc@5 100.000 (98.803)	Mem 4879MB
[2022-05-31 08:12:02 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.094 (0.097)	Loss 0.7423 (0.5998)	Acc@1 84.375 (87.530)	Acc@5 96.875 (98.786)	Mem 4879MB
[2022-05-31 08:12:03 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.097 (0.097)	Loss 0.8110 (0.6052)	Acc@1 75.000 (87.373)	Acc@5 100.000 (98.727)	Mem 4879MB
[2022-05-31 08:12:03 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.097)	Loss 0.4362 (0.6036)	Acc@1 93.750 (87.432)	Acc@5 100.000 (98.742)	Mem 4879MB
[2022-05-31 08:12:04 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.6753 (0.6044)	Acc@1 84.375 (87.331)	Acc@5 100.000 (98.742)	Mem 4879MB
[2022-05-31 08:12:05 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.102 (0.097)	Loss 0.5444 (0.6050)	Acc@1 90.625 (87.288)	Acc@5 96.875 (98.755)	Mem 4879MB
[2022-05-31 08:12:06 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.097)	Loss 0.8025 (0.6064)	Acc@1 81.250 (87.273)	Acc@5 96.875 (98.719)	Mem 4879MB
[2022-05-31 08:12:07 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.097)	Loss 0.6367 (0.6067)	Acc@1 87.500 (87.315)	Acc@5 96.875 (98.732)	Mem 4879MB
[2022-05-31 08:12:08 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 0.8573 (0.6069)	Acc@1 84.375 (87.322)	Acc@5 93.750 (98.710)	Mem 4879MB
[2022-05-31 08:12:09 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.087 (0.096)	Loss 0.7394 (0.6100)	Acc@1 87.500 (87.221)	Acc@5 100.000 (98.711)	Mem 4879MB
[2022-05-31 08:12:10 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.101 (0.096)	Loss 0.7037 (0.6115)	Acc@1 84.375 (87.147)	Acc@5 96.875 (98.713)	Mem 4879MB
[2022-05-31 08:12:11 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5690 (0.6083)	Acc@1 81.250 (87.219)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 08:12:11 MetaFG_0] (main.py 330): INFO  * Acc@1 87.220 Acc@5 98.740
[2022-05-31 08:12:11 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.2%
[2022-05-31 08:12:11 MetaFG_0] (main.py 171): INFO Max accuracy: 87.38%
[2022-05-31 08:12:12 MetaFG_0] (main.py 265): INFO Train: [70/300][0/1562]	eta 0:25:36 lr 0.000005	time 0.9835 (0.9835)	loss 1.6740 (1.6740)	grad_norm 37.1503 (37.1503)	mem 4879MB
[2022-05-31 08:12:15 MetaFG_0] (main.py 265): INFO Train: [70/300][10/1562]	eta 0:09:37 lr 0.000005	time 0.2929 (0.3722)	loss 1.4515 (1.4092)	grad_norm 27.0298 (25.1867)	mem 4879MB
[2022-05-31 08:12:18 MetaFG_0] (main.py 265): INFO Train: [70/300][20/1562]	eta 0:08:43 lr 0.000005	time 0.2929 (0.3393)	loss 1.4481 (1.3482)	grad_norm 21.1572 (37.5155)	mem 4879MB
[2022-05-31 08:12:21 MetaFG_0] (main.py 265): INFO Train: [70/300][30/1562]	eta 0:08:22 lr 0.000005	time 0.2926 (0.3281)	loss 1.5560 (1.3457)	grad_norm 35.2133 (37.0312)	mem 4879MB
[2022-05-31 08:12:24 MetaFG_0] (main.py 265): INFO Train: [70/300][40/1562]	eta 0:08:10 lr 0.000005	time 0.2991 (0.3226)	loss 1.2079 (1.3127)	grad_norm 14.9985 (34.0228)	mem 4879MB
[2022-05-31 08:12:28 MetaFG_0] (main.py 265): INFO Train: [70/300][50/1562]	eta 0:08:02 lr 0.000005	time 0.3004 (0.3190)	loss 1.5750 (1.3099)	grad_norm 29.5275 (33.6501)	mem 4879MB
[2022-05-31 08:12:31 MetaFG_0] (main.py 265): INFO Train: [70/300][60/1562]	eta 0:07:54 lr 0.000005	time 0.2935 (0.3161)	loss 1.2521 (1.3137)	grad_norm 25.3977 (32.5500)	mem 4879MB
[2022-05-31 08:12:34 MetaFG_0] (main.py 265): INFO Train: [70/300][70/1562]	eta 0:07:49 lr 0.000005	time 0.2931 (0.3146)	loss 1.4819 (1.3198)	grad_norm 29.6055 (32.2431)	mem 4879MB
[2022-05-31 08:12:37 MetaFG_0] (main.py 265): INFO Train: [70/300][80/1562]	eta 0:07:44 lr 0.000005	time 0.2994 (0.3134)	loss 1.2012 (1.3241)	grad_norm 51.1566 (31.6607)	mem 4879MB
[2022-05-31 08:12:40 MetaFG_0] (main.py 265): INFO Train: [70/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.2990 (0.3126)	loss 1.4043 (1.3255)	grad_norm 52.5799 (31.4953)	mem 4879MB
[2022-05-31 08:12:43 MetaFG_0] (main.py 265): INFO Train: [70/300][100/1562]	eta 0:07:35 lr 0.000005	time 0.2937 (0.3119)	loss 1.3795 (1.3261)	grad_norm 24.0485 (31.2695)	mem 4879MB
[2022-05-31 08:12:46 MetaFG_0] (main.py 265): INFO Train: [70/300][110/1562]	eta 0:07:31 lr 0.000005	time 0.2921 (0.3112)	loss 1.3077 (1.3229)	grad_norm 30.6321 (31.0226)	mem 4879MB
[2022-05-31 08:12:49 MetaFG_0] (main.py 265): INFO Train: [70/300][120/1562]	eta 0:07:28 lr 0.000005	time 0.2986 (0.3107)	loss 1.3822 (1.3257)	grad_norm 30.9835 (30.6452)	mem 4879MB
[2022-05-31 08:12:52 MetaFG_0] (main.py 265): INFO Train: [70/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.2939 (0.3104)	loss 1.6323 (1.3264)	grad_norm 23.4738 (30.8237)	mem 4879MB
[2022-05-31 08:12:55 MetaFG_0] (main.py 265): INFO Train: [70/300][140/1562]	eta 0:07:20 lr 0.000005	time 0.2978 (0.3099)	loss 1.3239 (1.3283)	grad_norm 30.3598 (30.7470)	mem 4879MB
[2022-05-31 08:12:58 MetaFG_0] (main.py 265): INFO Train: [70/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2920 (0.3095)	loss 1.4470 (1.3356)	grad_norm 34.0163 (30.7681)	mem 4879MB
[2022-05-31 08:13:01 MetaFG_0] (main.py 265): INFO Train: [70/300][160/1562]	eta 0:07:15 lr 0.000005	time 0.2984 (0.3104)	loss 1.0182 (1.3304)	grad_norm 31.3657 (30.5253)	mem 4879MB
[2022-05-31 08:13:04 MetaFG_0] (main.py 265): INFO Train: [70/300][170/1562]	eta 0:07:11 lr 0.000005	time 0.2950 (0.3101)	loss 1.7000 (1.3296)	grad_norm 27.2513 (30.5552)	mem 4879MB
[2022-05-31 08:13:07 MetaFG_0] (main.py 265): INFO Train: [70/300][180/1562]	eta 0:07:08 lr 0.000005	time 0.3006 (0.3099)	loss 0.9948 (1.3193)	grad_norm 17.6746 (30.3155)	mem 4879MB
[2022-05-31 08:13:10 MetaFG_0] (main.py 265): INFO Train: [70/300][190/1562]	eta 0:07:04 lr 0.000005	time 0.2954 (0.3096)	loss 1.5017 (1.3237)	grad_norm 20.4500 (29.9927)	mem 4879MB
[2022-05-31 08:13:13 MetaFG_0] (main.py 265): INFO Train: [70/300][200/1562]	eta 0:07:01 lr 0.000005	time 0.2924 (0.3093)	loss 0.9592 (1.3224)	grad_norm 23.1617 (29.7176)	mem 4879MB
[2022-05-31 08:13:16 MetaFG_0] (main.py 265): INFO Train: [70/300][210/1562]	eta 0:06:58 lr 0.000005	time 0.2929 (0.3092)	loss 1.2492 (1.3300)	grad_norm 37.9798 (29.8501)	mem 4879MB
[2022-05-31 08:13:20 MetaFG_0] (main.py 265): INFO Train: [70/300][220/1562]	eta 0:06:54 lr 0.000005	time 0.2999 (0.3090)	loss 1.2176 (1.3326)	grad_norm 21.9399 (30.0106)	mem 4879MB
[2022-05-31 08:13:23 MetaFG_0] (main.py 265): INFO Train: [70/300][230/1562]	eta 0:06:51 lr 0.000005	time 0.2982 (0.3087)	loss 1.5620 (1.3336)	grad_norm 23.7900 (29.7391)	mem 4879MB
[2022-05-31 08:13:26 MetaFG_0] (main.py 265): INFO Train: [70/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2927 (0.3085)	loss 1.2265 (1.3302)	grad_norm 29.2582 (30.0410)	mem 4879MB
[2022-05-31 08:13:29 MetaFG_0] (main.py 265): INFO Train: [70/300][250/1562]	eta 0:06:44 lr 0.000005	time 0.3053 (0.3084)	loss 1.0538 (1.3265)	grad_norm 56.0532 (30.2195)	mem 4879MB
[2022-05-31 08:13:32 MetaFG_0] (main.py 265): INFO Train: [70/300][260/1562]	eta 0:06:41 lr 0.000005	time 0.2973 (0.3082)	loss 1.4694 (1.3299)	grad_norm 30.0775 (30.3123)	mem 4879MB
[2022-05-31 08:13:35 MetaFG_0] (main.py 265): INFO Train: [70/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2939 (0.3080)	loss 1.5401 (1.3301)	grad_norm 33.0323 (30.3033)	mem 4879MB
[2022-05-31 08:13:38 MetaFG_0] (main.py 265): INFO Train: [70/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2988 (0.3078)	loss 1.2127 (1.3297)	grad_norm 26.7251 (30.0849)	mem 4879MB
[2022-05-31 08:13:41 MetaFG_0] (main.py 265): INFO Train: [70/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2991 (0.3078)	loss 1.6053 (1.3263)	grad_norm 30.8848 (30.0423)	mem 4879MB
[2022-05-31 08:13:44 MetaFG_0] (main.py 265): INFO Train: [70/300][300/1562]	eta 0:06:28 lr 0.000005	time 0.2985 (0.3077)	loss 0.7822 (1.3245)	grad_norm 30.6431 (29.9247)	mem 4879MB
[2022-05-31 08:13:47 MetaFG_0] (main.py 265): INFO Train: [70/300][310/1562]	eta 0:06:25 lr 0.000005	time 0.2985 (0.3077)	loss 0.8606 (1.3191)	grad_norm 41.2967 (29.9536)	mem 4879MB
[2022-05-31 08:13:50 MetaFG_0] (main.py 265): INFO Train: [70/300][320/1562]	eta 0:06:22 lr 0.000005	time 0.2925 (0.3076)	loss 1.2530 (1.3180)	grad_norm 28.9373 (29.9658)	mem 4879MB
[2022-05-31 08:13:53 MetaFG_0] (main.py 265): INFO Train: [70/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2921 (0.3075)	loss 1.1428 (1.3168)	grad_norm 24.4836 (30.0352)	mem 4879MB
[2022-05-31 08:13:56 MetaFG_0] (main.py 265): INFO Train: [70/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2940 (0.3074)	loss 1.5686 (1.3184)	grad_norm 23.8152 (30.0529)	mem 4879MB
[2022-05-31 08:13:59 MetaFG_0] (main.py 265): INFO Train: [70/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2981 (0.3073)	loss 1.3091 (1.3172)	grad_norm 33.0961 (30.0787)	mem 4879MB
[2022-05-31 08:14:02 MetaFG_0] (main.py 265): INFO Train: [70/300][360/1562]	eta 0:06:09 lr 0.000005	time 0.2929 (0.3072)	loss 0.9349 (1.3164)	grad_norm 23.1090 (30.1526)	mem 4879MB
[2022-05-31 08:14:05 MetaFG_0] (main.py 265): INFO Train: [70/300][370/1562]	eta 0:06:06 lr 0.000005	time 0.2998 (0.3072)	loss 1.3644 (1.3194)	grad_norm 38.2913 (30.2354)	mem 4879MB
[2022-05-31 08:14:08 MetaFG_0] (main.py 265): INFO Train: [70/300][380/1562]	eta 0:06:03 lr 0.000005	time 0.2919 (0.3071)	loss 0.9310 (1.3145)	grad_norm 22.4481 (30.1953)	mem 4879MB
[2022-05-31 08:14:11 MetaFG_0] (main.py 265): INFO Train: [70/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2926 (0.3071)	loss 1.4215 (1.3137)	grad_norm 20.4589 (30.2244)	mem 4879MB
[2022-05-31 08:14:14 MetaFG_0] (main.py 265): INFO Train: [70/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2941 (0.3071)	loss 1.3966 (1.3140)	grad_norm 21.3663 (30.0967)	mem 4879MB
[2022-05-31 08:14:17 MetaFG_0] (main.py 265): INFO Train: [70/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2919 (0.3070)	loss 1.2964 (1.3142)	grad_norm 28.0719 (30.0832)	mem 4879MB
[2022-05-31 08:14:20 MetaFG_0] (main.py 265): INFO Train: [70/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2924 (0.3069)	loss 1.0995 (1.3128)	grad_norm 21.6040 (29.9896)	mem 4879MB
[2022-05-31 08:14:24 MetaFG_0] (main.py 265): INFO Train: [70/300][430/1562]	eta 0:05:47 lr 0.000005	time 0.2985 (0.3068)	loss 1.5856 (1.3127)	grad_norm 49.2278 (29.9364)	mem 4879MB
[2022-05-31 08:14:27 MetaFG_0] (main.py 265): INFO Train: [70/300][440/1562]	eta 0:05:44 lr 0.000005	time 0.2934 (0.3068)	loss 1.1278 (1.3141)	grad_norm 62.4058 (30.2135)	mem 4879MB
[2022-05-31 08:14:30 MetaFG_0] (main.py 265): INFO Train: [70/300][450/1562]	eta 0:05:41 lr 0.000005	time 0.2993 (0.3068)	loss 1.3654 (1.3176)	grad_norm 29.9871 (30.1990)	mem 4879MB
[2022-05-31 08:14:33 MetaFG_0] (main.py 265): INFO Train: [70/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2934 (0.3067)	loss 1.4133 (1.3167)	grad_norm 38.9118 (30.2178)	mem 4879MB
[2022-05-31 08:14:36 MetaFG_0] (main.py 265): INFO Train: [70/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2936 (0.3066)	loss 1.4464 (1.3160)	grad_norm 39.9242 (30.1934)	mem 4879MB
[2022-05-31 08:14:39 MetaFG_0] (main.py 265): INFO Train: [70/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2931 (0.3066)	loss 1.3239 (1.3169)	grad_norm 26.0901 (30.2188)	mem 4879MB
[2022-05-31 08:14:42 MetaFG_0] (main.py 265): INFO Train: [70/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.3015 (0.3066)	loss 1.2727 (1.3159)	grad_norm 24.9204 (30.2866)	mem 4879MB
[2022-05-31 08:14:45 MetaFG_0] (main.py 265): INFO Train: [70/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2925 (0.3065)	loss 1.4959 (1.3168)	grad_norm 23.8226 (30.3526)	mem 4879MB
[2022-05-31 08:14:48 MetaFG_0] (main.py 265): INFO Train: [70/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2932 (0.3065)	loss 1.1335 (1.3180)	grad_norm 22.1251 (30.3599)	mem 4879MB
[2022-05-31 08:14:51 MetaFG_0] (main.py 265): INFO Train: [70/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.2912 (0.3065)	loss 1.4455 (1.3177)	grad_norm 26.2244 (30.2656)	mem 4879MB
[2022-05-31 08:14:54 MetaFG_0] (main.py 265): INFO Train: [70/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2931 (0.3064)	loss 1.4358 (1.3179)	grad_norm 17.6061 (30.2434)	mem 4879MB
[2022-05-31 08:14:57 MetaFG_0] (main.py 265): INFO Train: [70/300][540/1562]	eta 0:05:13 lr 0.000005	time 0.2938 (0.3064)	loss 1.5665 (1.3202)	grad_norm 21.9811 (30.2277)	mem 4879MB
[2022-05-31 08:15:00 MetaFG_0] (main.py 265): INFO Train: [70/300][550/1562]	eta 0:05:10 lr 0.000005	time 0.2918 (0.3064)	loss 1.5005 (1.3216)	grad_norm 41.3413 (30.2592)	mem 4879MB
[2022-05-31 08:15:03 MetaFG_0] (main.py 265): INFO Train: [70/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2995 (0.3063)	loss 0.9443 (1.3185)	grad_norm 21.7443 (30.1634)	mem 4879MB
[2022-05-31 08:15:06 MetaFG_0] (main.py 265): INFO Train: [70/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2936 (0.3063)	loss 1.1709 (1.3197)	grad_norm 35.0515 (30.1212)	mem 4879MB
[2022-05-31 08:15:09 MetaFG_0] (main.py 265): INFO Train: [70/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2992 (0.3063)	loss 1.7504 (1.3194)	grad_norm 29.3329 (30.0393)	mem 4879MB
[2022-05-31 08:15:12 MetaFG_0] (main.py 265): INFO Train: [70/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2984 (0.3063)	loss 1.3855 (1.3195)	grad_norm 22.0234 (29.9654)	mem 4879MB
[2022-05-31 08:15:15 MetaFG_0] (main.py 265): INFO Train: [70/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2987 (0.3062)	loss 1.4590 (1.3200)	grad_norm 29.6650 (29.9192)	mem 4879MB
[2022-05-31 08:15:18 MetaFG_0] (main.py 265): INFO Train: [70/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2922 (0.3062)	loss 1.4350 (1.3199)	grad_norm 21.6541 (29.9581)	mem 4879MB
[2022-05-31 08:15:21 MetaFG_0] (main.py 265): INFO Train: [70/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2918 (0.3061)	loss 1.3022 (1.3210)	grad_norm 31.6352 (29.9705)	mem 4879MB
[2022-05-31 08:15:24 MetaFG_0] (main.py 265): INFO Train: [70/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2926 (0.3061)	loss 1.1016 (1.3206)	grad_norm 27.1753 (29.9504)	mem 4879MB
[2022-05-31 08:15:27 MetaFG_0] (main.py 265): INFO Train: [70/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2918 (0.3061)	loss 1.5308 (1.3208)	grad_norm 21.0720 (29.9434)	mem 4879MB
[2022-05-31 08:15:31 MetaFG_0] (main.py 265): INFO Train: [70/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.3003 (0.3061)	loss 1.3463 (1.3206)	grad_norm 28.4156 (29.9413)	mem 4879MB
[2022-05-31 08:15:34 MetaFG_0] (main.py 265): INFO Train: [70/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2951 (0.3061)	loss 1.0328 (1.3210)	grad_norm 18.4868 (29.8465)	mem 4879MB
[2022-05-31 08:15:37 MetaFG_0] (main.py 265): INFO Train: [70/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2986 (0.3061)	loss 0.8531 (1.3216)	grad_norm 14.6994 (29.8799)	mem 4879MB
[2022-05-31 08:15:40 MetaFG_0] (main.py 265): INFO Train: [70/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2999 (0.3061)	loss 0.8858 (1.3197)	grad_norm 40.2986 (29.9445)	mem 4879MB
[2022-05-31 08:15:43 MetaFG_0] (main.py 265): INFO Train: [70/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2929 (0.3060)	loss 1.2900 (1.3201)	grad_norm 29.3903 (29.9384)	mem 4879MB
[2022-05-31 08:15:46 MetaFG_0] (main.py 265): INFO Train: [70/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2934 (0.3060)	loss 1.2983 (1.3194)	grad_norm 28.2626 (29.9143)	mem 4879MB
[2022-05-31 08:15:49 MetaFG_0] (main.py 265): INFO Train: [70/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2957 (0.3060)	loss 1.2961 (1.3199)	grad_norm 31.3072 (29.8969)	mem 4879MB
[2022-05-31 08:15:52 MetaFG_0] (main.py 265): INFO Train: [70/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2973 (0.3060)	loss 1.4845 (1.3196)	grad_norm 31.8126 (29.8354)	mem 4879MB
[2022-05-31 08:15:55 MetaFG_0] (main.py 265): INFO Train: [70/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.3059 (0.3059)	loss 1.3368 (1.3203)	grad_norm 18.6694 (29.8546)	mem 4879MB
[2022-05-31 08:15:58 MetaFG_0] (main.py 265): INFO Train: [70/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2962 (0.3059)	loss 1.6154 (1.3202)	grad_norm 24.7342 (29.8594)	mem 4879MB
[2022-05-31 08:16:01 MetaFG_0] (main.py 265): INFO Train: [70/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2921 (0.3059)	loss 1.3906 (1.3198)	grad_norm 28.0020 (29.8906)	mem 4879MB
[2022-05-31 08:16:04 MetaFG_0] (main.py 265): INFO Train: [70/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2932 (0.3059)	loss 1.3587 (1.3191)	grad_norm 16.2237 (29.9398)	mem 4879MB
[2022-05-31 08:16:07 MetaFG_0] (main.py 265): INFO Train: [70/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2982 (0.3059)	loss 1.2166 (1.3184)	grad_norm 18.5047 (29.8884)	mem 4879MB
[2022-05-31 08:16:10 MetaFG_0] (main.py 265): INFO Train: [70/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2989 (0.3058)	loss 1.5344 (1.3179)	grad_norm 43.3306 (29.8734)	mem 4879MB
[2022-05-31 08:16:13 MetaFG_0] (main.py 265): INFO Train: [70/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.2930 (0.3058)	loss 1.3264 (1.3197)	grad_norm 44.3980 (29.9023)	mem 4879MB
[2022-05-31 08:16:16 MetaFG_0] (main.py 265): INFO Train: [70/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2919 (0.3058)	loss 1.0398 (1.3199)	grad_norm 36.8285 (29.9365)	mem 4879MB
[2022-05-31 08:16:19 MetaFG_0] (main.py 265): INFO Train: [70/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2994 (0.3058)	loss 1.0582 (1.3195)	grad_norm 48.5758 (29.9184)	mem 4879MB
[2022-05-31 08:16:22 MetaFG_0] (main.py 265): INFO Train: [70/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2997 (0.3058)	loss 0.9321 (1.3188)	grad_norm 35.7527 (29.9354)	mem 4879MB
[2022-05-31 08:16:25 MetaFG_0] (main.py 265): INFO Train: [70/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2936 (0.3057)	loss 1.1660 (1.3193)	grad_norm 16.1390 (29.9022)	mem 4879MB
[2022-05-31 08:16:28 MetaFG_0] (main.py 265): INFO Train: [70/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2933 (0.3057)	loss 1.0186 (1.3203)	grad_norm 30.8944 (29.9409)	mem 4879MB
[2022-05-31 08:16:31 MetaFG_0] (main.py 265): INFO Train: [70/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2932 (0.3057)	loss 1.5169 (1.3199)	grad_norm 31.3670 (29.9140)	mem 4879MB
[2022-05-31 08:16:35 MetaFG_0] (main.py 265): INFO Train: [70/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2986 (0.3057)	loss 1.5390 (1.3212)	grad_norm 21.4967 (29.9466)	mem 4879MB
[2022-05-31 08:16:38 MetaFG_0] (main.py 265): INFO Train: [70/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2923 (0.3057)	loss 0.8778 (1.3203)	grad_norm 22.2411 (29.9345)	mem 4879MB
[2022-05-31 08:16:41 MetaFG_0] (main.py 265): INFO Train: [70/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2989 (0.3057)	loss 1.4583 (1.3202)	grad_norm 24.7312 (29.9237)	mem 4879MB
[2022-05-31 08:16:44 MetaFG_0] (main.py 265): INFO Train: [70/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2989 (0.3057)	loss 1.5062 (1.3201)	grad_norm 15.9952 (29.9120)	mem 4879MB
[2022-05-31 08:16:47 MetaFG_0] (main.py 265): INFO Train: [70/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2944 (0.3057)	loss 0.9627 (1.3202)	grad_norm 30.8508 (29.9293)	mem 4879MB
[2022-05-31 08:16:50 MetaFG_0] (main.py 265): INFO Train: [70/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2936 (0.3057)	loss 0.9986 (1.3207)	grad_norm 26.9645 (29.9166)	mem 4879MB
[2022-05-31 08:16:53 MetaFG_0] (main.py 265): INFO Train: [70/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2999 (0.3057)	loss 1.3973 (1.3201)	grad_norm 27.4171 (29.9278)	mem 4879MB
[2022-05-31 08:16:56 MetaFG_0] (main.py 265): INFO Train: [70/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2923 (0.3057)	loss 1.5755 (1.3201)	grad_norm 36.1269 (29.9099)	mem 4879MB
[2022-05-31 08:16:59 MetaFG_0] (main.py 265): INFO Train: [70/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.3000 (0.3057)	loss 1.2721 (1.3204)	grad_norm 20.3120 (29.9145)	mem 4879MB
[2022-05-31 08:17:02 MetaFG_0] (main.py 265): INFO Train: [70/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2920 (0.3057)	loss 1.8161 (1.3211)	grad_norm 44.0198 (29.9190)	mem 4879MB
[2022-05-31 08:17:05 MetaFG_0] (main.py 265): INFO Train: [70/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2922 (0.3056)	loss 1.5379 (1.3212)	grad_norm 34.3127 (29.9167)	mem 4879MB
[2022-05-31 08:17:08 MetaFG_0] (main.py 265): INFO Train: [70/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2987 (0.3056)	loss 1.3851 (1.3208)	grad_norm 66.1138 (29.8949)	mem 4879MB
[2022-05-31 08:17:11 MetaFG_0] (main.py 265): INFO Train: [70/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2919 (0.3056)	loss 1.2202 (1.3210)	grad_norm 39.8776 (29.8947)	mem 4879MB
[2022-05-31 08:17:14 MetaFG_0] (main.py 265): INFO Train: [70/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2927 (0.3056)	loss 1.3410 (1.3198)	grad_norm 21.6618 (29.8961)	mem 4879MB
[2022-05-31 08:17:17 MetaFG_0] (main.py 265): INFO Train: [70/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.3014 (0.3056)	loss 1.6974 (1.3204)	grad_norm 44.8693 (29.9309)	mem 4879MB
[2022-05-31 08:17:20 MetaFG_0] (main.py 265): INFO Train: [70/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2991 (0.3056)	loss 1.3619 (1.3199)	grad_norm 30.0129 (29.9963)	mem 4879MB
[2022-05-31 08:17:23 MetaFG_0] (main.py 265): INFO Train: [70/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2937 (0.3056)	loss 1.3920 (1.3201)	grad_norm 38.9010 (29.9310)	mem 4879MB
[2022-05-31 08:17:26 MetaFG_0] (main.py 265): INFO Train: [70/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2946 (0.3056)	loss 0.9424 (1.3203)	grad_norm 37.9575 (29.9361)	mem 4879MB
[2022-05-31 08:17:29 MetaFG_0] (main.py 265): INFO Train: [70/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2946 (0.3056)	loss 1.2672 (1.3201)	grad_norm 23.3770 (29.9320)	mem 4879MB
[2022-05-31 08:17:32 MetaFG_0] (main.py 265): INFO Train: [70/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2997 (0.3056)	loss 1.3027 (1.3192)	grad_norm 30.1255 (29.9455)	mem 4879MB
[2022-05-31 08:17:36 MetaFG_0] (main.py 265): INFO Train: [70/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2974 (0.3056)	loss 0.8351 (1.3179)	grad_norm 34.2278 (29.9255)	mem 4879MB
[2022-05-31 08:17:39 MetaFG_0] (main.py 265): INFO Train: [70/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2922 (0.3056)	loss 1.0959 (1.3169)	grad_norm 36.9144 (29.9583)	mem 4879MB
[2022-05-31 08:17:42 MetaFG_0] (main.py 265): INFO Train: [70/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.3010 (0.3056)	loss 1.3085 (1.3169)	grad_norm 29.8860 (29.9686)	mem 4879MB
[2022-05-31 08:17:45 MetaFG_0] (main.py 265): INFO Train: [70/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2943 (0.3056)	loss 1.3289 (1.3165)	grad_norm 16.6418 (29.9493)	mem 4879MB
[2022-05-31 08:17:48 MetaFG_0] (main.py 265): INFO Train: [70/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2937 (0.3056)	loss 1.4524 (1.3163)	grad_norm 29.3192 (29.9782)	mem 4879MB
[2022-05-31 08:17:51 MetaFG_0] (main.py 265): INFO Train: [70/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.3003 (0.3055)	loss 1.4176 (1.3158)	grad_norm 20.2517 (29.9826)	mem 4879MB
[2022-05-31 08:17:54 MetaFG_0] (main.py 265): INFO Train: [70/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2977 (0.3055)	loss 1.3236 (1.3165)	grad_norm 14.7980 (29.9411)	mem 4879MB
[2022-05-31 08:17:57 MetaFG_0] (main.py 265): INFO Train: [70/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2927 (0.3055)	loss 1.3759 (1.3177)	grad_norm 47.3815 (29.9620)	mem 4879MB
[2022-05-31 08:18:00 MetaFG_0] (main.py 265): INFO Train: [70/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2984 (0.3055)	loss 1.3173 (1.3174)	grad_norm 42.5487 (29.9569)	mem 4879MB
[2022-05-31 08:18:03 MetaFG_0] (main.py 265): INFO Train: [70/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2951 (0.3055)	loss 1.6167 (1.3169)	grad_norm 40.3788 (29.9499)	mem 4879MB
[2022-05-31 08:18:06 MetaFG_0] (main.py 265): INFO Train: [70/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.3000 (0.3055)	loss 1.5783 (1.3170)	grad_norm 26.0025 (29.9463)	mem 4879MB
[2022-05-31 08:18:09 MetaFG_0] (main.py 265): INFO Train: [70/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2944 (0.3055)	loss 1.1906 (1.3169)	grad_norm 34.1299 (29.9299)	mem 4879MB
[2022-05-31 08:18:12 MetaFG_0] (main.py 265): INFO Train: [70/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2974 (0.3055)	loss 1.5315 (1.3173)	grad_norm 25.0141 (29.9692)	mem 4879MB
[2022-05-31 08:18:15 MetaFG_0] (main.py 265): INFO Train: [70/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2988 (0.3055)	loss 1.4187 (1.3169)	grad_norm 17.7323 (29.9468)	mem 4879MB
[2022-05-31 08:18:18 MetaFG_0] (main.py 265): INFO Train: [70/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2929 (0.3055)	loss 1.4499 (1.3180)	grad_norm 26.9451 (29.9165)	mem 4879MB
[2022-05-31 08:18:21 MetaFG_0] (main.py 265): INFO Train: [70/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2990 (0.3055)	loss 1.1099 (1.3169)	grad_norm 30.8404 (29.9260)	mem 4879MB
[2022-05-31 08:18:24 MetaFG_0] (main.py 265): INFO Train: [70/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2946 (0.3055)	loss 1.5427 (1.3174)	grad_norm 27.4939 (29.9386)	mem 4879MB
[2022-05-31 08:18:27 MetaFG_0] (main.py 265): INFO Train: [70/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2924 (0.3055)	loss 1.6080 (1.3174)	grad_norm 26.5143 (29.9519)	mem 4879MB
[2022-05-31 08:18:30 MetaFG_0] (main.py 265): INFO Train: [70/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2975 (0.3055)	loss 1.0866 (1.3179)	grad_norm 20.4106 (29.9493)	mem 4879MB
[2022-05-31 08:18:33 MetaFG_0] (main.py 265): INFO Train: [70/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2994 (0.3055)	loss 1.7187 (1.3174)	grad_norm 29.0100 (29.9533)	mem 4879MB
[2022-05-31 08:18:36 MetaFG_0] (main.py 265): INFO Train: [70/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2934 (0.3055)	loss 1.4000 (1.3172)	grad_norm 17.7739 (29.9574)	mem 4879MB
[2022-05-31 08:18:40 MetaFG_0] (main.py 265): INFO Train: [70/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2997 (0.3055)	loss 1.3996 (1.3170)	grad_norm 28.1896 (29.9283)	mem 4879MB
[2022-05-31 08:18:43 MetaFG_0] (main.py 265): INFO Train: [70/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2934 (0.3055)	loss 1.4277 (1.3177)	grad_norm 30.8609 (29.9309)	mem 4879MB
[2022-05-31 08:18:46 MetaFG_0] (main.py 265): INFO Train: [70/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2941 (0.3054)	loss 1.6279 (1.3181)	grad_norm 20.0308 (29.9377)	mem 4879MB
[2022-05-31 08:18:49 MetaFG_0] (main.py 265): INFO Train: [70/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2922 (0.3054)	loss 1.3116 (1.3180)	grad_norm 19.9687 (29.9172)	mem 4879MB
[2022-05-31 08:18:52 MetaFG_0] (main.py 265): INFO Train: [70/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2993 (0.3054)	loss 1.3913 (1.3181)	grad_norm 22.3382 (29.9339)	mem 4879MB
[2022-05-31 08:18:55 MetaFG_0] (main.py 265): INFO Train: [70/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.3026 (0.3054)	loss 1.6810 (1.3178)	grad_norm 29.4442 (nan)	mem 4879MB
[2022-05-31 08:18:58 MetaFG_0] (main.py 265): INFO Train: [70/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2937 (0.3054)	loss 1.2881 (1.3182)	grad_norm 27.5792 (nan)	mem 4879MB
[2022-05-31 08:19:01 MetaFG_0] (main.py 265): INFO Train: [70/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2955 (0.3054)	loss 1.5700 (1.3187)	grad_norm 35.7128 (nan)	mem 4879MB
[2022-05-31 08:19:04 MetaFG_0] (main.py 265): INFO Train: [70/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2981 (0.3054)	loss 1.1276 (1.3183)	grad_norm 18.5409 (nan)	mem 4879MB
[2022-05-31 08:19:07 MetaFG_0] (main.py 265): INFO Train: [70/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2998 (0.3054)	loss 1.3143 (1.3183)	grad_norm 19.6509 (nan)	mem 4879MB
[2022-05-31 08:19:10 MetaFG_0] (main.py 265): INFO Train: [70/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2937 (0.3054)	loss 1.4522 (1.3176)	grad_norm 36.1724 (nan)	mem 4879MB
[2022-05-31 08:19:13 MetaFG_0] (main.py 265): INFO Train: [70/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2995 (0.3054)	loss 1.5195 (1.3170)	grad_norm 27.5008 (nan)	mem 4879MB
[2022-05-31 08:19:16 MetaFG_0] (main.py 265): INFO Train: [70/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2922 (0.3054)	loss 1.3735 (1.3165)	grad_norm 25.8756 (nan)	mem 4879MB
[2022-05-31 08:19:19 MetaFG_0] (main.py 265): INFO Train: [70/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2964 (0.3054)	loss 1.1681 (1.3168)	grad_norm 57.6147 (nan)	mem 4879MB
[2022-05-31 08:19:22 MetaFG_0] (main.py 265): INFO Train: [70/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2920 (0.3053)	loss 1.5187 (1.3163)	grad_norm 49.4105 (nan)	mem 4879MB
[2022-05-31 08:19:25 MetaFG_0] (main.py 265): INFO Train: [70/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.3012 (0.3053)	loss 1.4617 (1.3168)	grad_norm 31.8040 (nan)	mem 4879MB
[2022-05-31 08:19:28 MetaFG_0] (main.py 265): INFO Train: [70/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2926 (0.3054)	loss 1.3074 (1.3175)	grad_norm 24.2075 (nan)	mem 4879MB
[2022-05-31 08:19:31 MetaFG_0] (main.py 265): INFO Train: [70/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2935 (0.3053)	loss 1.4162 (1.3173)	grad_norm 26.7746 (nan)	mem 4879MB
[2022-05-31 08:19:34 MetaFG_0] (main.py 265): INFO Train: [70/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2922 (0.3053)	loss 0.9815 (1.3177)	grad_norm 19.2071 (nan)	mem 4879MB
[2022-05-31 08:19:37 MetaFG_0] (main.py 265): INFO Train: [70/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2934 (0.3053)	loss 0.9450 (1.3179)	grad_norm 28.6512 (nan)	mem 4879MB
[2022-05-31 08:19:40 MetaFG_0] (main.py 265): INFO Train: [70/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2984 (0.3053)	loss 1.4921 (1.3178)	grad_norm 48.7871 (nan)	mem 4879MB
[2022-05-31 08:19:43 MetaFG_0] (main.py 265): INFO Train: [70/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2942 (0.3053)	loss 0.7536 (1.3172)	grad_norm 21.3332 (nan)	mem 4879MB
[2022-05-31 08:19:46 MetaFG_0] (main.py 265): INFO Train: [70/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2926 (0.3053)	loss 1.5013 (1.3169)	grad_norm 36.5721 (nan)	mem 4879MB
[2022-05-31 08:19:50 MetaFG_0] (main.py 265): INFO Train: [70/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2990 (0.3055)	loss 1.4494 (1.3169)	grad_norm 16.1795 (nan)	mem 4879MB
[2022-05-31 08:19:53 MetaFG_0] (main.py 265): INFO Train: [70/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2984 (0.3055)	loss 1.4070 (1.3171)	grad_norm 31.4592 (nan)	mem 4879MB
[2022-05-31 08:19:56 MetaFG_0] (main.py 265): INFO Train: [70/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2933 (0.3055)	loss 1.1750 (1.3174)	grad_norm 23.5035 (nan)	mem 4879MB
[2022-05-31 08:19:59 MetaFG_0] (main.py 265): INFO Train: [70/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2976 (0.3055)	loss 1.0049 (1.3183)	grad_norm 26.5910 (nan)	mem 4879MB
[2022-05-31 08:20:02 MetaFG_0] (main.py 265): INFO Train: [70/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2932 (0.3055)	loss 1.0505 (1.3185)	grad_norm 37.5523 (nan)	mem 4879MB
[2022-05-31 08:20:05 MetaFG_0] (main.py 265): INFO Train: [70/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2998 (0.3055)	loss 1.4806 (1.3184)	grad_norm 24.4628 (nan)	mem 4879MB
[2022-05-31 08:20:08 MetaFG_0] (main.py 265): INFO Train: [70/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2910 (0.3054)	loss 1.5199 (1.3189)	grad_norm 30.0425 (nan)	mem 4879MB
[2022-05-31 08:20:08 MetaFG_0] (main.py 272): INFO EPOCH 70 training takes 0:07:57
[2022-05-31 08:20:08 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_70.pth saving......
[2022-05-31 08:20:09 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_70.pth saved !!!
[2022-05-31 08:20:09 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:20:11 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:20:11 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:20:12 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.622 (0.622)	Loss 0.3410 (0.3410)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 08:20:13 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.098 (0.147)	Loss 0.4787 (0.5503)	Acc@1 90.625 (88.920)	Acc@5 100.000 (99.716)	Mem 4879MB
[2022-05-31 08:20:14 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.123)	Loss 0.5580 (0.5826)	Acc@1 90.625 (87.946)	Acc@5 100.000 (99.405)	Mem 4879MB
[2022-05-31 08:20:15 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.097 (0.114)	Loss 0.4480 (0.5836)	Acc@1 93.750 (87.601)	Acc@5 100.000 (99.194)	Mem 4879MB
[2022-05-31 08:20:16 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.109)	Loss 0.9012 (0.6057)	Acc@1 78.125 (87.348)	Acc@5 96.875 (99.085)	Mem 4879MB
[2022-05-31 08:20:16 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.106)	Loss 0.5126 (0.6038)	Acc@1 90.625 (87.623)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 08:20:17 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.088 (0.104)	Loss 0.6585 (0.6085)	Acc@1 87.500 (87.551)	Acc@5 100.000 (98.975)	Mem 4879MB
[2022-05-31 08:20:18 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.092 (0.103)	Loss 0.5355 (0.6092)	Acc@1 90.625 (87.632)	Acc@5 96.875 (98.856)	Mem 4879MB
[2022-05-31 08:20:19 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.112 (0.102)	Loss 0.7221 (0.6113)	Acc@1 84.375 (87.654)	Acc@5 100.000 (98.804)	Mem 4879MB
[2022-05-31 08:20:20 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.088 (0.101)	Loss 0.7785 (0.6153)	Acc@1 84.375 (87.466)	Acc@5 100.000 (98.832)	Mem 4879MB
[2022-05-31 08:20:21 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.5903 (0.6145)	Acc@1 90.625 (87.407)	Acc@5 96.875 (98.886)	Mem 4879MB
[2022-05-31 08:20:22 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.099)	Loss 0.6638 (0.6184)	Acc@1 84.375 (87.190)	Acc@5 96.875 (98.874)	Mem 4879MB
[2022-05-31 08:20:23 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.6025 (0.6169)	Acc@1 90.625 (87.035)	Acc@5 100.000 (98.967)	Mem 4879MB
[2022-05-31 08:20:24 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.095 (0.099)	Loss 0.7125 (0.6190)	Acc@1 87.500 (86.975)	Acc@5 96.875 (98.879)	Mem 4879MB
[2022-05-31 08:20:25 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.088 (0.099)	Loss 0.4771 (0.6183)	Acc@1 96.875 (86.990)	Acc@5 100.000 (98.870)	Mem 4879MB
[2022-05-31 08:20:26 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.098)	Loss 0.9101 (0.6214)	Acc@1 81.250 (86.921)	Acc@5 96.875 (98.862)	Mem 4879MB
[2022-05-31 08:20:27 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.5493 (0.6161)	Acc@1 90.625 (87.209)	Acc@5 100.000 (98.874)	Mem 4879MB
[2022-05-31 08:20:28 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.098)	Loss 0.6520 (0.6194)	Acc@1 87.500 (87.116)	Acc@5 100.000 (98.849)	Mem 4879MB
[2022-05-31 08:20:29 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.094 (0.098)	Loss 0.5787 (0.6145)	Acc@1 90.625 (87.155)	Acc@5 100.000 (98.895)	Mem 4879MB
[2022-05-31 08:20:30 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.102 (0.098)	Loss 0.8257 (0.6124)	Acc@1 75.000 (87.287)	Acc@5 96.875 (98.904)	Mem 4879MB
[2022-05-31 08:20:31 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.097)	Loss 0.7489 (0.6116)	Acc@1 81.250 (87.236)	Acc@5 100.000 (98.943)	Mem 4879MB
[2022-05-31 08:20:32 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.097 (0.097)	Loss 0.2896 (0.6121)	Acc@1 100.000 (87.219)	Acc@5 100.000 (98.919)	Mem 4879MB
[2022-05-31 08:20:32 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.097)	Loss 0.5349 (0.6115)	Acc@1 87.500 (87.217)	Acc@5 100.000 (98.939)	Mem 4879MB
[2022-05-31 08:20:33 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.097)	Loss 0.6958 (0.6126)	Acc@1 90.625 (87.256)	Acc@5 96.875 (98.904)	Mem 4879MB
[2022-05-31 08:20:34 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.4594 (0.6115)	Acc@1 93.750 (87.318)	Acc@5 100.000 (98.924)	Mem 4879MB
[2022-05-31 08:20:35 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.100 (0.097)	Loss 0.6510 (0.6108)	Acc@1 87.500 (87.400)	Acc@5 96.875 (98.904)	Mem 4879MB
[2022-05-31 08:20:36 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.099 (0.097)	Loss 0.5491 (0.6095)	Acc@1 87.500 (87.416)	Acc@5 100.000 (98.910)	Mem 4879MB
[2022-05-31 08:20:37 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.087 (0.096)	Loss 0.6186 (0.6092)	Acc@1 90.625 (87.385)	Acc@5 100.000 (98.905)	Mem 4879MB
[2022-05-31 08:20:38 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.3870 (0.6075)	Acc@1 93.750 (87.478)	Acc@5 100.000 (98.888)	Mem 4879MB
[2022-05-31 08:20:39 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.6612 (0.6087)	Acc@1 87.500 (87.500)	Acc@5 100.000 (98.883)	Mem 4879MB
[2022-05-31 08:20:40 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.087 (0.096)	Loss 0.8702 (0.6086)	Acc@1 81.250 (87.542)	Acc@5 93.750 (98.837)	Mem 4879MB
[2022-05-31 08:20:41 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.7576 (0.6116)	Acc@1 84.375 (87.420)	Acc@5 100.000 (98.834)	Mem 4879MB
[2022-05-31 08:20:41 MetaFG_0] (main.py 330): INFO  * Acc@1 87.430 Acc@5 98.840
[2022-05-31 08:20:41 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.4%
[2022-05-31 08:20:41 MetaFG_0] (main.py 171): INFO Max accuracy: 87.43%
[2022-05-31 08:20:42 MetaFG_0] (main.py 265): INFO Train: [71/300][0/1562]	eta 0:27:22 lr 0.000005	time 1.0518 (1.0518)	loss 1.4581 (1.4581)	grad_norm 39.9827 (39.9827)	mem 4879MB
[2022-05-31 08:20:45 MetaFG_0] (main.py 265): INFO Train: [71/300][10/1562]	eta 0:09:47 lr 0.000005	time 0.3010 (0.3787)	loss 1.4020 (1.3757)	grad_norm 28.1728 (29.2953)	mem 4879MB
[2022-05-31 08:20:48 MetaFG_0] (main.py 265): INFO Train: [71/300][20/1562]	eta 0:08:50 lr 0.000005	time 0.2993 (0.3441)	loss 1.5981 (1.3300)	grad_norm 38.2626 (30.8977)	mem 4879MB
[2022-05-31 08:20:51 MetaFG_0] (main.py 265): INFO Train: [71/300][30/1562]	eta 0:08:27 lr 0.000005	time 0.2980 (0.3314)	loss 1.3962 (1.3356)	grad_norm 27.7890 (31.5203)	mem 4879MB
[2022-05-31 08:20:54 MetaFG_0] (main.py 265): INFO Train: [71/300][40/1562]	eta 0:08:14 lr 0.000005	time 0.2927 (0.3247)	loss 1.4700 (1.3617)	grad_norm 21.6232 (31.6679)	mem 4879MB
[2022-05-31 08:20:57 MetaFG_0] (main.py 265): INFO Train: [71/300][50/1562]	eta 0:08:05 lr 0.000005	time 0.3005 (0.3212)	loss 1.5584 (1.3528)	grad_norm 35.2551 (31.2655)	mem 4879MB
[2022-05-31 08:21:01 MetaFG_0] (main.py 265): INFO Train: [71/300][60/1562]	eta 0:07:58 lr 0.000005	time 0.2933 (0.3183)	loss 1.4160 (1.3481)	grad_norm 28.7590 (30.7831)	mem 4879MB
[2022-05-31 08:21:04 MetaFG_0] (main.py 265): INFO Train: [71/300][70/1562]	eta 0:07:52 lr 0.000005	time 0.3008 (0.3168)	loss 1.3022 (1.3464)	grad_norm 18.5130 (30.9385)	mem 4879MB
[2022-05-31 08:21:07 MetaFG_0] (main.py 265): INFO Train: [71/300][80/1562]	eta 0:07:47 lr 0.000005	time 0.2993 (0.3153)	loss 1.4465 (1.3617)	grad_norm 24.1593 (30.4944)	mem 4879MB
[2022-05-31 08:21:10 MetaFG_0] (main.py 265): INFO Train: [71/300][90/1562]	eta 0:07:42 lr 0.000005	time 0.3009 (0.3141)	loss 1.4443 (1.3648)	grad_norm 47.0958 (30.5760)	mem 4879MB
[2022-05-31 08:21:13 MetaFG_0] (main.py 265): INFO Train: [71/300][100/1562]	eta 0:07:37 lr 0.000005	time 0.2953 (0.3131)	loss 1.1082 (1.3646)	grad_norm 50.3383 (30.8163)	mem 4879MB
[2022-05-31 08:21:16 MetaFG_0] (main.py 265): INFO Train: [71/300][110/1562]	eta 0:07:33 lr 0.000005	time 0.2940 (0.3121)	loss 1.3595 (1.3675)	grad_norm 29.3707 (30.7541)	mem 4879MB
[2022-05-31 08:21:19 MetaFG_0] (main.py 265): INFO Train: [71/300][120/1562]	eta 0:07:29 lr 0.000005	time 0.2961 (0.3114)	loss 1.1089 (1.3598)	grad_norm 27.2933 (31.3171)	mem 4879MB
[2022-05-31 08:21:22 MetaFG_0] (main.py 265): INFO Train: [71/300][130/1562]	eta 0:07:25 lr 0.000005	time 0.2999 (0.3110)	loss 1.2621 (1.3544)	grad_norm 28.3171 (31.2060)	mem 4879MB
[2022-05-31 08:21:25 MetaFG_0] (main.py 265): INFO Train: [71/300][140/1562]	eta 0:07:21 lr 0.000005	time 0.2924 (0.3106)	loss 1.5309 (1.3531)	grad_norm 23.8961 (30.9038)	mem 4879MB
[2022-05-31 08:21:28 MetaFG_0] (main.py 265): INFO Train: [71/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2950 (0.3102)	loss 1.4775 (1.3561)	grad_norm 25.0305 (30.9044)	mem 4879MB
[2022-05-31 08:21:31 MetaFG_0] (main.py 265): INFO Train: [71/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.3019 (0.3099)	loss 1.5005 (1.3561)	grad_norm 26.3725 (30.6732)	mem 4879MB
[2022-05-31 08:21:34 MetaFG_0] (main.py 265): INFO Train: [71/300][170/1562]	eta 0:07:11 lr 0.000005	time 0.2983 (0.3096)	loss 1.4381 (1.3599)	grad_norm 17.6270 (30.4080)	mem 4879MB
[2022-05-31 08:21:37 MetaFG_0] (main.py 265): INFO Train: [71/300][180/1562]	eta 0:07:07 lr 0.000005	time 0.2994 (0.3094)	loss 1.0869 (1.3587)	grad_norm 28.0211 (30.1042)	mem 4879MB
[2022-05-31 08:21:40 MetaFG_0] (main.py 265): INFO Train: [71/300][190/1562]	eta 0:07:04 lr 0.000005	time 0.3063 (0.3093)	loss 1.4806 (1.3597)	grad_norm 22.3982 (30.1838)	mem 4879MB
[2022-05-31 08:21:43 MetaFG_0] (main.py 265): INFO Train: [71/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2938 (0.3090)	loss 1.2693 (1.3575)	grad_norm 27.4010 (30.4576)	mem 4879MB
[2022-05-31 08:21:46 MetaFG_0] (main.py 265): INFO Train: [71/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2937 (0.3088)	loss 1.5023 (1.3621)	grad_norm 50.8504 (30.4912)	mem 4879MB
[2022-05-31 08:21:49 MetaFG_0] (main.py 265): INFO Train: [71/300][220/1562]	eta 0:06:54 lr 0.000005	time 0.2924 (0.3085)	loss 1.5226 (1.3636)	grad_norm 25.3807 (30.7015)	mem 4879MB
[2022-05-31 08:21:52 MetaFG_0] (main.py 265): INFO Train: [71/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2921 (0.3084)	loss 1.1115 (1.3623)	grad_norm 18.6080 (30.5412)	mem 4879MB
[2022-05-31 08:21:55 MetaFG_0] (main.py 265): INFO Train: [71/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2930 (0.3082)	loss 1.5492 (1.3634)	grad_norm 33.5809 (30.4004)	mem 4879MB
[2022-05-31 08:21:58 MetaFG_0] (main.py 265): INFO Train: [71/300][250/1562]	eta 0:06:44 lr 0.000005	time 0.2938 (0.3080)	loss 1.2979 (1.3636)	grad_norm 30.0272 (30.1120)	mem 4879MB
[2022-05-31 08:22:01 MetaFG_0] (main.py 265): INFO Train: [71/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2941 (0.3079)	loss 1.4490 (1.3616)	grad_norm 20.8382 (30.0358)	mem 4879MB
[2022-05-31 08:22:04 MetaFG_0] (main.py 265): INFO Train: [71/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2935 (0.3077)	loss 1.5532 (1.3631)	grad_norm 26.3010 (29.9722)	mem 4879MB
[2022-05-31 08:22:08 MetaFG_0] (main.py 265): INFO Train: [71/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2940 (0.3076)	loss 1.3714 (1.3633)	grad_norm 23.9072 (29.8475)	mem 4879MB
[2022-05-31 08:22:11 MetaFG_0] (main.py 265): INFO Train: [71/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2978 (0.3075)	loss 0.9885 (1.3571)	grad_norm 36.3684 (29.9182)	mem 4879MB
[2022-05-31 08:22:14 MetaFG_0] (main.py 265): INFO Train: [71/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2926 (0.3074)	loss 0.7982 (1.3535)	grad_norm 29.4682 (30.2186)	mem 4879MB
[2022-05-31 08:22:17 MetaFG_0] (main.py 265): INFO Train: [71/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2930 (0.3072)	loss 1.1250 (1.3518)	grad_norm 24.7420 (30.3000)	mem 4879MB
[2022-05-31 08:22:20 MetaFG_0] (main.py 265): INFO Train: [71/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2981 (0.3072)	loss 1.0933 (1.3493)	grad_norm 37.2779 (30.3613)	mem 4879MB
[2022-05-31 08:22:23 MetaFG_0] (main.py 265): INFO Train: [71/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2980 (0.3071)	loss 1.4701 (1.3494)	grad_norm 25.2387 (30.2975)	mem 4879MB
[2022-05-31 08:22:26 MetaFG_0] (main.py 265): INFO Train: [71/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.3004 (0.3070)	loss 1.6694 (1.3522)	grad_norm 36.1318 (30.7596)	mem 4879MB
[2022-05-31 08:22:29 MetaFG_0] (main.py 265): INFO Train: [71/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2931 (0.3069)	loss 1.6164 (1.3519)	grad_norm 60.7324 (30.9097)	mem 4879MB
[2022-05-31 08:22:32 MetaFG_0] (main.py 265): INFO Train: [71/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2925 (0.3068)	loss 1.5594 (1.3527)	grad_norm 44.1626 (30.8561)	mem 4879MB
[2022-05-31 08:22:35 MetaFG_0] (main.py 265): INFO Train: [71/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2932 (0.3068)	loss 1.4749 (1.3489)	grad_norm 22.8802 (30.8499)	mem 4879MB
[2022-05-31 08:22:38 MetaFG_0] (main.py 265): INFO Train: [71/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2980 (0.3068)	loss 1.0587 (1.3470)	grad_norm 43.9603 (30.8684)	mem 4879MB
[2022-05-31 08:22:41 MetaFG_0] (main.py 265): INFO Train: [71/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2990 (0.3068)	loss 1.0907 (1.3453)	grad_norm 21.6642 (30.9842)	mem 4879MB
[2022-05-31 08:22:44 MetaFG_0] (main.py 265): INFO Train: [71/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2941 (0.3067)	loss 1.3404 (1.3434)	grad_norm 41.1078 (31.0881)	mem 4879MB
[2022-05-31 08:22:47 MetaFG_0] (main.py 265): INFO Train: [71/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2923 (0.3067)	loss 1.6730 (1.3432)	grad_norm 34.6815 (31.0380)	mem 4879MB
[2022-05-31 08:30:50 MetaFG_0] (main.py 265): INFO Train: [72/300][320/1562]	eta 0:06:22 lr 0.000005	time 0.3043 (0.3078)	loss 1.5476 (1.3031)	grad_norm 18.5877 (30.2378)	mem 4879MB
[2022-05-31 08:30:53 MetaFG_0] (main.py 265): INFO Train: [72/300][330/1562]	eta 0:06:19 lr 0.000005	time 0.2932 (0.3077)	loss 1.3622 (1.3047)	grad_norm 28.1433 (30.4697)	mem 4879MB
[2022-05-31 08:30:56 MetaFG_0] (main.py 265): INFO Train: [72/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2996 (0.3077)	loss 1.7662 (1.3058)	grad_norm 34.6497 (30.5166)	mem 4879MB
[2022-05-31 08:30:59 MetaFG_0] (main.py 265): INFO Train: [72/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2997 (0.3076)	loss 1.1688 (1.3054)	grad_norm 36.8062 (30.5825)	mem 4879MB
[2022-05-31 08:31:02 MetaFG_0] (main.py 265): INFO Train: [72/300][360/1562]	eta 0:06:09 lr 0.000005	time 0.2927 (0.3075)	loss 1.2984 (1.3065)	grad_norm 27.8728 (30.5478)	mem 4879MB
[2022-05-31 08:31:05 MetaFG_0] (main.py 265): INFO Train: [72/300][370/1562]	eta 0:06:06 lr 0.000005	time 0.2997 (0.3074)	loss 1.4163 (1.3052)	grad_norm 21.3058 (30.5635)	mem 4879MB
[2022-05-31 08:31:08 MetaFG_0] (main.py 265): INFO Train: [72/300][380/1562]	eta 0:06:03 lr 0.000005	time 0.2927 (0.3074)	loss 1.1561 (1.3067)	grad_norm 52.2638 (30.5792)	mem 4879MB
[2022-05-31 08:31:11 MetaFG_0] (main.py 265): INFO Train: [72/300][390/1562]	eta 0:06:00 lr 0.000005	time 0.2923 (0.3073)	loss 0.8908 (1.3068)	grad_norm 23.4290 (30.6152)	mem 4879MB
[2022-05-31 08:31:14 MetaFG_0] (main.py 265): INFO Train: [72/300][400/1562]	eta 0:05:57 lr 0.000005	time 0.2914 (0.3072)	loss 0.9561 (1.3086)	grad_norm 24.4489 (30.5993)	mem 4879MB
[2022-05-31 08:31:17 MetaFG_0] (main.py 265): INFO Train: [72/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2939 (0.3071)	loss 1.5237 (1.3088)	grad_norm 23.7148 (30.5555)	mem 4879MB
[2022-05-31 08:31:20 MetaFG_0] (main.py 265): INFO Train: [72/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.3002 (0.3071)	loss 1.3443 (1.3110)	grad_norm 15.3306 (30.4731)	mem 4879MB
[2022-05-31 08:31:23 MetaFG_0] (main.py 265): INFO Train: [72/300][430/1562]	eta 0:05:47 lr 0.000005	time 0.2940 (0.3070)	loss 1.5541 (1.3117)	grad_norm 39.7850 (30.6096)	mem 4879MB
[2022-05-31 08:31:26 MetaFG_0] (main.py 265): INFO Train: [72/300][440/1562]	eta 0:05:44 lr 0.000005	time 0.2997 (0.3069)	loss 1.0620 (1.3128)	grad_norm 40.9587 (30.5821)	mem 4879MB
[2022-05-31 08:31:29 MetaFG_0] (main.py 265): INFO Train: [72/300][450/1562]	eta 0:05:41 lr 0.000005	time 0.2947 (0.3069)	loss 1.6896 (1.3149)	grad_norm 43.9848 (30.5561)	mem 4879MB
[2022-05-31 08:31:32 MetaFG_0] (main.py 265): INFO Train: [72/300][460/1562]	eta 0:05:38 lr 0.000005	time 0.2921 (0.3068)	loss 1.2492 (1.3150)	grad_norm 33.3692 (30.5434)	mem 4879MB
[2022-05-31 08:31:35 MetaFG_0] (main.py 265): INFO Train: [72/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2929 (0.3068)	loss 1.2943 (1.3134)	grad_norm 63.0512 (30.5573)	mem 4879MB
[2022-05-31 08:31:38 MetaFG_0] (main.py 265): INFO Train: [72/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2926 (0.3067)	loss 1.2927 (1.3129)	grad_norm 23.3460 (30.4894)	mem 4879MB
[2022-05-31 08:31:41 MetaFG_0] (main.py 265): INFO Train: [72/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.3010 (0.3067)	loss 1.4489 (1.3115)	grad_norm 12.0073 (30.5079)	mem 4879MB
[2022-05-31 08:31:45 MetaFG_0] (main.py 265): INFO Train: [72/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2949 (0.3066)	loss 1.1038 (1.3101)	grad_norm 24.9514 (30.4464)	mem 4879MB
[2022-05-31 08:31:48 MetaFG_0] (main.py 265): INFO Train: [72/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2950 (0.3066)	loss 1.6758 (1.3115)	grad_norm 25.3540 (30.3463)	mem 4879MB
[2022-05-31 08:31:51 MetaFG_0] (main.py 265): INFO Train: [72/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.2941 (0.3066)	loss 1.5017 (1.3123)	grad_norm 19.4494 (30.2961)	mem 4879MB
[2022-05-31 08:31:54 MetaFG_0] (main.py 265): INFO Train: [72/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2935 (0.3066)	loss 1.6009 (1.3131)	grad_norm 26.3908 (30.3530)	mem 4879MB
[2022-05-31 08:31:57 MetaFG_0] (main.py 265): INFO Train: [72/300][540/1562]	eta 0:05:13 lr 0.000005	time 0.2997 (0.3065)	loss 0.9058 (1.3142)	grad_norm 16.5219 (30.2879)	mem 4879MB
[2022-05-31 08:32:00 MetaFG_0] (main.py 265): INFO Train: [72/300][550/1562]	eta 0:05:10 lr 0.000005	time 0.2990 (0.3065)	loss 1.0760 (1.3145)	grad_norm 49.6994 (30.2703)	mem 4879MB
[2022-05-31 08:32:03 MetaFG_0] (main.py 265): INFO Train: [72/300][560/1562]	eta 0:05:07 lr 0.000005	time 0.3001 (0.3065)	loss 1.3245 (1.3156)	grad_norm 22.1226 (30.3008)	mem 4879MB
[2022-05-31 08:32:06 MetaFG_0] (main.py 265): INFO Train: [72/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2988 (0.3064)	loss 1.5671 (1.3176)	grad_norm 18.9484 (30.2171)	mem 4879MB
[2022-05-31 08:32:09 MetaFG_0] (main.py 265): INFO Train: [72/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2948 (0.3064)	loss 1.1973 (1.3182)	grad_norm 27.6233 (30.1973)	mem 4879MB
[2022-05-31 08:32:12 MetaFG_0] (main.py 265): INFO Train: [72/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2985 (0.3064)	loss 1.3652 (1.3186)	grad_norm 20.0667 (30.1467)	mem 4879MB
[2022-05-31 08:32:15 MetaFG_0] (main.py 265): INFO Train: [72/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2931 (0.3063)	loss 1.4389 (1.3185)	grad_norm 23.0373 (30.1244)	mem 4879MB
[2022-05-31 08:32:18 MetaFG_0] (main.py 265): INFO Train: [72/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2998 (0.3063)	loss 1.1255 (1.3179)	grad_norm 29.3155 (30.2062)	mem 4879MB
[2022-05-31 08:32:21 MetaFG_0] (main.py 265): INFO Train: [72/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2995 (0.3063)	loss 0.8422 (1.3165)	grad_norm 32.1769 (30.2033)	mem 4879MB
[2022-05-31 08:32:24 MetaFG_0] (main.py 265): INFO Train: [72/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2980 (0.3062)	loss 1.7149 (1.3168)	grad_norm 29.7655 (30.1724)	mem 4879MB
[2022-05-31 08:32:27 MetaFG_0] (main.py 265): INFO Train: [72/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.3002 (0.3062)	loss 0.8235 (1.3175)	grad_norm 44.2189 (30.2390)	mem 4879MB
[2022-05-31 08:32:30 MetaFG_0] (main.py 265): INFO Train: [72/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.2943 (0.3061)	loss 1.1265 (1.3186)	grad_norm 31.2191 (30.2574)	mem 4879MB
[2022-05-31 08:32:33 MetaFG_0] (main.py 265): INFO Train: [72/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2919 (0.3061)	loss 1.4684 (1.3178)	grad_norm 22.7573 (30.2514)	mem 4879MB
[2022-05-31 08:32:36 MetaFG_0] (main.py 265): INFO Train: [72/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2981 (0.3061)	loss 1.4098 (1.3160)	grad_norm 41.0353 (30.2434)	mem 4879MB
[2022-05-31 08:32:39 MetaFG_0] (main.py 265): INFO Train: [72/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2928 (0.3061)	loss 1.1990 (1.3160)	grad_norm 26.6586 (30.2255)	mem 4879MB
[2022-05-31 08:32:42 MetaFG_0] (main.py 265): INFO Train: [72/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2930 (0.3061)	loss 1.5780 (1.3155)	grad_norm 20.8618 (30.1805)	mem 4879MB
[2022-05-31 08:32:45 MetaFG_0] (main.py 265): INFO Train: [72/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2931 (0.3060)	loss 1.4854 (1.3150)	grad_norm 27.3589 (30.1719)	mem 4879MB
[2022-05-31 08:32:49 MetaFG_0] (main.py 265): INFO Train: [72/300][710/1562]	eta 0:04:21 lr 0.000005	time 0.2992 (0.3064)	loss 1.3601 (1.3139)	grad_norm 28.3328 (30.2668)	mem 4879MB
[2022-05-31 08:32:52 MetaFG_0] (main.py 265): INFO Train: [72/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2984 (0.3063)	loss 0.9755 (1.3138)	grad_norm 20.4091 (30.3010)	mem 4879MB
[2022-05-31 08:32:55 MetaFG_0] (main.py 265): INFO Train: [72/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.3000 (0.3063)	loss 1.4475 (1.3147)	grad_norm 23.1587 (30.3151)	mem 4879MB
[2022-05-31 08:32:58 MetaFG_0] (main.py 265): INFO Train: [72/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.3014 (0.3063)	loss 0.9385 (1.3148)	grad_norm 32.3817 (30.3315)	mem 4879MB
[2022-05-31 08:33:01 MetaFG_0] (main.py 265): INFO Train: [72/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.3008 (0.3062)	loss 1.4431 (1.3159)	grad_norm 20.9604 (30.3135)	mem 4879MB
[2022-05-31 08:33:04 MetaFG_0] (main.py 265): INFO Train: [72/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2981 (0.3062)	loss 1.3174 (1.3155)	grad_norm 26.0818 (30.3761)	mem 4879MB
[2022-05-31 08:33:07 MetaFG_0] (main.py 265): INFO Train: [72/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2930 (0.3062)	loss 1.0855 (1.3142)	grad_norm 39.8501 (30.4020)	mem 4879MB
[2022-05-31 08:33:10 MetaFG_0] (main.py 265): INFO Train: [72/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2982 (0.3062)	loss 1.3728 (1.3157)	grad_norm 30.9224 (30.3630)	mem 4879MB
[2022-05-31 08:33:13 MetaFG_0] (main.py 265): INFO Train: [72/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.2950 (0.3061)	loss 1.5787 (1.3156)	grad_norm 34.7644 (30.4078)	mem 4879MB
[2022-05-31 08:33:16 MetaFG_0] (main.py 265): INFO Train: [72/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2921 (0.3061)	loss 1.4247 (1.3157)	grad_norm 25.9647 (30.4310)	mem 4879MB
[2022-05-31 08:33:19 MetaFG_0] (main.py 265): INFO Train: [72/300][810/1562]	eta 0:03:50 lr 0.000005	time 0.2977 (0.3061)	loss 1.2841 (1.3140)	grad_norm 29.3955 (30.3964)	mem 4879MB
[2022-05-31 08:33:22 MetaFG_0] (main.py 265): INFO Train: [72/300][820/1562]	eta 0:03:47 lr 0.000005	time 0.2995 (0.3061)	loss 0.8657 (1.3145)	grad_norm 37.7696 (30.4203)	mem 4879MB
[2022-05-31 08:33:25 MetaFG_0] (main.py 265): INFO Train: [72/300][830/1562]	eta 0:03:44 lr 0.000005	time 0.2958 (0.3061)	loss 1.3671 (1.3153)	grad_norm 27.6110 (30.4154)	mem 4879MB
[2022-05-31 08:33:28 MetaFG_0] (main.py 265): INFO Train: [72/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2991 (0.3061)	loss 1.0689 (1.3148)	grad_norm 35.0892 (30.4031)	mem 4879MB
[2022-05-31 08:33:31 MetaFG_0] (main.py 265): INFO Train: [72/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2952 (0.3061)	loss 1.3502 (1.3155)	grad_norm 42.7098 (30.4498)	mem 4879MB
[2022-05-31 08:33:34 MetaFG_0] (main.py 265): INFO Train: [72/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2983 (0.3060)	loss 1.0300 (1.3156)	grad_norm 24.0732 (30.4505)	mem 4879MB
[2022-05-31 08:33:37 MetaFG_0] (main.py 265): INFO Train: [72/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2932 (0.3061)	loss 1.1519 (1.3153)	grad_norm 28.8140 (30.4083)	mem 4879MB
[2022-05-31 08:33:40 MetaFG_0] (main.py 265): INFO Train: [72/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2925 (0.3060)	loss 1.1334 (1.3153)	grad_norm 35.7973 (30.4085)	mem 4879MB
[2022-05-31 08:33:44 MetaFG_0] (main.py 265): INFO Train: [72/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2914 (0.3060)	loss 1.5782 (1.3148)	grad_norm 27.4881 (30.3742)	mem 4879MB
[2022-05-31 08:33:47 MetaFG_0] (main.py 265): INFO Train: [72/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2919 (0.3060)	loss 1.0663 (1.3142)	grad_norm 20.7851 (30.3388)	mem 4879MB
[2022-05-31 08:33:50 MetaFG_0] (main.py 265): INFO Train: [72/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2923 (0.3060)	loss 1.2416 (1.3134)	grad_norm 37.2520 (30.3285)	mem 4879MB
[2022-05-31 08:33:53 MetaFG_0] (main.py 265): INFO Train: [72/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2980 (0.3060)	loss 1.3822 (1.3137)	grad_norm 18.1132 (30.2816)	mem 4879MB
[2022-05-31 08:33:56 MetaFG_0] (main.py 265): INFO Train: [72/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2926 (0.3059)	loss 1.4771 (1.3138)	grad_norm 31.9348 (30.3320)	mem 4879MB
[2022-05-31 08:33:59 MetaFG_0] (main.py 265): INFO Train: [72/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.3009 (0.3059)	loss 1.3629 (1.3145)	grad_norm 31.8117 (30.3963)	mem 4879MB
[2022-05-31 08:34:02 MetaFG_0] (main.py 265): INFO Train: [72/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2939 (0.3059)	loss 1.4136 (1.3143)	grad_norm 23.4235 (30.4144)	mem 4879MB
[2022-05-31 08:34:05 MetaFG_0] (main.py 265): INFO Train: [72/300][960/1562]	eta 0:03:04 lr 0.000005	time 0.2987 (0.3059)	loss 1.4797 (1.3150)	grad_norm 32.5161 (30.4286)	mem 4879MB
[2022-05-31 08:34:08 MetaFG_0] (main.py 265): INFO Train: [72/300][970/1562]	eta 0:03:01 lr 0.000005	time 0.2984 (0.3059)	loss 1.4434 (1.3152)	grad_norm 29.4102 (30.4002)	mem 4879MB
[2022-05-31 08:34:11 MetaFG_0] (main.py 265): INFO Train: [72/300][980/1562]	eta 0:02:58 lr 0.000005	time 0.2999 (0.3059)	loss 1.1986 (1.3150)	grad_norm 46.3694 (30.4503)	mem 4879MB
[2022-05-31 08:34:14 MetaFG_0] (main.py 265): INFO Train: [72/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2985 (0.3059)	loss 1.2708 (1.3136)	grad_norm 23.1813 (30.4434)	mem 4879MB
[2022-05-31 08:34:17 MetaFG_0] (main.py 265): INFO Train: [72/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2919 (0.3059)	loss 1.5194 (1.3137)	grad_norm 43.1916 (30.4718)	mem 4879MB
[2022-05-31 08:34:20 MetaFG_0] (main.py 265): INFO Train: [72/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2939 (0.3058)	loss 1.1758 (1.3131)	grad_norm 28.3675 (30.4635)	mem 4879MB
[2022-05-31 08:34:23 MetaFG_0] (main.py 265): INFO Train: [72/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2928 (0.3058)	loss 1.2848 (1.3130)	grad_norm 25.5890 (30.4605)	mem 4879MB
[2022-05-31 08:34:26 MetaFG_0] (main.py 265): INFO Train: [72/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2923 (0.3058)	loss 1.1315 (1.3139)	grad_norm 33.0263 (30.4810)	mem 4879MB
[2022-05-31 08:34:29 MetaFG_0] (main.py 265): INFO Train: [72/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2935 (0.3058)	loss 1.0094 (1.3128)	grad_norm 23.1280 (30.4544)	mem 4879MB
[2022-05-31 08:34:32 MetaFG_0] (main.py 265): INFO Train: [72/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2926 (0.3058)	loss 1.5602 (1.3128)	grad_norm 27.5744 (30.4763)	mem 4879MB
[2022-05-31 08:34:35 MetaFG_0] (main.py 265): INFO Train: [72/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2938 (0.3058)	loss 1.4900 (1.3131)	grad_norm 18.8861 (30.4863)	mem 4879MB
[2022-05-31 08:34:38 MetaFG_0] (main.py 265): INFO Train: [72/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2990 (0.3057)	loss 1.5296 (1.3131)	grad_norm 28.1050 (30.5391)	mem 4879MB
[2022-05-31 08:34:41 MetaFG_0] (main.py 265): INFO Train: [72/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2919 (0.3057)	loss 0.6890 (1.3130)	grad_norm 19.2287 (30.5184)	mem 4879MB
[2022-05-31 08:34:44 MetaFG_0] (main.py 265): INFO Train: [72/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2920 (0.3057)	loss 0.9056 (1.3119)	grad_norm 35.9073 (30.5520)	mem 4879MB
[2022-05-31 08:34:47 MetaFG_0] (main.py 265): INFO Train: [72/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2945 (0.3057)	loss 1.2297 (1.3125)	grad_norm 23.1601 (30.5667)	mem 4879MB
[2022-05-31 08:34:50 MetaFG_0] (main.py 265): INFO Train: [72/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2918 (0.3057)	loss 1.3087 (1.3120)	grad_norm 34.9123 (30.5671)	mem 4879MB
[2022-05-31 08:34:54 MetaFG_0] (main.py 265): INFO Train: [72/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2921 (0.3056)	loss 1.6113 (1.3119)	grad_norm 28.3845 (30.6008)	mem 4879MB
[2022-05-31 08:34:57 MetaFG_0] (main.py 265): INFO Train: [72/300][1130/1562]	eta 0:02:12 lr 0.000005	time 0.2948 (0.3056)	loss 1.2328 (1.3113)	grad_norm 29.7580 (30.5980)	mem 4879MB
[2022-05-31 08:35:00 MetaFG_0] (main.py 265): INFO Train: [72/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2977 (0.3056)	loss 1.4942 (1.3112)	grad_norm 69.5719 (30.6476)	mem 4879MB
[2022-05-31 08:35:03 MetaFG_0] (main.py 265): INFO Train: [72/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2991 (0.3056)	loss 0.9302 (1.3106)	grad_norm 30.9922 (30.6354)	mem 4879MB
[2022-05-31 08:35:06 MetaFG_0] (main.py 265): INFO Train: [72/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.3004 (0.3056)	loss 1.1738 (1.3101)	grad_norm 26.2918 (30.6341)	mem 4879MB
[2022-05-31 08:35:09 MetaFG_0] (main.py 265): INFO Train: [72/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2984 (0.3056)	loss 1.0123 (1.3106)	grad_norm 24.2526 (30.6266)	mem 4879MB
[2022-05-31 08:35:12 MetaFG_0] (main.py 265): INFO Train: [72/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2941 (0.3056)	loss 1.3778 (1.3111)	grad_norm 15.7709 (30.6045)	mem 4879MB
[2022-05-31 08:35:15 MetaFG_0] (main.py 265): INFO Train: [72/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.3015 (0.3056)	loss 0.7597 (1.3107)	grad_norm 30.5395 (30.6144)	mem 4879MB
[2022-05-31 08:35:18 MetaFG_0] (main.py 265): INFO Train: [72/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2915 (0.3056)	loss 1.7878 (1.3118)	grad_norm 49.5652 (30.6135)	mem 4879MB
[2022-05-31 08:35:21 MetaFG_0] (main.py 265): INFO Train: [72/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2981 (0.3056)	loss 1.7483 (1.3118)	grad_norm 44.6989 (30.6611)	mem 4879MB
[2022-05-31 08:35:24 MetaFG_0] (main.py 265): INFO Train: [72/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2923 (0.3056)	loss 1.6599 (1.3124)	grad_norm 40.7644 (30.6866)	mem 4879MB
[2022-05-31 08:35:27 MetaFG_0] (main.py 265): INFO Train: [72/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2983 (0.3056)	loss 1.4305 (1.3119)	grad_norm 35.6827 (30.6971)	mem 4879MB
[2022-05-31 08:35:30 MetaFG_0] (main.py 265): INFO Train: [72/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2994 (0.3056)	loss 1.3461 (1.3121)	grad_norm 31.7876 (30.6552)	mem 4879MB
[2022-05-31 08:35:33 MetaFG_0] (main.py 265): INFO Train: [72/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2937 (0.3056)	loss 1.4657 (1.3114)	grad_norm 23.4214 (30.6830)	mem 4879MB
[2022-05-31 08:35:36 MetaFG_0] (main.py 265): INFO Train: [72/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2934 (0.3056)	loss 1.2159 (1.3099)	grad_norm 23.3263 (30.6899)	mem 4879MB
[2022-05-31 08:35:39 MetaFG_0] (main.py 265): INFO Train: [72/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.3005 (0.3055)	loss 1.0484 (1.3102)	grad_norm 16.5948 (30.6783)	mem 4879MB
[2022-05-31 08:35:42 MetaFG_0] (main.py 265): INFO Train: [72/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2977 (0.3055)	loss 1.5378 (1.3103)	grad_norm 57.3371 (30.7173)	mem 4879MB
[2022-05-31 08:35:45 MetaFG_0] (main.py 265): INFO Train: [72/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2922 (0.3055)	loss 1.7167 (1.3108)	grad_norm 37.0094 (30.7109)	mem 4879MB
[2022-05-31 08:35:48 MetaFG_0] (main.py 265): INFO Train: [72/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2925 (0.3055)	loss 1.3569 (1.3108)	grad_norm 32.6318 (30.6907)	mem 4879MB
[2022-05-31 08:35:51 MetaFG_0] (main.py 265): INFO Train: [72/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2974 (0.3055)	loss 1.2172 (1.3106)	grad_norm 21.6575 (30.6701)	mem 4879MB
[2022-05-31 08:35:54 MetaFG_0] (main.py 265): INFO Train: [72/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2928 (0.3055)	loss 1.2972 (1.3104)	grad_norm 30.3721 (30.6551)	mem 4879MB
[2022-05-31 08:35:57 MetaFG_0] (main.py 265): INFO Train: [72/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2928 (0.3055)	loss 1.5541 (1.3107)	grad_norm 33.0321 (30.6173)	mem 4879MB
[2022-05-31 08:36:01 MetaFG_0] (main.py 265): INFO Train: [72/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2919 (0.3055)	loss 0.9109 (1.3102)	grad_norm 22.1986 (30.6373)	mem 4879MB
[2022-05-31 08:36:04 MetaFG_0] (main.py 265): INFO Train: [72/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2991 (0.3055)	loss 1.5394 (1.3109)	grad_norm 20.3883 (30.5906)	mem 4879MB
[2022-05-31 08:36:07 MetaFG_0] (main.py 265): INFO Train: [72/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.3060 (0.3054)	loss 0.9534 (1.3102)	grad_norm 21.6353 (30.6052)	mem 4879MB
[2022-05-31 08:36:10 MetaFG_0] (main.py 265): INFO Train: [72/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2978 (0.3054)	loss 1.1460 (1.3103)	grad_norm 30.1120 (30.6136)	mem 4879MB
[2022-05-31 08:36:13 MetaFG_0] (main.py 265): INFO Train: [72/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2922 (0.3054)	loss 0.9967 (1.3104)	grad_norm 19.8808 (30.6108)	mem 4879MB
[2022-05-31 08:36:16 MetaFG_0] (main.py 265): INFO Train: [72/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2932 (0.3054)	loss 1.4752 (1.3112)	grad_norm 22.1046 (30.6163)	mem 4879MB
[2022-05-31 08:36:19 MetaFG_0] (main.py 265): INFO Train: [72/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2991 (0.3054)	loss 1.6450 (1.3114)	grad_norm 28.4196 (30.5993)	mem 4879MB
[2022-05-31 08:36:22 MetaFG_0] (main.py 265): INFO Train: [72/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2988 (0.3054)	loss 1.3595 (1.3114)	grad_norm 22.7830 (30.5697)	mem 4879MB
[2022-05-31 08:36:25 MetaFG_0] (main.py 265): INFO Train: [72/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2934 (0.3054)	loss 1.3438 (1.3104)	grad_norm 54.6617 (30.5410)	mem 4879MB
[2022-05-31 08:36:28 MetaFG_0] (main.py 265): INFO Train: [72/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2931 (0.3054)	loss 1.6337 (1.3116)	grad_norm 29.4890 (30.5527)	mem 4879MB
[2022-05-31 08:36:31 MetaFG_0] (main.py 265): INFO Train: [72/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2988 (0.3054)	loss 1.3843 (1.3122)	grad_norm 26.7451 (30.5374)	mem 4879MB
[2022-05-31 08:36:34 MetaFG_0] (main.py 265): INFO Train: [72/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2981 (0.3054)	loss 1.1344 (1.3128)	grad_norm 43.5880 (30.5385)	mem 4879MB
[2022-05-31 08:36:37 MetaFG_0] (main.py 265): INFO Train: [72/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2975 (0.3054)	loss 1.1728 (1.3131)	grad_norm 27.5569 (30.5455)	mem 4879MB
[2022-05-31 08:36:40 MetaFG_0] (main.py 265): INFO Train: [72/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2993 (0.3054)	loss 1.2559 (1.3127)	grad_norm 24.6795 (30.5346)	mem 4879MB
[2022-05-31 08:36:43 MetaFG_0] (main.py 265): INFO Train: [72/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2943 (0.3053)	loss 1.1841 (1.3124)	grad_norm 21.6374 (30.4973)	mem 4879MB
[2022-05-31 08:36:46 MetaFG_0] (main.py 265): INFO Train: [72/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.3000 (0.3053)	loss 1.5864 (1.3131)	grad_norm 24.9391 (30.4874)	mem 4879MB
[2022-05-31 08:36:49 MetaFG_0] (main.py 265): INFO Train: [72/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2990 (0.3053)	loss 1.4328 (1.3130)	grad_norm 24.6068 (30.4555)	mem 4879MB
[2022-05-31 08:36:52 MetaFG_0] (main.py 265): INFO Train: [72/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2947 (0.3053)	loss 1.2663 (1.3134)	grad_norm 23.9118 (30.4455)	mem 4879MB
[2022-05-31 08:36:55 MetaFG_0] (main.py 265): INFO Train: [72/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2942 (0.3053)	loss 1.2810 (1.3128)	grad_norm 16.0132 (30.4324)	mem 4879MB
[2022-05-31 08:36:58 MetaFG_0] (main.py 265): INFO Train: [72/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2994 (0.3053)	loss 1.5247 (1.3128)	grad_norm 20.6099 (30.4122)	mem 4879MB
[2022-05-31 08:37:01 MetaFG_0] (main.py 265): INFO Train: [72/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2989 (0.3053)	loss 1.2921 (1.3125)	grad_norm 24.7535 (30.4143)	mem 4879MB
[2022-05-31 08:37:04 MetaFG_0] (main.py 265): INFO Train: [72/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2936 (0.3053)	loss 1.4479 (1.3127)	grad_norm 24.2587 (30.4089)	mem 4879MB
[2022-05-31 08:37:07 MetaFG_0] (main.py 265): INFO Train: [72/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2920 (0.3053)	loss 0.8114 (1.3131)	grad_norm 37.0899 (30.4155)	mem 4879MB
[2022-05-31 08:37:08 MetaFG_0] (main.py 272): INFO EPOCH 72 training takes 0:07:56
[2022-05-31 08:37:08 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_72.pth saving......
[2022-05-31 08:37:09 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_72.pth saved !!!
[2022-05-31 08:37:09 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:37:10 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:37:10 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:37:11 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.613 (0.613)	Loss 0.8575 (0.8575)	Acc@1 87.500 (87.500)	Acc@5 93.750 (93.750)	Mem 4879MB
[2022-05-31 08:37:12 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.087 (0.144)	Loss 0.7047 (0.6735)	Acc@1 84.375 (86.648)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 08:37:13 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.120)	Loss 0.5933 (0.6552)	Acc@1 90.625 (87.351)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 08:37:14 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.091 (0.111)	Loss 0.7470 (0.6425)	Acc@1 78.125 (87.601)	Acc@5 100.000 (98.992)	Mem 4879MB
[2022-05-31 08:37:15 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.108)	Loss 0.7387 (0.6422)	Acc@1 90.625 (87.652)	Acc@5 96.875 (98.857)	Mem 4879MB
[2022-05-31 08:37:16 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.102 (0.105)	Loss 0.8007 (0.6379)	Acc@1 78.125 (87.623)	Acc@5 100.000 (98.897)	Mem 4879MB
[2022-05-31 08:37:17 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.089 (0.103)	Loss 0.6605 (0.6287)	Acc@1 84.375 (87.807)	Acc@5 100.000 (99.027)	Mem 4879MB
[2022-05-31 08:37:18 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.104 (0.102)	Loss 0.5282 (0.6374)	Acc@1 90.625 (87.808)	Acc@5 100.000 (98.988)	Mem 4879MB
[2022-05-31 08:37:19 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.089 (0.101)	Loss 0.6054 (0.6453)	Acc@1 87.500 (87.616)	Acc@5 100.000 (98.881)	Mem 4879MB
[2022-05-31 08:37:20 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.100)	Loss 0.7397 (0.6516)	Acc@1 87.500 (87.500)	Acc@5 100.000 (98.867)	Mem 4879MB
[2022-05-31 08:37:20 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.093 (0.100)	Loss 0.9868 (0.6467)	Acc@1 78.125 (87.593)	Acc@5 93.750 (98.886)	Mem 4879MB
[2022-05-31 08:37:21 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.095 (0.099)	Loss 0.8940 (0.6418)	Acc@1 78.125 (87.697)	Acc@5 100.000 (98.986)	Mem 4879MB
[2022-05-31 08:37:22 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.099)	Loss 0.4958 (0.6419)	Acc@1 87.500 (87.655)	Acc@5 100.000 (98.967)	Mem 4879MB
[2022-05-31 08:37:23 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.098)	Loss 0.7240 (0.6433)	Acc@1 87.500 (87.595)	Acc@5 100.000 (98.998)	Mem 4879MB
[2022-05-31 08:37:24 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.096 (0.098)	Loss 0.7641 (0.6465)	Acc@1 84.375 (87.411)	Acc@5 96.875 (99.003)	Mem 4879MB
[2022-05-31 08:37:25 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.087 (0.098)	Loss 0.7667 (0.6483)	Acc@1 81.250 (87.293)	Acc@5 100.000 (98.945)	Mem 4879MB
[2022-05-31 08:37:26 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.097)	Loss 0.6783 (0.6517)	Acc@1 84.375 (87.112)	Acc@5 100.000 (98.952)	Mem 4879MB
[2022-05-31 08:37:27 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.096 (0.097)	Loss 0.6778 (0.6518)	Acc@1 84.375 (87.116)	Acc@5 96.875 (98.940)	Mem 4879MB
[2022-05-31 08:37:28 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.087 (0.097)	Loss 0.5261 (0.6551)	Acc@1 93.750 (86.930)	Acc@5 100.000 (98.930)	Mem 4879MB
[2022-05-31 08:37:29 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.097)	Loss 0.7073 (0.6546)	Acc@1 87.500 (86.878)	Acc@5 96.875 (98.953)	Mem 4879MB
[2022-05-31 08:37:30 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.5739 (0.6524)	Acc@1 90.625 (86.940)	Acc@5 96.875 (98.974)	Mem 4879MB
[2022-05-31 08:37:31 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.096)	Loss 0.7417 (0.6516)	Acc@1 84.375 (87.026)	Acc@5 96.875 (98.978)	Mem 4879MB
[2022-05-31 08:37:32 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.087 (0.096)	Loss 0.4921 (0.6512)	Acc@1 87.500 (86.991)	Acc@5 100.000 (98.982)	Mem 4879MB
[2022-05-31 08:37:33 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.096 (0.096)	Loss 0.6886 (0.6538)	Acc@1 84.375 (86.878)	Acc@5 100.000 (98.972)	Mem 4879MB
[2022-05-31 08:37:34 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.099 (0.096)	Loss 0.7119 (0.6522)	Acc@1 87.500 (86.929)	Acc@5 96.875 (98.963)	Mem 4879MB
[2022-05-31 08:37:35 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.092 (0.096)	Loss 0.6312 (0.6539)	Acc@1 90.625 (86.877)	Acc@5 96.875 (98.929)	Mem 4879MB
[2022-05-31 08:37:35 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.096 (0.096)	Loss 0.5795 (0.6527)	Acc@1 90.625 (86.961)	Acc@5 100.000 (98.934)	Mem 4879MB
[2022-05-31 08:37:36 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.100 (0.096)	Loss 0.6301 (0.6504)	Acc@1 90.625 (87.085)	Acc@5 96.875 (98.893)	Mem 4879MB
[2022-05-31 08:37:37 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.099 (0.096)	Loss 0.6515 (0.6497)	Acc@1 87.500 (87.066)	Acc@5 100.000 (98.910)	Mem 4879MB
[2022-05-31 08:37:38 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.8105 (0.6503)	Acc@1 84.375 (87.060)	Acc@5 96.875 (98.894)	Mem 4879MB
[2022-05-31 08:37:39 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.5735 (0.6497)	Acc@1 87.500 (87.095)	Acc@5 100.000 (98.889)	Mem 4879MB
[2022-05-31 08:37:40 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5655 (0.6496)	Acc@1 93.750 (87.158)	Acc@5 100.000 (98.895)	Mem 4879MB
[2022-05-31 08:37:40 MetaFG_0] (main.py 330): INFO  * Acc@1 87.150 Acc@5 98.890
[2022-05-31 08:37:40 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.2%
[2022-05-31 08:37:40 MetaFG_0] (main.py 171): INFO Max accuracy: 87.43%
[2022-05-31 08:37:41 MetaFG_0] (main.py 265): INFO Train: [73/300][0/1562]	eta 0:23:23 lr 0.000005	time 0.8984 (0.8984)	loss 1.0530 (1.0530)	grad_norm 33.4094 (33.4094)	mem 4879MB
[2022-05-31 08:37:44 MetaFG_0] (main.py 265): INFO Train: [73/300][10/1562]	eta 0:09:30 lr 0.000005	time 0.2922 (0.3678)	loss 1.6700 (1.2579)	grad_norm 36.6772 (28.5564)	mem 4879MB
[2022-05-31 08:37:48 MetaFG_0] (main.py 265): INFO Train: [73/300][20/1562]	eta 0:08:40 lr 0.000005	time 0.2945 (0.3378)	loss 0.8093 (1.2781)	grad_norm 36.5311 (31.1964)	mem 4879MB
[2022-05-31 08:37:51 MetaFG_0] (main.py 265): INFO Train: [73/300][30/1562]	eta 0:08:21 lr 0.000005	time 0.2989 (0.3276)	loss 1.1673 (1.3100)	grad_norm 40.8045 (30.4939)	mem 4879MB
[2022-05-31 08:37:54 MetaFG_0] (main.py 265): INFO Train: [73/300][40/1562]	eta 0:08:09 lr 0.000005	time 0.3003 (0.3218)	loss 1.1193 (1.2757)	grad_norm 62.9048 (31.0623)	mem 4879MB
[2022-05-31 08:37:57 MetaFG_0] (main.py 265): INFO Train: [73/300][50/1562]	eta 0:08:01 lr 0.000005	time 0.2926 (0.3186)	loss 1.3632 (1.2873)	grad_norm 30.9046 (30.1476)	mem 4879MB
[2022-05-31 08:38:00 MetaFG_0] (main.py 265): INFO Train: [73/300][60/1562]	eta 0:07:55 lr 0.000005	time 0.2975 (0.3165)	loss 1.6176 (1.3001)	grad_norm 25.6756 (29.3718)	mem 4879MB
[2022-05-31 08:38:03 MetaFG_0] (main.py 265): INFO Train: [73/300][70/1562]	eta 0:07:49 lr 0.000005	time 0.2931 (0.3146)	loss 1.2073 (1.3000)	grad_norm 27.7268 (30.3727)	mem 4879MB
[2022-05-31 08:38:06 MetaFG_0] (main.py 265): INFO Train: [73/300][80/1562]	eta 0:07:44 lr 0.000005	time 0.2930 (0.3132)	loss 1.2967 (1.3125)	grad_norm 18.4315 (30.6291)	mem 4879MB
[2022-05-31 08:38:09 MetaFG_0] (main.py 265): INFO Train: [73/300][90/1562]	eta 0:07:39 lr 0.000005	time 0.2977 (0.3121)	loss 0.9130 (1.3025)	grad_norm 21.8821 (30.1945)	mem 4879MB
[2022-05-31 08:38:12 MetaFG_0] (main.py 265): INFO Train: [73/300][100/1562]	eta 0:07:35 lr 0.000005	time 0.2939 (0.3112)	loss 1.2228 (1.3127)	grad_norm 44.8785 (30.3728)	mem 4879MB
[2022-05-31 08:38:15 MetaFG_0] (main.py 265): INFO Train: [73/300][110/1562]	eta 0:07:30 lr 0.000005	time 0.2918 (0.3106)	loss 0.8127 (1.3084)	grad_norm 20.3147 (30.5547)	mem 4879MB
[2022-05-31 08:38:18 MetaFG_0] (main.py 265): INFO Train: [73/300][120/1562]	eta 0:07:27 lr 0.000005	time 0.2971 (0.3100)	loss 1.3702 (1.3185)	grad_norm 19.2000 (30.0384)	mem 4879MB
[2022-05-31 08:38:21 MetaFG_0] (main.py 265): INFO Train: [73/300][130/1562]	eta 0:07:23 lr 0.000005	time 0.2976 (0.3095)	loss 1.4071 (1.3209)	grad_norm 32.4825 (29.8000)	mem 4879MB
[2022-05-31 08:38:24 MetaFG_0] (main.py 265): INFO Train: [73/300][140/1562]	eta 0:07:19 lr 0.000005	time 0.2935 (0.3091)	loss 1.2460 (1.3221)	grad_norm 31.2273 (29.8472)	mem 4879MB
[2022-05-31 08:38:27 MetaFG_0] (main.py 265): INFO Train: [73/300][150/1562]	eta 0:07:15 lr 0.000005	time 0.2925 (0.3087)	loss 1.4870 (1.3187)	grad_norm 34.1158 (29.4737)	mem 4879MB
[2022-05-31 08:38:30 MetaFG_0] (main.py 265): INFO Train: [73/300][160/1562]	eta 0:07:12 lr 0.000005	time 0.2942 (0.3084)	loss 1.2160 (1.3185)	grad_norm 16.1101 (29.2211)	mem 4879MB
[2022-05-31 08:38:33 MetaFG_0] (main.py 265): INFO Train: [73/300][170/1562]	eta 0:07:09 lr 0.000005	time 0.2937 (0.3082)	loss 1.2889 (1.3209)	grad_norm 20.7671 (29.3964)	mem 4879MB
[2022-05-31 08:38:36 MetaFG_0] (main.py 265): INFO Train: [73/300][180/1562]	eta 0:07:05 lr 0.000005	time 0.2981 (0.3080)	loss 1.0729 (1.3170)	grad_norm 62.1113 (29.4074)	mem 4879MB
[2022-05-31 08:38:39 MetaFG_0] (main.py 265): INFO Train: [73/300][190/1562]	eta 0:07:02 lr 0.000005	time 0.2984 (0.3078)	loss 1.3083 (1.3197)	grad_norm 31.6714 (29.3494)	mem 4879MB
[2022-05-31 08:38:42 MetaFG_0] (main.py 265): INFO Train: [73/300][200/1562]	eta 0:06:58 lr 0.000005	time 0.2995 (0.3076)	loss 1.0385 (1.3243)	grad_norm 26.3010 (29.6192)	mem 4879MB
[2022-05-31 08:38:45 MetaFG_0] (main.py 265): INFO Train: [73/300][210/1562]	eta 0:06:55 lr 0.000005	time 0.2925 (0.3074)	loss 1.2657 (1.3262)	grad_norm 27.3009 (29.6961)	mem 4879MB
[2022-05-31 08:38:48 MetaFG_0] (main.py 265): INFO Train: [73/300][220/1562]	eta 0:06:52 lr 0.000005	time 0.2986 (0.3074)	loss 1.3157 (1.3247)	grad_norm 36.0509 (30.0400)	mem 4879MB
[2022-05-31 08:38:51 MetaFG_0] (main.py 265): INFO Train: [73/300][230/1562]	eta 0:06:49 lr 0.000005	time 0.3017 (0.3073)	loss 1.0258 (1.3204)	grad_norm 44.1498 (30.1679)	mem 4879MB
[2022-05-31 08:38:54 MetaFG_0] (main.py 265): INFO Train: [73/300][240/1562]	eta 0:06:46 lr 0.000005	time 0.2976 (0.3071)	loss 1.5403 (1.3224)	grad_norm 25.8752 (30.2501)	mem 4879MB
[2022-05-31 08:38:57 MetaFG_0] (main.py 265): INFO Train: [73/300][250/1562]	eta 0:06:42 lr 0.000005	time 0.2932 (0.3070)	loss 1.3433 (1.3220)	grad_norm 18.9508 (30.0287)	mem 4879MB
[2022-05-31 08:39:01 MetaFG_0] (main.py 265): INFO Train: [73/300][260/1562]	eta 0:06:39 lr 0.000005	time 0.3012 (0.3069)	loss 1.2847 (1.3235)	grad_norm 42.4388 (30.0748)	mem 4879MB
[2022-05-31 08:39:04 MetaFG_0] (main.py 265): INFO Train: [73/300][270/1562]	eta 0:06:36 lr 0.000005	time 0.2995 (0.3067)	loss 1.1435 (1.3241)	grad_norm 29.7368 (30.0263)	mem 4879MB
[2022-05-31 08:39:07 MetaFG_0] (main.py 265): INFO Train: [73/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2924 (0.3066)	loss 1.0154 (1.3271)	grad_norm 18.7856 (30.1216)	mem 4879MB
[2022-05-31 08:39:10 MetaFG_0] (main.py 265): INFO Train: [73/300][290/1562]	eta 0:06:29 lr 0.000005	time 0.2927 (0.3065)	loss 1.2150 (1.3261)	grad_norm 17.6506 (29.9023)	mem 4879MB
[2022-05-31 08:39:13 MetaFG_0] (main.py 265): INFO Train: [73/300][300/1562]	eta 0:06:26 lr 0.000005	time 0.3000 (0.3064)	loss 1.1646 (1.3277)	grad_norm 30.0554 (29.9199)	mem 4879MB
[2022-05-31 08:39:16 MetaFG_0] (main.py 265): INFO Train: [73/300][310/1562]	eta 0:06:23 lr 0.000005	time 0.2986 (0.3063)	loss 1.4790 (1.3301)	grad_norm 24.6847 (29.9958)	mem 4879MB
[2022-05-31 08:39:19 MetaFG_0] (main.py 265): INFO Train: [73/300][320/1562]	eta 0:06:20 lr 0.000005	time 0.2942 (0.3062)	loss 1.6553 (1.3316)	grad_norm 28.3013 (30.0658)	mem 4879MB
[2022-05-31 08:39:22 MetaFG_0] (main.py 265): INFO Train: [73/300][330/1562]	eta 0:06:17 lr 0.000005	time 0.2982 (0.3062)	loss 1.5617 (1.3341)	grad_norm 23.3794 (30.0713)	mem 4879MB
[2022-05-31 08:39:25 MetaFG_0] (main.py 265): INFO Train: [73/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.3032 (0.3062)	loss 1.5413 (1.3350)	grad_norm 17.7421 (30.0258)	mem 4879MB
[2022-05-31 08:39:28 MetaFG_0] (main.py 265): INFO Train: [73/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2986 (0.3068)	loss 1.3501 (1.3363)	grad_norm 31.9234 (30.1062)	mem 4879MB
[2022-05-31 08:39:31 MetaFG_0] (main.py 265): INFO Train: [73/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2917 (0.3067)	loss 1.6276 (1.3376)	grad_norm 26.3137 (30.1826)	mem 4879MB
[2022-05-31 08:39:34 MetaFG_0] (main.py 265): INFO Train: [73/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2936 (0.3066)	loss 1.3376 (1.3356)	grad_norm 45.9935 (30.2117)	mem 4879MB
[2022-05-31 08:39:37 MetaFG_0] (main.py 265): INFO Train: [73/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.3009 (0.3066)	loss 1.5630 (1.3354)	grad_norm 29.6354 (30.1758)	mem 4879MB
[2022-05-31 08:39:40 MetaFG_0] (main.py 265): INFO Train: [73/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2988 (0.3066)	loss 1.1154 (1.3348)	grad_norm 36.1914 (30.1134)	mem 4879MB
[2022-05-31 08:39:43 MetaFG_0] (main.py 265): INFO Train: [73/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.3015 (0.3066)	loss 1.6135 (1.3345)	grad_norm 26.6322 (30.1841)	mem 4879MB
[2022-05-31 08:39:46 MetaFG_0] (main.py 265): INFO Train: [73/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2919 (0.3066)	loss 1.5274 (1.3362)	grad_norm 20.6193 (30.1380)	mem 4879MB
[2022-05-31 08:39:49 MetaFG_0] (main.py 265): INFO Train: [73/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.3069 (0.3065)	loss 1.2761 (1.3377)	grad_norm 24.8864 (30.1056)	mem 4879MB
[2022-05-31 08:39:53 MetaFG_0] (main.py 265): INFO Train: [73/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2981 (0.3065)	loss 1.1017 (1.3350)	grad_norm 25.6718 (30.0730)	mem 4879MB
[2022-05-31 08:39:56 MetaFG_0] (main.py 265): INFO Train: [73/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2930 (0.3064)	loss 1.5040 (1.3339)	grad_norm 19.5377 (30.0446)	mem 4879MB
[2022-05-31 08:39:59 MetaFG_0] (main.py 265): INFO Train: [73/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2978 (0.3064)	loss 1.4278 (1.3354)	grad_norm 27.6017 (29.9944)	mem 4879MB
[2022-05-31 08:40:02 MetaFG_0] (main.py 265): INFO Train: [73/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2932 (0.3063)	loss 1.3053 (1.3381)	grad_norm 28.6290 (29.8963)	mem 4879MB
[2022-05-31 08:40:05 MetaFG_0] (main.py 265): INFO Train: [73/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2990 (0.3063)	loss 0.8926 (1.3365)	grad_norm 33.1592 (29.8790)	mem 4879MB
[2022-05-31 08:40:08 MetaFG_0] (main.py 265): INFO Train: [73/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2949 (0.3062)	loss 0.9076 (1.3359)	grad_norm 16.7832 (29.7963)	mem 4879MB
[2022-05-31 08:40:11 MetaFG_0] (main.py 265): INFO Train: [73/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2936 (0.3062)	loss 1.5506 (1.3375)	grad_norm 37.5432 (30.0850)	mem 4879MB
[2022-05-31 08:40:14 MetaFG_0] (main.py 265): INFO Train: [73/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2926 (0.3062)	loss 1.4281 (1.3368)	grad_norm 23.7373 (30.0795)	mem 4879MB
[2022-05-31 08:40:17 MetaFG_0] (main.py 265): INFO Train: [73/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2991 (0.3062)	loss 1.1754 (1.3365)	grad_norm 47.7128 (30.2305)	mem 4879MB
[2022-05-31 08:40:20 MetaFG_0] (main.py 265): INFO Train: [73/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.3003 (0.3061)	loss 1.0964 (1.3378)	grad_norm 32.4092 (30.2146)	mem 4879MB
[2022-05-31 08:40:23 MetaFG_0] (main.py 265): INFO Train: [73/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2994 (0.3061)	loss 1.1523 (1.3393)	grad_norm 21.9350 (30.2591)	mem 4879MB
[2022-05-31 08:40:26 MetaFG_0] (main.py 265): INFO Train: [73/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2938 (0.3061)	loss 1.4569 (1.3370)	grad_norm 26.2906 (30.3274)	mem 4879MB
[2022-05-31 08:40:29 MetaFG_0] (main.py 265): INFO Train: [73/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2936 (0.3061)	loss 1.3552 (1.3377)	grad_norm 18.0195 (30.2806)	mem 4879MB
[2022-05-31 08:40:32 MetaFG_0] (main.py 265): INFO Train: [73/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2940 (0.3060)	loss 1.2600 (1.3383)	grad_norm 40.3016 (30.3447)	mem 4879MB
[2022-05-31 08:40:35 MetaFG_0] (main.py 265): INFO Train: [73/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2986 (0.3060)	loss 1.1434 (1.3368)	grad_norm 30.6508 (30.3100)	mem 4879MB
[2022-05-31 08:40:38 MetaFG_0] (main.py 265): INFO Train: [73/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2920 (0.3060)	loss 1.6787 (1.3374)	grad_norm 44.2540 (30.3226)	mem 4879MB
[2022-05-31 08:40:41 MetaFG_0] (main.py 265): INFO Train: [73/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2928 (0.3060)	loss 0.7762 (1.3379)	grad_norm 48.4869 (30.3106)	mem 4879MB
[2022-05-31 08:40:44 MetaFG_0] (main.py 265): INFO Train: [73/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2939 (0.3060)	loss 1.4213 (1.3391)	grad_norm 48.8211 (30.3682)	mem 4879MB
[2022-05-31 08:40:47 MetaFG_0] (main.py 265): INFO Train: [73/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2949 (0.3059)	loss 1.5418 (1.3391)	grad_norm 25.5309 (30.3576)	mem 4879MB
[2022-05-31 08:40:50 MetaFG_0] (main.py 265): INFO Train: [73/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2933 (0.3059)	loss 1.3091 (1.3401)	grad_norm 27.3885 (30.2975)	mem 4879MB
[2022-05-31 08:40:53 MetaFG_0] (main.py 265): INFO Train: [73/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2936 (0.3058)	loss 1.5356 (1.3401)	grad_norm 18.3180 (30.3170)	mem 4879MB
[2022-05-31 08:40:56 MetaFG_0] (main.py 265): INFO Train: [73/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.2918 (0.3058)	loss 1.5900 (1.3413)	grad_norm 42.8749 (30.3702)	mem 4879MB
[2022-05-31 08:41:00 MetaFG_0] (main.py 265): INFO Train: [73/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2980 (0.3058)	loss 1.3198 (1.3398)	grad_norm 34.5667 (30.3498)	mem 4879MB
[2022-05-31 08:41:03 MetaFG_0] (main.py 265): INFO Train: [73/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2997 (0.3058)	loss 1.7665 (1.3418)	grad_norm 33.7935 (30.2954)	mem 4879MB
[2022-05-31 08:41:06 MetaFG_0] (main.py 265): INFO Train: [73/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2991 (0.3058)	loss 1.6144 (1.3426)	grad_norm 28.0883 (30.2758)	mem 4879MB
[2022-05-31 08:41:09 MetaFG_0] (main.py 265): INFO Train: [73/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2994 (0.3058)	loss 1.3974 (1.3431)	grad_norm 19.6593 (30.1518)	mem 4879MB
[2022-05-31 08:41:12 MetaFG_0] (main.py 265): INFO Train: [73/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.3002 (0.3058)	loss 1.5450 (1.3413)	grad_norm 29.3375 (30.1966)	mem 4879MB
[2022-05-31 08:41:15 MetaFG_0] (main.py 265): INFO Train: [73/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2919 (0.3058)	loss 1.1620 (1.3414)	grad_norm 38.0999 (nan)	mem 4879MB
[2022-05-31 08:41:18 MetaFG_0] (main.py 265): INFO Train: [73/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2933 (0.3058)	loss 1.0777 (1.3421)	grad_norm 21.4965 (nan)	mem 4879MB
[2022-05-31 08:41:21 MetaFG_0] (main.py 265): INFO Train: [73/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.3007 (0.3058)	loss 1.5662 (1.3418)	grad_norm 30.3186 (nan)	mem 4879MB
[2022-05-31 08:41:24 MetaFG_0] (main.py 265): INFO Train: [73/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2929 (0.3058)	loss 1.1594 (1.3405)	grad_norm 23.1148 (nan)	mem 4879MB
[2022-05-31 08:41:27 MetaFG_0] (main.py 265): INFO Train: [73/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2987 (0.3057)	loss 1.2580 (1.3397)	grad_norm 25.9839 (nan)	mem 4879MB
[2022-05-31 08:41:30 MetaFG_0] (main.py 265): INFO Train: [73/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2938 (0.3057)	loss 1.0964 (1.3396)	grad_norm 55.8341 (nan)	mem 4879MB
[2022-05-31 08:41:33 MetaFG_0] (main.py 265): INFO Train: [73/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2977 (0.3057)	loss 1.2008 (1.3394)	grad_norm 50.5826 (nan)	mem 4879MB
[2022-05-31 08:41:36 MetaFG_0] (main.py 265): INFO Train: [73/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2936 (0.3057)	loss 1.3981 (1.3403)	grad_norm 29.6552 (nan)	mem 4879MB
[2022-05-31 08:41:39 MetaFG_0] (main.py 265): INFO Train: [73/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.3000 (0.3057)	loss 1.1585 (1.3407)	grad_norm 24.5591 (nan)	mem 4879MB
[2022-05-31 08:41:42 MetaFG_0] (main.py 265): INFO Train: [73/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2916 (0.3057)	loss 0.9839 (1.3405)	grad_norm 38.2791 (nan)	mem 4879MB
[2022-05-31 08:41:45 MetaFG_0] (main.py 265): INFO Train: [73/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2929 (0.3057)	loss 1.4052 (1.3408)	grad_norm 21.4287 (nan)	mem 4879MB
[2022-05-31 08:41:48 MetaFG_0] (main.py 265): INFO Train: [73/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2980 (0.3056)	loss 1.2168 (1.3413)	grad_norm 18.8271 (nan)	mem 4879MB
[2022-05-31 08:41:51 MetaFG_0] (main.py 265): INFO Train: [73/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2918 (0.3056)	loss 1.3872 (1.3422)	grad_norm 30.2794 (nan)	mem 4879MB
[2022-05-31 08:41:54 MetaFG_0] (main.py 265): INFO Train: [73/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2955 (0.3056)	loss 1.3539 (1.3416)	grad_norm 26.1252 (nan)	mem 4879MB
[2022-05-31 08:41:57 MetaFG_0] (main.py 265): INFO Train: [73/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2983 (0.3056)	loss 1.4428 (1.3422)	grad_norm 30.6790 (nan)	mem 4879MB
[2022-05-31 08:42:00 MetaFG_0] (main.py 265): INFO Train: [73/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2978 (0.3056)	loss 1.1153 (1.3415)	grad_norm 58.5685 (nan)	mem 4879MB
[2022-05-31 08:42:04 MetaFG_0] (main.py 265): INFO Train: [73/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2986 (0.3056)	loss 1.5283 (1.3400)	grad_norm 26.3785 (nan)	mem 4879MB
[2022-05-31 08:42:07 MetaFG_0] (main.py 265): INFO Train: [73/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2917 (0.3056)	loss 1.3267 (1.3410)	grad_norm 23.4101 (nan)	mem 4879MB
[2022-05-31 08:42:10 MetaFG_0] (main.py 265): INFO Train: [73/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2925 (0.3055)	loss 1.2540 (1.3405)	grad_norm 13.3368 (nan)	mem 4879MB
[2022-05-31 08:42:13 MetaFG_0] (main.py 265): INFO Train: [73/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2960 (0.3055)	loss 1.6191 (1.3418)	grad_norm 34.1413 (nan)	mem 4879MB
[2022-05-31 08:42:16 MetaFG_0] (main.py 265): INFO Train: [73/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2946 (0.3055)	loss 0.8575 (1.3415)	grad_norm 39.9935 (nan)	mem 4879MB
[2022-05-31 08:42:19 MetaFG_0] (main.py 265): INFO Train: [73/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2959 (0.3055)	loss 1.3305 (1.3406)	grad_norm 26.3970 (nan)	mem 4879MB
[2022-05-31 08:42:22 MetaFG_0] (main.py 265): INFO Train: [73/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2950 (0.3054)	loss 1.4748 (1.3412)	grad_norm 21.4348 (nan)	mem 4879MB
[2022-05-31 08:42:25 MetaFG_0] (main.py 265): INFO Train: [73/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2932 (0.3054)	loss 1.5474 (1.3418)	grad_norm 32.4256 (nan)	mem 4879MB
[2022-05-31 08:42:28 MetaFG_0] (main.py 265): INFO Train: [73/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2946 (0.3054)	loss 1.4953 (1.3421)	grad_norm 46.1858 (nan)	mem 4879MB
[2022-05-31 08:42:31 MetaFG_0] (main.py 265): INFO Train: [73/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.3001 (0.3054)	loss 1.5019 (1.3420)	grad_norm 43.1428 (nan)	mem 4879MB
[2022-05-31 08:42:34 MetaFG_0] (main.py 265): INFO Train: [73/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2972 (0.3054)	loss 1.0608 (1.3413)	grad_norm 30.8650 (nan)	mem 4879MB
[2022-05-31 08:42:37 MetaFG_0] (main.py 265): INFO Train: [73/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2923 (0.3054)	loss 1.3006 (1.3403)	grad_norm 25.6008 (nan)	mem 4879MB
[2022-05-31 08:42:40 MetaFG_0] (main.py 265): INFO Train: [73/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2941 (0.3053)	loss 1.3653 (1.3408)	grad_norm 35.7631 (nan)	mem 4879MB
[2022-05-31 08:42:43 MetaFG_0] (main.py 265): INFO Train: [73/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2973 (0.3053)	loss 1.1887 (1.3404)	grad_norm 31.1400 (nan)	mem 4879MB
[2022-05-31 08:42:46 MetaFG_0] (main.py 265): INFO Train: [73/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2928 (0.3053)	loss 1.3343 (1.3402)	grad_norm 18.9362 (nan)	mem 4879MB
[2022-05-31 08:42:49 MetaFG_0] (main.py 265): INFO Train: [73/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2990 (0.3053)	loss 0.8393 (1.3385)	grad_norm 41.6442 (nan)	mem 4879MB
[2022-05-31 08:42:52 MetaFG_0] (main.py 265): INFO Train: [73/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2928 (0.3053)	loss 1.4824 (1.3383)	grad_norm 34.6114 (nan)	mem 4879MB
[2022-05-31 08:42:55 MetaFG_0] (main.py 265): INFO Train: [73/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2931 (0.3052)	loss 1.1492 (1.3377)	grad_norm 12.5268 (nan)	mem 4879MB
[2022-05-31 08:42:58 MetaFG_0] (main.py 265): INFO Train: [73/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2928 (0.3052)	loss 1.4817 (1.3385)	grad_norm 29.4676 (nan)	mem 4879MB
[2022-05-31 08:43:01 MetaFG_0] (main.py 265): INFO Train: [73/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2929 (0.3052)	loss 1.1191 (1.3377)	grad_norm 28.3993 (nan)	mem 4879MB
[2022-05-31 08:43:04 MetaFG_0] (main.py 265): INFO Train: [73/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2934 (0.3052)	loss 1.4327 (1.3366)	grad_norm 21.5846 (nan)	mem 4879MB
[2022-05-31 08:43:07 MetaFG_0] (main.py 265): INFO Train: [73/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2939 (0.3052)	loss 1.4580 (1.3369)	grad_norm 28.8029 (nan)	mem 4879MB
[2022-05-31 08:43:10 MetaFG_0] (main.py 265): INFO Train: [73/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2941 (0.3052)	loss 1.5056 (1.3367)	grad_norm 47.0692 (nan)	mem 4879MB
[2022-05-31 08:43:13 MetaFG_0] (main.py 265): INFO Train: [73/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2916 (0.3052)	loss 1.5663 (1.3365)	grad_norm 24.7930 (nan)	mem 4879MB
[2022-05-31 08:43:16 MetaFG_0] (main.py 265): INFO Train: [73/300][1100/1562]	eta 0:02:20 lr 0.000005	time 0.2980 (0.3052)	loss 0.9213 (1.3357)	grad_norm 27.1381 (nan)	mem 4879MB
[2022-05-31 08:43:19 MetaFG_0] (main.py 265): INFO Train: [73/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2999 (0.3052)	loss 1.1832 (1.3365)	grad_norm 61.5168 (nan)	mem 4879MB
[2022-05-31 08:43:23 MetaFG_0] (main.py 265): INFO Train: [73/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2978 (0.3052)	loss 0.9844 (1.3351)	grad_norm 27.9396 (nan)	mem 4879MB
[2022-05-31 08:43:26 MetaFG_0] (main.py 265): INFO Train: [73/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2929 (0.3051)	loss 1.3165 (1.3356)	grad_norm 18.2149 (nan)	mem 4879MB
[2022-05-31 08:43:29 MetaFG_0] (main.py 265): INFO Train: [73/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2933 (0.3051)	loss 1.2745 (1.3354)	grad_norm 35.0116 (nan)	mem 4879MB
[2022-05-31 08:43:32 MetaFG_0] (main.py 265): INFO Train: [73/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2919 (0.3051)	loss 1.6193 (1.3348)	grad_norm 20.3014 (nan)	mem 4879MB
[2022-05-31 08:43:35 MetaFG_0] (main.py 265): INFO Train: [73/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2948 (0.3051)	loss 0.8784 (1.3333)	grad_norm 31.9129 (nan)	mem 4879MB
[2022-05-31 08:43:38 MetaFG_0] (main.py 265): INFO Train: [73/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2927 (0.3051)	loss 1.3725 (1.3335)	grad_norm 22.1514 (nan)	mem 4879MB
[2022-05-31 08:43:41 MetaFG_0] (main.py 265): INFO Train: [73/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2986 (0.3051)	loss 1.5439 (1.3345)	grad_norm 38.0731 (nan)	mem 4879MB
[2022-05-31 08:43:44 MetaFG_0] (main.py 265): INFO Train: [73/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.3001 (0.3051)	loss 1.3949 (1.3352)	grad_norm 24.1273 (nan)	mem 4879MB
[2022-05-31 08:43:47 MetaFG_0] (main.py 265): INFO Train: [73/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2985 (0.3051)	loss 1.5298 (1.3358)	grad_norm 37.0137 (nan)	mem 4879MB
[2022-05-31 08:43:50 MetaFG_0] (main.py 265): INFO Train: [73/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2918 (0.3051)	loss 1.0408 (1.3355)	grad_norm 29.8434 (nan)	mem 4879MB
[2022-05-31 08:43:53 MetaFG_0] (main.py 265): INFO Train: [73/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.3003 (0.3051)	loss 1.3686 (1.3357)	grad_norm 28.5926 (nan)	mem 4879MB
[2022-05-31 08:43:56 MetaFG_0] (main.py 265): INFO Train: [73/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2996 (0.3051)	loss 0.6303 (1.3356)	grad_norm 20.6783 (nan)	mem 4879MB
[2022-05-31 08:43:59 MetaFG_0] (main.py 265): INFO Train: [73/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2975 (0.3051)	loss 1.5214 (1.3362)	grad_norm 53.3828 (nan)	mem 4879MB
[2022-05-31 08:44:02 MetaFG_0] (main.py 265): INFO Train: [73/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2932 (0.3051)	loss 1.6044 (1.3367)	grad_norm 38.6124 (nan)	mem 4879MB
[2022-05-31 08:44:05 MetaFG_0] (main.py 265): INFO Train: [73/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2993 (0.3051)	loss 0.8723 (1.3354)	grad_norm 30.8313 (nan)	mem 4879MB
[2022-05-31 08:44:08 MetaFG_0] (main.py 265): INFO Train: [73/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2931 (0.3051)	loss 1.6990 (1.3360)	grad_norm 27.6626 (nan)	mem 4879MB
[2022-05-31 08:44:11 MetaFG_0] (main.py 265): INFO Train: [73/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2925 (0.3051)	loss 1.5847 (1.3362)	grad_norm 33.9142 (nan)	mem 4879MB
[2022-05-31 08:44:14 MetaFG_0] (main.py 265): INFO Train: [73/300][1290/1562]	eta 0:01:22 lr 0.000005	time 0.2924 (0.3051)	loss 1.5216 (1.3368)	grad_norm 22.5268 (nan)	mem 4879MB
[2022-05-31 08:44:17 MetaFG_0] (main.py 265): INFO Train: [73/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2999 (0.3051)	loss 1.5630 (1.3369)	grad_norm 23.8161 (nan)	mem 4879MB
[2022-05-31 08:44:20 MetaFG_0] (main.py 265): INFO Train: [73/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2942 (0.3051)	loss 1.1497 (1.3367)	grad_norm 33.8635 (nan)	mem 4879MB
[2022-05-31 08:44:23 MetaFG_0] (main.py 265): INFO Train: [73/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2948 (0.3050)	loss 1.3456 (1.3374)	grad_norm 33.2181 (nan)	mem 4879MB
[2022-05-31 08:44:26 MetaFG_0] (main.py 265): INFO Train: [73/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2977 (0.3050)	loss 1.3623 (1.3375)	grad_norm 26.2168 (nan)	mem 4879MB
[2022-05-31 08:44:29 MetaFG_0] (main.py 265): INFO Train: [73/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2937 (0.3050)	loss 1.0869 (1.3374)	grad_norm 20.3667 (nan)	mem 4879MB
[2022-05-31 08:44:32 MetaFG_0] (main.py 265): INFO Train: [73/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2922 (0.3050)	loss 1.5210 (1.3375)	grad_norm 22.1200 (nan)	mem 4879MB
[2022-05-31 08:44:36 MetaFG_0] (main.py 265): INFO Train: [73/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2946 (0.3050)	loss 0.9897 (1.3372)	grad_norm 28.5012 (nan)	mem 4879MB
[2022-05-31 08:44:39 MetaFG_0] (main.py 265): INFO Train: [73/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.3011 (0.3050)	loss 0.9860 (1.3372)	grad_norm 20.9413 (nan)	mem 4879MB
[2022-05-31 08:44:42 MetaFG_0] (main.py 265): INFO Train: [73/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2936 (0.3050)	loss 1.1147 (1.3380)	grad_norm 20.1427 (nan)	mem 4879MB
[2022-05-31 08:44:45 MetaFG_0] (main.py 265): INFO Train: [73/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2933 (0.3050)	loss 0.9976 (1.3374)	grad_norm 39.0482 (nan)	mem 4879MB
[2022-05-31 08:44:48 MetaFG_0] (main.py 265): INFO Train: [73/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.3002 (0.3050)	loss 1.3815 (1.3372)	grad_norm 26.7957 (nan)	mem 4879MB
[2022-05-31 08:44:51 MetaFG_0] (main.py 265): INFO Train: [73/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2934 (0.3050)	loss 1.5022 (1.3374)	grad_norm 34.4856 (nan)	mem 4879MB
[2022-05-31 08:44:54 MetaFG_0] (main.py 265): INFO Train: [73/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2936 (0.3050)	loss 1.5699 (1.3375)	grad_norm 29.6684 (nan)	mem 4879MB
[2022-05-31 08:44:57 MetaFG_0] (main.py 265): INFO Train: [73/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2925 (0.3049)	loss 1.5644 (1.3375)	grad_norm 34.0539 (nan)	mem 4879MB
[2022-05-31 08:45:00 MetaFG_0] (main.py 265): INFO Train: [73/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2934 (0.3049)	loss 1.3896 (1.3375)	grad_norm 15.5314 (nan)	mem 4879MB
[2022-05-31 08:45:03 MetaFG_0] (main.py 265): INFO Train: [73/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2946 (0.3050)	loss 1.3584 (1.3367)	grad_norm 25.9047 (nan)	mem 4879MB
[2022-05-31 08:45:06 MetaFG_0] (main.py 265): INFO Train: [73/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2997 (0.3049)	loss 1.6374 (1.3372)	grad_norm 19.6428 (nan)	mem 4879MB
[2022-05-31 08:45:09 MetaFG_0] (main.py 265): INFO Train: [73/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2926 (0.3049)	loss 1.2318 (1.3369)	grad_norm 22.5228 (nan)	mem 4879MB
[2022-05-31 08:45:12 MetaFG_0] (main.py 265): INFO Train: [73/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2935 (0.3049)	loss 1.2760 (1.3364)	grad_norm 42.6605 (nan)	mem 4879MB
[2022-05-31 08:45:15 MetaFG_0] (main.py 265): INFO Train: [73/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2935 (0.3049)	loss 0.9650 (1.3364)	grad_norm 28.1008 (nan)	mem 4879MB
[2022-05-31 08:45:18 MetaFG_0] (main.py 265): INFO Train: [73/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2920 (0.3049)	loss 1.3936 (1.3368)	grad_norm 41.6363 (nan)	mem 4879MB
[2022-05-31 08:45:21 MetaFG_0] (main.py 265): INFO Train: [73/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2922 (0.3049)	loss 1.2252 (1.3370)	grad_norm 47.4419 (nan)	mem 4879MB
[2022-05-31 08:45:24 MetaFG_0] (main.py 265): INFO Train: [73/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2928 (0.3049)	loss 1.2841 (1.3363)	grad_norm 34.6543 (nan)	mem 4879MB
[2022-05-31 08:45:27 MetaFG_0] (main.py 265): INFO Train: [73/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2914 (0.3049)	loss 1.3855 (1.3364)	grad_norm 42.6918 (nan)	mem 4879MB
[2022-05-31 08:45:30 MetaFG_0] (main.py 265): INFO Train: [73/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2924 (0.3049)	loss 1.2241 (1.3370)	grad_norm 43.2033 (nan)	mem 4879MB
[2022-05-31 08:45:33 MetaFG_0] (main.py 265): INFO Train: [73/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2939 (0.3049)	loss 1.1988 (1.3372)	grad_norm 27.2817 (nan)	mem 4879MB
[2022-05-31 08:45:36 MetaFG_0] (main.py 265): INFO Train: [73/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2911 (0.3049)	loss 1.1374 (1.3372)	grad_norm 29.4546 (nan)	mem 4879MB
[2022-05-31 08:45:37 MetaFG_0] (main.py 272): INFO EPOCH 73 training takes 0:07:56
[2022-05-31 08:45:37 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_73.pth saving......
[2022-05-31 08:45:38 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_73.pth saved !!!
[2022-05-31 08:45:38 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:45:39 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:45:39 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:45:40 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.554 (0.554)	Loss 0.6187 (0.6187)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 08:45:41 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.099 (0.141)	Loss 0.6231 (0.6459)	Acc@1 87.500 (88.636)	Acc@5 100.000 (98.864)	Mem 4879MB
[2022-05-31 08:45:42 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.119)	Loss 0.5822 (0.6643)	Acc@1 90.625 (87.351)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 08:45:42 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.102 (0.111)	Loss 0.6057 (0.6795)	Acc@1 87.500 (87.298)	Acc@5 100.000 (98.488)	Mem 4879MB
[2022-05-31 08:45:43 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.098 (0.107)	Loss 0.9163 (0.7001)	Acc@1 78.125 (86.662)	Acc@5 96.875 (98.323)	Mem 4879MB
[2022-05-31 08:45:44 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.096 (0.105)	Loss 0.6564 (0.6993)	Acc@1 87.500 (86.581)	Acc@5 100.000 (98.407)	Mem 4879MB
[2022-05-31 08:45:45 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.098 (0.103)	Loss 0.4575 (0.6934)	Acc@1 96.875 (86.885)	Acc@5 100.000 (98.514)	Mem 4879MB
[2022-05-31 08:45:46 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.097 (0.101)	Loss 0.8564 (0.6919)	Acc@1 78.125 (86.752)	Acc@5 96.875 (98.592)	Mem 4879MB
[2022-05-31 08:45:47 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.100)	Loss 0.8311 (0.6876)	Acc@1 81.250 (87.037)	Acc@5 96.875 (98.650)	Mem 4879MB
[2022-05-31 08:45:48 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.112 (0.100)	Loss 0.7285 (0.6928)	Acc@1 84.375 (87.054)	Acc@5 100.000 (98.592)	Mem 4879MB
[2022-05-31 08:45:49 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.096 (0.100)	Loss 0.4996 (0.6885)	Acc@1 90.625 (87.067)	Acc@5 100.000 (98.670)	Mem 4879MB
[2022-05-31 08:45:50 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.094 (0.099)	Loss 0.9418 (0.6917)	Acc@1 81.250 (86.937)	Acc@5 96.875 (98.592)	Mem 4879MB
[2022-05-31 08:45:51 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.088 (0.099)	Loss 0.6479 (0.6838)	Acc@1 87.500 (87.242)	Acc@5 100.000 (98.683)	Mem 4879MB
[2022-05-31 08:45:52 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.098)	Loss 0.6730 (0.6783)	Acc@1 90.625 (87.476)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 08:45:53 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.087 (0.098)	Loss 0.7644 (0.6753)	Acc@1 78.125 (87.411)	Acc@5 100.000 (98.759)	Mem 4879MB
[2022-05-31 08:45:54 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.094 (0.097)	Loss 0.6717 (0.6729)	Acc@1 87.500 (87.438)	Acc@5 100.000 (98.779)	Mem 4879MB
[2022-05-31 08:45:55 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.088 (0.097)	Loss 0.4860 (0.6720)	Acc@1 90.625 (87.384)	Acc@5 100.000 (98.758)	Mem 4879MB
[2022-05-31 08:45:56 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.103 (0.097)	Loss 0.6941 (0.6698)	Acc@1 87.500 (87.610)	Acc@5 100.000 (98.702)	Mem 4879MB
[2022-05-31 08:45:57 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.091 (0.097)	Loss 0.7498 (0.6718)	Acc@1 87.500 (87.604)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 08:45:58 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.092 (0.097)	Loss 0.4709 (0.6714)	Acc@1 93.750 (87.664)	Acc@5 100.000 (98.707)	Mem 4879MB
[2022-05-31 08:45:58 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.094 (0.097)	Loss 0.8447 (0.6737)	Acc@1 81.250 (87.609)	Acc@5 100.000 (98.725)	Mem 4879MB
[2022-05-31 08:45:59 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 0.5001 (0.6769)	Acc@1 93.750 (87.426)	Acc@5 100.000 (98.697)	Mem 4879MB
[2022-05-31 08:46:00 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.096)	Loss 0.7212 (0.6779)	Acc@1 84.375 (87.373)	Acc@5 100.000 (98.713)	Mem 4879MB
[2022-05-31 08:46:01 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.096)	Loss 0.6314 (0.6830)	Acc@1 84.375 (87.229)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 08:46:02 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.096)	Loss 0.6273 (0.6803)	Acc@1 87.500 (87.305)	Acc@5 96.875 (98.729)	Mem 4879MB
[2022-05-31 08:46:03 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.102 (0.096)	Loss 0.7673 (0.6808)	Acc@1 87.500 (87.288)	Acc@5 96.875 (98.705)	Mem 4879MB
[2022-05-31 08:46:04 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.093 (0.096)	Loss 0.6400 (0.6801)	Acc@1 87.500 (87.261)	Acc@5 100.000 (98.743)	Mem 4879MB
[2022-05-31 08:46:05 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.093 (0.096)	Loss 0.6429 (0.6811)	Acc@1 84.375 (87.212)	Acc@5 100.000 (98.755)	Mem 4879MB
[2022-05-31 08:46:06 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.096 (0.096)	Loss 0.5159 (0.6802)	Acc@1 90.625 (87.233)	Acc@5 100.000 (98.743)	Mem 4879MB
[2022-05-31 08:46:07 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.6124 (0.6784)	Acc@1 93.750 (87.285)	Acc@5 100.000 (98.754)	Mem 4879MB
[2022-05-31 08:46:08 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.096 (0.096)	Loss 0.8245 (0.6792)	Acc@1 84.375 (87.261)	Acc@5 93.750 (98.723)	Mem 4879MB
[2022-05-31 08:46:09 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 0.5695 (0.6785)	Acc@1 96.875 (87.249)	Acc@5 96.875 (98.714)	Mem 4879MB
[2022-05-31 08:46:09 MetaFG_0] (main.py 330): INFO  * Acc@1 87.240 Acc@5 98.710
[2022-05-31 08:46:09 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.2%
[2022-05-31 08:46:09 MetaFG_0] (main.py 171): INFO Max accuracy: 87.43%
[2022-05-31 08:46:10 MetaFG_0] (main.py 265): INFO Train: [74/300][0/1562]	eta 0:27:46 lr 0.000005	time 1.0672 (1.0672)	loss 1.5658 (1.5658)	grad_norm 38.9835 (38.9835)	mem 4879MB
[2022-05-31 08:46:13 MetaFG_0] (main.py 265): INFO Train: [74/300][10/1562]	eta 0:09:49 lr 0.000005	time 0.2989 (0.3796)	loss 1.3421 (1.2968)	grad_norm 17.2674 (31.3661)	mem 4879MB
[2022-05-31 08:46:16 MetaFG_0] (main.py 265): INFO Train: [74/300][20/1562]	eta 0:08:51 lr 0.000005	time 0.2936 (0.3444)	loss 1.6388 (1.3355)	grad_norm 52.2534 (33.7141)	mem 4879MB
[2022-05-31 08:46:19 MetaFG_0] (main.py 265): INFO Train: [74/300][30/1562]	eta 0:08:34 lr 0.000005	time 0.3439 (0.3358)	loss 1.0967 (1.3215)	grad_norm 41.5623 (31.9684)	mem 4879MB
[2022-05-31 08:46:22 MetaFG_0] (main.py 265): INFO Train: [74/300][40/1562]	eta 0:08:20 lr 0.000005	time 0.2917 (0.3292)	loss 1.4359 (1.3359)	grad_norm 22.5238 (32.3514)	mem 4879MB
[2022-05-31 08:46:26 MetaFG_0] (main.py 265): INFO Train: [74/300][50/1562]	eta 0:08:10 lr 0.000005	time 0.3010 (0.3246)	loss 1.0585 (1.3249)	grad_norm 14.1393 (31.2098)	mem 4879MB
[2022-05-31 08:46:29 MetaFG_0] (main.py 265): INFO Train: [74/300][60/1562]	eta 0:08:02 lr 0.000005	time 0.2993 (0.3212)	loss 1.1209 (1.3465)	grad_norm 35.4756 (30.6164)	mem 4879MB
[2022-05-31 08:46:32 MetaFG_0] (main.py 265): INFO Train: [74/300][70/1562]	eta 0:07:55 lr 0.000005	time 0.2936 (0.3189)	loss 1.3646 (1.3351)	grad_norm 28.6163 (30.7127)	mem 4879MB
[2022-05-31 08:46:35 MetaFG_0] (main.py 265): INFO Train: [74/300][80/1562]	eta 0:07:49 lr 0.000005	time 0.2991 (0.3170)	loss 1.4706 (1.3271)	grad_norm 27.9859 (30.5032)	mem 4879MB
[2022-05-31 08:46:38 MetaFG_0] (main.py 265): INFO Train: [74/300][90/1562]	eta 0:07:44 lr 0.000005	time 0.2921 (0.3157)	loss 1.3784 (1.3293)	grad_norm 25.0024 (30.0930)	mem 4879MB
[2022-05-31 08:46:41 MetaFG_0] (main.py 265): INFO Train: [74/300][100/1562]	eta 0:07:39 lr 0.000005	time 0.2977 (0.3146)	loss 1.4129 (1.3318)	grad_norm 43.3443 (30.2245)	mem 4879MB
[2022-05-31 08:46:44 MetaFG_0] (main.py 265): INFO Train: [74/300][110/1562]	eta 0:07:35 lr 0.000005	time 0.2946 (0.3138)	loss 1.4354 (1.3415)	grad_norm 18.3051 (30.0288)	mem 4879MB
[2022-05-31 08:46:47 MetaFG_0] (main.py 265): INFO Train: [74/300][120/1562]	eta 0:07:31 lr 0.000005	time 0.2926 (0.3130)	loss 1.0529 (1.3415)	grad_norm 32.9592 (30.1762)	mem 4879MB
[2022-05-31 08:46:50 MetaFG_0] (main.py 265): INFO Train: [74/300][130/1562]	eta 0:07:27 lr 0.000005	time 0.2923 (0.3122)	loss 1.5279 (1.3326)	grad_norm 27.5284 (29.9830)	mem 4879MB
[2022-05-31 08:46:53 MetaFG_0] (main.py 265): INFO Train: [74/300][140/1562]	eta 0:07:23 lr 0.000005	time 0.2955 (0.3118)	loss 1.4603 (1.3335)	grad_norm 22.4608 (29.8783)	mem 4879MB
[2022-05-31 08:46:56 MetaFG_0] (main.py 265): INFO Train: [74/300][150/1562]	eta 0:07:19 lr 0.000005	time 0.2972 (0.3114)	loss 1.4411 (1.3296)	grad_norm 28.0965 (29.4802)	mem 4879MB
[2022-05-31 08:46:59 MetaFG_0] (main.py 265): INFO Train: [74/300][160/1562]	eta 0:07:16 lr 0.000005	time 0.2998 (0.3110)	loss 1.5566 (1.3347)	grad_norm 47.6097 (29.7749)	mem 4879MB
[2022-05-31 08:47:02 MetaFG_0] (main.py 265): INFO Train: [74/300][170/1562]	eta 0:07:12 lr 0.000005	time 0.2919 (0.3106)	loss 1.3017 (1.3372)	grad_norm 26.4054 (29.5924)	mem 4879MB
[2022-05-31 08:47:05 MetaFG_0] (main.py 265): INFO Train: [74/300][180/1562]	eta 0:07:08 lr 0.000005	time 0.2925 (0.3103)	loss 1.0230 (1.3398)	grad_norm 40.7418 (29.8897)	mem 4879MB
[2022-05-31 08:47:08 MetaFG_0] (main.py 265): INFO Train: [74/300][190/1562]	eta 0:07:05 lr 0.000005	time 0.2922 (0.3098)	loss 1.0244 (1.3356)	grad_norm 28.1489 (nan)	mem 4879MB
[2022-05-31 08:47:11 MetaFG_0] (main.py 265): INFO Train: [74/300][200/1562]	eta 0:07:01 lr 0.000005	time 0.2922 (0.3096)	loss 0.9074 (1.3362)	grad_norm 42.1859 (nan)	mem 4879MB
[2022-05-31 08:47:14 MetaFG_0] (main.py 265): INFO Train: [74/300][210/1562]	eta 0:06:58 lr 0.000005	time 0.2930 (0.3093)	loss 1.6237 (1.3364)	grad_norm 18.6162 (nan)	mem 4879MB
[2022-05-31 08:47:17 MetaFG_0] (main.py 265): INFO Train: [74/300][220/1562]	eta 0:06:54 lr 0.000005	time 0.2923 (0.3090)	loss 0.9729 (1.3348)	grad_norm 23.3333 (nan)	mem 4879MB
[2022-05-31 08:47:20 MetaFG_0] (main.py 265): INFO Train: [74/300][230/1562]	eta 0:06:51 lr 0.000005	time 0.2948 (0.3088)	loss 1.3496 (1.3352)	grad_norm 35.0866 (nan)	mem 4879MB
[2022-05-31 08:47:23 MetaFG_0] (main.py 265): INFO Train: [74/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2927 (0.3085)	loss 1.4480 (1.3334)	grad_norm 19.5862 (nan)	mem 4879MB
[2022-05-31 08:47:26 MetaFG_0] (main.py 265): INFO Train: [74/300][250/1562]	eta 0:06:44 lr 0.000005	time 0.3025 (0.3083)	loss 1.7564 (1.3341)	grad_norm 45.9711 (nan)	mem 4879MB
[2022-05-31 08:47:29 MetaFG_0] (main.py 265): INFO Train: [74/300][260/1562]	eta 0:06:41 lr 0.000005	time 0.3000 (0.3082)	loss 1.4173 (1.3330)	grad_norm 33.6523 (nan)	mem 4879MB
[2022-05-31 08:47:32 MetaFG_0] (main.py 265): INFO Train: [74/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2933 (0.3080)	loss 1.1429 (1.3310)	grad_norm 33.2361 (nan)	mem 4879MB
[2022-05-31 08:47:35 MetaFG_0] (main.py 265): INFO Train: [74/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2939 (0.3079)	loss 1.5801 (1.3254)	grad_norm 22.9107 (nan)	mem 4879MB
[2022-05-31 08:47:39 MetaFG_0] (main.py 265): INFO Train: [74/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2923 (0.3077)	loss 1.2526 (1.3262)	grad_norm 19.8755 (nan)	mem 4879MB
[2022-05-31 08:47:42 MetaFG_0] (main.py 265): INFO Train: [74/300][300/1562]	eta 0:06:28 lr 0.000005	time 0.2928 (0.3076)	loss 1.6455 (1.3302)	grad_norm 22.5928 (nan)	mem 4879MB
[2022-05-31 08:47:45 MetaFG_0] (main.py 265): INFO Train: [74/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.3003 (0.3075)	loss 1.5911 (1.3263)	grad_norm 34.3695 (nan)	mem 4879MB
[2022-05-31 08:47:48 MetaFG_0] (main.py 265): INFO Train: [74/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2995 (0.3074)	loss 1.2729 (1.3275)	grad_norm 21.8472 (nan)	mem 4879MB
[2022-05-31 08:47:51 MetaFG_0] (main.py 265): INFO Train: [74/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2926 (0.3073)	loss 1.0145 (1.3270)	grad_norm 15.2037 (nan)	mem 4879MB
[2022-05-31 08:47:54 MetaFG_0] (main.py 265): INFO Train: [74/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2926 (0.3072)	loss 1.0005 (1.3247)	grad_norm 29.3519 (nan)	mem 4879MB
[2022-05-31 08:47:57 MetaFG_0] (main.py 265): INFO Train: [74/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.2980 (0.3072)	loss 1.1372 (1.3223)	grad_norm 44.1237 (nan)	mem 4879MB
[2022-05-31 08:48:00 MetaFG_0] (main.py 265): INFO Train: [74/300][360/1562]	eta 0:06:09 lr 0.000005	time 0.2944 (0.3072)	loss 0.9190 (1.3240)	grad_norm 25.6939 (nan)	mem 4879MB
[2022-05-31 08:48:03 MetaFG_0] (main.py 265): INFO Train: [74/300][370/1562]	eta 0:06:06 lr 0.000005	time 0.2917 (0.3071)	loss 1.4696 (1.3244)	grad_norm 30.3619 (nan)	mem 4879MB
[2022-05-31 08:48:06 MetaFG_0] (main.py 265): INFO Train: [74/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2989 (0.3070)	loss 1.3203 (1.3241)	grad_norm 45.5295 (nan)	mem 4879MB
[2022-05-31 08:48:09 MetaFG_0] (main.py 265): INFO Train: [74/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2930 (0.3069)	loss 1.0380 (1.3251)	grad_norm 25.5768 (nan)	mem 4879MB
[2022-05-31 08:48:12 MetaFG_0] (main.py 265): INFO Train: [74/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2918 (0.3069)	loss 0.9444 (1.3261)	grad_norm 27.0323 (nan)	mem 4879MB
[2022-05-31 08:48:15 MetaFG_0] (main.py 265): INFO Train: [74/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.3006 (0.3068)	loss 1.3219 (1.3266)	grad_norm 18.7091 (nan)	mem 4879MB
[2022-05-31 08:48:18 MetaFG_0] (main.py 265): INFO Train: [74/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2924 (0.3068)	loss 1.5818 (1.3245)	grad_norm 37.4342 (nan)	mem 4879MB
[2022-05-31 08:48:21 MetaFG_0] (main.py 265): INFO Train: [74/300][430/1562]	eta 0:05:47 lr 0.000005	time 0.2920 (0.3067)	loss 0.9378 (1.3242)	grad_norm 21.2971 (nan)	mem 4879MB
[2022-05-31 08:48:24 MetaFG_0] (main.py 265): INFO Train: [74/300][440/1562]	eta 0:05:44 lr 0.000005	time 0.2935 (0.3067)	loss 1.4500 (1.3237)	grad_norm 30.1130 (nan)	mem 4879MB
[2022-05-31 08:48:27 MetaFG_0] (main.py 265): INFO Train: [74/300][450/1562]	eta 0:05:41 lr 0.000005	time 0.2991 (0.3067)	loss 1.6310 (1.3227)	grad_norm 44.7675 (nan)	mem 4879MB
[2022-05-31 08:48:30 MetaFG_0] (main.py 265): INFO Train: [74/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2919 (0.3066)	loss 1.3586 (1.3223)	grad_norm 19.6591 (nan)	mem 4879MB
[2022-05-31 08:48:33 MetaFG_0] (main.py 265): INFO Train: [74/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2966 (0.3065)	loss 1.4722 (1.3186)	grad_norm 24.7443 (nan)	mem 4879MB
[2022-05-31 08:48:36 MetaFG_0] (main.py 265): INFO Train: [74/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2945 (0.3064)	loss 1.3193 (1.3198)	grad_norm 24.7027 (nan)	mem 4879MB
[2022-05-31 08:48:39 MetaFG_0] (main.py 265): INFO Train: [74/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2941 (0.3064)	loss 1.3521 (1.3210)	grad_norm 19.4939 (nan)	mem 4879MB
[2022-05-31 08:48:42 MetaFG_0] (main.py 265): INFO Train: [74/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2963 (0.3063)	loss 1.5588 (1.3214)	grad_norm 42.7342 (nan)	mem 4879MB
[2022-05-31 08:48:45 MetaFG_0] (main.py 265): INFO Train: [74/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.3004 (0.3063)	loss 1.2904 (1.3192)	grad_norm 30.1724 (nan)	mem 4879MB
[2022-05-31 08:48:49 MetaFG_0] (main.py 265): INFO Train: [74/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.2927 (0.3062)	loss 1.5065 (1.3212)	grad_norm 26.7519 (nan)	mem 4879MB
[2022-05-31 08:48:52 MetaFG_0] (main.py 265): INFO Train: [74/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2927 (0.3062)	loss 1.4559 (1.3222)	grad_norm 23.1877 (nan)	mem 4879MB
[2022-05-31 08:48:55 MetaFG_0] (main.py 265): INFO Train: [74/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2984 (0.3062)	loss 1.1992 (1.3217)	grad_norm 30.4145 (nan)	mem 4879MB
[2022-05-31 08:48:58 MetaFG_0] (main.py 265): INFO Train: [74/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2919 (0.3062)	loss 1.4746 (1.3235)	grad_norm 22.3255 (nan)	mem 4879MB
[2022-05-31 08:49:01 MetaFG_0] (main.py 265): INFO Train: [74/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2926 (0.3061)	loss 1.0765 (1.3220)	grad_norm 41.3542 (nan)	mem 4879MB
[2022-05-31 08:49:04 MetaFG_0] (main.py 265): INFO Train: [74/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2931 (0.3061)	loss 1.4871 (1.3223)	grad_norm 42.8812 (nan)	mem 4879MB
[2022-05-31 08:49:07 MetaFG_0] (main.py 265): INFO Train: [74/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2936 (0.3060)	loss 0.9120 (1.3219)	grad_norm 25.2369 (nan)	mem 4879MB
[2022-05-31 08:49:10 MetaFG_0] (main.py 265): INFO Train: [74/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2995 (0.3060)	loss 1.3320 (1.3217)	grad_norm 22.7036 (nan)	mem 4879MB
[2022-05-31 08:49:13 MetaFG_0] (main.py 265): INFO Train: [74/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2941 (0.3060)	loss 1.1822 (1.3221)	grad_norm 25.8004 (nan)	mem 4879MB
[2022-05-31 08:49:16 MetaFG_0] (main.py 265): INFO Train: [74/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2923 (0.3060)	loss 1.4815 (1.3226)	grad_norm 16.5616 (nan)	mem 4879MB
[2022-05-31 08:49:19 MetaFG_0] (main.py 265): INFO Train: [74/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2943 (0.3059)	loss 1.1192 (1.3207)	grad_norm 22.7018 (nan)	mem 4879MB
[2022-05-31 08:49:22 MetaFG_0] (main.py 265): INFO Train: [74/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2976 (0.3059)	loss 1.3064 (1.3208)	grad_norm 23.3995 (nan)	mem 4879MB
[2022-05-31 08:49:25 MetaFG_0] (main.py 265): INFO Train: [74/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2982 (0.3059)	loss 1.5672 (1.3217)	grad_norm 35.9418 (nan)	mem 4879MB
[2022-05-31 08:49:28 MetaFG_0] (main.py 265): INFO Train: [74/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2929 (0.3059)	loss 1.4652 (1.3220)	grad_norm 40.6264 (nan)	mem 4879MB
[2022-05-31 08:49:31 MetaFG_0] (main.py 265): INFO Train: [74/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2987 (0.3059)	loss 1.3005 (1.3219)	grad_norm 47.9058 (nan)	mem 4879MB
[2022-05-31 08:49:34 MetaFG_0] (main.py 265): INFO Train: [74/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2966 (0.3058)	loss 1.6591 (1.3225)	grad_norm 51.1641 (nan)	mem 4879MB
[2022-05-31 08:49:37 MetaFG_0] (main.py 265): INFO Train: [74/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2984 (0.3058)	loss 1.5703 (1.3227)	grad_norm 18.8995 (nan)	mem 4879MB
[2022-05-31 08:49:40 MetaFG_0] (main.py 265): INFO Train: [74/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2918 (0.3058)	loss 1.3297 (1.3213)	grad_norm 26.8208 (nan)	mem 4879MB
[2022-05-31 08:49:43 MetaFG_0] (main.py 265): INFO Train: [74/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2996 (0.3058)	loss 1.3266 (1.3199)	grad_norm 37.2419 (nan)	mem 4879MB
[2022-05-31 08:49:46 MetaFG_0] (main.py 265): INFO Train: [74/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2983 (0.3057)	loss 1.3907 (1.3196)	grad_norm 22.8049 (nan)	mem 4879MB
[2022-05-31 08:49:49 MetaFG_0] (main.py 265): INFO Train: [74/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.3004 (0.3058)	loss 1.5482 (1.3198)	grad_norm 21.9472 (nan)	mem 4879MB
[2022-05-31 08:49:52 MetaFG_0] (main.py 265): INFO Train: [74/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2980 (0.3058)	loss 1.2500 (1.3203)	grad_norm 18.2719 (nan)	mem 4879MB
[2022-05-31 08:49:56 MetaFG_0] (main.py 265): INFO Train: [74/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2952 (0.3057)	loss 0.7887 (1.3192)	grad_norm 78.0832 (nan)	mem 4879MB
[2022-05-31 08:49:59 MetaFG_0] (main.py 265): INFO Train: [74/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2925 (0.3057)	loss 1.2335 (1.3191)	grad_norm 13.3817 (nan)	mem 4879MB
[2022-05-31 08:50:02 MetaFG_0] (main.py 265): INFO Train: [74/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2922 (0.3057)	loss 1.2532 (1.3194)	grad_norm 32.1481 (nan)	mem 4879MB
[2022-05-31 08:50:05 MetaFG_0] (main.py 265): INFO Train: [74/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2925 (0.3057)	loss 1.0317 (1.3195)	grad_norm 50.8196 (nan)	mem 4879MB
[2022-05-31 08:50:08 MetaFG_0] (main.py 265): INFO Train: [74/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2924 (0.3057)	loss 1.0627 (1.3187)	grad_norm 17.8387 (nan)	mem 4879MB
[2022-05-31 08:50:11 MetaFG_0] (main.py 265): INFO Train: [74/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2940 (0.3056)	loss 1.5843 (1.3196)	grad_norm 27.3712 (nan)	mem 4879MB
[2022-05-31 08:50:14 MetaFG_0] (main.py 265): INFO Train: [74/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2996 (0.3056)	loss 1.4225 (1.3199)	grad_norm 25.0951 (nan)	mem 4879MB
[2022-05-31 08:50:17 MetaFG_0] (main.py 265): INFO Train: [74/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2925 (0.3056)	loss 1.6330 (1.3200)	grad_norm 24.7062 (nan)	mem 4879MB
[2022-05-31 08:50:20 MetaFG_0] (main.py 265): INFO Train: [74/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2927 (0.3056)	loss 1.3533 (1.3207)	grad_norm 32.8962 (nan)	mem 4879MB
[2022-05-31 08:50:23 MetaFG_0] (main.py 265): INFO Train: [74/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2933 (0.3056)	loss 0.9075 (1.3211)	grad_norm 37.4903 (nan)	mem 4879MB
[2022-05-31 08:50:26 MetaFG_0] (main.py 265): INFO Train: [74/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2924 (0.3055)	loss 1.5507 (1.3219)	grad_norm 37.6401 (nan)	mem 4879MB
[2022-05-31 08:50:29 MetaFG_0] (main.py 265): INFO Train: [74/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2947 (0.3055)	loss 1.6843 (1.3229)	grad_norm 28.4338 (nan)	mem 4879MB
[2022-05-31 08:50:32 MetaFG_0] (main.py 265): INFO Train: [74/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2930 (0.3055)	loss 0.7587 (1.3227)	grad_norm 28.4161 (nan)	mem 4879MB
[2022-05-31 08:50:35 MetaFG_0] (main.py 265): INFO Train: [74/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2925 (0.3055)	loss 1.4079 (1.3232)	grad_norm 27.4867 (nan)	mem 4879MB
[2022-05-31 08:50:38 MetaFG_0] (main.py 265): INFO Train: [74/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.3000 (0.3055)	loss 1.3021 (1.3227)	grad_norm 11.9641 (nan)	mem 4879MB
[2022-05-31 08:50:41 MetaFG_0] (main.py 265): INFO Train: [74/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2984 (0.3055)	loss 0.9846 (1.3223)	grad_norm 37.5652 (nan)	mem 4879MB
[2022-05-31 08:50:44 MetaFG_0] (main.py 265): INFO Train: [74/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2979 (0.3055)	loss 1.5245 (1.3233)	grad_norm 27.5515 (nan)	mem 4879MB
[2022-05-31 08:50:47 MetaFG_0] (main.py 265): INFO Train: [74/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2922 (0.3054)	loss 1.1783 (1.3231)	grad_norm 43.4487 (nan)	mem 4879MB
[2022-05-31 08:50:50 MetaFG_0] (main.py 265): INFO Train: [74/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2921 (0.3054)	loss 0.9653 (1.3227)	grad_norm 35.2688 (nan)	mem 4879MB
[2022-05-31 08:50:53 MetaFG_0] (main.py 265): INFO Train: [74/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2926 (0.3054)	loss 1.1316 (1.3234)	grad_norm 37.6103 (nan)	mem 4879MB
[2022-05-31 08:50:56 MetaFG_0] (main.py 265): INFO Train: [74/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2925 (0.3054)	loss 1.7385 (1.3245)	grad_norm 23.8299 (nan)	mem 4879MB
[2022-05-31 08:50:59 MetaFG_0] (main.py 265): INFO Train: [74/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.2937 (0.3054)	loss 1.0199 (1.3230)	grad_norm 17.4246 (nan)	mem 4879MB
[2022-05-31 08:51:03 MetaFG_0] (main.py 265): INFO Train: [74/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2988 (0.3054)	loss 1.5808 (1.3229)	grad_norm 18.3350 (nan)	mem 4879MB
[2022-05-31 08:51:06 MetaFG_0] (main.py 265): INFO Train: [74/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2925 (0.3054)	loss 1.4047 (1.3232)	grad_norm 39.7795 (nan)	mem 4879MB
[2022-05-31 08:51:09 MetaFG_0] (main.py 265): INFO Train: [74/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2921 (0.3054)	loss 1.5024 (1.3229)	grad_norm 31.9317 (nan)	mem 4879MB
[2022-05-31 08:51:12 MetaFG_0] (main.py 265): INFO Train: [74/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2998 (0.3054)	loss 1.5649 (1.3232)	grad_norm 42.5971 (nan)	mem 4879MB
[2022-05-31 08:51:15 MetaFG_0] (main.py 265): INFO Train: [74/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.3003 (0.3054)	loss 1.5893 (1.3223)	grad_norm 34.1037 (nan)	mem 4879MB
[2022-05-31 08:51:18 MetaFG_0] (main.py 265): INFO Train: [74/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2926 (0.3054)	loss 1.3331 (1.3231)	grad_norm 32.5087 (nan)	mem 4879MB
[2022-05-31 08:51:21 MetaFG_0] (main.py 265): INFO Train: [74/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2927 (0.3054)	loss 1.5873 (1.3238)	grad_norm 37.9333 (nan)	mem 4879MB
[2022-05-31 08:51:24 MetaFG_0] (main.py 265): INFO Train: [74/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2983 (0.3054)	loss 1.3018 (1.3243)	grad_norm 15.7893 (nan)	mem 4879MB
[2022-05-31 08:51:27 MetaFG_0] (main.py 265): INFO Train: [74/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.3013 (0.3054)	loss 1.5970 (1.3248)	grad_norm 30.8965 (nan)	mem 4879MB
[2022-05-31 08:51:30 MetaFG_0] (main.py 265): INFO Train: [74/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2921 (0.3054)	loss 1.6722 (1.3257)	grad_norm 35.7010 (nan)	mem 4879MB
[2022-05-31 08:51:33 MetaFG_0] (main.py 265): INFO Train: [74/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2923 (0.3053)	loss 1.6443 (1.3257)	grad_norm 36.1000 (nan)	mem 4879MB
[2022-05-31 08:51:36 MetaFG_0] (main.py 265): INFO Train: [74/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2926 (0.3053)	loss 1.3120 (1.3269)	grad_norm 31.0908 (nan)	mem 4879MB
[2022-05-31 08:51:39 MetaFG_0] (main.py 265): INFO Train: [74/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2984 (0.3053)	loss 1.5105 (1.3260)	grad_norm 39.5631 (nan)	mem 4879MB
[2022-05-31 08:51:42 MetaFG_0] (main.py 265): INFO Train: [74/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2930 (0.3053)	loss 1.3443 (1.3266)	grad_norm 31.4596 (nan)	mem 4879MB
[2022-05-31 08:51:45 MetaFG_0] (main.py 265): INFO Train: [74/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.3004 (0.3053)	loss 1.0225 (1.3267)	grad_norm 36.6333 (nan)	mem 4879MB
[2022-05-31 08:51:48 MetaFG_0] (main.py 265): INFO Train: [74/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2938 (0.3053)	loss 1.3497 (1.3278)	grad_norm 28.7341 (nan)	mem 4879MB
[2022-05-31 08:51:51 MetaFG_0] (main.py 265): INFO Train: [74/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2971 (0.3052)	loss 1.2901 (1.3286)	grad_norm 32.7095 (nan)	mem 4879MB
[2022-05-31 08:51:54 MetaFG_0] (main.py 265): INFO Train: [74/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2984 (0.3052)	loss 1.6243 (1.3268)	grad_norm 26.2043 (nan)	mem 4879MB
[2022-05-31 08:51:57 MetaFG_0] (main.py 265): INFO Train: [74/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2925 (0.3052)	loss 1.1655 (1.3262)	grad_norm 50.2208 (nan)	mem 4879MB
[2022-05-31 08:52:00 MetaFG_0] (main.py 265): INFO Train: [74/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2918 (0.3052)	loss 1.6659 (1.3264)	grad_norm 32.6057 (nan)	mem 4879MB
[2022-05-31 08:52:03 MetaFG_0] (main.py 265): INFO Train: [74/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2949 (0.3052)	loss 1.1668 (1.3268)	grad_norm 23.3139 (nan)	mem 4879MB
[2022-05-31 08:52:06 MetaFG_0] (main.py 265): INFO Train: [74/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2997 (0.3052)	loss 1.1945 (1.3270)	grad_norm 24.0950 (nan)	mem 4879MB
[2022-05-31 08:52:09 MetaFG_0] (main.py 265): INFO Train: [74/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.3041 (0.3052)	loss 1.3246 (1.3265)	grad_norm 19.6905 (nan)	mem 4879MB
[2022-05-31 08:52:12 MetaFG_0] (main.py 265): INFO Train: [74/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2990 (0.3052)	loss 0.7125 (1.3263)	grad_norm 22.9961 (nan)	mem 4879MB
[2022-05-31 08:52:15 MetaFG_0] (main.py 265): INFO Train: [74/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2950 (0.3052)	loss 0.8396 (1.3258)	grad_norm 18.2814 (nan)	mem 4879MB
[2022-05-31 08:52:19 MetaFG_0] (main.py 265): INFO Train: [74/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2958 (0.3052)	loss 1.2763 (1.3266)	grad_norm 22.4686 (nan)	mem 4879MB
[2022-05-31 08:52:22 MetaFG_0] (main.py 265): INFO Train: [74/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2931 (0.3052)	loss 1.0126 (1.3268)	grad_norm 46.8682 (nan)	mem 4879MB
[2022-05-31 08:52:25 MetaFG_0] (main.py 265): INFO Train: [74/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2944 (0.3052)	loss 1.3890 (1.3272)	grad_norm 32.4164 (nan)	mem 4879MB
[2022-05-31 08:52:28 MetaFG_0] (main.py 265): INFO Train: [74/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2984 (0.3052)	loss 1.2081 (1.3271)	grad_norm 28.4123 (nan)	mem 4879MB
[2022-05-31 08:52:31 MetaFG_0] (main.py 265): INFO Train: [74/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2928 (0.3052)	loss 1.4866 (1.3275)	grad_norm 20.5119 (nan)	mem 4879MB
[2022-05-31 08:52:34 MetaFG_0] (main.py 265): INFO Train: [74/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2950 (0.3052)	loss 1.0522 (1.3270)	grad_norm 38.1988 (nan)	mem 4879MB
[2022-05-31 08:52:37 MetaFG_0] (main.py 265): INFO Train: [74/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2982 (0.3052)	loss 1.1735 (1.3266)	grad_norm 29.4887 (nan)	mem 4879MB
[2022-05-31 08:52:40 MetaFG_0] (main.py 265): INFO Train: [74/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.3038 (0.3052)	loss 1.0537 (1.3269)	grad_norm 43.3167 (nan)	mem 4879MB
[2022-05-31 08:52:43 MetaFG_0] (main.py 265): INFO Train: [74/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.3030 (0.3052)	loss 1.5289 (1.3263)	grad_norm 42.9955 (nan)	mem 4879MB
[2022-05-31 08:52:46 MetaFG_0] (main.py 265): INFO Train: [74/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2915 (0.3052)	loss 1.5312 (1.3263)	grad_norm 50.9016 (nan)	mem 4879MB
[2022-05-31 08:52:49 MetaFG_0] (main.py 265): INFO Train: [74/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2993 (0.3052)	loss 1.7508 (1.3262)	grad_norm 151.7284 (nan)	mem 4879MB
[2022-05-31 08:52:52 MetaFG_0] (main.py 265): INFO Train: [74/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2930 (0.3051)	loss 1.3481 (1.3266)	grad_norm 54.8814 (nan)	mem 4879MB
[2022-05-31 08:52:55 MetaFG_0] (main.py 265): INFO Train: [74/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2922 (0.3051)	loss 1.1800 (1.3264)	grad_norm 35.1727 (nan)	mem 4879MB
[2022-05-31 08:52:58 MetaFG_0] (main.py 265): INFO Train: [74/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2943 (0.3053)	loss 1.4889 (1.3261)	grad_norm 29.0033 (nan)	mem 4879MB
[2022-05-31 08:53:01 MetaFG_0] (main.py 265): INFO Train: [74/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2940 (0.3053)	loss 1.4607 (1.3269)	grad_norm 29.5626 (nan)	mem 4879MB
[2022-05-31 08:53:04 MetaFG_0] (main.py 265): INFO Train: [74/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2987 (0.3053)	loss 1.3636 (1.3272)	grad_norm 20.1824 (nan)	mem 4879MB
[2022-05-31 08:53:07 MetaFG_0] (main.py 265): INFO Train: [74/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2942 (0.3053)	loss 1.0342 (1.3273)	grad_norm 21.7584 (nan)	mem 4879MB
[2022-05-31 08:53:11 MetaFG_0] (main.py 265): INFO Train: [74/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2939 (0.3053)	loss 1.5777 (1.3269)	grad_norm 18.9972 (nan)	mem 4879MB
[2022-05-31 08:53:14 MetaFG_0] (main.py 265): INFO Train: [74/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2991 (0.3053)	loss 0.9292 (1.3271)	grad_norm 23.7682 (nan)	mem 4879MB
[2022-05-31 08:53:17 MetaFG_0] (main.py 265): INFO Train: [74/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2976 (0.3053)	loss 1.6697 (1.3277)	grad_norm 31.0080 (nan)	mem 4879MB
[2022-05-31 08:53:20 MetaFG_0] (main.py 265): INFO Train: [74/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2945 (0.3053)	loss 1.5059 (1.3275)	grad_norm 32.8292 (nan)	mem 4879MB
[2022-05-31 08:53:23 MetaFG_0] (main.py 265): INFO Train: [74/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2939 (0.3052)	loss 1.1132 (1.3268)	grad_norm 22.3922 (nan)	mem 4879MB
[2022-05-31 08:53:26 MetaFG_0] (main.py 265): INFO Train: [74/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2927 (0.3052)	loss 1.5132 (1.3278)	grad_norm 31.8723 (nan)	mem 4879MB
[2022-05-31 08:53:29 MetaFG_0] (main.py 265): INFO Train: [74/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2939 (0.3052)	loss 1.2182 (1.3279)	grad_norm 52.5193 (nan)	mem 4879MB
[2022-05-31 08:53:32 MetaFG_0] (main.py 265): INFO Train: [74/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2993 (0.3052)	loss 1.0971 (1.3275)	grad_norm 24.6223 (nan)	mem 4879MB
[2022-05-31 08:53:35 MetaFG_0] (main.py 265): INFO Train: [74/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2933 (0.3052)	loss 1.2513 (1.3277)	grad_norm 29.4478 (nan)	mem 4879MB
[2022-05-31 08:53:38 MetaFG_0] (main.py 265): INFO Train: [74/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2925 (0.3052)	loss 1.3410 (1.3269)	grad_norm 33.5761 (nan)	mem 4879MB
[2022-05-31 08:53:41 MetaFG_0] (main.py 265): INFO Train: [74/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2931 (0.3052)	loss 1.2553 (1.3274)	grad_norm 21.4274 (nan)	mem 4879MB
[2022-05-31 08:53:44 MetaFG_0] (main.py 265): INFO Train: [74/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2990 (0.3052)	loss 1.0089 (1.3273)	grad_norm 46.6568 (nan)	mem 4879MB
[2022-05-31 08:53:47 MetaFG_0] (main.py 265): INFO Train: [74/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2920 (0.3052)	loss 1.5723 (1.3270)	grad_norm 47.5100 (nan)	mem 4879MB
[2022-05-31 08:53:50 MetaFG_0] (main.py 265): INFO Train: [74/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2934 (0.3052)	loss 1.4797 (1.3274)	grad_norm 26.9049 (nan)	mem 4879MB
[2022-05-31 08:53:53 MetaFG_0] (main.py 265): INFO Train: [74/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2933 (0.3052)	loss 1.4963 (1.3278)	grad_norm 22.4093 (nan)	mem 4879MB
[2022-05-31 08:53:56 MetaFG_0] (main.py 265): INFO Train: [74/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2982 (0.3052)	loss 1.4935 (1.3278)	grad_norm 33.1344 (nan)	mem 4879MB
[2022-05-31 08:53:59 MetaFG_0] (main.py 265): INFO Train: [74/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2923 (0.3051)	loss 1.5747 (1.3276)	grad_norm 36.3020 (nan)	mem 4879MB
[2022-05-31 08:54:02 MetaFG_0] (main.py 265): INFO Train: [74/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2988 (0.3051)	loss 1.5417 (1.3280)	grad_norm 38.2249 (nan)	mem 4879MB
[2022-05-31 08:54:05 MetaFG_0] (main.py 265): INFO Train: [74/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2923 (0.3051)	loss 1.4245 (1.3281)	grad_norm 20.9176 (nan)	mem 4879MB
[2022-05-31 08:54:06 MetaFG_0] (main.py 272): INFO EPOCH 74 training takes 0:07:56
[2022-05-31 08:54:06 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_74.pth saving......
[2022-05-31 08:54:06 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_74.pth saved !!!
[2022-05-31 08:54:06 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 08:54:08 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 08:54:08 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 08:54:09 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.635 (0.635)	Loss 0.6260 (0.6260)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 08:54:10 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.146)	Loss 0.3899 (0.5798)	Acc@1 93.750 (88.352)	Acc@5 100.000 (99.432)	Mem 4879MB
[2022-05-31 08:54:11 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.096 (0.121)	Loss 0.4145 (0.5886)	Acc@1 93.750 (87.500)	Acc@5 100.000 (99.107)	Mem 4879MB
[2022-05-31 08:54:12 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.087 (0.113)	Loss 0.5465 (0.5990)	Acc@1 90.625 (86.593)	Acc@5 100.000 (99.093)	Mem 4879MB
[2022-05-31 08:54:13 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.108)	Loss 0.5752 (0.6062)	Acc@1 87.500 (86.585)	Acc@5 100.000 (99.085)	Mem 4879MB
[2022-05-31 08:54:14 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.094 (0.106)	Loss 0.7106 (0.5928)	Acc@1 81.250 (87.132)	Acc@5 100.000 (99.265)	Mem 4879MB
[2022-05-31 08:54:15 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.104)	Loss 0.6127 (0.5952)	Acc@1 87.500 (87.346)	Acc@5 100.000 (98.975)	Mem 4879MB
[2022-05-31 08:54:16 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.093 (0.102)	Loss 0.6484 (0.6027)	Acc@1 84.375 (87.148)	Acc@5 100.000 (98.856)	Mem 4879MB
[2022-05-31 08:54:17 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.096 (0.101)	Loss 0.6227 (0.5940)	Acc@1 84.375 (87.577)	Acc@5 100.000 (98.804)	Mem 4879MB
[2022-05-31 08:54:17 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.098 (0.101)	Loss 0.8111 (0.5976)	Acc@1 81.250 (87.397)	Acc@5 100.000 (98.764)	Mem 4879MB
[2022-05-31 08:54:18 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.4789 (0.5938)	Acc@1 90.625 (87.593)	Acc@5 96.875 (98.762)	Mem 4879MB
[2022-05-31 08:54:19 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.092 (0.100)	Loss 0.4919 (0.5952)	Acc@1 90.625 (87.528)	Acc@5 100.000 (98.761)	Mem 4879MB
[2022-05-31 08:54:20 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.087 (0.099)	Loss 0.4595 (0.6009)	Acc@1 90.625 (87.371)	Acc@5 100.000 (98.760)	Mem 4879MB
[2022-05-31 08:54:21 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.087 (0.099)	Loss 0.5604 (0.6038)	Acc@1 87.500 (87.285)	Acc@5 100.000 (98.760)	Mem 4879MB
[2022-05-31 08:54:22 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.098)	Loss 0.7550 (0.6058)	Acc@1 87.500 (87.301)	Acc@5 96.875 (98.692)	Mem 4879MB
[2022-05-31 08:54:23 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.093 (0.098)	Loss 0.6718 (0.6053)	Acc@1 84.375 (87.314)	Acc@5 96.875 (98.675)	Mem 4879MB
[2022-05-31 08:54:24 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.096 (0.098)	Loss 0.7010 (0.6042)	Acc@1 81.250 (87.364)	Acc@5 96.875 (98.680)	Mem 4879MB
[2022-05-31 08:54:25 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 0.4193 (0.6031)	Acc@1 96.875 (87.445)	Acc@5 100.000 (98.629)	Mem 4879MB
[2022-05-31 08:54:26 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.097)	Loss 0.5308 (0.6033)	Acc@1 90.625 (87.500)	Acc@5 100.000 (98.653)	Mem 4879MB
[2022-05-31 08:54:27 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.103 (0.097)	Loss 0.6169 (0.6025)	Acc@1 84.375 (87.385)	Acc@5 96.875 (98.707)	Mem 4879MB
[2022-05-31 08:54:28 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.100 (0.097)	Loss 0.8288 (0.6021)	Acc@1 81.250 (87.453)	Acc@5 93.750 (98.694)	Mem 4879MB
[2022-05-31 08:54:29 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.097)	Loss 0.3833 (0.5977)	Acc@1 93.750 (87.559)	Acc@5 100.000 (98.741)	Mem 4879MB
[2022-05-31 08:54:30 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.093 (0.097)	Loss 0.6399 (0.5972)	Acc@1 93.750 (87.528)	Acc@5 100.000 (98.756)	Mem 4879MB
[2022-05-31 08:54:31 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.097)	Loss 0.4756 (0.5963)	Acc@1 93.750 (87.554)	Acc@5 100.000 (98.769)	Mem 4879MB
[2022-05-31 08:54:32 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.102 (0.097)	Loss 0.7544 (0.5981)	Acc@1 84.375 (87.422)	Acc@5 100.000 (98.820)	Mem 4879MB
[2022-05-31 08:54:33 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.098 (0.096)	Loss 0.5629 (0.5947)	Acc@1 90.625 (87.488)	Acc@5 96.875 (98.830)	Mem 4879MB
[2022-05-31 08:54:33 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.096)	Loss 0.8104 (0.5956)	Acc@1 78.125 (87.440)	Acc@5 96.875 (98.815)	Mem 4879MB
[2022-05-31 08:54:34 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 0.4556 (0.5963)	Acc@1 87.500 (87.373)	Acc@5 100.000 (98.801)	Mem 4879MB
[2022-05-31 08:54:35 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.096)	Loss 0.5617 (0.5958)	Acc@1 87.500 (87.433)	Acc@5 100.000 (98.777)	Mem 4879MB
[2022-05-31 08:54:36 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.096)	Loss 0.4217 (0.5951)	Acc@1 96.875 (87.468)	Acc@5 100.000 (98.797)	Mem 4879MB
[2022-05-31 08:54:37 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.091 (0.096)	Loss 0.7809 (0.5937)	Acc@1 81.250 (87.531)	Acc@5 93.750 (98.796)	Mem 4879MB
[2022-05-31 08:54:38 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5248 (0.5941)	Acc@1 90.625 (87.500)	Acc@5 100.000 (98.794)	Mem 4879MB
[2022-05-31 08:54:38 MetaFG_0] (main.py 330): INFO  * Acc@1 87.520 Acc@5 98.800
[2022-05-31 08:54:38 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.5%
[2022-05-31 08:54:38 MetaFG_0] (main.py 171): INFO Max accuracy: 87.52%
[2022-05-31 08:54:39 MetaFG_0] (main.py 265): INFO Train: [75/300][0/1562]	eta 0:25:07 lr 0.000005	time 0.9651 (0.9651)	loss 1.2316 (1.2316)	grad_norm 28.7066 (28.7066)	mem 4879MB
[2022-05-31 08:54:42 MetaFG_0] (main.py 265): INFO Train: [75/300][10/1562]	eta 0:09:37 lr 0.000005	time 0.2938 (0.3718)	loss 1.4565 (1.2110)	grad_norm 19.4648 (27.9944)	mem 4879MB
[2022-05-31 08:54:45 MetaFG_0] (main.py 265): INFO Train: [75/300][20/1562]	eta 0:08:42 lr 0.000005	time 0.2934 (0.3387)	loss 1.1138 (1.2648)	grad_norm 31.2315 (28.6918)	mem 4879MB
[2022-05-31 08:54:49 MetaFG_0] (main.py 265): INFO Train: [75/300][30/1562]	eta 0:08:22 lr 0.000005	time 0.2941 (0.3283)	loss 1.2879 (1.2962)	grad_norm 45.1576 (29.4584)	mem 4879MB
[2022-05-31 08:54:52 MetaFG_0] (main.py 265): INFO Train: [75/300][40/1562]	eta 0:08:11 lr 0.000005	time 0.2932 (0.3229)	loss 1.6283 (1.3036)	grad_norm 26.5290 (29.7855)	mem 4879MB
[2022-05-31 08:54:55 MetaFG_0] (main.py 265): INFO Train: [75/300][50/1562]	eta 0:08:02 lr 0.000005	time 0.2932 (0.3191)	loss 1.4587 (1.3247)	grad_norm 51.8636 (31.1250)	mem 4879MB
[2022-05-31 08:54:58 MetaFG_0] (main.py 265): INFO Train: [75/300][60/1562]	eta 0:07:56 lr 0.000005	time 0.3017 (0.3170)	loss 1.6750 (1.3313)	grad_norm 24.5819 (31.3913)	mem 4879MB
[2022-05-31 08:55:01 MetaFG_0] (main.py 265): INFO Train: [75/300][70/1562]	eta 0:07:50 lr 0.000005	time 0.3005 (0.3154)	loss 1.0090 (1.3209)	grad_norm 19.1536 (31.7134)	mem 4879MB
[2022-05-31 08:55:04 MetaFG_0] (main.py 265): INFO Train: [75/300][80/1562]	eta 0:07:45 lr 0.000005	time 0.2926 (0.3141)	loss 1.3374 (1.3233)	grad_norm 29.9461 (31.5939)	mem 4879MB
[2022-05-31 08:55:07 MetaFG_0] (main.py 265): INFO Train: [75/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.2992 (0.3131)	loss 1.6096 (1.3281)	grad_norm 16.9414 (30.9149)	mem 4879MB
[2022-05-31 08:55:10 MetaFG_0] (main.py 265): INFO Train: [75/300][100/1562]	eta 0:07:36 lr 0.000005	time 0.2986 (0.3126)	loss 1.0095 (1.3181)	grad_norm 50.5128 (31.3020)	mem 4879MB
[2022-05-31 08:55:13 MetaFG_0] (main.py 265): INFO Train: [75/300][110/1562]	eta 0:07:32 lr 0.000005	time 0.2976 (0.3119)	loss 1.1373 (1.3149)	grad_norm 39.4561 (30.9412)	mem 4879MB
[2022-05-31 08:55:16 MetaFG_0] (main.py 265): INFO Train: [75/300][120/1562]	eta 0:07:28 lr 0.000005	time 0.3001 (0.3112)	loss 1.6021 (1.3135)	grad_norm 38.1613 (31.2127)	mem 4879MB
[2022-05-31 08:55:19 MetaFG_0] (main.py 265): INFO Train: [75/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.2944 (0.3106)	loss 1.0760 (1.3149)	grad_norm 17.3251 (31.0782)	mem 4879MB
[2022-05-31 08:55:22 MetaFG_0] (main.py 265): INFO Train: [75/300][140/1562]	eta 0:07:21 lr 0.000005	time 0.2916 (0.3102)	loss 1.2664 (1.3178)	grad_norm 50.4880 (31.2563)	mem 4879MB
[2022-05-31 08:55:25 MetaFG_0] (main.py 265): INFO Train: [75/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2981 (0.3100)	loss 1.4695 (1.3172)	grad_norm 17.2862 (30.8142)	mem 4879MB
[2022-05-31 08:55:28 MetaFG_0] (main.py 265): INFO Train: [75/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.2922 (0.3096)	loss 1.3470 (1.3167)	grad_norm 23.3776 (30.6886)	mem 4879MB
[2022-05-31 08:55:31 MetaFG_0] (main.py 265): INFO Train: [75/300][170/1562]	eta 0:07:10 lr 0.000005	time 0.2928 (0.3092)	loss 1.4848 (1.3172)	grad_norm 28.4327 (30.9905)	mem 4879MB
[2022-05-31 08:55:34 MetaFG_0] (main.py 265): INFO Train: [75/300][180/1562]	eta 0:07:06 lr 0.000005	time 0.2987 (0.3089)	loss 1.0284 (1.3204)	grad_norm 32.1540 (30.9300)	mem 4879MB
[2022-05-31 08:55:37 MetaFG_0] (main.py 265): INFO Train: [75/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2937 (0.3086)	loss 1.1023 (1.3162)	grad_norm 24.6233 (30.7769)	mem 4879MB
[2022-05-31 08:55:40 MetaFG_0] (main.py 265): INFO Train: [75/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2981 (0.3084)	loss 0.8227 (1.3100)	grad_norm 28.3964 (30.4494)	mem 4879MB
[2022-05-31 08:55:43 MetaFG_0] (main.py 265): INFO Train: [75/300][210/1562]	eta 0:06:56 lr 0.000005	time 0.2924 (0.3081)	loss 1.4501 (1.3097)	grad_norm 32.7228 (30.2309)	mem 4879MB
[2022-05-31 08:55:46 MetaFG_0] (main.py 265): INFO Train: [75/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2921 (0.3079)	loss 1.2945 (1.3048)	grad_norm 24.8872 (30.1667)	mem 4879MB
[2022-05-31 08:55:49 MetaFG_0] (main.py 265): INFO Train: [75/300][230/1562]	eta 0:06:49 lr 0.000005	time 0.2934 (0.3077)	loss 1.2634 (1.3098)	grad_norm 26.8248 (30.1405)	mem 4879MB
[2022-05-31 08:55:52 MetaFG_0] (main.py 265): INFO Train: [75/300][240/1562]	eta 0:06:46 lr 0.000005	time 0.2997 (0.3076)	loss 1.4197 (1.3126)	grad_norm 27.1832 (29.9457)	mem 4879MB
[2022-05-31 08:55:56 MetaFG_0] (main.py 265): INFO Train: [75/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2932 (0.3076)	loss 1.3978 (1.3120)	grad_norm 42.5836 (30.0249)	mem 4879MB
[2022-05-31 08:55:59 MetaFG_0] (main.py 265): INFO Train: [75/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2978 (0.3075)	loss 1.6164 (1.3139)	grad_norm 27.2856 (29.9101)	mem 4879MB
[2022-05-31 08:56:02 MetaFG_0] (main.py 265): INFO Train: [75/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2925 (0.3074)	loss 1.4983 (1.3152)	grad_norm 26.5556 (29.9334)	mem 4879MB
[2022-05-31 08:56:05 MetaFG_0] (main.py 265): INFO Train: [75/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2971 (0.3073)	loss 1.5451 (1.3145)	grad_norm 23.1424 (29.9775)	mem 4879MB
[2022-05-31 08:56:08 MetaFG_0] (main.py 265): INFO Train: [75/300][290/1562]	eta 0:06:30 lr 0.000005	time 0.2991 (0.3072)	loss 1.3790 (1.3146)	grad_norm 31.4525 (30.0770)	mem 4879MB
[2022-05-31 08:56:11 MetaFG_0] (main.py 265): INFO Train: [75/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.3010 (0.3071)	loss 1.2229 (1.3150)	grad_norm 24.1027 (29.9220)	mem 4879MB
[2022-05-31 08:56:14 MetaFG_0] (main.py 265): INFO Train: [75/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2976 (0.3070)	loss 1.0315 (1.3124)	grad_norm 22.5187 (30.0430)	mem 4879MB
[2022-05-31 08:56:17 MetaFG_0] (main.py 265): INFO Train: [75/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.3017 (0.3070)	loss 1.3388 (1.3135)	grad_norm 22.6214 (30.1159)	mem 4879MB
[2022-05-31 08:56:20 MetaFG_0] (main.py 265): INFO Train: [75/300][330/1562]	eta 0:06:18 lr 0.000005	time 0.2984 (0.3069)	loss 1.1279 (1.3128)	grad_norm 57.0291 (30.2093)	mem 4879MB
[2022-05-31 08:56:23 MetaFG_0] (main.py 265): INFO Train: [75/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.3051 (0.3069)	loss 1.5475 (1.3150)	grad_norm 68.3210 (30.2894)	mem 4879MB
[2022-05-31 08:56:26 MetaFG_0] (main.py 265): INFO Train: [75/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2987 (0.3069)	loss 1.5978 (1.3143)	grad_norm 32.6665 (30.2233)	mem 4879MB
[2022-05-31 08:56:29 MetaFG_0] (main.py 265): INFO Train: [75/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2937 (0.3067)	loss 1.7420 (1.3192)	grad_norm 70.4542 (30.3297)	mem 4879MB
[2022-05-31 08:56:32 MetaFG_0] (main.py 265): INFO Train: [75/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2975 (0.3067)	loss 1.2197 (1.3209)	grad_norm 44.8010 (30.4538)	mem 4879MB
[2022-05-31 08:56:35 MetaFG_0] (main.py 265): INFO Train: [75/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2925 (0.3066)	loss 1.1248 (1.3197)	grad_norm 34.5803 (30.5523)	mem 4879MB
[2022-05-31 08:56:38 MetaFG_0] (main.py 265): INFO Train: [75/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2979 (0.3066)	loss 1.4929 (1.3181)	grad_norm 20.8454 (30.4356)	mem 4879MB
[2022-05-31 08:56:41 MetaFG_0] (main.py 265): INFO Train: [75/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2943 (0.3065)	loss 1.3417 (1.3176)	grad_norm 28.6983 (30.5916)	mem 4879MB
[2022-05-31 08:56:44 MetaFG_0] (main.py 265): INFO Train: [75/300][410/1562]	eta 0:05:52 lr 0.000005	time 0.2916 (0.3064)	loss 1.3586 (1.3202)	grad_norm 31.1295 (30.7246)	mem 4879MB
[2022-05-31 08:56:47 MetaFG_0] (main.py 265): INFO Train: [75/300][420/1562]	eta 0:05:49 lr 0.000005	time 0.2993 (0.3064)	loss 1.3285 (1.3217)	grad_norm 127.0819 (30.8763)	mem 4879MB
[2022-05-31 08:56:50 MetaFG_0] (main.py 265): INFO Train: [75/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2983 (0.3063)	loss 1.3049 (1.3200)	grad_norm 19.9370 (30.7396)	mem 4879MB
[2022-05-31 08:56:53 MetaFG_0] (main.py 265): INFO Train: [75/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2959 (0.3063)	loss 1.6510 (1.3210)	grad_norm 21.2224 (30.7198)	mem 4879MB
[2022-05-31 08:56:56 MetaFG_0] (main.py 265): INFO Train: [75/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2933 (0.3063)	loss 0.9808 (1.3189)	grad_norm 21.2268 (30.7298)	mem 4879MB
[2022-05-31 08:57:00 MetaFG_0] (main.py 265): INFO Train: [75/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2924 (0.3062)	loss 0.7105 (1.3168)	grad_norm 31.0893 (30.5912)	mem 4879MB
[2022-05-31 08:57:03 MetaFG_0] (main.py 265): INFO Train: [75/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2927 (0.3062)	loss 1.5722 (1.3180)	grad_norm 28.7954 (30.6715)	mem 4879MB
[2022-05-31 08:57:06 MetaFG_0] (main.py 265): INFO Train: [75/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2980 (0.3061)	loss 0.9293 (1.3172)	grad_norm 41.6844 (30.7248)	mem 4879MB
[2022-05-31 08:57:09 MetaFG_0] (main.py 265): INFO Train: [75/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2927 (0.3061)	loss 1.6140 (1.3175)	grad_norm 42.7978 (30.6903)	mem 4879MB
[2022-05-31 08:57:12 MetaFG_0] (main.py 265): INFO Train: [75/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2929 (0.3060)	loss 1.5508 (1.3165)	grad_norm 43.1656 (30.6859)	mem 4879MB
[2022-05-31 08:57:15 MetaFG_0] (main.py 265): INFO Train: [75/300][510/1562]	eta 0:05:21 lr 0.000005	time 0.2978 (0.3060)	loss 1.2984 (1.3177)	grad_norm 33.4142 (30.6927)	mem 4879MB
[2022-05-31 08:57:18 MetaFG_0] (main.py 265): INFO Train: [75/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.3012 (0.3060)	loss 1.6668 (1.3171)	grad_norm 34.5759 (30.6697)	mem 4879MB
[2022-05-31 08:57:21 MetaFG_0] (main.py 265): INFO Train: [75/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2992 (0.3060)	loss 1.4702 (1.3174)	grad_norm 22.0058 (30.6783)	mem 4879MB
[2022-05-31 08:57:24 MetaFG_0] (main.py 265): INFO Train: [75/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2994 (0.3059)	loss 1.4687 (1.3173)	grad_norm 25.8914 (30.6842)	mem 4879MB
[2022-05-31 08:57:27 MetaFG_0] (main.py 265): INFO Train: [75/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2999 (0.3060)	loss 1.2759 (1.3175)	grad_norm 19.0356 (30.7287)	mem 4879MB
[2022-05-31 08:57:30 MetaFG_0] (main.py 265): INFO Train: [75/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2924 (0.3059)	loss 1.2709 (1.3160)	grad_norm 30.8089 (30.6560)	mem 4879MB
[2022-05-31 08:57:33 MetaFG_0] (main.py 265): INFO Train: [75/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2927 (0.3059)	loss 1.3261 (1.3166)	grad_norm 31.4489 (30.7564)	mem 4879MB
[2022-05-31 08:57:36 MetaFG_0] (main.py 265): INFO Train: [75/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2946 (0.3058)	loss 1.1315 (1.3190)	grad_norm 35.5808 (30.7635)	mem 4879MB
[2022-05-31 08:57:39 MetaFG_0] (main.py 265): INFO Train: [75/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.3002 (0.3059)	loss 1.3402 (1.3208)	grad_norm 17.2376 (30.7515)	mem 4879MB
[2022-05-31 08:57:42 MetaFG_0] (main.py 265): INFO Train: [75/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2925 (0.3058)	loss 1.4120 (1.3210)	grad_norm 37.3690 (30.6777)	mem 4879MB
[2022-05-31 08:57:45 MetaFG_0] (main.py 265): INFO Train: [75/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2937 (0.3058)	loss 1.2772 (1.3222)	grad_norm 29.4977 (30.8109)	mem 4879MB
[2022-05-31 08:57:48 MetaFG_0] (main.py 265): INFO Train: [75/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2930 (0.3058)	loss 1.6452 (1.3207)	grad_norm 29.4608 (30.7538)	mem 4879MB
[2022-05-31 08:57:51 MetaFG_0] (main.py 265): INFO Train: [75/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2932 (0.3058)	loss 1.6710 (1.3209)	grad_norm 15.4398 (30.6876)	mem 4879MB
[2022-05-31 08:57:54 MetaFG_0] (main.py 265): INFO Train: [75/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.2929 (0.3058)	loss 1.2630 (1.3213)	grad_norm 13.5400 (30.5501)	mem 4879MB
[2022-05-31 08:57:57 MetaFG_0] (main.py 265): INFO Train: [75/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2916 (0.3058)	loss 1.3391 (1.3208)	grad_norm 33.6277 (30.5616)	mem 4879MB
[2022-05-31 08:58:00 MetaFG_0] (main.py 265): INFO Train: [75/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2933 (0.3058)	loss 1.6286 (1.3220)	grad_norm 32.6271 (30.5890)	mem 4879MB
[2022-05-31 08:58:03 MetaFG_0] (main.py 265): INFO Train: [75/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2935 (0.3057)	loss 1.2554 (1.3213)	grad_norm 36.5603 (30.5697)	mem 4879MB
[2022-05-31 08:58:06 MetaFG_0] (main.py 265): INFO Train: [75/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2944 (0.3056)	loss 1.0259 (1.3190)	grad_norm 31.7343 (30.5603)	mem 4879MB
[2022-05-31 08:58:10 MetaFG_0] (main.py 265): INFO Train: [75/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2991 (0.3056)	loss 1.3561 (1.3206)	grad_norm 39.6457 (30.6206)	mem 4879MB
[2022-05-31 08:58:13 MetaFG_0] (main.py 265): INFO Train: [75/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2924 (0.3056)	loss 1.3743 (1.3209)	grad_norm 35.0307 (30.6666)	mem 4879MB
[2022-05-31 08:58:16 MetaFG_0] (main.py 265): INFO Train: [75/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2929 (0.3056)	loss 1.1622 (1.3190)	grad_norm 27.6724 (30.6565)	mem 4879MB
[2022-05-31 08:58:19 MetaFG_0] (main.py 265): INFO Train: [75/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2938 (0.3056)	loss 1.4941 (1.3202)	grad_norm 29.9342 (30.6817)	mem 4879MB
[2022-05-31 08:58:22 MetaFG_0] (main.py 265): INFO Train: [75/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.3088 (0.3055)	loss 1.2557 (1.3204)	grad_norm 28.5190 (30.6129)	mem 4879MB
[2022-05-31 08:58:25 MetaFG_0] (main.py 265): INFO Train: [75/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2987 (0.3055)	loss 1.4858 (1.3211)	grad_norm 31.0010 (30.6200)	mem 4879MB
[2022-05-31 08:58:28 MetaFG_0] (main.py 265): INFO Train: [75/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2992 (0.3055)	loss 1.3745 (1.3225)	grad_norm 28.1681 (30.5958)	mem 4879MB
[2022-05-31 08:58:31 MetaFG_0] (main.py 265): INFO Train: [75/300][760/1562]	eta 0:04:04 lr 0.000005	time 0.2923 (0.3055)	loss 1.5550 (1.3232)	grad_norm 17.5144 (30.6006)	mem 4879MB
[2022-05-31 08:58:34 MetaFG_0] (main.py 265): INFO Train: [75/300][770/1562]	eta 0:04:01 lr 0.000005	time 0.3002 (0.3055)	loss 1.2484 (1.3239)	grad_norm 19.8266 (30.6013)	mem 4879MB
[2022-05-31 08:58:37 MetaFG_0] (main.py 265): INFO Train: [75/300][780/1562]	eta 0:03:58 lr 0.000005	time 0.2992 (0.3055)	loss 1.2746 (1.3248)	grad_norm 20.5461 (30.5732)	mem 4879MB
[2022-05-31 08:58:40 MetaFG_0] (main.py 265): INFO Train: [75/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2928 (0.3055)	loss 1.3776 (1.3247)	grad_norm 23.7187 (30.5876)	mem 4879MB
[2022-05-31 08:58:43 MetaFG_0] (main.py 265): INFO Train: [75/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2980 (0.3055)	loss 1.3748 (1.3239)	grad_norm 20.0955 (30.5523)	mem 4879MB
[2022-05-31 08:58:46 MetaFG_0] (main.py 265): INFO Train: [75/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.3003 (0.3054)	loss 1.3001 (1.3233)	grad_norm 19.6038 (30.4323)	mem 4879MB
[2022-05-31 08:58:49 MetaFG_0] (main.py 265): INFO Train: [75/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2916 (0.3054)	loss 1.4807 (1.3248)	grad_norm 38.6757 (30.4276)	mem 4879MB
[2022-05-31 08:58:52 MetaFG_0] (main.py 265): INFO Train: [75/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2985 (0.3054)	loss 1.4831 (1.3248)	grad_norm 45.3772 (30.3843)	mem 4879MB
[2022-05-31 08:58:55 MetaFG_0] (main.py 265): INFO Train: [75/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.3000 (0.3054)	loss 1.5452 (1.3261)	grad_norm 27.9371 (30.3585)	mem 4879MB
[2022-05-31 08:58:58 MetaFG_0] (main.py 265): INFO Train: [75/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2926 (0.3054)	loss 1.5688 (1.3252)	grad_norm 31.2650 (30.3741)	mem 4879MB
[2022-05-31 08:59:01 MetaFG_0] (main.py 265): INFO Train: [75/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.3064 (0.3054)	loss 1.0607 (1.3240)	grad_norm 22.6324 (30.3456)	mem 4879MB
[2022-05-31 08:59:04 MetaFG_0] (main.py 265): INFO Train: [75/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2936 (0.3054)	loss 1.5064 (1.3244)	grad_norm 44.0956 (30.3383)	mem 4879MB
[2022-05-31 08:59:07 MetaFG_0] (main.py 265): INFO Train: [75/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2976 (0.3054)	loss 1.1173 (1.3242)	grad_norm 22.6747 (30.3354)	mem 4879MB
[2022-05-31 08:59:10 MetaFG_0] (main.py 265): INFO Train: [75/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2921 (0.3053)	loss 1.6076 (1.3250)	grad_norm 30.0667 (30.2875)	mem 4879MB
[2022-05-31 08:59:13 MetaFG_0] (main.py 265): INFO Train: [75/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2942 (0.3053)	loss 0.8867 (1.3241)	grad_norm 47.8536 (30.3057)	mem 4879MB
[2022-05-31 08:59:16 MetaFG_0] (main.py 265): INFO Train: [75/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2915 (0.3053)	loss 0.9837 (1.3237)	grad_norm 26.0261 (30.3188)	mem 4879MB
[2022-05-31 08:59:19 MetaFG_0] (main.py 265): INFO Train: [75/300][920/1562]	eta 0:03:15 lr 0.000005	time 0.2946 (0.3053)	loss 0.9030 (1.3222)	grad_norm 35.1964 (30.3462)	mem 4879MB
[2022-05-31 08:59:23 MetaFG_0] (main.py 265): INFO Train: [75/300][930/1562]	eta 0:03:12 lr 0.000005	time 0.2925 (0.3053)	loss 1.3548 (1.3210)	grad_norm 24.1029 (30.3260)	mem 4879MB
[2022-05-31 08:59:26 MetaFG_0] (main.py 265): INFO Train: [75/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2982 (0.3053)	loss 1.1983 (1.3201)	grad_norm 26.0971 (30.3007)	mem 4879MB
[2022-05-31 08:59:29 MetaFG_0] (main.py 265): INFO Train: [75/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.2954 (0.3053)	loss 1.5587 (1.3206)	grad_norm 20.0494 (30.2713)	mem 4879MB
[2022-05-31 08:59:32 MetaFG_0] (main.py 265): INFO Train: [75/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2990 (0.3053)	loss 1.4428 (1.3214)	grad_norm 23.1677 (30.2601)	mem 4879MB
[2022-05-31 08:59:35 MetaFG_0] (main.py 265): INFO Train: [75/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2924 (0.3053)	loss 0.8485 (1.3211)	grad_norm 51.6737 (30.3502)	mem 4879MB
[2022-05-31 08:59:38 MetaFG_0] (main.py 265): INFO Train: [75/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2941 (0.3053)	loss 1.6432 (1.3214)	grad_norm 30.4180 (30.3576)	mem 4879MB
[2022-05-31 08:59:41 MetaFG_0] (main.py 265): INFO Train: [75/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.3204 (0.3055)	loss 1.1015 (1.3212)	grad_norm 30.8418 (30.3731)	mem 4879MB
[2022-05-31 08:59:44 MetaFG_0] (main.py 265): INFO Train: [75/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2977 (0.3055)	loss 1.3891 (1.3205)	grad_norm 19.5203 (30.3946)	mem 4879MB
[2022-05-31 08:59:47 MetaFG_0] (main.py 265): INFO Train: [75/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2921 (0.3054)	loss 1.3605 (1.3195)	grad_norm 23.9667 (30.3735)	mem 4879MB
[2022-05-31 08:59:50 MetaFG_0] (main.py 265): INFO Train: [75/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2929 (0.3054)	loss 0.8838 (1.3190)	grad_norm 28.0408 (30.3669)	mem 4879MB
[2022-05-31 08:59:53 MetaFG_0] (main.py 265): INFO Train: [75/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2932 (0.3054)	loss 1.4650 (1.3181)	grad_norm 32.5784 (30.3851)	mem 4879MB
[2022-05-31 08:59:56 MetaFG_0] (main.py 265): INFO Train: [75/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2984 (0.3054)	loss 1.5949 (1.3189)	grad_norm 39.7785 (30.3853)	mem 4879MB
[2022-05-31 08:59:59 MetaFG_0] (main.py 265): INFO Train: [75/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2929 (0.3053)	loss 1.4091 (1.3187)	grad_norm 21.6100 (30.3592)	mem 4879MB
[2022-05-31 09:00:02 MetaFG_0] (main.py 265): INFO Train: [75/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2975 (0.3053)	loss 1.4484 (1.3191)	grad_norm 16.1023 (30.3077)	mem 4879MB
[2022-05-31 09:00:05 MetaFG_0] (main.py 265): INFO Train: [75/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.3005 (0.3053)	loss 1.2593 (1.3197)	grad_norm 38.1797 (30.3092)	mem 4879MB
[2022-05-31 09:00:08 MetaFG_0] (main.py 265): INFO Train: [75/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2925 (0.3053)	loss 1.0049 (1.3188)	grad_norm 25.7079 (30.2843)	mem 4879MB
[2022-05-31 09:00:11 MetaFG_0] (main.py 265): INFO Train: [75/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2993 (0.3053)	loss 1.7255 (1.3191)	grad_norm 46.9003 (30.3021)	mem 4879MB
[2022-05-31 09:00:14 MetaFG_0] (main.py 265): INFO Train: [75/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2940 (0.3053)	loss 1.3560 (1.3190)	grad_norm 28.9286 (30.3820)	mem 4879MB
[2022-05-31 09:00:18 MetaFG_0] (main.py 265): INFO Train: [75/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2992 (0.3053)	loss 1.5922 (1.3198)	grad_norm 35.2118 (30.4202)	mem 4879MB
[2022-05-31 09:00:21 MetaFG_0] (main.py 265): INFO Train: [75/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2978 (0.3053)	loss 0.8393 (1.3193)	grad_norm 36.4805 (30.4297)	mem 4879MB
[2022-05-31 09:00:24 MetaFG_0] (main.py 265): INFO Train: [75/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2924 (0.3053)	loss 1.6465 (1.3206)	grad_norm 32.5352 (30.4859)	mem 4879MB
[2022-05-31 09:00:27 MetaFG_0] (main.py 265): INFO Train: [75/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2935 (0.3053)	loss 1.4798 (1.3215)	grad_norm 26.8905 (30.4955)	mem 4879MB
[2022-05-31 09:00:30 MetaFG_0] (main.py 265): INFO Train: [75/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2984 (0.3053)	loss 1.5139 (1.3220)	grad_norm 37.5180 (30.4610)	mem 4879MB
[2022-05-31 09:00:33 MetaFG_0] (main.py 265): INFO Train: [75/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2937 (0.3052)	loss 1.4354 (1.3227)	grad_norm 29.8350 (nan)	mem 4879MB
[2022-05-31 09:00:36 MetaFG_0] (main.py 265): INFO Train: [75/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2929 (0.3052)	loss 1.5691 (1.3237)	grad_norm 22.7381 (nan)	mem 4879MB
[2022-05-31 09:00:39 MetaFG_0] (main.py 265): INFO Train: [75/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2931 (0.3052)	loss 1.4805 (1.3232)	grad_norm 36.9546 (nan)	mem 4879MB
[2022-05-31 09:00:42 MetaFG_0] (main.py 265): INFO Train: [75/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2985 (0.3052)	loss 1.0595 (1.3229)	grad_norm 46.2123 (nan)	mem 4879MB
[2022-05-31 09:00:45 MetaFG_0] (main.py 265): INFO Train: [75/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2936 (0.3052)	loss 1.4173 (1.3237)	grad_norm 45.0171 (nan)	mem 4879MB
[2022-05-31 09:00:48 MetaFG_0] (main.py 265): INFO Train: [75/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2944 (0.3052)	loss 1.1791 (1.3231)	grad_norm 22.1913 (nan)	mem 4879MB
[2022-05-31 09:00:51 MetaFG_0] (main.py 265): INFO Train: [75/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2925 (0.3052)	loss 1.3412 (1.3239)	grad_norm 26.2240 (nan)	mem 4879MB
[2022-05-31 09:00:54 MetaFG_0] (main.py 265): INFO Train: [75/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2951 (0.3052)	loss 1.3163 (1.3238)	grad_norm 21.6447 (nan)	mem 4879MB
[2022-05-31 09:00:57 MetaFG_0] (main.py 265): INFO Train: [75/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2926 (0.3052)	loss 1.5222 (1.3235)	grad_norm 60.2149 (nan)	mem 4879MB
[2022-05-31 09:01:00 MetaFG_0] (main.py 265): INFO Train: [75/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2924 (0.3052)	loss 0.7041 (1.3234)	grad_norm 18.1871 (nan)	mem 4879MB
[2022-05-31 09:01:03 MetaFG_0] (main.py 265): INFO Train: [75/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2992 (0.3052)	loss 1.4440 (1.3228)	grad_norm 19.5521 (nan)	mem 4879MB
[2022-05-31 09:01:06 MetaFG_0] (main.py 265): INFO Train: [75/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.3002 (0.3052)	loss 1.6029 (1.3226)	grad_norm 43.0862 (nan)	mem 4879MB
[2022-05-31 09:01:09 MetaFG_0] (main.py 265): INFO Train: [75/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2922 (0.3051)	loss 0.8734 (1.3222)	grad_norm 25.2749 (nan)	mem 4879MB
[2022-05-31 09:01:12 MetaFG_0] (main.py 265): INFO Train: [75/300][1290/1562]	eta 0:01:22 lr 0.000005	time 0.2924 (0.3051)	loss 1.1116 (1.3208)	grad_norm 42.4501 (nan)	mem 4879MB
[2022-05-31 09:01:15 MetaFG_0] (main.py 265): INFO Train: [75/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2936 (0.3051)	loss 1.1002 (1.3210)	grad_norm 22.2711 (nan)	mem 4879MB
[2022-05-31 09:01:18 MetaFG_0] (main.py 265): INFO Train: [75/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2919 (0.3051)	loss 1.1206 (1.3214)	grad_norm 39.8264 (nan)	mem 4879MB
[2022-05-31 09:01:21 MetaFG_0] (main.py 265): INFO Train: [75/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2980 (0.3051)	loss 1.6268 (1.3210)	grad_norm 20.9053 (nan)	mem 4879MB
[2022-05-31 09:01:24 MetaFG_0] (main.py 265): INFO Train: [75/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2997 (0.3051)	loss 1.2673 (1.3207)	grad_norm 21.1454 (nan)	mem 4879MB
[2022-05-31 09:01:27 MetaFG_0] (main.py 265): INFO Train: [75/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.3006 (0.3051)	loss 1.2243 (1.3207)	grad_norm 23.9731 (nan)	mem 4879MB
[2022-05-31 09:01:31 MetaFG_0] (main.py 265): INFO Train: [75/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2938 (0.3051)	loss 1.4387 (1.3206)	grad_norm 27.4594 (nan)	mem 4879MB
[2022-05-31 09:01:34 MetaFG_0] (main.py 265): INFO Train: [75/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2932 (0.3051)	loss 1.1243 (1.3202)	grad_norm 15.0263 (nan)	mem 4879MB
[2022-05-31 09:01:37 MetaFG_0] (main.py 265): INFO Train: [75/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2990 (0.3051)	loss 1.0548 (1.3193)	grad_norm 23.6671 (nan)	mem 4879MB
[2022-05-31 09:01:40 MetaFG_0] (main.py 265): INFO Train: [75/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.3003 (0.3051)	loss 1.3518 (1.3190)	grad_norm 36.4360 (nan)	mem 4879MB
[2022-05-31 09:01:43 MetaFG_0] (main.py 265): INFO Train: [75/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2994 (0.3051)	loss 1.2839 (1.3191)	grad_norm 49.7227 (nan)	mem 4879MB
[2022-05-31 09:01:46 MetaFG_0] (main.py 265): INFO Train: [75/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2927 (0.3051)	loss 1.3089 (1.3187)	grad_norm 15.9850 (nan)	mem 4879MB
[2022-05-31 09:01:49 MetaFG_0] (main.py 265): INFO Train: [75/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.3007 (0.3051)	loss 0.9435 (1.3191)	grad_norm 21.1807 (nan)	mem 4879MB
[2022-05-31 09:01:52 MetaFG_0] (main.py 265): INFO Train: [75/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2990 (0.3051)	loss 0.8501 (1.3189)	grad_norm 29.1022 (nan)	mem 4879MB
[2022-05-31 09:01:55 MetaFG_0] (main.py 265): INFO Train: [75/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2977 (0.3051)	loss 1.2741 (1.3190)	grad_norm 20.9344 (nan)	mem 4879MB
[2022-05-31 09:01:58 MetaFG_0] (main.py 265): INFO Train: [75/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.3018 (0.3051)	loss 1.6017 (1.3193)	grad_norm 44.6655 (nan)	mem 4879MB
[2022-05-31 09:02:01 MetaFG_0] (main.py 265): INFO Train: [75/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2974 (0.3051)	loss 0.9821 (1.3198)	grad_norm 29.3462 (nan)	mem 4879MB
[2022-05-31 09:02:04 MetaFG_0] (main.py 265): INFO Train: [75/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2993 (0.3051)	loss 1.1752 (1.3190)	grad_norm 32.0644 (nan)	mem 4879MB
[2022-05-31 09:02:07 MetaFG_0] (main.py 265): INFO Train: [75/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2929 (0.3051)	loss 1.4622 (1.3188)	grad_norm 19.8704 (nan)	mem 4879MB
[2022-05-31 09:02:10 MetaFG_0] (main.py 265): INFO Train: [75/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2921 (0.3051)	loss 1.6050 (1.3184)	grad_norm 36.9850 (nan)	mem 4879MB
[2022-05-31 09:02:13 MetaFG_0] (main.py 265): INFO Train: [75/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2938 (0.3051)	loss 1.5836 (1.3193)	grad_norm 54.7531 (nan)	mem 4879MB
[2022-05-31 09:02:16 MetaFG_0] (main.py 265): INFO Train: [75/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2984 (0.3051)	loss 1.5066 (1.3195)	grad_norm 14.2304 (nan)	mem 4879MB
[2022-05-31 09:02:19 MetaFG_0] (main.py 265): INFO Train: [75/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2922 (0.3051)	loss 0.9355 (1.3192)	grad_norm 30.7590 (nan)	mem 4879MB
[2022-05-31 09:02:22 MetaFG_0] (main.py 265): INFO Train: [75/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2934 (0.3051)	loss 1.1651 (1.3189)	grad_norm 36.8162 (nan)	mem 4879MB
[2022-05-31 09:02:25 MetaFG_0] (main.py 265): INFO Train: [75/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2932 (0.3051)	loss 1.1855 (1.3183)	grad_norm 27.7275 (nan)	mem 4879MB
[2022-05-31 09:02:28 MetaFG_0] (main.py 265): INFO Train: [75/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2923 (0.3051)	loss 1.5379 (1.3192)	grad_norm 17.4008 (nan)	mem 4879MB
[2022-05-31 09:02:32 MetaFG_0] (main.py 265): INFO Train: [75/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2951 (0.3051)	loss 1.1010 (1.3198)	grad_norm 13.0358 (nan)	mem 4879MB
[2022-05-31 09:02:35 MetaFG_0] (main.py 265): INFO Train: [75/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2909 (0.3051)	loss 1.3243 (1.3195)	grad_norm 56.0293 (nan)	mem 4879MB
[2022-05-31 09:02:35 MetaFG_0] (main.py 272): INFO EPOCH 75 training takes 0:07:56
[2022-05-31 09:02:35 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_75.pth saving......
[2022-05-31 09:02:36 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_75.pth saved !!!
[2022-05-31 09:02:36 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 09:02:38 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 09:02:38 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 09:02:38 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.694 (0.694)	Loss 0.5239 (0.5239)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 09:02:39 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.093 (0.151)	Loss 0.4343 (0.6110)	Acc@1 93.750 (86.080)	Acc@5 100.000 (98.295)	Mem 4879MB
[2022-05-31 09:02:40 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.124)	Loss 0.7738 (0.5618)	Acc@1 90.625 (88.095)	Acc@5 96.875 (98.958)	Mem 4879MB
[2022-05-31 09:02:41 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.115)	Loss 0.5158 (0.5770)	Acc@1 87.500 (88.004)	Acc@5 100.000 (98.891)	Mem 4879MB
[2022-05-31 09:02:42 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.094 (0.109)	Loss 0.8155 (0.5841)	Acc@1 71.875 (87.881)	Acc@5 100.000 (99.009)	Mem 4879MB
[2022-05-31 09:02:43 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.106)	Loss 0.6357 (0.5893)	Acc@1 84.375 (87.623)	Acc@5 100.000 (98.775)	Mem 4879MB
[2022-05-31 09:02:44 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.095 (0.104)	Loss 0.5645 (0.5854)	Acc@1 84.375 (87.705)	Acc@5 100.000 (98.822)	Mem 4879MB
[2022-05-31 09:02:45 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.103)	Loss 0.7090 (0.5833)	Acc@1 81.250 (87.632)	Acc@5 96.875 (98.812)	Mem 4879MB
[2022-05-31 09:02:46 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.098 (0.102)	Loss 0.5632 (0.5924)	Acc@1 87.500 (87.269)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 09:02:47 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.095 (0.101)	Loss 0.3290 (0.5907)	Acc@1 96.875 (87.260)	Acc@5 100.000 (98.729)	Mem 4879MB
[2022-05-31 09:02:48 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.100)	Loss 0.5919 (0.5921)	Acc@1 87.500 (87.252)	Acc@5 100.000 (98.762)	Mem 4879MB
[2022-05-31 09:02:49 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.087 (0.100)	Loss 0.7708 (0.5964)	Acc@1 75.000 (87.106)	Acc@5 96.875 (98.620)	Mem 4879MB
[2022-05-31 09:02:50 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.112 (0.099)	Loss 0.3381 (0.5942)	Acc@1 96.875 (87.190)	Acc@5 100.000 (98.631)	Mem 4879MB
[2022-05-31 09:02:51 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.092 (0.099)	Loss 0.4514 (0.5921)	Acc@1 90.625 (87.238)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 09:02:51 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.099)	Loss 0.5736 (0.5957)	Acc@1 90.625 (87.123)	Acc@5 96.875 (98.670)	Mem 4879MB
[2022-05-31 09:02:52 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.096 (0.098)	Loss 0.4753 (0.5972)	Acc@1 90.625 (87.045)	Acc@5 100.000 (98.696)	Mem 4879MB
[2022-05-31 09:02:53 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.110 (0.098)	Loss 0.5717 (0.5975)	Acc@1 87.500 (86.995)	Acc@5 100.000 (98.680)	Mem 4879MB
[2022-05-31 09:02:54 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.097 (0.098)	Loss 0.5715 (0.6006)	Acc@1 90.625 (86.915)	Acc@5 100.000 (98.666)	Mem 4879MB
[2022-05-31 09:02:55 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.100 (0.098)	Loss 0.4837 (0.5993)	Acc@1 87.500 (86.930)	Acc@5 100.000 (98.688)	Mem 4879MB
[2022-05-31 09:02:56 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.092 (0.098)	Loss 0.7229 (0.5999)	Acc@1 81.250 (86.911)	Acc@5 100.000 (98.724)	Mem 4879MB
[2022-05-31 09:02:57 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.098)	Loss 0.9659 (0.5981)	Acc@1 78.125 (86.971)	Acc@5 93.750 (98.694)	Mem 4879MB
[2022-05-31 09:02:58 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.095 (0.097)	Loss 0.5153 (0.5943)	Acc@1 84.375 (87.100)	Acc@5 100.000 (98.711)	Mem 4879MB
[2022-05-31 09:02:59 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.097)	Loss 0.5667 (0.5917)	Acc@1 84.375 (87.189)	Acc@5 100.000 (98.727)	Mem 4879MB
[2022-05-31 09:03:00 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.093 (0.097)	Loss 0.4551 (0.5893)	Acc@1 90.625 (87.270)	Acc@5 100.000 (98.769)	Mem 4879MB
[2022-05-31 09:03:01 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.4966 (0.5877)	Acc@1 93.750 (87.344)	Acc@5 100.000 (98.781)	Mem 4879MB
[2022-05-31 09:03:02 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.088 (0.097)	Loss 0.3321 (0.5820)	Acc@1 96.875 (87.537)	Acc@5 100.000 (98.817)	Mem 4879MB
[2022-05-31 09:03:03 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.097)	Loss 0.6812 (0.5831)	Acc@1 87.500 (87.500)	Acc@5 96.875 (98.827)	Mem 4879MB
[2022-05-31 09:03:04 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.091 (0.097)	Loss 0.9391 (0.5861)	Acc@1 84.375 (87.419)	Acc@5 96.875 (98.812)	Mem 4879MB
[2022-05-31 09:03:05 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 0.6787 (0.5851)	Acc@1 87.500 (87.489)	Acc@5 96.875 (98.810)	Mem 4879MB
[2022-05-31 09:03:06 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.097)	Loss 0.4377 (0.5827)	Acc@1 96.875 (87.640)	Acc@5 100.000 (98.829)	Mem 4879MB
[2022-05-31 09:03:07 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.107 (0.096)	Loss 0.5485 (0.5849)	Acc@1 87.500 (87.593)	Acc@5 100.000 (98.796)	Mem 4879MB
[2022-05-31 09:03:07 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.5374 (0.5851)	Acc@1 87.500 (87.550)	Acc@5 100.000 (98.804)	Mem 4879MB
[2022-05-31 09:03:08 MetaFG_0] (main.py 330): INFO  * Acc@1 87.580 Acc@5 98.800
[2022-05-31 09:03:08 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.6%
[2022-05-31 09:03:08 MetaFG_0] (main.py 171): INFO Max accuracy: 87.58%
[2022-05-31 09:03:09 MetaFG_0] (main.py 265): INFO Train: [76/300][0/1562]	eta 0:23:27 lr 0.000005	time 0.9009 (0.9009)	loss 1.1553 (1.1553)	grad_norm 22.7412 (22.7412)	mem 4879MB
[2022-05-31 09:03:12 MetaFG_0] (main.py 265): INFO Train: [76/300][10/1562]	eta 0:09:29 lr 0.000005	time 0.2977 (0.3669)	loss 1.4665 (1.2600)	grad_norm 20.5139 (25.6324)	mem 4879MB
[2022-05-31 09:03:15 MetaFG_0] (main.py 265): INFO Train: [76/300][20/1562]	eta 0:08:40 lr 0.000005	time 0.2969 (0.3375)	loss 1.2377 (1.2582)	grad_norm 36.7614 (27.4150)	mem 4879MB
[2022-05-31 09:03:18 MetaFG_0] (main.py 265): INFO Train: [76/300][30/1562]	eta 0:08:20 lr 0.000005	time 0.2928 (0.3264)	loss 1.3614 (1.2586)	grad_norm 28.6355 (26.5210)	mem 4879MB
[2022-05-31 09:03:21 MetaFG_0] (main.py 265): INFO Train: [76/300][40/1562]	eta 0:08:07 lr 0.000005	time 0.2981 (0.3204)	loss 1.3653 (1.2723)	grad_norm 20.2815 (27.1558)	mem 4879MB
[2022-05-31 09:03:24 MetaFG_0] (main.py 265): INFO Train: [76/300][50/1562]	eta 0:07:59 lr 0.000005	time 0.2935 (0.3172)	loss 0.9442 (1.2564)	grad_norm 28.1393 (28.7984)	mem 4879MB
[2022-05-31 09:03:27 MetaFG_0] (main.py 265): INFO Train: [76/300][60/1562]	eta 0:07:53 lr 0.000005	time 0.3008 (0.3154)	loss 0.8380 (1.2530)	grad_norm 20.2318 (29.7534)	mem 4879MB
[2022-05-31 09:03:30 MetaFG_0] (main.py 265): INFO Train: [76/300][70/1562]	eta 0:07:48 lr 0.000005	time 0.2987 (0.3140)	loss 1.1774 (1.2616)	grad_norm 25.5692 (30.6396)	mem 4879MB
[2022-05-31 09:03:33 MetaFG_0] (main.py 265): INFO Train: [76/300][80/1562]	eta 0:07:44 lr 0.000005	time 0.2940 (0.3133)	loss 1.7924 (1.2815)	grad_norm 62.3302 (31.3883)	mem 4879MB
[2022-05-31 09:03:36 MetaFG_0] (main.py 265): INFO Train: [76/300][90/1562]	eta 0:07:39 lr 0.000005	time 0.2986 (0.3124)	loss 1.3985 (1.2923)	grad_norm 21.3451 (31.2972)	mem 4879MB
[2022-05-31 09:03:39 MetaFG_0] (main.py 265): INFO Train: [76/300][100/1562]	eta 0:07:35 lr 0.000005	time 0.2914 (0.3116)	loss 0.7680 (1.2872)	grad_norm 21.1149 (30.8375)	mem 4879MB
[2022-05-31 09:03:42 MetaFG_0] (main.py 265): INFO Train: [76/300][110/1562]	eta 0:07:31 lr 0.000005	time 0.2929 (0.3108)	loss 0.8778 (1.2824)	grad_norm 44.5483 (30.8471)	mem 4879MB
[2022-05-31 09:03:45 MetaFG_0] (main.py 265): INFO Train: [76/300][120/1562]	eta 0:07:27 lr 0.000005	time 0.2926 (0.3101)	loss 1.3073 (1.2834)	grad_norm 22.4176 (30.8788)	mem 4879MB
[2022-05-31 09:03:48 MetaFG_0] (main.py 265): INFO Train: [76/300][130/1562]	eta 0:07:23 lr 0.000005	time 0.2938 (0.3097)	loss 1.0439 (1.2879)	grad_norm 20.9072 (30.5192)	mem 4879MB
[2022-05-31 09:03:51 MetaFG_0] (main.py 265): INFO Train: [76/300][140/1562]	eta 0:07:19 lr 0.000005	time 0.2992 (0.3094)	loss 1.0090 (1.2828)	grad_norm 20.0103 (30.1715)	mem 4879MB
[2022-05-31 09:03:54 MetaFG_0] (main.py 265): INFO Train: [76/300][150/1562]	eta 0:07:16 lr 0.000005	time 0.2925 (0.3091)	loss 1.5873 (1.2822)	grad_norm 26.2706 (29.8499)	mem 4879MB
[2022-05-31 09:03:57 MetaFG_0] (main.py 265): INFO Train: [76/300][160/1562]	eta 0:07:12 lr 0.000005	time 0.3002 (0.3088)	loss 1.2006 (1.2801)	grad_norm 25.3887 (29.7684)	mem 4879MB
[2022-05-31 09:04:00 MetaFG_0] (main.py 265): INFO Train: [76/300][170/1562]	eta 0:07:09 lr 0.000005	time 0.2925 (0.3086)	loss 1.3366 (1.2846)	grad_norm 26.6864 (29.6796)	mem 4879MB
[2022-05-31 09:04:04 MetaFG_0] (main.py 265): INFO Train: [76/300][180/1562]	eta 0:07:06 lr 0.000005	time 0.2942 (0.3083)	loss 1.3078 (1.2891)	grad_norm 23.0335 (29.9603)	mem 4879MB
[2022-05-31 09:04:07 MetaFG_0] (main.py 265): INFO Train: [76/300][190/1562]	eta 0:07:02 lr 0.000005	time 0.2924 (0.3080)	loss 1.0559 (1.2937)	grad_norm 44.7693 (30.0866)	mem 4879MB
[2022-05-31 09:04:10 MetaFG_0] (main.py 265): INFO Train: [76/300][200/1562]	eta 0:06:59 lr 0.000005	time 0.2924 (0.3079)	loss 1.4387 (1.2963)	grad_norm 22.1391 (29.9496)	mem 4879MB
[2022-05-31 09:04:13 MetaFG_0] (main.py 265): INFO Train: [76/300][210/1562]	eta 0:06:56 lr 0.000005	time 0.2930 (0.3077)	loss 1.3092 (1.2981)	grad_norm 21.5100 (30.0615)	mem 4879MB
[2022-05-31 09:04:16 MetaFG_0] (main.py 265): INFO Train: [76/300][220/1562]	eta 0:06:52 lr 0.000005	time 0.3009 (0.3077)	loss 1.6336 (1.3049)	grad_norm 45.5272 (30.2153)	mem 4879MB
[2022-05-31 09:04:19 MetaFG_0] (main.py 265): INFO Train: [76/300][230/1562]	eta 0:06:49 lr 0.000005	time 0.2929 (0.3076)	loss 1.4781 (1.3063)	grad_norm 25.0301 (30.0494)	mem 4879MB
[2022-05-31 09:04:22 MetaFG_0] (main.py 265): INFO Train: [76/300][240/1562]	eta 0:06:46 lr 0.000005	time 0.2929 (0.3075)	loss 1.1636 (1.3099)	grad_norm 27.0161 (29.9003)	mem 4879MB
[2022-05-31 09:04:25 MetaFG_0] (main.py 265): INFO Train: [76/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2932 (0.3074)	loss 0.9373 (1.3069)	grad_norm 18.1342 (30.0318)	mem 4879MB
[2022-05-31 09:04:28 MetaFG_0] (main.py 265): INFO Train: [76/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2944 (0.3073)	loss 1.4918 (1.3067)	grad_norm 23.6658 (29.8545)	mem 4879MB
[2022-05-31 09:04:31 MetaFG_0] (main.py 265): INFO Train: [76/300][270/1562]	eta 0:06:36 lr 0.000005	time 0.3000 (0.3072)	loss 0.8577 (1.3045)	grad_norm 22.2380 (29.8972)	mem 4879MB
[2022-05-31 09:04:34 MetaFG_0] (main.py 265): INFO Train: [76/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2931 (0.3071)	loss 1.4732 (1.3069)	grad_norm 31.5106 (29.7131)	mem 4879MB
[2022-05-31 09:04:37 MetaFG_0] (main.py 265): INFO Train: [76/300][290/1562]	eta 0:06:30 lr 0.000005	time 0.2925 (0.3071)	loss 0.9290 (1.3031)	grad_norm 27.6926 (29.6697)	mem 4879MB
[2022-05-31 09:04:40 MetaFG_0] (main.py 265): INFO Train: [76/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2921 (0.3070)	loss 1.1933 (1.3053)	grad_norm 44.5092 (29.7804)	mem 4879MB
[2022-05-31 09:04:43 MetaFG_0] (main.py 265): INFO Train: [76/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2928 (0.3069)	loss 1.3948 (1.3048)	grad_norm 31.9390 (29.7064)	mem 4879MB
[2022-05-31 09:04:46 MetaFG_0] (main.py 265): INFO Train: [76/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2932 (0.3068)	loss 0.9622 (1.3044)	grad_norm 31.0397 (29.6337)	mem 4879MB
[2022-05-31 09:04:49 MetaFG_0] (main.py 265): INFO Train: [76/300][330/1562]	eta 0:06:17 lr 0.000005	time 0.2999 (0.3068)	loss 1.3703 (1.3082)	grad_norm 36.6048 (29.7621)	mem 4879MB
[2022-05-31 09:04:52 MetaFG_0] (main.py 265): INFO Train: [76/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.2928 (0.3067)	loss 1.3332 (1.3064)	grad_norm 23.0220 (29.8603)	mem 4879MB
[2022-05-31 09:04:55 MetaFG_0] (main.py 265): INFO Train: [76/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2985 (0.3066)	loss 1.5233 (1.3099)	grad_norm 61.0270 (30.0361)	mem 4879MB
[2022-05-31 09:04:58 MetaFG_0] (main.py 265): INFO Train: [76/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2923 (0.3065)	loss 1.3715 (1.3104)	grad_norm 31.4485 (30.0237)	mem 4879MB
[2022-05-31 09:05:01 MetaFG_0] (main.py 265): INFO Train: [76/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2937 (0.3065)	loss 1.4295 (1.3110)	grad_norm 27.1012 (29.9025)	mem 4879MB
[2022-05-31 09:05:04 MetaFG_0] (main.py 265): INFO Train: [76/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2923 (0.3064)	loss 1.5368 (1.3129)	grad_norm 42.8430 (29.9770)	mem 4879MB
[2022-05-31 09:05:08 MetaFG_0] (main.py 265): INFO Train: [76/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2986 (0.3064)	loss 0.8421 (1.3113)	grad_norm 32.4439 (30.0113)	mem 4879MB
[2022-05-31 09:05:11 MetaFG_0] (main.py 265): INFO Train: [76/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.3055 (0.3064)	loss 1.3297 (1.3128)	grad_norm 34.8529 (30.0814)	mem 4879MB
[2022-05-31 09:05:14 MetaFG_0] (main.py 265): INFO Train: [76/300][410/1562]	eta 0:05:52 lr 0.000005	time 0.3021 (0.3064)	loss 1.4418 (1.3132)	grad_norm 28.3005 (30.0609)	mem 4879MB
[2022-05-31 09:05:17 MetaFG_0] (main.py 265): INFO Train: [76/300][420/1562]	eta 0:05:49 lr 0.000005	time 0.2962 (0.3063)	loss 0.8683 (1.3153)	grad_norm 63.7741 (30.1587)	mem 4879MB
[2022-05-31 09:05:20 MetaFG_0] (main.py 265): INFO Train: [76/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.3020 (0.3063)	loss 1.3813 (1.3177)	grad_norm 18.3549 (30.0336)	mem 4879MB
[2022-05-31 09:05:23 MetaFG_0] (main.py 265): INFO Train: [76/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2972 (0.3063)	loss 1.2743 (1.3191)	grad_norm 21.5659 (30.0695)	mem 4879MB
[2022-05-31 09:05:26 MetaFG_0] (main.py 265): INFO Train: [76/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2954 (0.3062)	loss 1.3956 (1.3196)	grad_norm 35.2059 (30.0224)	mem 4879MB
[2022-05-31 09:05:29 MetaFG_0] (main.py 265): INFO Train: [76/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.3020 (0.3062)	loss 1.5101 (1.3190)	grad_norm 33.0083 (30.2117)	mem 4879MB
[2022-05-31 09:05:32 MetaFG_0] (main.py 265): INFO Train: [76/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.3035 (0.3062)	loss 1.4163 (1.3217)	grad_norm 31.7458 (30.2071)	mem 4879MB
[2022-05-31 09:05:35 MetaFG_0] (main.py 265): INFO Train: [76/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.3034 (0.3062)	loss 1.2505 (1.3221)	grad_norm 21.9120 (30.3389)	mem 4879MB
[2022-05-31 09:05:38 MetaFG_0] (main.py 265): INFO Train: [76/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.3004 (0.3061)	loss 1.6124 (1.3242)	grad_norm 17.9038 (30.3381)	mem 4879MB
[2022-05-31 09:05:41 MetaFG_0] (main.py 265): INFO Train: [76/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2915 (0.3061)	loss 1.3251 (1.3242)	grad_norm 20.4997 (30.3463)	mem 4879MB
[2022-05-31 09:05:44 MetaFG_0] (main.py 265): INFO Train: [76/300][510/1562]	eta 0:05:21 lr 0.000005	time 0.2990 (0.3060)	loss 0.8741 (1.3237)	grad_norm 49.7349 (30.4381)	mem 4879MB
[2022-05-31 09:05:47 MetaFG_0] (main.py 265): INFO Train: [76/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.2975 (0.3060)	loss 1.6964 (1.3252)	grad_norm 31.7946 (30.4080)	mem 4879MB
[2022-05-31 09:05:50 MetaFG_0] (main.py 265): INFO Train: [76/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2940 (0.3059)	loss 1.1329 (1.3219)	grad_norm 24.5112 (30.3090)	mem 4879MB
[2022-05-31 09:05:53 MetaFG_0] (main.py 265): INFO Train: [76/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2932 (0.3059)	loss 1.4180 (1.3214)	grad_norm 21.3557 (30.3662)	mem 4879MB
[2022-05-31 09:05:56 MetaFG_0] (main.py 265): INFO Train: [76/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.3011 (0.3058)	loss 1.7399 (1.3198)	grad_norm 18.1860 (30.3231)	mem 4879MB
[2022-05-31 09:05:59 MetaFG_0] (main.py 265): INFO Train: [76/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2992 (0.3058)	loss 1.0005 (1.3189)	grad_norm 11.8081 (30.2978)	mem 4879MB
[2022-05-31 09:06:02 MetaFG_0] (main.py 265): INFO Train: [76/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2938 (0.3058)	loss 1.2405 (1.3191)	grad_norm 32.9663 (30.2371)	mem 4879MB
[2022-05-31 09:06:05 MetaFG_0] (main.py 265): INFO Train: [76/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.3014 (0.3058)	loss 1.5588 (1.3192)	grad_norm 16.2740 (30.3973)	mem 4879MB
[2022-05-31 09:06:08 MetaFG_0] (main.py 265): INFO Train: [76/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.3001 (0.3058)	loss 1.4663 (1.3192)	grad_norm 45.5851 (30.4465)	mem 4879MB
[2022-05-31 09:06:11 MetaFG_0] (main.py 265): INFO Train: [76/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2930 (0.3058)	loss 1.3783 (1.3201)	grad_norm 20.2758 (30.3974)	mem 4879MB
[2022-05-31 09:06:15 MetaFG_0] (main.py 265): INFO Train: [76/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2926 (0.3057)	loss 1.6543 (1.3194)	grad_norm 17.4185 (30.4024)	mem 4879MB
[2022-05-31 09:06:18 MetaFG_0] (main.py 265): INFO Train: [76/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2921 (0.3057)	loss 1.1284 (1.3184)	grad_norm 37.1380 (30.4395)	mem 4879MB
[2022-05-31 09:06:21 MetaFG_0] (main.py 265): INFO Train: [76/300][630/1562]	eta 0:04:44 lr 0.000005	time 0.2924 (0.3057)	loss 1.2532 (1.3186)	grad_norm 22.8958 (30.4682)	mem 4879MB
[2022-05-31 09:06:24 MetaFG_0] (main.py 265): INFO Train: [76/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.2938 (0.3057)	loss 1.2422 (1.3169)	grad_norm 49.4361 (30.4833)	mem 4879MB
[2022-05-31 09:06:27 MetaFG_0] (main.py 265): INFO Train: [76/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.3085 (0.3057)	loss 1.4587 (1.3180)	grad_norm 42.1284 (30.5006)	mem 4879MB
[2022-05-31 09:06:30 MetaFG_0] (main.py 265): INFO Train: [76/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2976 (0.3059)	loss 1.4664 (1.3188)	grad_norm 36.3453 (30.4623)	mem 4879MB
[2022-05-31 09:06:33 MetaFG_0] (main.py 265): INFO Train: [76/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.2996 (0.3059)	loss 1.4562 (1.3192)	grad_norm 28.3938 (30.5281)	mem 4879MB
[2022-05-31 09:06:36 MetaFG_0] (main.py 265): INFO Train: [76/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2938 (0.3059)	loss 1.6638 (1.3215)	grad_norm 25.6616 (30.4743)	mem 4879MB
[2022-05-31 09:06:39 MetaFG_0] (main.py 265): INFO Train: [76/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2994 (0.3059)	loss 1.3360 (1.3213)	grad_norm 33.8282 (30.4555)	mem 4879MB
[2022-05-31 09:06:42 MetaFG_0] (main.py 265): INFO Train: [76/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2924 (0.3059)	loss 1.2071 (1.3213)	grad_norm 20.4597 (30.4727)	mem 4879MB
[2022-05-31 09:06:45 MetaFG_0] (main.py 265): INFO Train: [76/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2921 (0.3059)	loss 1.6924 (1.3222)	grad_norm 41.5277 (30.4404)	mem 4879MB
[2022-05-31 09:06:48 MetaFG_0] (main.py 265): INFO Train: [76/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2932 (0.3058)	loss 1.2917 (1.3223)	grad_norm 39.1596 (30.4572)	mem 4879MB
[2022-05-31 09:06:51 MetaFG_0] (main.py 265): INFO Train: [76/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2935 (0.3058)	loss 1.1979 (1.3203)	grad_norm 24.3002 (30.5520)	mem 4879MB
[2022-05-31 09:06:54 MetaFG_0] (main.py 265): INFO Train: [76/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2929 (0.3058)	loss 1.5265 (1.3189)	grad_norm 21.4189 (30.5261)	mem 4879MB
[2022-05-31 09:06:57 MetaFG_0] (main.py 265): INFO Train: [76/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2985 (0.3058)	loss 1.2743 (1.3188)	grad_norm 48.0552 (30.5643)	mem 4879MB
[2022-05-31 09:07:00 MetaFG_0] (main.py 265): INFO Train: [76/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2999 (0.3057)	loss 1.0667 (1.3188)	grad_norm 29.4268 (30.4957)	mem 4879MB
[2022-05-31 09:07:03 MetaFG_0] (main.py 265): INFO Train: [76/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2925 (0.3057)	loss 1.3108 (1.3193)	grad_norm 38.0993 (30.4634)	mem 4879MB
[2022-05-31 09:07:06 MetaFG_0] (main.py 265): INFO Train: [76/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2920 (0.3057)	loss 1.1767 (1.3186)	grad_norm 30.9931 (30.4676)	mem 4879MB
[2022-05-31 09:07:10 MetaFG_0] (main.py 265): INFO Train: [76/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2927 (0.3057)	loss 1.4018 (1.3201)	grad_norm 41.8181 (30.4251)	mem 4879MB
[2022-05-31 09:07:13 MetaFG_0] (main.py 265): INFO Train: [76/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2980 (0.3057)	loss 1.3264 (1.3190)	grad_norm 15.0307 (30.3263)	mem 4879MB
[2022-05-31 09:07:16 MetaFG_0] (main.py 265): INFO Train: [76/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2939 (0.3057)	loss 1.3458 (1.3190)	grad_norm 22.9510 (30.2731)	mem 4879MB
[2022-05-31 09:07:19 MetaFG_0] (main.py 265): INFO Train: [76/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.3014 (0.3056)	loss 0.9064 (1.3176)	grad_norm 33.3698 (30.2840)	mem 4879MB
[2022-05-31 09:07:22 MetaFG_0] (main.py 265): INFO Train: [76/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2923 (0.3056)	loss 1.5450 (1.3179)	grad_norm 15.8592 (30.2581)	mem 4879MB
[2022-05-31 09:07:25 MetaFG_0] (main.py 265): INFO Train: [76/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2937 (0.3056)	loss 1.2257 (1.3167)	grad_norm 40.5683 (30.2109)	mem 4879MB
[2022-05-31 09:07:28 MetaFG_0] (main.py 265): INFO Train: [76/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2993 (0.3056)	loss 1.1513 (1.3165)	grad_norm 36.3620 (30.2229)	mem 4879MB
[2022-05-31 09:07:31 MetaFG_0] (main.py 265): INFO Train: [76/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2916 (0.3056)	loss 0.9671 (1.3158)	grad_norm 23.5515 (30.1714)	mem 4879MB
[2022-05-31 09:07:34 MetaFG_0] (main.py 265): INFO Train: [76/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2918 (0.3056)	loss 1.5076 (1.3148)	grad_norm 21.7901 (30.1417)	mem 4879MB
[2022-05-31 09:07:37 MetaFG_0] (main.py 265): INFO Train: [76/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2928 (0.3056)	loss 0.9368 (1.3150)	grad_norm 21.3774 (30.0694)	mem 4879MB
[2022-05-31 09:07:40 MetaFG_0] (main.py 265): INFO Train: [76/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2936 (0.3055)	loss 0.7240 (1.3134)	grad_norm 58.0414 (30.0906)	mem 4879MB
[2022-05-31 09:07:43 MetaFG_0] (main.py 265): INFO Train: [76/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2928 (0.3055)	loss 1.3675 (1.3137)	grad_norm 22.1616 (30.0691)	mem 4879MB
[2022-05-31 09:07:46 MetaFG_0] (main.py 265): INFO Train: [76/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2935 (0.3055)	loss 1.3074 (1.3142)	grad_norm 10.6548 (30.0791)	mem 4879MB
[2022-05-31 09:07:49 MetaFG_0] (main.py 265): INFO Train: [76/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2985 (0.3055)	loss 1.4048 (1.3143)	grad_norm 52.1507 (30.0442)	mem 4879MB
[2022-05-31 09:07:52 MetaFG_0] (main.py 265): INFO Train: [76/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2932 (0.3055)	loss 1.3685 (1.3146)	grad_norm 30.8174 (30.0677)	mem 4879MB
[2022-05-31 09:07:55 MetaFG_0] (main.py 265): INFO Train: [76/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2986 (0.3054)	loss 1.1688 (1.3150)	grad_norm 30.8963 (30.0443)	mem 4879MB
[2022-05-31 09:07:58 MetaFG_0] (main.py 265): INFO Train: [76/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.2986 (0.3054)	loss 1.3581 (1.3145)	grad_norm 23.5091 (30.0724)	mem 4879MB
[2022-05-31 09:08:01 MetaFG_0] (main.py 265): INFO Train: [76/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2982 (0.3054)	loss 1.2844 (1.3145)	grad_norm 34.9366 (30.0533)	mem 4879MB
[2022-05-31 09:08:04 MetaFG_0] (main.py 265): INFO Train: [76/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.3120 (0.3054)	loss 1.5320 (1.3154)	grad_norm 47.6957 (30.0396)	mem 4879MB
[2022-05-31 09:08:07 MetaFG_0] (main.py 265): INFO Train: [76/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2998 (0.3054)	loss 0.6681 (1.3150)	grad_norm 17.0014 (30.1002)	mem 4879MB
[2022-05-31 09:08:10 MetaFG_0] (main.py 265): INFO Train: [76/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2934 (0.3054)	loss 1.2682 (1.3156)	grad_norm 19.5019 (30.0401)	mem 4879MB
[2022-05-31 09:08:13 MetaFG_0] (main.py 265): INFO Train: [76/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2981 (0.3054)	loss 1.3118 (1.3154)	grad_norm 44.2027 (30.0688)	mem 4879MB
[2022-05-31 09:08:16 MetaFG_0] (main.py 265): INFO Train: [76/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2927 (0.3054)	loss 1.1507 (1.3141)	grad_norm 27.2063 (30.0191)	mem 4879MB
[2022-05-31 09:08:20 MetaFG_0] (main.py 265): INFO Train: [76/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2987 (0.3054)	loss 1.5254 (1.3126)	grad_norm 39.9080 (30.0943)	mem 4879MB
[2022-05-31 09:08:23 MetaFG_0] (main.py 265): INFO Train: [76/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2980 (0.3054)	loss 1.3446 (1.3140)	grad_norm 22.6882 (30.1125)	mem 4879MB
[2022-05-31 09:08:26 MetaFG_0] (main.py 265): INFO Train: [76/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2939 (0.3054)	loss 1.4078 (1.3143)	grad_norm 21.0460 (30.0678)	mem 4879MB
[2022-05-31 09:08:29 MetaFG_0] (main.py 265): INFO Train: [76/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.3042 (0.3054)	loss 1.5343 (1.3150)	grad_norm 51.7546 (30.0737)	mem 4879MB
[2022-05-31 09:08:32 MetaFG_0] (main.py 265): INFO Train: [76/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2929 (0.3054)	loss 1.7945 (1.3159)	grad_norm 25.8453 (30.0846)	mem 4879MB
[2022-05-31 09:08:35 MetaFG_0] (main.py 265): INFO Train: [76/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2936 (0.3054)	loss 1.8449 (1.3170)	grad_norm 27.4918 (30.1024)	mem 4879MB
[2022-05-31 09:08:38 MetaFG_0] (main.py 265): INFO Train: [76/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2933 (0.3054)	loss 1.3126 (1.3175)	grad_norm 43.8950 (30.1248)	mem 4879MB
[2022-05-31 09:08:41 MetaFG_0] (main.py 265): INFO Train: [76/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2993 (0.3053)	loss 0.8740 (1.3175)	grad_norm 23.1695 (30.1880)	mem 4879MB
[2022-05-31 09:08:44 MetaFG_0] (main.py 265): INFO Train: [76/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2940 (0.3053)	loss 1.1460 (1.3167)	grad_norm 31.5972 (30.2365)	mem 4879MB
[2022-05-31 09:08:47 MetaFG_0] (main.py 265): INFO Train: [76/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2934 (0.3053)	loss 1.4384 (1.3161)	grad_norm 17.0124 (30.2445)	mem 4879MB
[2022-05-31 09:08:50 MetaFG_0] (main.py 265): INFO Train: [76/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2977 (0.3053)	loss 1.1742 (1.3148)	grad_norm 45.4778 (30.2503)	mem 4879MB
[2022-05-31 09:08:53 MetaFG_0] (main.py 265): INFO Train: [76/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.3010 (0.3053)	loss 1.5740 (1.3148)	grad_norm 41.5366 (30.2861)	mem 4879MB
[2022-05-31 09:08:56 MetaFG_0] (main.py 265): INFO Train: [76/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2997 (0.3053)	loss 1.3190 (1.3150)	grad_norm 28.3187 (30.2347)	mem 4879MB
[2022-05-31 09:08:59 MetaFG_0] (main.py 265): INFO Train: [76/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2992 (0.3053)	loss 1.2837 (1.3147)	grad_norm 29.1889 (30.2558)	mem 4879MB
[2022-05-31 09:09:02 MetaFG_0] (main.py 265): INFO Train: [76/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2944 (0.3053)	loss 1.4784 (1.3144)	grad_norm 31.0813 (30.2742)	mem 4879MB
[2022-05-31 09:09:05 MetaFG_0] (main.py 265): INFO Train: [76/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2986 (0.3053)	loss 1.4598 (1.3148)	grad_norm 21.0080 (30.2943)	mem 4879MB
[2022-05-31 09:09:08 MetaFG_0] (main.py 265): INFO Train: [76/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2921 (0.3053)	loss 1.0885 (1.3150)	grad_norm 24.7079 (30.2729)	mem 4879MB
[2022-05-31 09:09:11 MetaFG_0] (main.py 265): INFO Train: [76/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2930 (0.3053)	loss 1.0671 (1.3150)	grad_norm 39.4881 (30.2799)	mem 4879MB
[2022-05-31 09:09:14 MetaFG_0] (main.py 265): INFO Train: [76/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2990 (0.3053)	loss 1.5037 (1.3156)	grad_norm 27.5811 (30.2437)	mem 4879MB
[2022-05-31 09:09:17 MetaFG_0] (main.py 265): INFO Train: [76/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2932 (0.3053)	loss 1.4859 (1.3155)	grad_norm 37.7479 (30.2033)	mem 4879MB
[2022-05-31 09:09:20 MetaFG_0] (main.py 265): INFO Train: [76/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2986 (0.3053)	loss 1.4835 (1.3152)	grad_norm 13.1078 (30.1602)	mem 4879MB
[2022-05-31 09:09:24 MetaFG_0] (main.py 265): INFO Train: [76/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2937 (0.3053)	loss 1.2897 (1.3156)	grad_norm 59.7835 (30.1650)	mem 4879MB
[2022-05-31 09:09:27 MetaFG_0] (main.py 265): INFO Train: [76/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2918 (0.3053)	loss 1.4536 (1.3160)	grad_norm 25.4290 (30.1850)	mem 4879MB
[2022-05-31 09:09:30 MetaFG_0] (main.py 265): INFO Train: [76/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2925 (0.3053)	loss 1.6004 (1.3159)	grad_norm 35.3519 (30.1648)	mem 4879MB
[2022-05-31 09:09:33 MetaFG_0] (main.py 265): INFO Train: [76/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2944 (0.3053)	loss 1.3948 (1.3164)	grad_norm 34.7744 (30.1812)	mem 4879MB
[2022-05-31 09:09:36 MetaFG_0] (main.py 265): INFO Train: [76/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2977 (0.3053)	loss 1.2642 (1.3165)	grad_norm 38.8909 (30.1773)	mem 4879MB
[2022-05-31 09:09:39 MetaFG_0] (main.py 265): INFO Train: [76/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2929 (0.3052)	loss 0.9180 (1.3164)	grad_norm 58.6648 (30.1799)	mem 4879MB
[2022-05-31 09:09:42 MetaFG_0] (main.py 265): INFO Train: [76/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2930 (0.3052)	loss 0.8303 (1.3157)	grad_norm 22.3611 (30.1532)	mem 4879MB
[2022-05-31 09:09:45 MetaFG_0] (main.py 265): INFO Train: [76/300][1300/1562]	eta 0:01:19 lr 0.000005	time 0.2926 (0.3052)	loss 1.4855 (1.3162)	grad_norm 34.8267 (30.1216)	mem 4879MB
[2022-05-31 09:09:48 MetaFG_0] (main.py 265): INFO Train: [76/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2933 (0.3052)	loss 0.9806 (1.3161)	grad_norm 18.4067 (30.1133)	mem 4879MB
[2022-05-31 09:09:51 MetaFG_0] (main.py 265): INFO Train: [76/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2925 (0.3052)	loss 1.5182 (1.3153)	grad_norm 25.2263 (30.1272)	mem 4879MB
[2022-05-31 09:09:54 MetaFG_0] (main.py 265): INFO Train: [76/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2934 (0.3052)	loss 1.3723 (1.3157)	grad_norm 20.1395 (30.1295)	mem 4879MB
[2022-05-31 09:09:57 MetaFG_0] (main.py 265): INFO Train: [76/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.3042 (0.3052)	loss 1.2381 (1.3161)	grad_norm 19.2556 (30.1129)	mem 4879MB
[2022-05-31 09:10:00 MetaFG_0] (main.py 265): INFO Train: [76/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2988 (0.3052)	loss 1.3926 (1.3171)	grad_norm 25.9077 (30.1029)	mem 4879MB
[2022-05-31 09:10:03 MetaFG_0] (main.py 265): INFO Train: [76/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2945 (0.3052)	loss 1.3499 (1.3169)	grad_norm 34.3663 (30.1152)	mem 4879MB
[2022-05-31 09:10:06 MetaFG_0] (main.py 265): INFO Train: [76/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2933 (0.3052)	loss 1.4073 (1.3170)	grad_norm 30.0919 (30.0984)	mem 4879MB
[2022-05-31 09:10:09 MetaFG_0] (main.py 265): INFO Train: [76/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2980 (0.3052)	loss 0.7642 (1.3168)	grad_norm 30.3266 (30.0969)	mem 4879MB
[2022-05-31 09:10:12 MetaFG_0] (main.py 265): INFO Train: [76/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2924 (0.3052)	loss 1.0779 (1.3170)	grad_norm 26.6082 (30.1104)	mem 4879MB
[2022-05-31 09:10:15 MetaFG_0] (main.py 265): INFO Train: [76/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2989 (0.3052)	loss 1.4400 (1.3169)	grad_norm 21.0732 (30.1120)	mem 4879MB
[2022-05-31 09:10:18 MetaFG_0] (main.py 265): INFO Train: [76/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2922 (0.3052)	loss 1.4883 (1.3173)	grad_norm 18.5569 (30.0806)	mem 4879MB
[2022-05-31 09:10:21 MetaFG_0] (main.py 265): INFO Train: [76/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2926 (0.3052)	loss 1.0738 (1.3168)	grad_norm 23.6249 (30.0844)	mem 4879MB
[2022-05-31 09:10:24 MetaFG_0] (main.py 265): INFO Train: [76/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2979 (0.3052)	loss 0.9114 (1.3156)	grad_norm 20.1813 (30.0691)	mem 4879MB
[2022-05-31 09:10:27 MetaFG_0] (main.py 265): INFO Train: [76/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2954 (0.3051)	loss 1.2551 (1.3155)	grad_norm 22.2100 (30.0422)	mem 4879MB
[2022-05-31 09:10:30 MetaFG_0] (main.py 265): INFO Train: [76/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2983 (0.3051)	loss 0.9124 (1.3151)	grad_norm 44.2569 (30.0428)	mem 4879MB
[2022-05-31 09:10:33 MetaFG_0] (main.py 265): INFO Train: [76/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2929 (0.3051)	loss 1.3747 (1.3144)	grad_norm 25.6748 (30.0242)	mem 4879MB
[2022-05-31 09:10:37 MetaFG_0] (main.py 265): INFO Train: [76/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2992 (0.3051)	loss 1.1552 (1.3144)	grad_norm 47.8091 (30.0393)	mem 4879MB
[2022-05-31 09:10:40 MetaFG_0] (main.py 265): INFO Train: [76/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.3004 (0.3051)	loss 0.9708 (1.3145)	grad_norm 48.9724 (30.0321)	mem 4879MB
[2022-05-31 09:10:43 MetaFG_0] (main.py 265): INFO Train: [76/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2961 (0.3051)	loss 1.3770 (1.3148)	grad_norm 32.5859 (30.0200)	mem 4879MB
[2022-05-31 09:10:46 MetaFG_0] (main.py 265): INFO Train: [76/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2987 (0.3051)	loss 1.3036 (1.3148)	grad_norm 23.2921 (29.9964)	mem 4879MB
[2022-05-31 09:10:49 MetaFG_0] (main.py 265): INFO Train: [76/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2931 (0.3051)	loss 1.0290 (1.3156)	grad_norm 24.5826 (29.9731)	mem 4879MB
[2022-05-31 09:10:52 MetaFG_0] (main.py 265): INFO Train: [76/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.3026 (0.3051)	loss 1.6364 (1.3158)	grad_norm 16.9446 (29.9807)	mem 4879MB
[2022-05-31 09:10:55 MetaFG_0] (main.py 265): INFO Train: [76/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.3004 (0.3051)	loss 1.2910 (1.3158)	grad_norm 26.7030 (30.0215)	mem 4879MB
[2022-05-31 09:10:58 MetaFG_0] (main.py 265): INFO Train: [76/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2935 (0.3051)	loss 1.0304 (1.3159)	grad_norm 27.4116 (30.0154)	mem 4879MB
[2022-05-31 09:11:01 MetaFG_0] (main.py 265): INFO Train: [76/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2923 (0.3051)	loss 1.1163 (1.3160)	grad_norm 20.9401 (30.0274)	mem 4879MB
[2022-05-31 09:11:04 MetaFG_0] (main.py 265): INFO Train: [76/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2925 (0.3051)	loss 1.7033 (1.3169)	grad_norm 35.9898 (30.0571)	mem 4879MB
[2022-05-31 09:11:04 MetaFG_0] (main.py 272): INFO EPOCH 76 training takes 0:07:56
[2022-05-31 09:11:04 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_76.pth saving......
[2022-05-31 09:11:05 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_76.pth saved !!!
[2022-05-31 09:11:05 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 09:11:07 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 09:11:07 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 09:11:07 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.710 (0.710)	Loss 0.6652 (0.6652)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 09:11:08 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.090 (0.154)	Loss 0.6180 (0.6684)	Acc@1 90.625 (88.068)	Acc@5 96.875 (98.864)	Mem 4879MB
[2022-05-31 09:11:09 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.126)	Loss 0.7234 (0.6810)	Acc@1 84.375 (87.798)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 09:11:10 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.096 (0.115)	Loss 0.7258 (0.6821)	Acc@1 87.500 (87.399)	Acc@5 100.000 (98.992)	Mem 4879MB
[2022-05-31 09:11:11 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.097 (0.111)	Loss 0.5907 (0.6768)	Acc@1 90.625 (87.652)	Acc@5 100.000 (98.780)	Mem 4879MB
[2022-05-31 09:11:12 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.097 (0.108)	Loss 0.5391 (0.6735)	Acc@1 90.625 (87.561)	Acc@5 100.000 (98.775)	Mem 4879MB
[2022-05-31 09:11:13 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.097 (0.106)	Loss 1.0305 (0.6709)	Acc@1 78.125 (87.859)	Acc@5 96.875 (98.873)	Mem 4879MB
[2022-05-31 09:11:14 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.101 (0.104)	Loss 0.8190 (0.6730)	Acc@1 93.750 (87.808)	Acc@5 96.875 (98.944)	Mem 4879MB
[2022-05-31 09:11:15 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.103)	Loss 0.8877 (0.6695)	Acc@1 75.000 (87.886)	Acc@5 96.875 (98.920)	Mem 4879MB
[2022-05-31 09:11:16 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.107 (0.102)	Loss 0.5219 (0.6737)	Acc@1 96.875 (87.569)	Acc@5 96.875 (98.935)	Mem 4879MB
[2022-05-31 09:11:17 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.092 (0.102)	Loss 0.5165 (0.6695)	Acc@1 96.875 (87.686)	Acc@5 100.000 (98.917)	Mem 4879MB
[2022-05-31 09:11:18 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.7281 (0.6750)	Acc@1 87.500 (87.528)	Acc@5 100.000 (98.846)	Mem 4879MB
[2022-05-31 09:11:19 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.100)	Loss 0.7194 (0.6769)	Acc@1 90.625 (87.474)	Acc@5 100.000 (98.889)	Mem 4879MB
[2022-05-31 09:11:20 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.100)	Loss 0.8744 (0.6731)	Acc@1 75.000 (87.381)	Acc@5 100.000 (98.950)	Mem 4879MB
[2022-05-31 09:11:21 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.094 (0.099)	Loss 0.7292 (0.6746)	Acc@1 87.500 (87.434)	Acc@5 100.000 (99.003)	Mem 4879MB
[2022-05-31 09:11:22 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.092 (0.099)	Loss 0.5255 (0.6687)	Acc@1 93.750 (87.686)	Acc@5 100.000 (99.007)	Mem 4879MB
[2022-05-31 09:11:22 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.094 (0.099)	Loss 0.6139 (0.6681)	Acc@1 90.625 (87.733)	Acc@5 100.000 (99.010)	Mem 4879MB
[2022-05-31 09:11:23 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 0.7807 (0.6714)	Acc@1 84.375 (87.573)	Acc@5 100.000 (98.995)	Mem 4879MB
[2022-05-31 09:11:24 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.098)	Loss 0.5812 (0.6743)	Acc@1 93.750 (87.535)	Acc@5 100.000 (98.947)	Mem 4879MB
[2022-05-31 09:11:25 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.097 (0.098)	Loss 0.8930 (0.6750)	Acc@1 84.375 (87.565)	Acc@5 96.875 (98.953)	Mem 4879MB
[2022-05-31 09:11:26 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.098)	Loss 0.7317 (0.6805)	Acc@1 84.375 (87.345)	Acc@5 96.875 (98.896)	Mem 4879MB
[2022-05-31 09:11:27 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.093 (0.098)	Loss 0.6664 (0.6820)	Acc@1 87.500 (87.263)	Acc@5 100.000 (98.904)	Mem 4879MB
[2022-05-31 09:11:28 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.094 (0.098)	Loss 0.6033 (0.6823)	Acc@1 90.625 (87.288)	Acc@5 100.000 (98.883)	Mem 4879MB
[2022-05-31 09:11:29 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.087 (0.097)	Loss 0.7444 (0.6839)	Acc@1 87.500 (87.243)	Acc@5 100.000 (98.904)	Mem 4879MB
[2022-05-31 09:11:30 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.087 (0.097)	Loss 0.6514 (0.6842)	Acc@1 87.500 (87.267)	Acc@5 96.875 (98.872)	Mem 4879MB
[2022-05-31 09:11:31 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.096 (0.097)	Loss 0.8138 (0.6832)	Acc@1 81.250 (87.363)	Acc@5 100.000 (98.867)	Mem 4879MB
[2022-05-31 09:11:32 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.098 (0.097)	Loss 0.5816 (0.6841)	Acc@1 93.750 (87.380)	Acc@5 100.000 (98.851)	Mem 4879MB
[2022-05-31 09:11:33 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.097)	Loss 0.6854 (0.6849)	Acc@1 93.750 (87.373)	Acc@5 96.875 (98.847)	Mem 4879MB
[2022-05-31 09:11:34 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.097)	Loss 0.8184 (0.6831)	Acc@1 81.250 (87.433)	Acc@5 96.875 (98.832)	Mem 4879MB
[2022-05-31 09:11:35 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.096 (0.097)	Loss 0.8007 (0.6854)	Acc@1 87.500 (87.382)	Acc@5 90.625 (98.744)	Mem 4879MB
[2022-05-31 09:11:36 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.101 (0.097)	Loss 0.6088 (0.6846)	Acc@1 87.500 (87.427)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 09:11:37 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.096)	Loss 0.8131 (0.6851)	Acc@1 75.000 (87.359)	Acc@5 100.000 (98.744)	Mem 4879MB
[2022-05-31 09:11:37 MetaFG_0] (main.py 330): INFO  * Acc@1 87.340 Acc@5 98.730
[2022-05-31 09:11:37 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.3%
[2022-05-31 09:11:37 MetaFG_0] (main.py 171): INFO Max accuracy: 87.58%
[2022-05-31 09:11:38 MetaFG_0] (main.py 265): INFO Train: [77/300][0/1562]	eta 0:25:41 lr 0.000005	time 0.9868 (0.9868)	loss 1.0439 (1.0439)	grad_norm 31.8356 (31.8356)	mem 4879MB
[2022-05-31 09:11:41 MetaFG_0] (main.py 265): INFO Train: [77/300][10/1562]	eta 0:09:40 lr 0.000005	time 0.3003 (0.3741)	loss 1.3782 (1.2649)	grad_norm 31.8934 (30.9896)	mem 4879MB
[2022-05-31 09:11:44 MetaFG_0] (main.py 265): INFO Train: [77/300][20/1562]	eta 0:08:45 lr 0.000005	time 0.2996 (0.3411)	loss 1.3808 (1.2936)	grad_norm 31.9974 (29.1192)	mem 4879MB
[2022-05-31 09:11:47 MetaFG_0] (main.py 265): INFO Train: [77/300][30/1562]	eta 0:08:24 lr 0.000005	time 0.2985 (0.3290)	loss 1.4132 (1.3051)	grad_norm 22.1427 (29.5484)	mem 4879MB
[2022-05-31 09:11:50 MetaFG_0] (main.py 265): INFO Train: [77/300][40/1562]	eta 0:08:10 lr 0.000005	time 0.2935 (0.3226)	loss 1.0472 (1.2968)	grad_norm 31.2840 (29.8383)	mem 4879MB
[2022-05-31 09:11:53 MetaFG_0] (main.py 265): INFO Train: [77/300][50/1562]	eta 0:08:01 lr 0.000005	time 0.2935 (0.3188)	loss 0.8275 (1.2815)	grad_norm 25.6448 (29.2496)	mem 4879MB
[2022-05-31 09:11:56 MetaFG_0] (main.py 265): INFO Train: [77/300][60/1562]	eta 0:07:55 lr 0.000005	time 0.2994 (0.3166)	loss 1.1592 (1.2812)	grad_norm 48.9675 (29.5861)	mem 4879MB
[2022-05-31 09:11:59 MetaFG_0] (main.py 265): INFO Train: [77/300][70/1562]	eta 0:07:49 lr 0.000005	time 0.2928 (0.3150)	loss 1.4479 (1.2771)	grad_norm 22.4695 (29.2368)	mem 4879MB
[2022-05-31 09:12:02 MetaFG_0] (main.py 265): INFO Train: [77/300][80/1562]	eta 0:07:45 lr 0.000005	time 0.2976 (0.3139)	loss 1.3993 (1.2840)	grad_norm 23.7642 (28.4416)	mem 4879MB
[2022-05-31 09:12:05 MetaFG_0] (main.py 265): INFO Train: [77/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.3012 (0.3129)	loss 1.2315 (1.2938)	grad_norm 23.4178 (29.2397)	mem 4879MB
[2022-05-31 09:12:08 MetaFG_0] (main.py 265): INFO Train: [77/300][100/1562]	eta 0:07:36 lr 0.000005	time 0.2988 (0.3123)	loss 1.3313 (1.2891)	grad_norm 28.0890 (29.4216)	mem 4879MB
[2022-05-31 09:12:11 MetaFG_0] (main.py 265): INFO Train: [77/300][110/1562]	eta 0:07:32 lr 0.000005	time 0.2946 (0.3117)	loss 1.3754 (1.2950)	grad_norm 20.9334 (29.5000)	mem 4879MB
[2022-05-31 09:12:14 MetaFG_0] (main.py 265): INFO Train: [77/300][120/1562]	eta 0:07:28 lr 0.000005	time 0.2930 (0.3112)	loss 1.3603 (1.2973)	grad_norm 38.1594 (29.8812)	mem 4879MB
[2022-05-31 09:12:18 MetaFG_0] (main.py 265): INFO Train: [77/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.2988 (0.3107)	loss 1.0081 (1.2938)	grad_norm 30.1107 (30.0290)	mem 4879MB
[2022-05-31 09:12:21 MetaFG_0] (main.py 265): INFO Train: [77/300][140/1562]	eta 0:07:21 lr 0.000005	time 0.2938 (0.3104)	loss 1.4301 (1.3062)	grad_norm 18.2597 (30.1171)	mem 4879MB
[2022-05-31 09:12:24 MetaFG_0] (main.py 265): INFO Train: [77/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2941 (0.3102)	loss 1.5526 (1.3056)	grad_norm 39.2967 (30.4201)	mem 4879MB
[2022-05-31 09:12:27 MetaFG_0] (main.py 265): INFO Train: [77/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.2935 (0.3099)	loss 1.2752 (1.3081)	grad_norm 27.5462 (30.5942)	mem 4879MB
[2022-05-31 09:12:30 MetaFG_0] (main.py 265): INFO Train: [77/300][170/1562]	eta 0:07:11 lr 0.000005	time 0.2919 (0.3097)	loss 1.3302 (1.3047)	grad_norm 26.8265 (30.5792)	mem 4879MB
[2022-05-31 09:12:33 MetaFG_0] (main.py 265): INFO Train: [77/300][180/1562]	eta 0:07:07 lr 0.000005	time 0.2929 (0.3094)	loss 1.2533 (1.3111)	grad_norm 28.6874 (30.4397)	mem 4879MB
[2022-05-31 09:12:36 MetaFG_0] (main.py 265): INFO Train: [77/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2925 (0.3090)	loss 1.3682 (1.3146)	grad_norm 38.6912 (30.2179)	mem 4879MB
[2022-05-31 09:12:39 MetaFG_0] (main.py 265): INFO Train: [77/300][200/1562]	eta 0:07:00 lr 0.000005	time 0.2922 (0.3087)	loss 1.6831 (1.3171)	grad_norm 29.0646 (30.4731)	mem 4879MB
[2022-05-31 09:12:42 MetaFG_0] (main.py 265): INFO Train: [77/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2950 (0.3084)	loss 1.3564 (1.3187)	grad_norm 31.6519 (30.4670)	mem 4879MB
[2022-05-31 09:12:45 MetaFG_0] (main.py 265): INFO Train: [77/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2983 (0.3084)	loss 1.4290 (1.3140)	grad_norm 31.5362 (30.5454)	mem 4879MB
[2022-05-31 09:12:48 MetaFG_0] (main.py 265): INFO Train: [77/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2999 (0.3082)	loss 1.6782 (1.3133)	grad_norm 36.1856 (31.0577)	mem 4879MB
[2022-05-31 09:12:51 MetaFG_0] (main.py 265): INFO Train: [77/300][240/1562]	eta 0:06:47 lr 0.000005	time 0.2931 (0.3080)	loss 1.1163 (1.3151)	grad_norm 42.0494 (31.3444)	mem 4879MB
[2022-05-31 09:12:54 MetaFG_0] (main.py 265): INFO Train: [77/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2947 (0.3078)	loss 1.6089 (1.3148)	grad_norm 33.1061 (31.4168)	mem 4879MB
[2022-05-31 09:12:57 MetaFG_0] (main.py 265): INFO Train: [77/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2988 (0.3077)	loss 1.3000 (1.3170)	grad_norm 47.5487 (31.4522)	mem 4879MB
[2022-05-31 09:13:00 MetaFG_0] (main.py 265): INFO Train: [77/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2936 (0.3076)	loss 1.5806 (1.3173)	grad_norm 22.8250 (31.5857)	mem 4879MB
[2022-05-31 09:13:03 MetaFG_0] (main.py 265): INFO Train: [77/300][280/1562]	eta 0:06:34 lr 0.000005	time 0.2935 (0.3075)	loss 1.3402 (1.3143)	grad_norm 21.7286 (31.4229)	mem 4879MB
[2022-05-31 09:13:06 MetaFG_0] (main.py 265): INFO Train: [77/300][290/1562]	eta 0:06:31 lr 0.000005	time 0.2928 (0.3074)	loss 0.8846 (1.3123)	grad_norm 26.0426 (31.4072)	mem 4879MB
[2022-05-31 09:13:10 MetaFG_0] (main.py 265): INFO Train: [77/300][300/1562]	eta 0:06:28 lr 0.000005	time 0.3097 (0.3080)	loss 1.3020 (1.3111)	grad_norm 40.9029 (31.2084)	mem 4879MB
[2022-05-31 09:13:13 MetaFG_0] (main.py 265): INFO Train: [77/300][310/1562]	eta 0:06:25 lr 0.000005	time 0.2919 (0.3079)	loss 1.4401 (1.3069)	grad_norm 31.1891 (31.4402)	mem 4879MB
[2022-05-31 09:13:16 MetaFG_0] (main.py 265): INFO Train: [77/300][320/1562]	eta 0:06:22 lr 0.000005	time 0.2986 (0.3078)	loss 1.2729 (1.3071)	grad_norm 16.2224 (31.4067)	mem 4879MB
[2022-05-31 09:13:19 MetaFG_0] (main.py 265): INFO Train: [77/300][330/1562]	eta 0:06:19 lr 0.000005	time 0.2934 (0.3077)	loss 1.4744 (1.3098)	grad_norm 31.0035 (31.2697)	mem 4879MB
[2022-05-31 09:13:22 MetaFG_0] (main.py 265): INFO Train: [77/300][340/1562]	eta 0:06:15 lr 0.000005	time 0.2985 (0.3075)	loss 1.3404 (1.3111)	grad_norm 23.9815 (31.3212)	mem 4879MB
[2022-05-31 09:13:25 MetaFG_0] (main.py 265): INFO Train: [77/300][350/1562]	eta 0:06:12 lr 0.000005	time 0.3003 (0.3075)	loss 1.5738 (1.3102)	grad_norm 24.8962 (31.2209)	mem 4879MB
[2022-05-31 09:13:28 MetaFG_0] (main.py 265): INFO Train: [77/300][360/1562]	eta 0:06:09 lr 0.000005	time 0.2923 (0.3075)	loss 1.3045 (1.3111)	grad_norm 18.8645 (31.0271)	mem 4879MB
[2022-05-31 09:13:31 MetaFG_0] (main.py 265): INFO Train: [77/300][370/1562]	eta 0:06:06 lr 0.000005	time 0.2931 (0.3073)	loss 1.4379 (1.3126)	grad_norm 19.9344 (31.0487)	mem 4879MB
[2022-05-31 09:13:34 MetaFG_0] (main.py 265): INFO Train: [77/300][380/1562]	eta 0:06:03 lr 0.000005	time 0.3006 (0.3073)	loss 0.9925 (1.3155)	grad_norm 65.4469 (31.0936)	mem 4879MB
[2022-05-31 09:13:37 MetaFG_0] (main.py 265): INFO Train: [77/300][390/1562]	eta 0:06:00 lr 0.000005	time 0.2934 (0.3072)	loss 1.3282 (1.3170)	grad_norm 26.0697 (30.9804)	mem 4879MB
[2022-05-31 09:13:40 MetaFG_0] (main.py 265): INFO Train: [77/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2984 (0.3072)	loss 0.9230 (1.3169)	grad_norm 24.1354 (30.9854)	mem 4879MB
[2022-05-31 09:13:43 MetaFG_0] (main.py 265): INFO Train: [77/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2940 (0.3071)	loss 1.2627 (1.3177)	grad_norm 37.8233 (31.0741)	mem 4879MB
[2022-05-31 09:13:46 MetaFG_0] (main.py 265): INFO Train: [77/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2916 (0.3071)	loss 1.4698 (1.3186)	grad_norm 33.3711 (31.0293)	mem 4879MB
[2022-05-31 09:13:49 MetaFG_0] (main.py 265): INFO Train: [77/300][430/1562]	eta 0:05:47 lr 0.000005	time 0.2990 (0.3070)	loss 1.4287 (1.3191)	grad_norm 21.0690 (30.9523)	mem 4879MB
[2022-05-31 09:13:52 MetaFG_0] (main.py 265): INFO Train: [77/300][440/1562]	eta 0:05:44 lr 0.000005	time 0.2990 (0.3069)	loss 1.4310 (1.3188)	grad_norm 28.8372 (31.0322)	mem 4879MB
[2022-05-31 09:13:55 MetaFG_0] (main.py 265): INFO Train: [77/300][450/1562]	eta 0:05:41 lr 0.000005	time 0.2930 (0.3068)	loss 1.0466 (1.3194)	grad_norm 44.0197 (31.0417)	mem 4879MB
[2022-05-31 09:13:58 MetaFG_0] (main.py 265): INFO Train: [77/300][460/1562]	eta 0:05:38 lr 0.000005	time 0.2987 (0.3069)	loss 1.4967 (1.3205)	grad_norm 24.6141 (31.0389)	mem 4879MB
[2022-05-31 09:14:01 MetaFG_0] (main.py 265): INFO Train: [77/300][470/1562]	eta 0:05:35 lr 0.000005	time 0.3003 (0.3069)	loss 1.4110 (1.3220)	grad_norm 23.5602 (31.1191)	mem 4879MB
[2022-05-31 09:14:04 MetaFG_0] (main.py 265): INFO Train: [77/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2923 (0.3068)	loss 1.6278 (1.3215)	grad_norm 23.8237 (31.0071)	mem 4879MB
[2022-05-31 09:14:07 MetaFG_0] (main.py 265): INFO Train: [77/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2924 (0.3067)	loss 1.0229 (1.3202)	grad_norm 18.1729 (31.0111)	mem 4879MB
[2022-05-31 09:14:10 MetaFG_0] (main.py 265): INFO Train: [77/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.2930 (0.3067)	loss 1.5619 (1.3190)	grad_norm 27.0181 (30.9638)	mem 4879MB
[2022-05-31 09:14:14 MetaFG_0] (main.py 265): INFO Train: [77/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2921 (0.3067)	loss 0.9671 (1.3171)	grad_norm 39.5039 (31.0510)	mem 4879MB
[2022-05-31 09:14:17 MetaFG_0] (main.py 265): INFO Train: [77/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.3003 (0.3067)	loss 1.6062 (1.3160)	grad_norm 31.2615 (30.9759)	mem 4879MB
[2022-05-31 09:14:20 MetaFG_0] (main.py 265): INFO Train: [77/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2930 (0.3067)	loss 1.7398 (1.3192)	grad_norm 41.9033 (30.8915)	mem 4879MB
[2022-05-31 09:14:23 MetaFG_0] (main.py 265): INFO Train: [77/300][540/1562]	eta 0:05:13 lr 0.000005	time 0.2940 (0.3066)	loss 1.1076 (1.3187)	grad_norm 26.1485 (30.9680)	mem 4879MB
[2022-05-31 09:14:26 MetaFG_0] (main.py 265): INFO Train: [77/300][550/1562]	eta 0:05:10 lr 0.000005	time 0.3061 (0.3066)	loss 0.7503 (1.3188)	grad_norm 21.9821 (31.0490)	mem 4879MB
[2022-05-31 09:14:29 MetaFG_0] (main.py 265): INFO Train: [77/300][560/1562]	eta 0:05:07 lr 0.000005	time 0.2992 (0.3065)	loss 1.3971 (1.3182)	grad_norm 34.6590 (31.1281)	mem 4879MB
[2022-05-31 09:14:32 MetaFG_0] (main.py 265): INFO Train: [77/300][570/1562]	eta 0:05:04 lr 0.000005	time 0.2924 (0.3065)	loss 1.4311 (1.3185)	grad_norm 30.3108 (31.2019)	mem 4879MB
[2022-05-31 09:14:35 MetaFG_0] (main.py 265): INFO Train: [77/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2934 (0.3064)	loss 1.4875 (1.3197)	grad_norm 17.7108 (31.1408)	mem 4879MB
[2022-05-31 09:14:38 MetaFG_0] (main.py 265): INFO Train: [77/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2995 (0.3064)	loss 1.3804 (1.3206)	grad_norm 23.0362 (31.1540)	mem 4879MB
[2022-05-31 09:14:41 MetaFG_0] (main.py 265): INFO Train: [77/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2993 (0.3064)	loss 1.2251 (1.3209)	grad_norm 27.8116 (31.1680)	mem 4879MB
[2022-05-31 09:14:44 MetaFG_0] (main.py 265): INFO Train: [77/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2989 (0.3064)	loss 1.1278 (1.3212)	grad_norm 20.3021 (31.1126)	mem 4879MB
[2022-05-31 09:14:47 MetaFG_0] (main.py 265): INFO Train: [77/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.3060 (0.3063)	loss 1.1661 (1.3221)	grad_norm 32.2859 (31.0748)	mem 4879MB
[2022-05-31 09:14:50 MetaFG_0] (main.py 265): INFO Train: [77/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.3004 (0.3063)	loss 0.8763 (1.3222)	grad_norm 16.3206 (31.0083)	mem 4879MB
[2022-05-31 09:14:53 MetaFG_0] (main.py 265): INFO Train: [77/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2923 (0.3063)	loss 1.1121 (1.3220)	grad_norm 32.3170 (30.9744)	mem 4879MB
[2022-05-31 09:14:56 MetaFG_0] (main.py 265): INFO Train: [77/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.2919 (0.3062)	loss 1.6281 (1.3226)	grad_norm 30.4028 (31.0131)	mem 4879MB
[2022-05-31 09:14:59 MetaFG_0] (main.py 265): INFO Train: [77/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2925 (0.3062)	loss 1.3717 (1.3240)	grad_norm 22.8082 (30.9782)	mem 4879MB
[2022-05-31 09:15:02 MetaFG_0] (main.py 265): INFO Train: [77/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2920 (0.3061)	loss 1.2738 (1.3229)	grad_norm 30.1011 (30.9927)	mem 4879MB
[2022-05-31 09:15:05 MetaFG_0] (main.py 265): INFO Train: [77/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2969 (0.3061)	loss 1.3216 (1.3228)	grad_norm 29.9837 (30.9620)	mem 4879MB
[2022-05-31 09:15:08 MetaFG_0] (main.py 265): INFO Train: [77/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2953 (0.3061)	loss 0.9117 (1.3224)	grad_norm 20.7898 (30.9303)	mem 4879MB
[2022-05-31 09:15:11 MetaFG_0] (main.py 265): INFO Train: [77/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2983 (0.3060)	loss 1.4306 (1.3233)	grad_norm 32.8665 (30.9615)	mem 4879MB
[2022-05-31 09:15:14 MetaFG_0] (main.py 265): INFO Train: [77/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.3013 (0.3060)	loss 1.3570 (1.3236)	grad_norm 32.4367 (30.9860)	mem 4879MB
[2022-05-31 09:15:17 MetaFG_0] (main.py 265): INFO Train: [77/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.3001 (0.3060)	loss 1.3825 (1.3246)	grad_norm 20.6690 (30.9296)	mem 4879MB
[2022-05-31 09:15:21 MetaFG_0] (main.py 265): INFO Train: [77/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2920 (0.3060)	loss 1.0316 (1.3241)	grad_norm 44.8782 (30.9740)	mem 4879MB
[2022-05-31 09:15:24 MetaFG_0] (main.py 265): INFO Train: [77/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2987 (0.3060)	loss 1.5737 (1.3243)	grad_norm 24.8539 (30.9907)	mem 4879MB
[2022-05-31 09:15:27 MetaFG_0] (main.py 265): INFO Train: [77/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2931 (0.3060)	loss 1.6094 (1.3246)	grad_norm 19.8754 (30.9608)	mem 4879MB
[2022-05-31 09:15:30 MetaFG_0] (main.py 265): INFO Train: [77/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2920 (0.3059)	loss 1.3899 (1.3249)	grad_norm 46.5730 (30.9999)	mem 4879MB
[2022-05-31 09:15:33 MetaFG_0] (main.py 265): INFO Train: [77/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2928 (0.3059)	loss 1.4471 (1.3250)	grad_norm 16.2286 (30.9787)	mem 4879MB
[2022-05-31 09:15:36 MetaFG_0] (main.py 265): INFO Train: [77/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.3021 (0.3059)	loss 1.6598 (1.3253)	grad_norm 30.9010 (30.9763)	mem 4879MB
[2022-05-31 09:15:39 MetaFG_0] (main.py 265): INFO Train: [77/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.3007 (0.3059)	loss 1.4859 (1.3244)	grad_norm 22.8364 (30.9281)	mem 4879MB
[2022-05-31 09:15:42 MetaFG_0] (main.py 265): INFO Train: [77/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.2937 (0.3058)	loss 0.9593 (1.3250)	grad_norm 45.4042 (30.9316)	mem 4879MB
[2022-05-31 09:15:45 MetaFG_0] (main.py 265): INFO Train: [77/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2929 (0.3058)	loss 1.1168 (1.3251)	grad_norm 30.0174 (30.9012)	mem 4879MB
[2022-05-31 09:15:48 MetaFG_0] (main.py 265): INFO Train: [77/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.2934 (0.3058)	loss 1.2743 (1.3245)	grad_norm 27.0627 (31.0270)	mem 4879MB
[2022-05-31 09:15:51 MetaFG_0] (main.py 265): INFO Train: [77/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2928 (0.3058)	loss 1.1826 (1.3249)	grad_norm 48.4743 (31.0068)	mem 4879MB
[2022-05-31 09:15:54 MetaFG_0] (main.py 265): INFO Train: [77/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2935 (0.3057)	loss 1.1109 (1.3250)	grad_norm 18.6213 (30.9492)	mem 4879MB
[2022-05-31 09:15:57 MetaFG_0] (main.py 265): INFO Train: [77/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2979 (0.3058)	loss 1.3715 (1.3250)	grad_norm 23.8157 (30.9163)	mem 4879MB
[2022-05-31 09:16:00 MetaFG_0] (main.py 265): INFO Train: [77/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.3012 (0.3057)	loss 1.2993 (1.3242)	grad_norm 30.2492 (30.9042)	mem 4879MB
[2022-05-31 09:16:03 MetaFG_0] (main.py 265): INFO Train: [77/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2999 (0.3057)	loss 1.4953 (1.3229)	grad_norm 25.0803 (30.8166)	mem 4879MB
[2022-05-31 09:16:06 MetaFG_0] (main.py 265): INFO Train: [77/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2921 (0.3057)	loss 1.3092 (1.3229)	grad_norm 27.0490 (30.8328)	mem 4879MB
[2022-05-31 09:16:09 MetaFG_0] (main.py 265): INFO Train: [77/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2938 (0.3057)	loss 1.5006 (1.3241)	grad_norm 23.7527 (30.7826)	mem 4879MB
[2022-05-31 09:16:12 MetaFG_0] (main.py 265): INFO Train: [77/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2983 (0.3057)	loss 1.5035 (1.3248)	grad_norm 25.0012 (30.7182)	mem 4879MB
[2022-05-31 09:16:15 MetaFG_0] (main.py 265): INFO Train: [77/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2991 (0.3057)	loss 1.0366 (1.3244)	grad_norm 32.4480 (30.6975)	mem 4879MB
[2022-05-31 09:16:18 MetaFG_0] (main.py 265): INFO Train: [77/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2944 (0.3057)	loss 1.0724 (1.3250)	grad_norm 25.7711 (30.6641)	mem 4879MB
[2022-05-31 09:16:21 MetaFG_0] (main.py 265): INFO Train: [77/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2930 (0.3057)	loss 1.5412 (1.3253)	grad_norm 57.4496 (30.7098)	mem 4879MB
[2022-05-31 09:16:24 MetaFG_0] (main.py 265): INFO Train: [77/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.2937 (0.3056)	loss 1.2584 (1.3255)	grad_norm 21.1751 (30.6957)	mem 4879MB
[2022-05-31 09:16:27 MetaFG_0] (main.py 265): INFO Train: [77/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.3004 (0.3056)	loss 1.5723 (1.3250)	grad_norm 26.2215 (30.6776)	mem 4879MB
[2022-05-31 09:16:31 MetaFG_0] (main.py 265): INFO Train: [77/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2922 (0.3056)	loss 1.5179 (1.3258)	grad_norm 24.0032 (30.6275)	mem 4879MB
[2022-05-31 09:16:34 MetaFG_0] (main.py 265): INFO Train: [77/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.3001 (0.3056)	loss 1.2181 (1.3259)	grad_norm 27.1555 (30.6351)	mem 4879MB
[2022-05-31 09:16:37 MetaFG_0] (main.py 265): INFO Train: [77/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2956 (0.3056)	loss 1.5195 (1.3260)	grad_norm 29.1272 (30.5952)	mem 4879MB
[2022-05-31 09:16:40 MetaFG_0] (main.py 265): INFO Train: [77/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2929 (0.3056)	loss 0.9656 (1.3246)	grad_norm 37.7784 (30.5439)	mem 4879MB
[2022-05-31 09:16:43 MetaFG_0] (main.py 265): INFO Train: [77/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2988 (0.3056)	loss 1.0433 (1.3250)	grad_norm 17.7597 (nan)	mem 4879MB
[2022-05-31 09:16:46 MetaFG_0] (main.py 265): INFO Train: [77/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2949 (0.3055)	loss 1.4749 (1.3266)	grad_norm 22.8121 (nan)	mem 4879MB
[2022-05-31 09:16:49 MetaFG_0] (main.py 265): INFO Train: [77/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2925 (0.3056)	loss 1.4438 (1.3268)	grad_norm 23.5762 (nan)	mem 4879MB
[2022-05-31 09:16:52 MetaFG_0] (main.py 265): INFO Train: [77/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.3023 (0.3056)	loss 1.6324 (1.3272)	grad_norm 29.6604 (nan)	mem 4879MB
[2022-05-31 09:16:55 MetaFG_0] (main.py 265): INFO Train: [77/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.3005 (0.3056)	loss 1.1480 (1.3271)	grad_norm 15.1549 (nan)	mem 4879MB
[2022-05-31 09:16:58 MetaFG_0] (main.py 265): INFO Train: [77/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2941 (0.3056)	loss 1.5210 (1.3270)	grad_norm 65.3807 (nan)	mem 4879MB
[2022-05-31 09:17:01 MetaFG_0] (main.py 265): INFO Train: [77/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2993 (0.3056)	loss 0.9785 (1.3263)	grad_norm 40.7740 (nan)	mem 4879MB
[2022-05-31 09:17:04 MetaFG_0] (main.py 265): INFO Train: [77/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2937 (0.3056)	loss 1.3904 (1.3255)	grad_norm 38.4398 (nan)	mem 4879MB
[2022-05-31 09:17:07 MetaFG_0] (main.py 265): INFO Train: [77/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.3027 (0.3056)	loss 1.1479 (1.3267)	grad_norm 36.1080 (nan)	mem 4879MB
[2022-05-31 09:17:10 MetaFG_0] (main.py 265): INFO Train: [77/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2986 (0.3056)	loss 1.4794 (1.3273)	grad_norm 31.7802 (nan)	mem 4879MB
[2022-05-31 09:17:13 MetaFG_0] (main.py 265): INFO Train: [77/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2974 (0.3056)	loss 1.6126 (1.3272)	grad_norm 27.7628 (nan)	mem 4879MB
[2022-05-31 09:17:16 MetaFG_0] (main.py 265): INFO Train: [77/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.2992 (0.3055)	loss 1.4254 (1.3277)	grad_norm 37.8621 (nan)	mem 4879MB
[2022-05-31 09:17:19 MetaFG_0] (main.py 265): INFO Train: [77/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.3014 (0.3055)	loss 1.4049 (1.3277)	grad_norm 24.0376 (nan)	mem 4879MB
[2022-05-31 09:17:22 MetaFG_0] (main.py 265): INFO Train: [77/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.2922 (0.3055)	loss 1.2241 (1.3279)	grad_norm 15.4034 (nan)	mem 4879MB
[2022-05-31 09:17:25 MetaFG_0] (main.py 265): INFO Train: [77/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.3010 (0.3055)	loss 1.1061 (1.3273)	grad_norm 33.6604 (nan)	mem 4879MB
[2022-05-31 09:17:28 MetaFG_0] (main.py 265): INFO Train: [77/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.3005 (0.3055)	loss 1.2825 (1.3267)	grad_norm 36.8542 (nan)	mem 4879MB
[2022-05-31 09:17:32 MetaFG_0] (main.py 265): INFO Train: [77/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2983 (0.3055)	loss 1.3615 (1.3267)	grad_norm 33.1414 (nan)	mem 4879MB
[2022-05-31 09:17:35 MetaFG_0] (main.py 265): INFO Train: [77/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2963 (0.3055)	loss 0.9124 (1.3260)	grad_norm 29.4826 (nan)	mem 4879MB
[2022-05-31 09:17:38 MetaFG_0] (main.py 265): INFO Train: [77/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2993 (0.3055)	loss 1.6357 (1.3264)	grad_norm 30.7087 (nan)	mem 4879MB
[2022-05-31 09:17:41 MetaFG_0] (main.py 265): INFO Train: [77/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.3052 (0.3055)	loss 1.3853 (1.3269)	grad_norm 55.7220 (nan)	mem 4879MB
[2022-05-31 09:17:44 MetaFG_0] (main.py 265): INFO Train: [77/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2994 (0.3055)	loss 1.6324 (1.3279)	grad_norm 33.3644 (nan)	mem 4879MB
[2022-05-31 09:17:47 MetaFG_0] (main.py 265): INFO Train: [77/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2950 (0.3054)	loss 1.0584 (1.3280)	grad_norm 38.7030 (nan)	mem 4879MB
[2022-05-31 09:17:50 MetaFG_0] (main.py 265): INFO Train: [77/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2928 (0.3055)	loss 1.3681 (1.3282)	grad_norm 23.3862 (nan)	mem 4879MB
[2022-05-31 09:17:53 MetaFG_0] (main.py 265): INFO Train: [77/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2920 (0.3054)	loss 1.1702 (1.3282)	grad_norm 26.0377 (nan)	mem 4879MB
[2022-05-31 09:17:56 MetaFG_0] (main.py 265): INFO Train: [77/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2942 (0.3054)	loss 1.4415 (1.3278)	grad_norm 25.8724 (nan)	mem 4879MB
[2022-05-31 09:17:59 MetaFG_0] (main.py 265): INFO Train: [77/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2933 (0.3054)	loss 1.3698 (1.3286)	grad_norm 21.1409 (nan)	mem 4879MB
[2022-05-31 09:18:02 MetaFG_0] (main.py 265): INFO Train: [77/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.3000 (0.3054)	loss 1.0805 (1.3280)	grad_norm 14.9418 (nan)	mem 4879MB
[2022-05-31 09:18:05 MetaFG_0] (main.py 265): INFO Train: [77/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2949 (0.3054)	loss 1.1772 (1.3290)	grad_norm 19.1244 (nan)	mem 4879MB
[2022-05-31 09:18:08 MetaFG_0] (main.py 265): INFO Train: [77/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2990 (0.3054)	loss 1.5059 (1.3292)	grad_norm 20.0045 (nan)	mem 4879MB
[2022-05-31 09:18:11 MetaFG_0] (main.py 265): INFO Train: [77/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2957 (0.3054)	loss 0.8361 (1.3284)	grad_norm 28.8615 (nan)	mem 4879MB
[2022-05-31 09:18:14 MetaFG_0] (main.py 265): INFO Train: [77/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2983 (0.3054)	loss 1.3971 (1.3285)	grad_norm 21.3850 (nan)	mem 4879MB
[2022-05-31 09:18:17 MetaFG_0] (main.py 265): INFO Train: [77/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2923 (0.3054)	loss 1.4566 (1.3292)	grad_norm 24.9058 (nan)	mem 4879MB
[2022-05-31 09:18:20 MetaFG_0] (main.py 265): INFO Train: [77/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2997 (0.3054)	loss 1.5119 (1.3290)	grad_norm 21.6207 (nan)	mem 4879MB
[2022-05-31 09:18:23 MetaFG_0] (main.py 265): INFO Train: [77/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2920 (0.3054)	loss 1.0389 (1.3295)	grad_norm 29.3175 (nan)	mem 4879MB
[2022-05-31 09:18:26 MetaFG_0] (main.py 265): INFO Train: [77/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.3008 (0.3054)	loss 1.2251 (1.3290)	grad_norm 20.5971 (nan)	mem 4879MB
[2022-05-31 09:18:29 MetaFG_0] (main.py 265): INFO Train: [77/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.3005 (0.3054)	loss 1.4053 (1.3297)	grad_norm 34.5524 (nan)	mem 4879MB
[2022-05-31 09:18:32 MetaFG_0] (main.py 265): INFO Train: [77/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2921 (0.3054)	loss 1.3831 (1.3297)	grad_norm 28.6595 (nan)	mem 4879MB
[2022-05-31 09:18:35 MetaFG_0] (main.py 265): INFO Train: [77/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2993 (0.3053)	loss 1.4954 (1.3300)	grad_norm 46.1869 (nan)	mem 4879MB
[2022-05-31 09:18:39 MetaFG_0] (main.py 265): INFO Train: [77/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.3002 (0.3053)	loss 1.2209 (1.3304)	grad_norm 51.4219 (nan)	mem 4879MB
[2022-05-31 09:18:42 MetaFG_0] (main.py 265): INFO Train: [77/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2927 (0.3053)	loss 0.9102 (1.3302)	grad_norm 32.3460 (nan)	mem 4879MB
[2022-05-31 09:18:45 MetaFG_0] (main.py 265): INFO Train: [77/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2922 (0.3053)	loss 1.4751 (1.3310)	grad_norm 33.6553 (nan)	mem 4879MB
[2022-05-31 09:18:48 MetaFG_0] (main.py 265): INFO Train: [77/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2987 (0.3053)	loss 1.4631 (1.3306)	grad_norm 18.4196 (nan)	mem 4879MB
[2022-05-31 09:18:51 MetaFG_0] (main.py 265): INFO Train: [77/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2920 (0.3053)	loss 1.1743 (1.3310)	grad_norm 31.2264 (nan)	mem 4879MB
[2022-05-31 09:18:54 MetaFG_0] (main.py 265): INFO Train: [77/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2947 (0.3053)	loss 1.6186 (1.3302)	grad_norm 44.4980 (nan)	mem 4879MB
[2022-05-31 09:18:57 MetaFG_0] (main.py 265): INFO Train: [77/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.3004 (0.3053)	loss 1.0020 (1.3301)	grad_norm 24.8189 (nan)	mem 4879MB
[2022-05-31 09:19:00 MetaFG_0] (main.py 265): INFO Train: [77/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.3006 (0.3053)	loss 1.4687 (1.3307)	grad_norm 28.2138 (nan)	mem 4879MB
[2022-05-31 09:19:03 MetaFG_0] (main.py 265): INFO Train: [77/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2933 (0.3053)	loss 1.2573 (1.3303)	grad_norm 34.8961 (nan)	mem 4879MB
[2022-05-31 09:19:06 MetaFG_0] (main.py 265): INFO Train: [77/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2955 (0.3053)	loss 1.2335 (1.3304)	grad_norm 26.6225 (nan)	mem 4879MB
[2022-05-31 09:19:09 MetaFG_0] (main.py 265): INFO Train: [77/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2923 (0.3053)	loss 1.7158 (1.3312)	grad_norm 21.2968 (nan)	mem 4879MB
[2022-05-31 09:19:12 MetaFG_0] (main.py 265): INFO Train: [77/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.2928 (0.3053)	loss 1.3151 (1.3304)	grad_norm 25.9736 (nan)	mem 4879MB
[2022-05-31 09:19:15 MetaFG_0] (main.py 265): INFO Train: [77/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2933 (0.3053)	loss 0.7882 (1.3296)	grad_norm 27.1769 (nan)	mem 4879MB
[2022-05-31 09:19:18 MetaFG_0] (main.py 265): INFO Train: [77/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2986 (0.3052)	loss 1.4321 (1.3295)	grad_norm 16.3894 (nan)	mem 4879MB
[2022-05-31 09:19:21 MetaFG_0] (main.py 265): INFO Train: [77/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2985 (0.3052)	loss 1.4145 (1.3297)	grad_norm 16.1264 (nan)	mem 4879MB
[2022-05-31 09:19:24 MetaFG_0] (main.py 265): INFO Train: [77/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2950 (0.3052)	loss 0.9403 (1.3297)	grad_norm 30.0791 (nan)	mem 4879MB
[2022-05-31 09:19:27 MetaFG_0] (main.py 265): INFO Train: [77/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.3002 (0.3052)	loss 1.4663 (1.3299)	grad_norm 43.2852 (nan)	mem 4879MB
[2022-05-31 09:19:30 MetaFG_0] (main.py 265): INFO Train: [77/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2991 (0.3053)	loss 1.2070 (1.3295)	grad_norm 14.8444 (nan)	mem 4879MB
[2022-05-31 09:19:33 MetaFG_0] (main.py 265): INFO Train: [77/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2914 (0.3053)	loss 1.5136 (1.3292)	grad_norm 29.7009 (nan)	mem 4879MB
[2022-05-31 09:19:34 MetaFG_0] (main.py 272): INFO EPOCH 77 training takes 0:07:56
[2022-05-31 09:19:34 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_77.pth saving......
[2022-05-31 09:19:35 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_77.pth saved !!!
[2022-05-31 09:19:35 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 09:19:36 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 09:19:36 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 09:19:37 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.574 (0.574)	Loss 0.3163 (0.3163)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 09:19:38 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.087 (0.136)	Loss 0.5655 (0.5194)	Acc@1 84.375 (89.773)	Acc@5 100.000 (99.148)	Mem 4879MB
[2022-05-31 09:19:38 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.093 (0.117)	Loss 0.3867 (0.5884)	Acc@1 90.625 (87.351)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 09:19:39 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.090 (0.110)	Loss 0.7147 (0.5987)	Acc@1 78.125 (86.996)	Acc@5 100.000 (98.589)	Mem 4879MB
[2022-05-31 09:19:40 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.087 (0.106)	Loss 0.6513 (0.5861)	Acc@1 87.500 (87.500)	Acc@5 93.750 (98.628)	Mem 4879MB
[2022-05-31 09:19:41 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.093 (0.103)	Loss 0.3729 (0.5745)	Acc@1 90.625 (87.806)	Acc@5 100.000 (98.713)	Mem 4879MB
[2022-05-31 09:19:42 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.087 (0.102)	Loss 0.9124 (0.5940)	Acc@1 75.000 (86.988)	Acc@5 93.750 (98.617)	Mem 4879MB
[2022-05-31 09:19:43 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.101)	Loss 0.7108 (0.5871)	Acc@1 81.250 (87.016)	Acc@5 96.875 (98.592)	Mem 4879MB
[2022-05-31 09:19:44 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.089 (0.100)	Loss 0.8018 (0.5856)	Acc@1 81.250 (86.998)	Acc@5 96.875 (98.688)	Mem 4879MB
[2022-05-31 09:19:45 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.087 (0.099)	Loss 0.5375 (0.5849)	Acc@1 87.500 (87.054)	Acc@5 100.000 (98.729)	Mem 4879MB
[2022-05-31 09:19:46 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.099)	Loss 0.7775 (0.5826)	Acc@1 81.250 (87.098)	Acc@5 96.875 (98.762)	Mem 4879MB
[2022-05-31 09:19:47 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.092 (0.098)	Loss 0.6243 (0.5803)	Acc@1 84.375 (87.134)	Acc@5 100.000 (98.818)	Mem 4879MB
[2022-05-31 09:19:48 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.098)	Loss 0.4916 (0.5854)	Acc@1 90.625 (86.906)	Acc@5 100.000 (98.812)	Mem 4879MB
[2022-05-31 09:19:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.092 (0.098)	Loss 1.0785 (0.5886)	Acc@1 78.125 (87.047)	Acc@5 93.750 (98.760)	Mem 4879MB
[2022-05-31 09:19:50 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.095 (0.097)	Loss 0.8161 (0.5957)	Acc@1 87.500 (86.791)	Acc@5 93.750 (98.759)	Mem 4879MB
[2022-05-31 09:19:51 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.097 (0.097)	Loss 0.5264 (0.5950)	Acc@1 90.625 (86.921)	Acc@5 100.000 (98.717)	Mem 4879MB
[2022-05-31 09:19:52 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.087 (0.097)	Loss 0.4358 (0.5919)	Acc@1 93.750 (86.995)	Acc@5 100.000 (98.719)	Mem 4879MB
[2022-05-31 09:19:53 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.092 (0.097)	Loss 0.6388 (0.5884)	Acc@1 81.250 (87.116)	Acc@5 100.000 (98.757)	Mem 4879MB
[2022-05-31 09:19:54 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.103 (0.097)	Loss 0.5507 (0.5883)	Acc@1 87.500 (87.086)	Acc@5 100.000 (98.774)	Mem 4879MB
[2022-05-31 09:19:54 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.096)	Loss 0.3347 (0.5874)	Acc@1 90.625 (87.156)	Acc@5 100.000 (98.806)	Mem 4879MB
[2022-05-31 09:19:55 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.102 (0.096)	Loss 0.4357 (0.5857)	Acc@1 93.750 (87.205)	Acc@5 100.000 (98.818)	Mem 4879MB
[2022-05-31 09:19:56 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.096 (0.096)	Loss 0.3246 (0.5831)	Acc@1 96.875 (87.307)	Acc@5 100.000 (98.830)	Mem 4879MB
[2022-05-31 09:19:57 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.088 (0.096)	Loss 0.4927 (0.5842)	Acc@1 90.625 (87.302)	Acc@5 96.875 (98.770)	Mem 4879MB
[2022-05-31 09:19:58 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.094 (0.096)	Loss 0.7232 (0.5868)	Acc@1 81.250 (87.229)	Acc@5 100.000 (98.728)	Mem 4879MB
[2022-05-31 09:19:59 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.094 (0.096)	Loss 0.7476 (0.5858)	Acc@1 84.375 (87.318)	Acc@5 100.000 (98.742)	Mem 4879MB
[2022-05-31 09:20:00 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.094 (0.096)	Loss 0.4193 (0.5833)	Acc@1 90.625 (87.438)	Acc@5 100.000 (98.755)	Mem 4879MB
[2022-05-31 09:20:01 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.096)	Loss 0.6192 (0.5815)	Acc@1 87.500 (87.440)	Acc@5 100.000 (98.791)	Mem 4879MB
[2022-05-31 09:20:02 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.094 (0.096)	Loss 0.5795 (0.5791)	Acc@1 84.375 (87.535)	Acc@5 100.000 (98.801)	Mem 4879MB
[2022-05-31 09:20:03 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.087 (0.096)	Loss 0.7741 (0.5792)	Acc@1 84.375 (87.522)	Acc@5 93.750 (98.799)	Mem 4879MB
[2022-05-31 09:20:04 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.096)	Loss 0.6287 (0.5806)	Acc@1 87.500 (87.479)	Acc@5 100.000 (98.787)	Mem 4879MB
[2022-05-31 09:20:05 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.090 (0.096)	Loss 0.5756 (0.5810)	Acc@1 93.750 (87.510)	Acc@5 96.875 (98.775)	Mem 4879MB
[2022-05-31 09:20:06 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.095)	Loss 0.4605 (0.5804)	Acc@1 93.750 (87.500)	Acc@5 100.000 (98.814)	Mem 4879MB
[2022-05-31 09:20:06 MetaFG_0] (main.py 330): INFO  * Acc@1 87.510 Acc@5 98.810
[2022-05-31 09:20:06 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.5%
[2022-05-31 09:20:06 MetaFG_0] (main.py 171): INFO Max accuracy: 87.58%
[2022-05-31 09:20:07 MetaFG_0] (main.py 265): INFO Train: [78/300][0/1562]	eta 0:25:04 lr 0.000005	time 0.9635 (0.9635)	loss 1.2428 (1.2428)	grad_norm 15.3268 (15.3268)	mem 4879MB
[2022-05-31 09:20:10 MetaFG_0] (main.py 265): INFO Train: [78/300][10/1562]	eta 0:09:39 lr 0.000005	time 0.2927 (0.3736)	loss 1.4445 (1.2818)	grad_norm 27.4931 (29.8663)	mem 4879MB
[2022-05-31 09:20:13 MetaFG_0] (main.py 265): INFO Train: [78/300][20/1562]	eta 0:08:45 lr 0.000005	time 0.2971 (0.3409)	loss 1.3085 (1.2536)	grad_norm 33.1490 (33.4173)	mem 4879MB
[2022-05-31 09:20:16 MetaFG_0] (main.py 265): INFO Train: [78/300][30/1562]	eta 0:08:25 lr 0.000005	time 0.3052 (0.3300)	loss 1.4883 (1.2777)	grad_norm 30.3906 (31.3954)	mem 4879MB
[2022-05-31 09:20:19 MetaFG_0] (main.py 265): INFO Train: [78/300][40/1562]	eta 0:08:13 lr 0.000005	time 0.2921 (0.3239)	loss 1.5449 (1.2995)	grad_norm 26.1540 (31.0008)	mem 4879MB
[2022-05-31 09:20:22 MetaFG_0] (main.py 265): INFO Train: [78/300][50/1562]	eta 0:08:03 lr 0.000005	time 0.2924 (0.3198)	loss 1.0918 (1.2768)	grad_norm 26.1280 (30.3027)	mem 4879MB
[2022-05-31 09:20:25 MetaFG_0] (main.py 265): INFO Train: [78/300][60/1562]	eta 0:07:56 lr 0.000005	time 0.2957 (0.3172)	loss 1.3678 (1.2815)	grad_norm 25.0166 (29.3713)	mem 4879MB
[2022-05-31 09:20:28 MetaFG_0] (main.py 265): INFO Train: [78/300][70/1562]	eta 0:07:50 lr 0.000005	time 0.2917 (0.3154)	loss 1.3990 (1.2921)	grad_norm 28.0922 (29.2736)	mem 4879MB
[2022-05-31 09:20:31 MetaFG_0] (main.py 265): INFO Train: [78/300][80/1562]	eta 0:07:45 lr 0.000005	time 0.2937 (0.3142)	loss 1.2109 (1.2901)	grad_norm 42.5193 (29.7828)	mem 4879MB
[2022-05-31 09:20:34 MetaFG_0] (main.py 265): INFO Train: [78/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.2989 (0.3130)	loss 0.9446 (1.2945)	grad_norm 12.5018 (29.3388)	mem 4879MB
[2022-05-31 09:20:38 MetaFG_0] (main.py 265): INFO Train: [78/300][100/1562]	eta 0:07:36 lr 0.000005	time 0.3006 (0.3121)	loss 1.0759 (1.2826)	grad_norm 23.2112 (29.9569)	mem 4879MB
[2022-05-31 09:20:41 MetaFG_0] (main.py 265): INFO Train: [78/300][110/1562]	eta 0:07:32 lr 0.000005	time 0.2977 (0.3114)	loss 1.6765 (1.2940)	grad_norm 45.2942 (30.0440)	mem 4879MB
[2022-05-31 09:20:44 MetaFG_0] (main.py 265): INFO Train: [78/300][120/1562]	eta 0:07:28 lr 0.000005	time 0.2931 (0.3108)	loss 1.5322 (1.3050)	grad_norm 25.3514 (29.9207)	mem 4879MB
[2022-05-31 09:20:47 MetaFG_0] (main.py 265): INFO Train: [78/300][130/1562]	eta 0:07:24 lr 0.000005	time 0.2925 (0.3104)	loss 1.1361 (1.3084)	grad_norm 27.9015 (29.5988)	mem 4879MB
[2022-05-31 09:20:50 MetaFG_0] (main.py 265): INFO Train: [78/300][140/1562]	eta 0:07:20 lr 0.000005	time 0.2928 (0.3100)	loss 1.2148 (1.3090)	grad_norm 36.9391 (30.0005)	mem 4879MB
[2022-05-31 09:20:53 MetaFG_0] (main.py 265): INFO Train: [78/300][150/1562]	eta 0:07:17 lr 0.000005	time 0.2990 (0.3097)	loss 1.4019 (1.3116)	grad_norm 25.8800 (29.8828)	mem 4879MB
[2022-05-31 09:20:56 MetaFG_0] (main.py 265): INFO Train: [78/300][160/1562]	eta 0:07:13 lr 0.000005	time 0.2942 (0.3092)	loss 1.0597 (1.3033)	grad_norm 21.5601 (29.8842)	mem 4879MB
[2022-05-31 09:20:59 MetaFG_0] (main.py 265): INFO Train: [78/300][170/1562]	eta 0:07:09 lr 0.000005	time 0.2927 (0.3089)	loss 1.1584 (1.3035)	grad_norm 25.5629 (29.8908)	mem 4879MB
[2022-05-31 09:21:02 MetaFG_0] (main.py 265): INFO Train: [78/300][180/1562]	eta 0:07:06 lr 0.000005	time 0.2952 (0.3087)	loss 1.1306 (1.3093)	grad_norm 31.3914 (29.9708)	mem 4879MB
[2022-05-31 09:21:05 MetaFG_0] (main.py 265): INFO Train: [78/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2917 (0.3084)	loss 1.4657 (1.3113)	grad_norm 25.7631 (29.9297)	mem 4879MB
[2022-05-31 09:21:08 MetaFG_0] (main.py 265): INFO Train: [78/300][200/1562]	eta 0:06:59 lr 0.000005	time 0.2937 (0.3082)	loss 1.0039 (1.3140)	grad_norm 23.2016 (29.9756)	mem 4879MB
[2022-05-31 09:21:11 MetaFG_0] (main.py 265): INFO Train: [78/300][210/1562]	eta 0:06:56 lr 0.000005	time 0.2935 (0.3080)	loss 1.4129 (1.3062)	grad_norm 32.0009 (30.3386)	mem 4879MB
[2022-05-31 09:21:14 MetaFG_0] (main.py 265): INFO Train: [78/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2921 (0.3078)	loss 1.3411 (1.3007)	grad_norm 30.0176 (30.2701)	mem 4879MB
[2022-05-31 09:21:17 MetaFG_0] (main.py 265): INFO Train: [78/300][230/1562]	eta 0:06:49 lr 0.000005	time 0.2923 (0.3077)	loss 1.4046 (1.3008)	grad_norm 23.0212 (30.2552)	mem 4879MB
[2022-05-31 09:21:20 MetaFG_0] (main.py 265): INFO Train: [78/300][240/1562]	eta 0:06:46 lr 0.000005	time 0.2934 (0.3075)	loss 1.3648 (1.3004)	grad_norm 30.4551 (30.0456)	mem 4879MB
[2022-05-31 09:21:23 MetaFG_0] (main.py 265): INFO Train: [78/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2990 (0.3073)	loss 1.2951 (1.2993)	grad_norm 17.9264 (29.9716)	mem 4879MB
[2022-05-31 09:21:26 MetaFG_0] (main.py 265): INFO Train: [78/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2935 (0.3073)	loss 1.2986 (1.2991)	grad_norm 25.6574 (29.9514)	mem 4879MB
[2022-05-31 09:21:29 MetaFG_0] (main.py 265): INFO Train: [78/300][270/1562]	eta 0:06:36 lr 0.000005	time 0.2988 (0.3071)	loss 0.9866 (1.3016)	grad_norm 19.9417 (29.9253)	mem 4879MB
[2022-05-31 09:21:32 MetaFG_0] (main.py 265): INFO Train: [78/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2937 (0.3070)	loss 1.4388 (1.3017)	grad_norm 28.7981 (29.8673)	mem 4879MB
[2022-05-31 09:21:35 MetaFG_0] (main.py 265): INFO Train: [78/300][290/1562]	eta 0:06:30 lr 0.000005	time 0.2929 (0.3069)	loss 1.3513 (1.3022)	grad_norm 26.5864 (29.6668)	mem 4879MB
[2022-05-31 09:21:38 MetaFG_0] (main.py 265): INFO Train: [78/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2988 (0.3069)	loss 1.5792 (1.3043)	grad_norm 26.8470 (29.5391)	mem 4879MB
[2022-05-31 09:21:41 MetaFG_0] (main.py 265): INFO Train: [78/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2930 (0.3068)	loss 1.1594 (1.3044)	grad_norm 46.1651 (29.5387)	mem 4879MB
[2022-05-31 09:21:44 MetaFG_0] (main.py 265): INFO Train: [78/300][320/1562]	eta 0:06:20 lr 0.000005	time 0.2993 (0.3067)	loss 1.4487 (1.3085)	grad_norm 29.9759 (29.4484)	mem 4879MB
[2022-05-31 09:21:47 MetaFG_0] (main.py 265): INFO Train: [78/300][330/1562]	eta 0:06:17 lr 0.000005	time 0.2919 (0.3067)	loss 1.4442 (1.3105)	grad_norm 26.6687 (29.4508)	mem 4879MB
[2022-05-31 09:21:51 MetaFG_0] (main.py 265): INFO Train: [78/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.2925 (0.3066)	loss 1.3361 (1.3077)	grad_norm 30.6818 (29.3538)	mem 4879MB
[2022-05-31 09:21:54 MetaFG_0] (main.py 265): INFO Train: [78/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.2926 (0.3066)	loss 1.4310 (1.3083)	grad_norm 18.5315 (29.3089)	mem 4879MB
[2022-05-31 09:21:57 MetaFG_0] (main.py 265): INFO Train: [78/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2940 (0.3065)	loss 1.2906 (1.3095)	grad_norm 14.4222 (29.3608)	mem 4879MB
[2022-05-31 09:22:00 MetaFG_0] (main.py 265): INFO Train: [78/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2942 (0.3064)	loss 1.3645 (1.3098)	grad_norm 20.2023 (29.2247)	mem 4879MB
[2022-05-31 09:22:03 MetaFG_0] (main.py 265): INFO Train: [78/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.3002 (0.3064)	loss 1.3253 (1.3107)	grad_norm 36.4750 (29.2432)	mem 4879MB
[2022-05-31 09:22:06 MetaFG_0] (main.py 265): INFO Train: [78/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.2984 (0.3064)	loss 1.3656 (1.3109)	grad_norm 38.6832 (29.2336)	mem 4879MB
[2022-05-31 09:22:09 MetaFG_0] (main.py 265): INFO Train: [78/300][400/1562]	eta 0:05:55 lr 0.000005	time 0.2936 (0.3063)	loss 0.9427 (1.3101)	grad_norm 49.7568 (29.2123)	mem 4879MB
[2022-05-31 09:22:12 MetaFG_0] (main.py 265): INFO Train: [78/300][410/1562]	eta 0:05:52 lr 0.000005	time 0.2941 (0.3063)	loss 1.4379 (1.3108)	grad_norm 18.1223 (29.1449)	mem 4879MB
[2022-05-31 09:22:15 MetaFG_0] (main.py 265): INFO Train: [78/300][420/1562]	eta 0:05:49 lr 0.000005	time 0.2919 (0.3062)	loss 1.6034 (1.3116)	grad_norm 23.1434 (29.1517)	mem 4879MB
[2022-05-31 09:22:18 MetaFG_0] (main.py 265): INFO Train: [78/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2936 (0.3062)	loss 1.6512 (1.3104)	grad_norm 19.4433 (29.2232)	mem 4879MB
[2022-05-31 09:22:21 MetaFG_0] (main.py 265): INFO Train: [78/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2978 (0.3061)	loss 1.4499 (1.3083)	grad_norm 43.0751 (29.2357)	mem 4879MB
[2022-05-31 09:22:24 MetaFG_0] (main.py 265): INFO Train: [78/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.2917 (0.3060)	loss 1.0202 (1.3057)	grad_norm 36.7680 (29.2831)	mem 4879MB
[2022-05-31 09:22:27 MetaFG_0] (main.py 265): INFO Train: [78/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2924 (0.3060)	loss 1.5243 (1.3058)	grad_norm 32.7649 (29.5095)	mem 4879MB
[2022-05-31 09:22:30 MetaFG_0] (main.py 265): INFO Train: [78/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2933 (0.3060)	loss 1.3992 (1.3066)	grad_norm 27.3149 (29.5054)	mem 4879MB
[2022-05-31 09:22:33 MetaFG_0] (main.py 265): INFO Train: [78/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.2922 (0.3059)	loss 1.2111 (1.3066)	grad_norm 20.9579 (29.5343)	mem 4879MB
[2022-05-31 09:22:36 MetaFG_0] (main.py 265): INFO Train: [78/300][490/1562]	eta 0:05:27 lr 0.000005	time 0.2933 (0.3059)	loss 1.4855 (1.3061)	grad_norm 25.9064 (29.4404)	mem 4879MB
[2022-05-31 09:22:39 MetaFG_0] (main.py 265): INFO Train: [78/300][500/1562]	eta 0:05:24 lr 0.000005	time 0.2926 (0.3059)	loss 1.2782 (1.3051)	grad_norm 81.6800 (29.4311)	mem 4879MB
[2022-05-31 09:22:42 MetaFG_0] (main.py 265): INFO Train: [78/300][510/1562]	eta 0:05:21 lr 0.000005	time 0.2919 (0.3058)	loss 1.4535 (1.3059)	grad_norm 39.6232 (29.4061)	mem 4879MB
[2022-05-31 09:22:45 MetaFG_0] (main.py 265): INFO Train: [78/300][520/1562]	eta 0:05:18 lr 0.000005	time 0.3007 (0.3058)	loss 1.3521 (1.3045)	grad_norm 29.1220 (29.4008)	mem 4879MB
[2022-05-31 09:22:48 MetaFG_0] (main.py 265): INFO Train: [78/300][530/1562]	eta 0:05:15 lr 0.000005	time 0.2994 (0.3058)	loss 1.5116 (1.3058)	grad_norm 27.2880 (29.5290)	mem 4879MB
[2022-05-31 09:22:51 MetaFG_0] (main.py 265): INFO Train: [78/300][540/1562]	eta 0:05:12 lr 0.000005	time 0.2928 (0.3058)	loss 1.5022 (1.3055)	grad_norm 22.9012 (29.4473)	mem 4879MB
[2022-05-31 09:22:54 MetaFG_0] (main.py 265): INFO Train: [78/300][550/1562]	eta 0:05:09 lr 0.000005	time 0.2929 (0.3057)	loss 1.4877 (1.3028)	grad_norm 31.0512 (29.4685)	mem 4879MB
[2022-05-31 09:22:57 MetaFG_0] (main.py 265): INFO Train: [78/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2987 (0.3057)	loss 1.2397 (1.3042)	grad_norm 31.8692 (29.4292)	mem 4879MB
[2022-05-31 09:23:01 MetaFG_0] (main.py 265): INFO Train: [78/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2928 (0.3057)	loss 0.6149 (1.3027)	grad_norm 22.7481 (29.3985)	mem 4879MB
[2022-05-31 09:23:04 MetaFG_0] (main.py 265): INFO Train: [78/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2937 (0.3056)	loss 1.4611 (1.3024)	grad_norm 56.7104 (29.4448)	mem 4879MB
[2022-05-31 09:23:07 MetaFG_0] (main.py 265): INFO Train: [78/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2930 (0.3056)	loss 1.3193 (1.3020)	grad_norm 42.4048 (29.4571)	mem 4879MB
[2022-05-31 09:23:10 MetaFG_0] (main.py 265): INFO Train: [78/300][600/1562]	eta 0:04:53 lr 0.000005	time 0.2940 (0.3056)	loss 1.0381 (1.3019)	grad_norm 24.1536 (29.5426)	mem 4879MB
[2022-05-31 09:23:13 MetaFG_0] (main.py 265): INFO Train: [78/300][610/1562]	eta 0:04:50 lr 0.000005	time 0.2984 (0.3056)	loss 0.7484 (1.3009)	grad_norm 16.2292 (29.5544)	mem 4879MB
[2022-05-31 09:23:16 MetaFG_0] (main.py 265): INFO Train: [78/300][620/1562]	eta 0:04:47 lr 0.000005	time 0.2921 (0.3056)	loss 1.3262 (1.2997)	grad_norm 24.9873 (29.5536)	mem 4879MB
[2022-05-31 09:23:19 MetaFG_0] (main.py 265): INFO Train: [78/300][630/1562]	eta 0:04:44 lr 0.000005	time 0.2982 (0.3055)	loss 1.3987 (1.2985)	grad_norm 41.9588 (29.5434)	mem 4879MB
[2022-05-31 09:23:22 MetaFG_0] (main.py 265): INFO Train: [78/300][640/1562]	eta 0:04:41 lr 0.000005	time 0.3006 (0.3055)	loss 1.3480 (1.3002)	grad_norm 20.8287 (29.6654)	mem 4879MB
[2022-05-31 09:23:25 MetaFG_0] (main.py 265): INFO Train: [78/300][650/1562]	eta 0:04:38 lr 0.000005	time 0.2938 (0.3055)	loss 0.9941 (1.3004)	grad_norm 43.8870 (29.6754)	mem 4879MB
[2022-05-31 09:23:28 MetaFG_0] (main.py 265): INFO Train: [78/300][660/1562]	eta 0:04:35 lr 0.000005	time 0.2946 (0.3055)	loss 0.9435 (1.3003)	grad_norm 16.2196 (29.6406)	mem 4879MB
[2022-05-31 09:23:31 MetaFG_0] (main.py 265): INFO Train: [78/300][670/1562]	eta 0:04:32 lr 0.000005	time 0.3001 (0.3056)	loss 1.3236 (1.3003)	grad_norm 23.2797 (29.6796)	mem 4879MB
[2022-05-31 09:23:34 MetaFG_0] (main.py 265): INFO Train: [78/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2921 (0.3055)	loss 1.2720 (1.2999)	grad_norm 25.2261 (29.6106)	mem 4879MB
[2022-05-31 09:23:37 MetaFG_0] (main.py 265): INFO Train: [78/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2919 (0.3055)	loss 1.4552 (1.3013)	grad_norm 20.4205 (29.6127)	mem 4879MB
[2022-05-31 09:23:40 MetaFG_0] (main.py 265): INFO Train: [78/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.2928 (0.3055)	loss 1.7127 (1.3034)	grad_norm 28.1371 (29.6922)	mem 4879MB
[2022-05-31 09:23:43 MetaFG_0] (main.py 265): INFO Train: [78/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2933 (0.3055)	loss 1.6591 (1.3045)	grad_norm 30.3639 (29.6384)	mem 4879MB
[2022-05-31 09:23:46 MetaFG_0] (main.py 265): INFO Train: [78/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2995 (0.3055)	loss 1.4571 (1.3042)	grad_norm 29.0483 (29.6324)	mem 4879MB
[2022-05-31 09:23:49 MetaFG_0] (main.py 265): INFO Train: [78/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2954 (0.3055)	loss 1.1988 (1.3039)	grad_norm 30.5783 (29.5778)	mem 4879MB
[2022-05-31 09:23:52 MetaFG_0] (main.py 265): INFO Train: [78/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2980 (0.3055)	loss 0.9308 (1.3033)	grad_norm 44.5424 (29.5512)	mem 4879MB
[2022-05-31 09:23:55 MetaFG_0] (main.py 265): INFO Train: [78/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2928 (0.3055)	loss 1.1467 (1.3024)	grad_norm 35.4679 (29.6593)	mem 4879MB
[2022-05-31 09:23:58 MetaFG_0] (main.py 265): INFO Train: [78/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.3005 (0.3055)	loss 1.7584 (1.3023)	grad_norm 27.2499 (29.6884)	mem 4879MB
[2022-05-31 09:24:02 MetaFG_0] (main.py 265): INFO Train: [78/300][770/1562]	eta 0:04:01 lr 0.000005	time 0.2924 (0.3055)	loss 0.8557 (1.3013)	grad_norm 23.9931 (29.6738)	mem 4879MB
[2022-05-31 09:24:05 MetaFG_0] (main.py 265): INFO Train: [78/300][780/1562]	eta 0:03:58 lr 0.000005	time 0.2923 (0.3055)	loss 1.4288 (1.3010)	grad_norm 19.1031 (29.6221)	mem 4879MB
[2022-05-31 09:24:08 MetaFG_0] (main.py 265): INFO Train: [78/300][790/1562]	eta 0:03:55 lr 0.000005	time 0.2938 (0.3055)	loss 1.1541 (1.3018)	grad_norm 22.1713 (29.5824)	mem 4879MB
[2022-05-31 09:24:11 MetaFG_0] (main.py 265): INFO Train: [78/300][800/1562]	eta 0:03:52 lr 0.000005	time 0.2919 (0.3055)	loss 1.5643 (1.3033)	grad_norm 21.0717 (29.5968)	mem 4879MB
[2022-05-31 09:24:14 MetaFG_0] (main.py 265): INFO Train: [78/300][810/1562]	eta 0:03:49 lr 0.000005	time 0.2918 (0.3055)	loss 1.3517 (1.3032)	grad_norm 27.1392 (29.5653)	mem 4879MB
[2022-05-31 09:24:17 MetaFG_0] (main.py 265): INFO Train: [78/300][820/1562]	eta 0:03:46 lr 0.000005	time 0.3001 (0.3055)	loss 1.4391 (1.3026)	grad_norm 37.4084 (29.5542)	mem 4879MB
[2022-05-31 09:24:20 MetaFG_0] (main.py 265): INFO Train: [78/300][830/1562]	eta 0:03:43 lr 0.000005	time 0.2993 (0.3055)	loss 0.8325 (1.3025)	grad_norm 23.9003 (29.5821)	mem 4879MB
[2022-05-31 09:24:23 MetaFG_0] (main.py 265): INFO Train: [78/300][840/1562]	eta 0:03:40 lr 0.000005	time 0.2922 (0.3055)	loss 1.4717 (1.3016)	grad_norm 25.5243 (29.5951)	mem 4879MB
[2022-05-31 09:24:26 MetaFG_0] (main.py 265): INFO Train: [78/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.2987 (0.3055)	loss 1.4112 (1.3022)	grad_norm 34.4886 (29.6185)	mem 4879MB
[2022-05-31 09:24:29 MetaFG_0] (main.py 265): INFO Train: [78/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.2993 (0.3055)	loss 1.1136 (1.3023)	grad_norm 59.6701 (29.7068)	mem 4879MB
[2022-05-31 09:24:32 MetaFG_0] (main.py 265): INFO Train: [78/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2966 (0.3055)	loss 1.3188 (1.3029)	grad_norm 20.7497 (29.7044)	mem 4879MB
[2022-05-31 09:24:35 MetaFG_0] (main.py 265): INFO Train: [78/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2943 (0.3054)	loss 1.4387 (1.3027)	grad_norm 30.3901 (29.7516)	mem 4879MB
[2022-05-31 09:24:38 MetaFG_0] (main.py 265): INFO Train: [78/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.3016 (0.3054)	loss 1.1094 (1.3034)	grad_norm 28.5765 (29.7975)	mem 4879MB
[2022-05-31 09:24:41 MetaFG_0] (main.py 265): INFO Train: [78/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.3081 (0.3054)	loss 1.0522 (1.3033)	grad_norm 34.4159 (29.8100)	mem 4879MB
[2022-05-31 09:24:44 MetaFG_0] (main.py 265): INFO Train: [78/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2986 (0.3054)	loss 1.5101 (1.3029)	grad_norm 33.3072 (29.8414)	mem 4879MB
[2022-05-31 09:24:47 MetaFG_0] (main.py 265): INFO Train: [78/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.3001 (0.3054)	loss 1.4847 (1.3030)	grad_norm 18.0495 (29.8049)	mem 4879MB
[2022-05-31 09:24:50 MetaFG_0] (main.py 265): INFO Train: [78/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2918 (0.3054)	loss 0.9850 (1.3028)	grad_norm 28.7559 (29.8271)	mem 4879MB
[2022-05-31 09:24:53 MetaFG_0] (main.py 265): INFO Train: [78/300][940/1562]	eta 0:03:09 lr 0.000005	time 0.2948 (0.3054)	loss 1.3796 (1.3025)	grad_norm 39.5677 (29.7762)	mem 4879MB
[2022-05-31 09:24:56 MetaFG_0] (main.py 265): INFO Train: [78/300][950/1562]	eta 0:03:06 lr 0.000005	time 0.2943 (0.3054)	loss 1.2934 (1.3038)	grad_norm 27.9452 (29.7520)	mem 4879MB
[2022-05-31 09:24:59 MetaFG_0] (main.py 265): INFO Train: [78/300][960/1562]	eta 0:03:03 lr 0.000005	time 0.2940 (0.3054)	loss 1.5679 (1.3027)	grad_norm 62.3101 (29.7785)	mem 4879MB
[2022-05-31 09:25:03 MetaFG_0] (main.py 265): INFO Train: [78/300][970/1562]	eta 0:03:00 lr 0.000005	time 0.2980 (0.3054)	loss 0.9320 (1.3029)	grad_norm 23.7355 (29.7738)	mem 4879MB
[2022-05-31 09:25:06 MetaFG_0] (main.py 265): INFO Train: [78/300][980/1562]	eta 0:02:57 lr 0.000005	time 0.2990 (0.3054)	loss 1.3075 (1.3036)	grad_norm 42.1760 (29.7652)	mem 4879MB
[2022-05-31 09:25:09 MetaFG_0] (main.py 265): INFO Train: [78/300][990/1562]	eta 0:02:54 lr 0.000005	time 0.2981 (0.3054)	loss 1.5206 (1.3038)	grad_norm 19.1657 (29.7263)	mem 4879MB
[2022-05-31 09:25:12 MetaFG_0] (main.py 265): INFO Train: [78/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2995 (0.3054)	loss 1.4478 (1.3037)	grad_norm 22.7580 (29.7104)	mem 4879MB
[2022-05-31 09:25:15 MetaFG_0] (main.py 265): INFO Train: [78/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.3026 (0.3054)	loss 1.4753 (1.3046)	grad_norm 33.0632 (29.9302)	mem 4879MB
[2022-05-31 09:25:18 MetaFG_0] (main.py 265): INFO Train: [78/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2924 (0.3053)	loss 0.9007 (1.3038)	grad_norm 41.2099 (29.9094)	mem 4879MB
[2022-05-31 09:25:21 MetaFG_0] (main.py 265): INFO Train: [78/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.2943 (0.3053)	loss 1.4237 (1.3035)	grad_norm 23.4052 (29.9259)	mem 4879MB
[2022-05-31 09:25:24 MetaFG_0] (main.py 265): INFO Train: [78/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2929 (0.3053)	loss 1.2040 (1.3040)	grad_norm 33.2489 (29.9494)	mem 4879MB
[2022-05-31 09:25:27 MetaFG_0] (main.py 265): INFO Train: [78/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.2918 (0.3053)	loss 1.4964 (1.3053)	grad_norm 25.4762 (29.9450)	mem 4879MB
[2022-05-31 09:25:30 MetaFG_0] (main.py 265): INFO Train: [78/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.3023 (0.3053)	loss 1.5966 (1.3057)	grad_norm 35.9951 (29.9794)	mem 4879MB
[2022-05-31 09:25:33 MetaFG_0] (main.py 265): INFO Train: [78/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2936 (0.3053)	loss 1.2824 (1.3052)	grad_norm 33.2676 (29.9722)	mem 4879MB
[2022-05-31 09:25:36 MetaFG_0] (main.py 265): INFO Train: [78/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2996 (0.3053)	loss 1.7555 (1.3051)	grad_norm 22.3298 (29.9426)	mem 4879MB
[2022-05-31 09:25:39 MetaFG_0] (main.py 265): INFO Train: [78/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.2927 (0.3053)	loss 1.5340 (1.3043)	grad_norm 29.5753 (29.9366)	mem 4879MB
[2022-05-31 09:25:42 MetaFG_0] (main.py 265): INFO Train: [78/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2940 (0.3053)	loss 1.0971 (1.3041)	grad_norm 42.2137 (29.9511)	mem 4879MB
[2022-05-31 09:25:45 MetaFG_0] (main.py 265): INFO Train: [78/300][1110/1562]	eta 0:02:17 lr 0.000005	time 0.2940 (0.3053)	loss 1.4280 (1.3050)	grad_norm 31.5636 (29.9407)	mem 4879MB
[2022-05-31 09:25:48 MetaFG_0] (main.py 265): INFO Train: [78/300][1120/1562]	eta 0:02:14 lr 0.000005	time 0.2929 (0.3053)	loss 1.3021 (1.3052)	grad_norm 32.8722 (29.8872)	mem 4879MB
[2022-05-31 09:25:51 MetaFG_0] (main.py 265): INFO Train: [78/300][1130/1562]	eta 0:02:11 lr 0.000005	time 0.3120 (0.3054)	loss 1.5134 (1.3049)	grad_norm 22.1900 (29.8636)	mem 4879MB
[2022-05-31 09:25:55 MetaFG_0] (main.py 265): INFO Train: [78/300][1140/1562]	eta 0:02:08 lr 0.000005	time 0.2927 (0.3055)	loss 1.4823 (1.3053)	grad_norm 30.9716 (29.8728)	mem 4879MB
[2022-05-31 09:25:58 MetaFG_0] (main.py 265): INFO Train: [78/300][1150/1562]	eta 0:02:05 lr 0.000005	time 0.2940 (0.3055)	loss 1.0703 (1.3053)	grad_norm 19.5996 (29.8450)	mem 4879MB
[2022-05-31 09:26:01 MetaFG_0] (main.py 265): INFO Train: [78/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2939 (0.3055)	loss 1.0875 (1.3061)	grad_norm 30.0996 (29.8212)	mem 4879MB
[2022-05-31 09:26:04 MetaFG_0] (main.py 265): INFO Train: [78/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.2999 (0.3055)	loss 1.5389 (1.3074)	grad_norm 13.8754 (29.8054)	mem 4879MB
[2022-05-31 09:26:07 MetaFG_0] (main.py 265): INFO Train: [78/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2922 (0.3055)	loss 1.0446 (1.3080)	grad_norm 43.8331 (29.7567)	mem 4879MB
[2022-05-31 09:26:10 MetaFG_0] (main.py 265): INFO Train: [78/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2948 (0.3055)	loss 1.0465 (1.3069)	grad_norm 14.1890 (29.7738)	mem 4879MB
[2022-05-31 09:26:13 MetaFG_0] (main.py 265): INFO Train: [78/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2989 (0.3055)	loss 1.5682 (1.3080)	grad_norm 42.5314 (29.7529)	mem 4879MB
[2022-05-31 09:26:16 MetaFG_0] (main.py 265): INFO Train: [78/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.3005 (0.3055)	loss 1.5123 (1.3085)	grad_norm 24.2285 (29.7469)	mem 4879MB
[2022-05-31 09:26:19 MetaFG_0] (main.py 265): INFO Train: [78/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.3014 (0.3055)	loss 1.4247 (1.3086)	grad_norm 29.4638 (29.7383)	mem 4879MB
[2022-05-31 09:26:22 MetaFG_0] (main.py 265): INFO Train: [78/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2919 (0.3055)	loss 0.8767 (1.3080)	grad_norm 38.7402 (29.7723)	mem 4879MB
[2022-05-31 09:26:25 MetaFG_0] (main.py 265): INFO Train: [78/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.3018 (0.3055)	loss 1.4610 (1.3083)	grad_norm 24.8817 (29.7557)	mem 4879MB
[2022-05-31 09:26:28 MetaFG_0] (main.py 265): INFO Train: [78/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2998 (0.3055)	loss 0.8583 (1.3077)	grad_norm 35.7645 (29.7630)	mem 4879MB
[2022-05-31 09:26:31 MetaFG_0] (main.py 265): INFO Train: [78/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2934 (0.3055)	loss 1.1324 (1.3074)	grad_norm 14.9995 (29.7446)	mem 4879MB
[2022-05-31 09:26:34 MetaFG_0] (main.py 265): INFO Train: [78/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2925 (0.3054)	loss 1.5421 (1.3073)	grad_norm 37.0726 (29.7357)	mem 4879MB
[2022-05-31 09:26:37 MetaFG_0] (main.py 265): INFO Train: [78/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.2999 (0.3054)	loss 0.9291 (1.3070)	grad_norm 18.1097 (29.6846)	mem 4879MB
[2022-05-31 09:26:40 MetaFG_0] (main.py 265): INFO Train: [78/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2929 (0.3055)	loss 1.1985 (1.3073)	grad_norm 25.5878 (29.7051)	mem 4879MB
[2022-05-31 09:26:43 MetaFG_0] (main.py 265): INFO Train: [78/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.2935 (0.3054)	loss 1.4048 (1.3075)	grad_norm 18.0657 (29.7227)	mem 4879MB
[2022-05-31 09:26:46 MetaFG_0] (main.py 265): INFO Train: [78/300][1310/1562]	eta 0:01:16 lr 0.000005	time 0.2948 (0.3054)	loss 0.9872 (1.3065)	grad_norm 26.8989 (29.7057)	mem 4879MB
[2022-05-31 09:26:49 MetaFG_0] (main.py 265): INFO Train: [78/300][1320/1562]	eta 0:01:13 lr 0.000005	time 0.2925 (0.3054)	loss 1.3354 (1.3060)	grad_norm 22.6837 (29.6809)	mem 4879MB
[2022-05-31 09:26:53 MetaFG_0] (main.py 265): INFO Train: [78/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.2936 (0.3054)	loss 1.6784 (1.3070)	grad_norm 39.2026 (29.6707)	mem 4879MB
[2022-05-31 09:26:56 MetaFG_0] (main.py 265): INFO Train: [78/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.3019 (0.3054)	loss 1.5526 (1.3069)	grad_norm 37.2516 (29.7032)	mem 4879MB
[2022-05-31 09:26:59 MetaFG_0] (main.py 265): INFO Train: [78/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2928 (0.3054)	loss 1.2909 (1.3068)	grad_norm 19.9559 (29.7489)	mem 4879MB
[2022-05-31 09:27:02 MetaFG_0] (main.py 265): INFO Train: [78/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2933 (0.3054)	loss 1.5963 (1.3077)	grad_norm 27.6981 (29.7821)	mem 4879MB
[2022-05-31 09:27:05 MetaFG_0] (main.py 265): INFO Train: [78/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2924 (0.3054)	loss 1.0539 (1.3077)	grad_norm 21.7042 (29.7408)	mem 4879MB
[2022-05-31 09:27:08 MetaFG_0] (main.py 265): INFO Train: [78/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.2944 (0.3054)	loss 1.3985 (1.3073)	grad_norm 23.9149 (29.7231)	mem 4879MB
[2022-05-31 09:27:11 MetaFG_0] (main.py 265): INFO Train: [78/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.3004 (0.3054)	loss 1.4094 (1.3068)	grad_norm 18.3924 (29.6893)	mem 4879MB
[2022-05-31 09:27:14 MetaFG_0] (main.py 265): INFO Train: [78/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2928 (0.3054)	loss 1.5273 (1.3068)	grad_norm 27.4500 (29.6579)	mem 4879MB
[2022-05-31 09:27:17 MetaFG_0] (main.py 265): INFO Train: [78/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2998 (0.3054)	loss 0.8542 (1.3058)	grad_norm 34.6294 (29.6439)	mem 4879MB
[2022-05-31 09:27:20 MetaFG_0] (main.py 265): INFO Train: [78/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2930 (0.3054)	loss 1.4105 (1.3065)	grad_norm 17.3752 (29.6095)	mem 4879MB
[2022-05-31 09:27:23 MetaFG_0] (main.py 265): INFO Train: [78/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2983 (0.3054)	loss 0.9972 (1.3063)	grad_norm 26.7706 (29.5756)	mem 4879MB
[2022-05-31 09:27:26 MetaFG_0] (main.py 265): INFO Train: [78/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2939 (0.3054)	loss 1.6618 (1.3065)	grad_norm 29.3743 (29.5422)	mem 4879MB
[2022-05-31 09:27:29 MetaFG_0] (main.py 265): INFO Train: [78/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2931 (0.3054)	loss 0.9697 (1.3070)	grad_norm 40.5989 (29.5595)	mem 4879MB
[2022-05-31 09:27:32 MetaFG_0] (main.py 265): INFO Train: [78/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2928 (0.3054)	loss 1.2414 (1.3074)	grad_norm 16.7837 (29.5263)	mem 4879MB
[2022-05-31 09:27:35 MetaFG_0] (main.py 265): INFO Train: [78/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2936 (0.3054)	loss 1.3398 (1.3073)	grad_norm 18.8440 (29.5134)	mem 4879MB
[2022-05-31 09:27:38 MetaFG_0] (main.py 265): INFO Train: [78/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2991 (0.3054)	loss 0.9273 (1.3073)	grad_norm 39.4748 (29.5688)	mem 4879MB
[2022-05-31 09:27:41 MetaFG_0] (main.py 265): INFO Train: [78/300][1490/1562]	eta 0:00:21 lr 0.000005	time 0.3053 (0.3054)	loss 1.5154 (1.3075)	grad_norm 22.5336 (29.5909)	mem 4879MB
[2022-05-31 09:27:44 MetaFG_0] (main.py 265): INFO Train: [78/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2988 (0.3054)	loss 1.5125 (1.3081)	grad_norm 28.0253 (29.5824)	mem 4879MB
[2022-05-31 09:27:47 MetaFG_0] (main.py 265): INFO Train: [78/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.3003 (0.3054)	loss 1.2889 (1.3079)	grad_norm 36.6926 (29.6153)	mem 4879MB
[2022-05-31 09:27:50 MetaFG_0] (main.py 265): INFO Train: [78/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2925 (0.3054)	loss 1.3638 (1.3069)	grad_norm 31.7347 (29.6012)	mem 4879MB
[2022-05-31 09:27:53 MetaFG_0] (main.py 265): INFO Train: [78/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.3011 (0.3054)	loss 1.0849 (1.3060)	grad_norm 44.4993 (29.5903)	mem 4879MB
[2022-05-31 09:27:57 MetaFG_0] (main.py 265): INFO Train: [78/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2993 (0.3053)	loss 1.2343 (1.3060)	grad_norm 19.4619 (29.5578)	mem 4879MB
[2022-05-31 09:28:00 MetaFG_0] (main.py 265): INFO Train: [78/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.2925 (0.3053)	loss 1.5372 (1.3072)	grad_norm 29.8026 (29.5609)	mem 4879MB
[2022-05-31 09:28:03 MetaFG_0] (main.py 265): INFO Train: [78/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2934 (0.3053)	loss 1.2179 (1.3075)	grad_norm 29.2545 (29.5375)	mem 4879MB
[2022-05-31 09:28:03 MetaFG_0] (main.py 272): INFO EPOCH 78 training takes 0:07:57
[2022-05-31 09:28:03 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_78.pth saving......
[2022-05-31 09:28:04 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_78.pth saved !!!
[2022-05-31 09:28:04 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 09:28:05 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 09:28:05 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 09:28:06 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.775 (0.775)	Loss 0.4422 (0.4422)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 09:28:07 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.096 (0.163)	Loss 0.7019 (0.5413)	Acc@1 78.125 (87.500)	Acc@5 100.000 (99.432)	Mem 4879MB
[2022-05-31 09:28:08 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.097 (0.130)	Loss 0.7065 (0.5600)	Acc@1 81.250 (87.649)	Acc@5 96.875 (98.661)	Mem 4879MB
[2022-05-31 09:28:09 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.093 (0.118)	Loss 0.3816 (0.5618)	Acc@1 93.750 (87.097)	Acc@5 100.000 (98.790)	Mem 4879MB
[2022-05-31 09:28:10 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.095 (0.112)	Loss 0.5296 (0.5526)	Acc@1 87.500 (87.652)	Acc@5 100.000 (98.933)	Mem 4879MB
[2022-05-31 09:28:11 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.087 (0.109)	Loss 0.7262 (0.5567)	Acc@1 81.250 (87.377)	Acc@5 93.750 (98.897)	Mem 4879MB
[2022-05-31 09:28:12 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.088 (0.106)	Loss 0.6279 (0.5453)	Acc@1 90.625 (87.961)	Acc@5 96.875 (99.027)	Mem 4879MB
[2022-05-31 09:28:13 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.096 (0.105)	Loss 0.4348 (0.5442)	Acc@1 87.500 (88.116)	Acc@5 100.000 (98.988)	Mem 4879MB
[2022-05-31 09:28:14 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.097 (0.104)	Loss 0.6110 (0.5499)	Acc@1 87.500 (87.963)	Acc@5 100.000 (98.881)	Mem 4879MB
[2022-05-31 09:28:15 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.092 (0.103)	Loss 0.4736 (0.5559)	Acc@1 93.750 (87.809)	Acc@5 100.000 (98.798)	Mem 4879MB
[2022-05-31 09:28:16 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.087 (0.102)	Loss 0.8212 (0.5667)	Acc@1 81.250 (87.593)	Acc@5 93.750 (98.731)	Mem 4879MB
[2022-05-31 09:28:17 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.096 (0.101)	Loss 0.4905 (0.5690)	Acc@1 87.500 (87.556)	Acc@5 100.000 (98.846)	Mem 4879MB
[2022-05-31 09:28:17 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.115 (0.101)	Loss 0.6336 (0.5678)	Acc@1 84.375 (87.629)	Acc@5 96.875 (98.838)	Mem 4879MB
[2022-05-31 09:28:18 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.097 (0.100)	Loss 0.5407 (0.5695)	Acc@1 90.625 (87.619)	Acc@5 96.875 (98.712)	Mem 4879MB
[2022-05-31 09:28:19 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.098 (0.100)	Loss 0.7510 (0.5755)	Acc@1 90.625 (87.655)	Acc@5 93.750 (98.670)	Mem 4879MB
[2022-05-31 09:28:20 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.103 (0.099)	Loss 0.4943 (0.5810)	Acc@1 90.625 (87.521)	Acc@5 96.875 (98.675)	Mem 4879MB
[2022-05-31 09:28:21 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.099)	Loss 0.3673 (0.5849)	Acc@1 93.750 (87.442)	Acc@5 100.000 (98.661)	Mem 4879MB
[2022-05-31 09:28:22 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.087 (0.099)	Loss 0.5986 (0.5819)	Acc@1 84.375 (87.591)	Acc@5 96.875 (98.702)	Mem 4879MB
[2022-05-31 09:28:23 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.097 (0.099)	Loss 0.8241 (0.5841)	Acc@1 81.250 (87.552)	Acc@5 96.875 (98.653)	Mem 4879MB
[2022-05-31 09:28:24 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.087 (0.098)	Loss 0.3114 (0.5793)	Acc@1 100.000 (87.729)	Acc@5 100.000 (98.724)	Mem 4879MB
[2022-05-31 09:28:25 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.103 (0.098)	Loss 0.6564 (0.5815)	Acc@1 84.375 (87.671)	Acc@5 100.000 (98.725)	Mem 4879MB
[2022-05-31 09:28:26 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.098 (0.098)	Loss 0.5663 (0.5806)	Acc@1 87.500 (87.693)	Acc@5 100.000 (98.741)	Mem 4879MB
[2022-05-31 09:28:27 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.097 (0.098)	Loss 0.8027 (0.5851)	Acc@1 84.375 (87.571)	Acc@5 96.875 (98.713)	Mem 4879MB
[2022-05-31 09:28:28 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.088 (0.098)	Loss 0.5254 (0.5846)	Acc@1 93.750 (87.676)	Acc@5 100.000 (98.715)	Mem 4879MB
[2022-05-31 09:28:29 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.098)	Loss 0.6051 (0.5839)	Acc@1 87.500 (87.707)	Acc@5 96.875 (98.742)	Mem 4879MB
[2022-05-31 09:28:30 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.105 (0.098)	Loss 0.7025 (0.5849)	Acc@1 81.250 (87.575)	Acc@5 96.875 (98.767)	Mem 4879MB
[2022-05-31 09:28:31 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.094 (0.098)	Loss 0.9098 (0.5842)	Acc@1 81.250 (87.656)	Acc@5 93.750 (98.779)	Mem 4879MB
[2022-05-31 09:28:32 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.096 (0.097)	Loss 0.4654 (0.5843)	Acc@1 90.625 (87.661)	Acc@5 100.000 (98.812)	Mem 4879MB
[2022-05-31 09:28:33 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.094 (0.097)	Loss 0.4555 (0.5845)	Acc@1 90.625 (87.645)	Acc@5 100.000 (98.810)	Mem 4879MB
[2022-05-31 09:28:34 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.099 (0.097)	Loss 0.6642 (0.5833)	Acc@1 78.125 (87.640)	Acc@5 100.000 (98.851)	Mem 4879MB
[2022-05-31 09:28:34 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.094 (0.097)	Loss 0.6344 (0.5818)	Acc@1 84.375 (87.676)	Acc@5 96.875 (98.858)	Mem 4879MB
[2022-05-31 09:28:35 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.7938 (0.5820)	Acc@1 81.250 (87.671)	Acc@5 96.875 (98.865)	Mem 4879MB
[2022-05-31 09:28:36 MetaFG_0] (main.py 330): INFO  * Acc@1 87.630 Acc@5 98.870
[2022-05-31 09:28:36 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.6%
[2022-05-31 09:28:36 MetaFG_0] (main.py 171): INFO Max accuracy: 87.63%
[2022-05-31 09:28:37 MetaFG_0] (main.py 265): INFO Train: [79/300][0/1562]	eta 0:27:05 lr 0.000005	time 1.0409 (1.0409)	loss 1.1093 (1.1093)	grad_norm 39.8387 (39.8387)	mem 4879MB
[2022-05-31 09:28:40 MetaFG_0] (main.py 265): INFO Train: [79/300][10/1562]	eta 0:09:46 lr 0.000005	time 0.3000 (0.3776)	loss 1.3091 (1.1022)	grad_norm 27.0252 (25.7010)	mem 4879MB
[2022-05-31 09:28:43 MetaFG_0] (main.py 265): INFO Train: [79/300][20/1562]	eta 0:08:48 lr 0.000005	time 0.2926 (0.3425)	loss 0.9493 (1.1736)	grad_norm 26.0887 (29.1144)	mem 4879MB
[2022-05-31 09:28:46 MetaFG_0] (main.py 265): INFO Train: [79/300][30/1562]	eta 0:08:25 lr 0.000005	time 0.2923 (0.3298)	loss 1.4560 (1.1912)	grad_norm 34.2393 (30.4433)	mem 4879MB
[2022-05-31 09:28:49 MetaFG_0] (main.py 265): INFO Train: [79/300][40/1562]	eta 0:08:13 lr 0.000005	time 0.2983 (0.3240)	loss 0.9425 (1.2055)	grad_norm 30.9267 (29.1995)	mem 4879MB
[2022-05-31 09:28:52 MetaFG_0] (main.py 265): INFO Train: [79/300][50/1562]	eta 0:08:03 lr 0.000005	time 0.2921 (0.3197)	loss 1.4223 (1.2359)	grad_norm 18.1033 (29.5176)	mem 4879MB
[2022-05-31 09:28:55 MetaFG_0] (main.py 265): INFO Train: [79/300][60/1562]	eta 0:07:56 lr 0.000005	time 0.2919 (0.3170)	loss 1.5439 (1.2571)	grad_norm 21.2830 (29.7191)	mem 4879MB
[2022-05-31 09:28:58 MetaFG_0] (main.py 265): INFO Train: [79/300][70/1562]	eta 0:07:49 lr 0.000005	time 0.2927 (0.3150)	loss 1.3683 (1.2589)	grad_norm 33.0456 (30.6462)	mem 4879MB
[2022-05-31 09:29:01 MetaFG_0] (main.py 265): INFO Train: [79/300][80/1562]	eta 0:07:44 lr 0.000005	time 0.2995 (0.3134)	loss 1.5478 (1.2781)	grad_norm 33.5810 (30.2328)	mem 4879MB
[2022-05-31 09:29:04 MetaFG_0] (main.py 265): INFO Train: [79/300][90/1562]	eta 0:07:40 lr 0.000005	time 0.3039 (0.3126)	loss 1.5560 (1.2787)	grad_norm 22.5536 (29.9522)	mem 4879MB
[2022-05-31 09:29:07 MetaFG_0] (main.py 265): INFO Train: [79/300][100/1562]	eta 0:07:35 lr 0.000005	time 0.2946 (0.3118)	loss 1.1903 (1.2797)	grad_norm 37.2757 (30.3739)	mem 4879MB
[2022-05-31 09:29:10 MetaFG_0] (main.py 265): INFO Train: [79/300][110/1562]	eta 0:07:31 lr 0.000005	time 0.2989 (0.3111)	loss 1.5009 (1.2785)	grad_norm 22.8773 (30.2155)	mem 4879MB
[2022-05-31 09:29:13 MetaFG_0] (main.py 265): INFO Train: [79/300][120/1562]	eta 0:07:27 lr 0.000005	time 0.2988 (0.3103)	loss 1.6764 (1.2811)	grad_norm 51.5235 (30.3282)	mem 4879MB
[2022-05-31 09:29:16 MetaFG_0] (main.py 265): INFO Train: [79/300][130/1562]	eta 0:07:23 lr 0.000005	time 0.2938 (0.3100)	loss 1.2808 (1.2912)	grad_norm 25.8439 (30.3458)	mem 4879MB
[2022-05-31 09:29:19 MetaFG_0] (main.py 265): INFO Train: [79/300][140/1562]	eta 0:07:20 lr 0.000005	time 0.2936 (0.3097)	loss 1.0626 (1.2981)	grad_norm 46.2044 (30.5737)	mem 4879MB
[2022-05-31 09:29:22 MetaFG_0] (main.py 265): INFO Train: [79/300][150/1562]	eta 0:07:16 lr 0.000005	time 0.2941 (0.3094)	loss 1.4526 (1.3090)	grad_norm 34.3271 (30.3113)	mem 4879MB
[2022-05-31 09:29:25 MetaFG_0] (main.py 265): INFO Train: [79/300][160/1562]	eta 0:07:13 lr 0.000005	time 0.2978 (0.3091)	loss 1.3170 (1.3111)	grad_norm 38.4328 (30.3135)	mem 4879MB
[2022-05-31 09:29:28 MetaFG_0] (main.py 265): INFO Train: [79/300][170/1562]	eta 0:07:09 lr 0.000005	time 0.2978 (0.3089)	loss 1.4493 (1.3078)	grad_norm 43.5965 (30.3312)	mem 4879MB
[2022-05-31 09:29:32 MetaFG_0] (main.py 265): INFO Train: [79/300][180/1562]	eta 0:07:06 lr 0.000005	time 0.2925 (0.3086)	loss 1.1516 (1.3128)	grad_norm 30.1977 (30.3755)	mem 4879MB
[2022-05-31 09:29:35 MetaFG_0] (main.py 265): INFO Train: [79/300][190/1562]	eta 0:07:03 lr 0.000005	time 0.2931 (0.3086)	loss 1.5878 (1.3102)	grad_norm 30.7565 (30.4540)	mem 4879MB
[2022-05-31 09:29:38 MetaFG_0] (main.py 265): INFO Train: [79/300][200/1562]	eta 0:06:59 lr 0.000005	time 0.3076 (0.3083)	loss 1.4964 (1.3137)	grad_norm 17.9440 (30.6209)	mem 4879MB
[2022-05-31 09:29:41 MetaFG_0] (main.py 265): INFO Train: [79/300][210/1562]	eta 0:06:56 lr 0.000005	time 0.3038 (0.3083)	loss 1.1210 (1.3168)	grad_norm 30.3079 (31.0207)	mem 4879MB
[2022-05-31 09:29:44 MetaFG_0] (main.py 265): INFO Train: [79/300][220/1562]	eta 0:06:53 lr 0.000005	time 0.2979 (0.3081)	loss 1.5877 (1.3179)	grad_norm 22.1595 (31.3325)	mem 4879MB
[2022-05-31 09:29:47 MetaFG_0] (main.py 265): INFO Train: [79/300][230/1562]	eta 0:06:50 lr 0.000005	time 0.2985 (0.3080)	loss 1.2851 (1.3170)	grad_norm 27.9288 (31.2540)	mem 4879MB
[2022-05-31 09:29:50 MetaFG_0] (main.py 265): INFO Train: [79/300][240/1562]	eta 0:06:46 lr 0.000005	time 0.2948 (0.3078)	loss 1.3902 (1.3190)	grad_norm 45.6193 (inf)	mem 4879MB
[2022-05-31 09:29:53 MetaFG_0] (main.py 265): INFO Train: [79/300][250/1562]	eta 0:06:43 lr 0.000005	time 0.2979 (0.3076)	loss 1.3252 (1.3196)	grad_norm 21.7750 (inf)	mem 4879MB
[2022-05-31 09:29:56 MetaFG_0] (main.py 265): INFO Train: [79/300][260/1562]	eta 0:06:40 lr 0.000005	time 0.2925 (0.3075)	loss 1.4638 (1.3182)	grad_norm 29.7624 (inf)	mem 4879MB
[2022-05-31 09:29:59 MetaFG_0] (main.py 265): INFO Train: [79/300][270/1562]	eta 0:06:37 lr 0.000005	time 0.2934 (0.3073)	loss 0.8914 (1.3157)	grad_norm 36.8783 (inf)	mem 4879MB
[2022-05-31 09:30:02 MetaFG_0] (main.py 265): INFO Train: [79/300][280/1562]	eta 0:06:33 lr 0.000005	time 0.2919 (0.3073)	loss 1.4263 (1.3206)	grad_norm 36.8268 (inf)	mem 4879MB
[2022-05-31 09:30:05 MetaFG_0] (main.py 265): INFO Train: [79/300][290/1562]	eta 0:06:30 lr 0.000005	time 0.2941 (0.3071)	loss 1.2485 (1.3196)	grad_norm 15.5868 (inf)	mem 4879MB
[2022-05-31 09:30:08 MetaFG_0] (main.py 265): INFO Train: [79/300][300/1562]	eta 0:06:27 lr 0.000005	time 0.2998 (0.3071)	loss 1.5150 (1.3198)	grad_norm 16.5507 (inf)	mem 4879MB
[2022-05-31 09:30:11 MetaFG_0] (main.py 265): INFO Train: [79/300][310/1562]	eta 0:06:24 lr 0.000005	time 0.2947 (0.3070)	loss 1.4163 (1.3208)	grad_norm 29.3226 (inf)	mem 4879MB
[2022-05-31 09:30:14 MetaFG_0] (main.py 265): INFO Train: [79/300][320/1562]	eta 0:06:21 lr 0.000005	time 0.2926 (0.3069)	loss 1.2785 (1.3217)	grad_norm 45.5452 (inf)	mem 4879MB
[2022-05-31 09:30:17 MetaFG_0] (main.py 265): INFO Train: [79/300][330/1562]	eta 0:06:17 lr 0.000005	time 0.3017 (0.3068)	loss 1.0829 (1.3209)	grad_norm 31.6571 (inf)	mem 4879MB
[2022-05-31 09:30:20 MetaFG_0] (main.py 265): INFO Train: [79/300][340/1562]	eta 0:06:14 lr 0.000005	time 0.2939 (0.3067)	loss 1.4119 (1.3221)	grad_norm 20.1554 (inf)	mem 4879MB
[2022-05-31 09:30:23 MetaFG_0] (main.py 265): INFO Train: [79/300][350/1562]	eta 0:06:11 lr 0.000005	time 0.3016 (0.3067)	loss 1.5222 (1.3219)	grad_norm 34.2850 (inf)	mem 4879MB
[2022-05-31 09:30:26 MetaFG_0] (main.py 265): INFO Train: [79/300][360/1562]	eta 0:06:08 lr 0.000005	time 0.2987 (0.3067)	loss 1.3958 (1.3183)	grad_norm 27.2107 (inf)	mem 4879MB
[2022-05-31 09:30:29 MetaFG_0] (main.py 265): INFO Train: [79/300][370/1562]	eta 0:06:05 lr 0.000005	time 0.2933 (0.3067)	loss 1.5663 (1.3195)	grad_norm 27.6570 (inf)	mem 4879MB
[2022-05-31 09:30:33 MetaFG_0] (main.py 265): INFO Train: [79/300][380/1562]	eta 0:06:02 lr 0.000005	time 0.2992 (0.3067)	loss 0.7412 (1.3198)	grad_norm 19.6278 (inf)	mem 4879MB
[2022-05-31 09:30:36 MetaFG_0] (main.py 265): INFO Train: [79/300][390/1562]	eta 0:05:59 lr 0.000005	time 0.3014 (0.3067)	loss 1.3919 (1.3188)	grad_norm 24.5188 (inf)	mem 4879MB
[2022-05-31 09:30:39 MetaFG_0] (main.py 265): INFO Train: [79/300][400/1562]	eta 0:05:56 lr 0.000005	time 0.2929 (0.3066)	loss 1.1262 (1.3209)	grad_norm 34.6454 (inf)	mem 4879MB
[2022-05-31 09:30:42 MetaFG_0] (main.py 265): INFO Train: [79/300][410/1562]	eta 0:05:53 lr 0.000005	time 0.2930 (0.3066)	loss 1.5436 (1.3179)	grad_norm 21.5562 (inf)	mem 4879MB
[2022-05-31 09:30:45 MetaFG_0] (main.py 265): INFO Train: [79/300][420/1562]	eta 0:05:50 lr 0.000005	time 0.2925 (0.3065)	loss 1.4006 (1.3169)	grad_norm 39.7479 (inf)	mem 4879MB
[2022-05-31 09:30:48 MetaFG_0] (main.py 265): INFO Train: [79/300][430/1562]	eta 0:05:46 lr 0.000005	time 0.2983 (0.3064)	loss 1.0234 (1.3159)	grad_norm 20.7702 (inf)	mem 4879MB
[2022-05-31 09:30:51 MetaFG_0] (main.py 265): INFO Train: [79/300][440/1562]	eta 0:05:43 lr 0.000005	time 0.2947 (0.3064)	loss 1.5053 (1.3134)	grad_norm 41.4042 (inf)	mem 4879MB
[2022-05-31 09:30:54 MetaFG_0] (main.py 265): INFO Train: [79/300][450/1562]	eta 0:05:40 lr 0.000005	time 0.3000 (0.3064)	loss 1.3326 (1.3155)	grad_norm 23.4764 (inf)	mem 4879MB
[2022-05-31 09:30:57 MetaFG_0] (main.py 265): INFO Train: [79/300][460/1562]	eta 0:05:37 lr 0.000005	time 0.2948 (0.3064)	loss 1.2861 (1.3130)	grad_norm 36.5231 (inf)	mem 4879MB
[2022-05-31 09:31:00 MetaFG_0] (main.py 265): INFO Train: [79/300][470/1562]	eta 0:05:34 lr 0.000005	time 0.2997 (0.3064)	loss 1.3483 (1.3146)	grad_norm 18.2786 (inf)	mem 4879MB
[2022-05-31 09:31:03 MetaFG_0] (main.py 265): INFO Train: [79/300][480/1562]	eta 0:05:31 lr 0.000005	time 0.3012 (0.3064)	loss 1.4617 (1.3155)	grad_norm 22.7442 (inf)	mem 4879MB
[2022-05-31 09:31:06 MetaFG_0] (main.py 265): INFO Train: [79/300][490/1562]	eta 0:05:28 lr 0.000005	time 0.2990 (0.3064)	loss 1.6220 (1.3172)	grad_norm 27.3566 (inf)	mem 4879MB
[2022-05-31 09:31:09 MetaFG_0] (main.py 265): INFO Train: [79/300][500/1562]	eta 0:05:25 lr 0.000005	time 0.3014 (0.3064)	loss 1.4736 (1.3185)	grad_norm 20.3762 (inf)	mem 4879MB
[2022-05-31 09:31:12 MetaFG_0] (main.py 265): INFO Train: [79/300][510/1562]	eta 0:05:22 lr 0.000005	time 0.2949 (0.3064)	loss 1.3195 (1.3165)	grad_norm 43.5232 (inf)	mem 4879MB
[2022-05-31 09:31:15 MetaFG_0] (main.py 265): INFO Train: [79/300][520/1562]	eta 0:05:19 lr 0.000005	time 0.2987 (0.3064)	loss 1.0963 (1.3179)	grad_norm 33.3473 (inf)	mem 4879MB
[2022-05-31 09:31:18 MetaFG_0] (main.py 265): INFO Train: [79/300][530/1562]	eta 0:05:16 lr 0.000005	time 0.2941 (0.3064)	loss 1.2712 (1.3179)	grad_norm 29.6469 (inf)	mem 4879MB
[2022-05-31 09:31:21 MetaFG_0] (main.py 265): INFO Train: [79/300][540/1562]	eta 0:05:13 lr 0.000005	time 0.2957 (0.3064)	loss 1.2882 (1.3174)	grad_norm 21.2530 (inf)	mem 4879MB
[2022-05-31 09:31:24 MetaFG_0] (main.py 265): INFO Train: [79/300][550/1562]	eta 0:05:10 lr 0.000005	time 0.2937 (0.3064)	loss 1.0509 (1.3144)	grad_norm 26.0327 (inf)	mem 4879MB
[2022-05-31 09:31:28 MetaFG_0] (main.py 265): INFO Train: [79/300][560/1562]	eta 0:05:06 lr 0.000005	time 0.2988 (0.3064)	loss 1.4000 (1.3142)	grad_norm 49.5739 (inf)	mem 4879MB
[2022-05-31 09:31:31 MetaFG_0] (main.py 265): INFO Train: [79/300][570/1562]	eta 0:05:03 lr 0.000005	time 0.2931 (0.3064)	loss 1.4803 (1.3148)	grad_norm 36.5352 (inf)	mem 4879MB
[2022-05-31 09:31:34 MetaFG_0] (main.py 265): INFO Train: [79/300][580/1562]	eta 0:05:00 lr 0.000005	time 0.2932 (0.3063)	loss 1.1250 (1.3114)	grad_norm 44.4768 (inf)	mem 4879MB
[2022-05-31 09:31:37 MetaFG_0] (main.py 265): INFO Train: [79/300][590/1562]	eta 0:04:57 lr 0.000005	time 0.2988 (0.3063)	loss 1.7361 (1.3096)	grad_norm 39.5190 (inf)	mem 4879MB
[2022-05-31 09:31:40 MetaFG_0] (main.py 265): INFO Train: [79/300][600/1562]	eta 0:04:54 lr 0.000005	time 0.2944 (0.3063)	loss 1.3593 (1.3086)	grad_norm 22.3267 (inf)	mem 4879MB
[2022-05-31 09:31:43 MetaFG_0] (main.py 265): INFO Train: [79/300][610/1562]	eta 0:04:51 lr 0.000005	time 0.2994 (0.3063)	loss 1.0958 (1.3095)	grad_norm 27.6560 (inf)	mem 4879MB
[2022-05-31 09:31:46 MetaFG_0] (main.py 265): INFO Train: [79/300][620/1562]	eta 0:04:48 lr 0.000005	time 0.2951 (0.3062)	loss 1.5712 (1.3110)	grad_norm 21.5806 (inf)	mem 4879MB
[2022-05-31 09:31:49 MetaFG_0] (main.py 265): INFO Train: [79/300][630/1562]	eta 0:04:45 lr 0.000005	time 0.2943 (0.3062)	loss 1.1696 (1.3099)	grad_norm 40.7877 (inf)	mem 4879MB
[2022-05-31 09:31:52 MetaFG_0] (main.py 265): INFO Train: [79/300][640/1562]	eta 0:04:42 lr 0.000005	time 0.2923 (0.3062)	loss 1.5029 (1.3118)	grad_norm 23.5245 (inf)	mem 4879MB
[2022-05-31 09:31:55 MetaFG_0] (main.py 265): INFO Train: [79/300][650/1562]	eta 0:04:39 lr 0.000005	time 0.3039 (0.3062)	loss 1.1948 (1.3102)	grad_norm 35.4871 (inf)	mem 4879MB
[2022-05-31 09:31:58 MetaFG_0] (main.py 265): INFO Train: [79/300][660/1562]	eta 0:04:36 lr 0.000005	time 0.2997 (0.3062)	loss 1.2814 (1.3099)	grad_norm 21.7840 (inf)	mem 4879MB
[2022-05-31 09:32:01 MetaFG_0] (main.py 265): INFO Train: [79/300][670/1562]	eta 0:04:33 lr 0.000005	time 0.2989 (0.3061)	loss 1.1185 (1.3085)	grad_norm 34.4157 (inf)	mem 4879MB
[2022-05-31 09:32:04 MetaFG_0] (main.py 265): INFO Train: [79/300][680/1562]	eta 0:04:29 lr 0.000005	time 0.2948 (0.3061)	loss 1.5165 (1.3074)	grad_norm 34.1487 (inf)	mem 4879MB
[2022-05-31 09:32:07 MetaFG_0] (main.py 265): INFO Train: [79/300][690/1562]	eta 0:04:26 lr 0.000005	time 0.2956 (0.3061)	loss 1.4698 (1.3093)	grad_norm 28.1774 (inf)	mem 4879MB
[2022-05-31 09:32:10 MetaFG_0] (main.py 265): INFO Train: [79/300][700/1562]	eta 0:04:23 lr 0.000005	time 0.3016 (0.3061)	loss 1.2116 (1.3103)	grad_norm 36.0669 (inf)	mem 4879MB
[2022-05-31 09:32:13 MetaFG_0] (main.py 265): INFO Train: [79/300][710/1562]	eta 0:04:20 lr 0.000005	time 0.2943 (0.3061)	loss 1.5612 (1.3114)	grad_norm 22.8847 (inf)	mem 4879MB
[2022-05-31 09:32:16 MetaFG_0] (main.py 265): INFO Train: [79/300][720/1562]	eta 0:04:17 lr 0.000005	time 0.2931 (0.3061)	loss 1.1255 (1.3116)	grad_norm 25.7344 (inf)	mem 4879MB
[2022-05-31 09:32:19 MetaFG_0] (main.py 265): INFO Train: [79/300][730/1562]	eta 0:04:14 lr 0.000005	time 0.2997 (0.3061)	loss 1.1576 (1.3109)	grad_norm 31.0285 (inf)	mem 4879MB
[2022-05-31 09:32:22 MetaFG_0] (main.py 265): INFO Train: [79/300][740/1562]	eta 0:04:11 lr 0.000005	time 0.2938 (0.3061)	loss 0.8758 (1.3093)	grad_norm 35.9636 (inf)	mem 4879MB
[2022-05-31 09:32:25 MetaFG_0] (main.py 265): INFO Train: [79/300][750/1562]	eta 0:04:08 lr 0.000005	time 0.2939 (0.3060)	loss 1.2937 (1.3084)	grad_norm 35.6772 (inf)	mem 4879MB
[2022-05-31 09:32:29 MetaFG_0] (main.py 265): INFO Train: [79/300][760/1562]	eta 0:04:05 lr 0.000005	time 0.2942 (0.3060)	loss 1.4706 (1.3092)	grad_norm 15.6651 (inf)	mem 4879MB
[2022-05-31 09:32:32 MetaFG_0] (main.py 265): INFO Train: [79/300][770/1562]	eta 0:04:02 lr 0.000005	time 0.2989 (0.3060)	loss 1.1391 (1.3093)	grad_norm 30.3895 (inf)	mem 4879MB
[2022-05-31 09:32:35 MetaFG_0] (main.py 265): INFO Train: [79/300][780/1562]	eta 0:03:59 lr 0.000005	time 0.2995 (0.3060)	loss 1.0620 (1.3093)	grad_norm 40.5522 (inf)	mem 4879MB
[2022-05-31 09:32:38 MetaFG_0] (main.py 265): INFO Train: [79/300][790/1562]	eta 0:03:56 lr 0.000005	time 0.2985 (0.3060)	loss 0.9768 (1.3099)	grad_norm 24.6244 (inf)	mem 4879MB
[2022-05-31 09:32:41 MetaFG_0] (main.py 265): INFO Train: [79/300][800/1562]	eta 0:03:53 lr 0.000005	time 0.3047 (0.3062)	loss 1.3889 (1.3100)	grad_norm 33.2652 (inf)	mem 4879MB
[2022-05-31 09:32:44 MetaFG_0] (main.py 265): INFO Train: [79/300][810/1562]	eta 0:03:50 lr 0.000005	time 0.2935 (0.3062)	loss 1.2423 (1.3082)	grad_norm 26.1706 (inf)	mem 4879MB
[2022-05-31 09:32:47 MetaFG_0] (main.py 265): INFO Train: [79/300][820/1562]	eta 0:03:47 lr 0.000005	time 0.3002 (0.3062)	loss 1.4911 (1.3095)	grad_norm 27.7281 (inf)	mem 4879MB
[2022-05-31 09:32:50 MetaFG_0] (main.py 265): INFO Train: [79/300][830/1562]	eta 0:03:44 lr 0.000005	time 0.2947 (0.3062)	loss 1.2489 (1.3108)	grad_norm 25.1251 (inf)	mem 4879MB
[2022-05-31 09:32:53 MetaFG_0] (main.py 265): INFO Train: [79/300][840/1562]	eta 0:03:41 lr 0.000005	time 0.2943 (0.3061)	loss 1.0915 (1.3113)	grad_norm 20.5431 (inf)	mem 4879MB
[2022-05-31 09:32:56 MetaFG_0] (main.py 265): INFO Train: [79/300][850/1562]	eta 0:03:37 lr 0.000005	time 0.3016 (0.3061)	loss 1.0639 (1.3127)	grad_norm 30.0271 (inf)	mem 4879MB
[2022-05-31 09:32:59 MetaFG_0] (main.py 265): INFO Train: [79/300][860/1562]	eta 0:03:34 lr 0.000005	time 0.3011 (0.3061)	loss 1.2296 (1.3126)	grad_norm 36.9727 (inf)	mem 4879MB
[2022-05-31 09:33:02 MetaFG_0] (main.py 265): INFO Train: [79/300][870/1562]	eta 0:03:31 lr 0.000005	time 0.2986 (0.3061)	loss 1.3630 (1.3121)	grad_norm 26.6862 (inf)	mem 4879MB
[2022-05-31 09:33:05 MetaFG_0] (main.py 265): INFO Train: [79/300][880/1562]	eta 0:03:28 lr 0.000005	time 0.2947 (0.3061)	loss 1.2205 (1.3125)	grad_norm 24.0690 (inf)	mem 4879MB
[2022-05-31 09:33:08 MetaFG_0] (main.py 265): INFO Train: [79/300][890/1562]	eta 0:03:25 lr 0.000005	time 0.2997 (0.3061)	loss 1.5078 (1.3119)	grad_norm 37.5075 (inf)	mem 4879MB
[2022-05-31 09:33:11 MetaFG_0] (main.py 265): INFO Train: [79/300][900/1562]	eta 0:03:22 lr 0.000005	time 0.2945 (0.3061)	loss 1.3102 (1.3125)	grad_norm 32.1223 (inf)	mem 4879MB
[2022-05-31 09:33:15 MetaFG_0] (main.py 265): INFO Train: [79/300][910/1562]	eta 0:03:19 lr 0.000005	time 0.2979 (0.3061)	loss 0.8909 (1.3127)	grad_norm 38.1256 (inf)	mem 4879MB
[2022-05-31 09:33:18 MetaFG_0] (main.py 265): INFO Train: [79/300][920/1562]	eta 0:03:16 lr 0.000005	time 0.2953 (0.3061)	loss 1.4417 (1.3123)	grad_norm 16.6675 (inf)	mem 4879MB
[2022-05-31 09:33:21 MetaFG_0] (main.py 265): INFO Train: [79/300][930/1562]	eta 0:03:13 lr 0.000005	time 0.2931 (0.3061)	loss 1.0024 (1.3110)	grad_norm 33.4671 (inf)	mem 4879MB
[2022-05-31 09:33:24 MetaFG_0] (main.py 265): INFO Train: [79/300][940/1562]	eta 0:03:10 lr 0.000005	time 0.2934 (0.3060)	loss 1.4709 (1.3121)	grad_norm 26.0299 (inf)	mem 4879MB
[2022-05-31 09:33:27 MetaFG_0] (main.py 265): INFO Train: [79/300][950/1562]	eta 0:03:07 lr 0.000005	time 0.2992 (0.3060)	loss 1.5268 (1.3118)	grad_norm 25.0168 (inf)	mem 4879MB
[2022-05-31 09:33:30 MetaFG_0] (main.py 265): INFO Train: [79/300][960/1562]	eta 0:03:04 lr 0.000005	time 0.2932 (0.3060)	loss 1.2788 (1.3110)	grad_norm 22.7007 (inf)	mem 4879MB
[2022-05-31 09:33:33 MetaFG_0] (main.py 265): INFO Train: [79/300][970/1562]	eta 0:03:01 lr 0.000005	time 0.2926 (0.3060)	loss 1.0070 (1.3111)	grad_norm 41.8430 (inf)	mem 4879MB
[2022-05-31 09:33:36 MetaFG_0] (main.py 265): INFO Train: [79/300][980/1562]	eta 0:02:58 lr 0.000005	time 0.2958 (0.3060)	loss 1.0574 (1.3092)	grad_norm 29.4162 (inf)	mem 4879MB
[2022-05-31 09:33:39 MetaFG_0] (main.py 265): INFO Train: [79/300][990/1562]	eta 0:02:55 lr 0.000005	time 0.2932 (0.3060)	loss 1.5625 (1.3082)	grad_norm 26.6770 (inf)	mem 4879MB
[2022-05-31 09:33:42 MetaFG_0] (main.py 265): INFO Train: [79/300][1000/1562]	eta 0:02:51 lr 0.000005	time 0.2946 (0.3060)	loss 1.2424 (1.3087)	grad_norm 36.1256 (inf)	mem 4879MB
[2022-05-31 09:33:45 MetaFG_0] (main.py 265): INFO Train: [79/300][1010/1562]	eta 0:02:48 lr 0.000005	time 0.2935 (0.3060)	loss 0.7133 (1.3081)	grad_norm 36.8200 (inf)	mem 4879MB
[2022-05-31 09:33:48 MetaFG_0] (main.py 265): INFO Train: [79/300][1020/1562]	eta 0:02:45 lr 0.000005	time 0.2948 (0.3059)	loss 1.4697 (1.3088)	grad_norm 31.3407 (inf)	mem 4879MB
[2022-05-31 09:33:51 MetaFG_0] (main.py 265): INFO Train: [79/300][1030/1562]	eta 0:02:42 lr 0.000005	time 0.3025 (0.3059)	loss 1.6653 (1.3091)	grad_norm 33.6064 (inf)	mem 4879MB
[2022-05-31 09:33:54 MetaFG_0] (main.py 265): INFO Train: [79/300][1040/1562]	eta 0:02:39 lr 0.000005	time 0.2928 (0.3059)	loss 1.1468 (1.3086)	grad_norm 16.2381 (inf)	mem 4879MB
[2022-05-31 09:33:57 MetaFG_0] (main.py 265): INFO Train: [79/300][1050/1562]	eta 0:02:36 lr 0.000005	time 0.3012 (0.3059)	loss 0.8969 (1.3086)	grad_norm 38.1566 (inf)	mem 4879MB
[2022-05-31 09:34:00 MetaFG_0] (main.py 265): INFO Train: [79/300][1060/1562]	eta 0:02:33 lr 0.000005	time 0.2994 (0.3059)	loss 1.4130 (1.3089)	grad_norm 23.8907 (inf)	mem 4879MB
[2022-05-31 09:34:03 MetaFG_0] (main.py 265): INFO Train: [79/300][1070/1562]	eta 0:02:30 lr 0.000005	time 0.2937 (0.3059)	loss 0.9230 (1.3077)	grad_norm 33.8527 (inf)	mem 4879MB
[2022-05-31 09:34:06 MetaFG_0] (main.py 265): INFO Train: [79/300][1080/1562]	eta 0:02:27 lr 0.000005	time 0.2991 (0.3059)	loss 0.9805 (1.3071)	grad_norm 41.9214 (inf)	mem 4879MB
[2022-05-31 09:34:09 MetaFG_0] (main.py 265): INFO Train: [79/300][1090/1562]	eta 0:02:24 lr 0.000005	time 0.3021 (0.3059)	loss 1.5454 (1.3077)	grad_norm 18.9231 (inf)	mem 4879MB
[2022-05-31 09:34:12 MetaFG_0] (main.py 265): INFO Train: [79/300][1100/1562]	eta 0:02:21 lr 0.000005	time 0.2927 (0.3059)	loss 1.0598 (1.3085)	grad_norm 18.0738 (inf)	mem 4879MB
[2022-05-31 09:34:16 MetaFG_0] (main.py 265): INFO Train: [79/300][1110/1562]	eta 0:02:18 lr 0.000005	time 0.3011 (0.3059)	loss 1.4905 (1.3087)	grad_norm 20.2720 (inf)	mem 4879MB
[2022-05-31 09:34:19 MetaFG_0] (main.py 265): INFO Train: [79/300][1120/1562]	eta 0:02:15 lr 0.000005	time 0.2931 (0.3059)	loss 1.4045 (1.3085)	grad_norm 39.5516 (inf)	mem 4879MB
[2022-05-31 09:34:22 MetaFG_0] (main.py 265): INFO Train: [79/300][1130/1562]	eta 0:02:12 lr 0.000005	time 0.3004 (0.3059)	loss 1.5034 (1.3076)	grad_norm 29.4287 (inf)	mem 4879MB
[2022-05-31 09:34:25 MetaFG_0] (main.py 265): INFO Train: [79/300][1140/1562]	eta 0:02:09 lr 0.000005	time 0.2987 (0.3059)	loss 1.4862 (1.3074)	grad_norm 42.9699 (inf)	mem 4879MB
[2022-05-31 09:34:28 MetaFG_0] (main.py 265): INFO Train: [79/300][1150/1562]	eta 0:02:06 lr 0.000005	time 0.2928 (0.3059)	loss 1.5607 (1.3084)	grad_norm 22.7872 (inf)	mem 4879MB
[2022-05-31 09:34:31 MetaFG_0] (main.py 265): INFO Train: [79/300][1160/1562]	eta 0:02:02 lr 0.000005	time 0.2946 (0.3059)	loss 1.6728 (1.3077)	grad_norm 28.2991 (inf)	mem 4879MB
[2022-05-31 09:34:34 MetaFG_0] (main.py 265): INFO Train: [79/300][1170/1562]	eta 0:01:59 lr 0.000005	time 0.3011 (0.3059)	loss 0.9381 (1.3073)	grad_norm 31.9981 (inf)	mem 4879MB
[2022-05-31 09:34:37 MetaFG_0] (main.py 265): INFO Train: [79/300][1180/1562]	eta 0:01:56 lr 0.000005	time 0.2940 (0.3059)	loss 0.8533 (1.3069)	grad_norm 48.6691 (inf)	mem 4879MB
[2022-05-31 09:34:40 MetaFG_0] (main.py 265): INFO Train: [79/300][1190/1562]	eta 0:01:53 lr 0.000005	time 0.2935 (0.3059)	loss 1.2990 (1.3081)	grad_norm 20.7940 (inf)	mem 4879MB
[2022-05-31 09:34:43 MetaFG_0] (main.py 265): INFO Train: [79/300][1200/1562]	eta 0:01:50 lr 0.000005	time 0.2933 (0.3059)	loss 1.7451 (1.3094)	grad_norm 112.6850 (inf)	mem 4879MB
[2022-05-31 09:34:46 MetaFG_0] (main.py 265): INFO Train: [79/300][1210/1562]	eta 0:01:47 lr 0.000005	time 0.2989 (0.3058)	loss 1.1990 (1.3091)	grad_norm 32.2804 (inf)	mem 4879MB
[2022-05-31 09:34:49 MetaFG_0] (main.py 265): INFO Train: [79/300][1220/1562]	eta 0:01:44 lr 0.000005	time 0.2988 (0.3058)	loss 0.6698 (1.3079)	grad_norm 78.7703 (inf)	mem 4879MB
[2022-05-31 09:34:52 MetaFG_0] (main.py 265): INFO Train: [79/300][1230/1562]	eta 0:01:41 lr 0.000005	time 0.2926 (0.3058)	loss 1.0682 (1.3081)	grad_norm 29.4177 (inf)	mem 4879MB
[2022-05-31 09:34:55 MetaFG_0] (main.py 265): INFO Train: [79/300][1240/1562]	eta 0:01:38 lr 0.000005	time 0.2937 (0.3058)	loss 1.5590 (1.3093)	grad_norm 38.9529 (inf)	mem 4879MB
[2022-05-31 09:34:58 MetaFG_0] (main.py 265): INFO Train: [79/300][1250/1562]	eta 0:01:35 lr 0.000005	time 0.2928 (0.3058)	loss 1.2118 (1.3094)	grad_norm 44.5118 (inf)	mem 4879MB
[2022-05-31 09:35:01 MetaFG_0] (main.py 265): INFO Train: [79/300][1260/1562]	eta 0:01:32 lr 0.000005	time 0.2931 (0.3058)	loss 1.3410 (1.3104)	grad_norm 28.0946 (inf)	mem 4879MB
[2022-05-31 09:35:04 MetaFG_0] (main.py 265): INFO Train: [79/300][1270/1562]	eta 0:01:29 lr 0.000005	time 0.2922 (0.3058)	loss 1.1908 (1.3099)	grad_norm 64.2863 (inf)	mem 4879MB
[2022-05-31 09:35:07 MetaFG_0] (main.py 265): INFO Train: [79/300][1280/1562]	eta 0:01:26 lr 0.000005	time 0.3006 (0.3058)	loss 1.5693 (1.3108)	grad_norm 39.5861 (inf)	mem 4879MB
[2022-05-31 09:35:10 MetaFG_0] (main.py 265): INFO Train: [79/300][1290/1562]	eta 0:01:23 lr 0.000005	time 0.2937 (0.3058)	loss 1.1469 (1.3103)	grad_norm 33.2169 (inf)	mem 4879MB
[2022-05-31 09:35:13 MetaFG_0] (main.py 265): INFO Train: [79/300][1300/1562]	eta 0:01:20 lr 0.000005	time 0.3008 (0.3058)	loss 1.4462 (1.3103)	grad_norm 42.8452 (inf)	mem 4879MB
[2022-05-31 09:35:17 MetaFG_0] (main.py 265): INFO Train: [79/300][1310/1562]	eta 0:01:17 lr 0.000005	time 0.2998 (0.3058)	loss 1.4877 (1.3109)	grad_norm 26.0092 (inf)	mem 4879MB
[2022-05-31 09:35:20 MetaFG_0] (main.py 265): INFO Train: [79/300][1320/1562]	eta 0:01:14 lr 0.000005	time 0.2943 (0.3058)	loss 1.2632 (1.3110)	grad_norm 30.8501 (inf)	mem 4879MB
[2022-05-31 09:35:23 MetaFG_0] (main.py 265): INFO Train: [79/300][1330/1562]	eta 0:01:10 lr 0.000005	time 0.3042 (0.3058)	loss 1.0647 (1.3108)	grad_norm 56.4116 (inf)	mem 4879MB
[2022-05-31 09:35:26 MetaFG_0] (main.py 265): INFO Train: [79/300][1340/1562]	eta 0:01:07 lr 0.000005	time 0.2954 (0.3058)	loss 1.5445 (1.3106)	grad_norm 26.3854 (inf)	mem 4879MB
[2022-05-31 09:35:29 MetaFG_0] (main.py 265): INFO Train: [79/300][1350/1562]	eta 0:01:04 lr 0.000005	time 0.2931 (0.3058)	loss 1.5358 (1.3112)	grad_norm 28.5283 (inf)	mem 4879MB
[2022-05-31 09:35:32 MetaFG_0] (main.py 265): INFO Train: [79/300][1360/1562]	eta 0:01:01 lr 0.000005	time 0.2992 (0.3058)	loss 1.5212 (1.3109)	grad_norm 28.7147 (inf)	mem 4879MB
[2022-05-31 09:35:35 MetaFG_0] (main.py 265): INFO Train: [79/300][1370/1562]	eta 0:00:58 lr 0.000005	time 0.2986 (0.3058)	loss 1.2309 (1.3109)	grad_norm 24.9220 (inf)	mem 4879MB
[2022-05-31 09:35:38 MetaFG_0] (main.py 265): INFO Train: [79/300][1380/1562]	eta 0:00:55 lr 0.000005	time 0.3005 (0.3058)	loss 1.2618 (1.3105)	grad_norm 28.3621 (inf)	mem 4879MB
[2022-05-31 09:35:41 MetaFG_0] (main.py 265): INFO Train: [79/300][1390/1562]	eta 0:00:52 lr 0.000005	time 0.2965 (0.3058)	loss 1.3050 (1.3107)	grad_norm 32.7523 (inf)	mem 4879MB
[2022-05-31 09:35:44 MetaFG_0] (main.py 265): INFO Train: [79/300][1400/1562]	eta 0:00:49 lr 0.000005	time 0.2993 (0.3058)	loss 1.6826 (1.3114)	grad_norm 41.8361 (inf)	mem 4879MB
[2022-05-31 09:35:47 MetaFG_0] (main.py 265): INFO Train: [79/300][1410/1562]	eta 0:00:46 lr 0.000005	time 0.2990 (0.3057)	loss 1.1900 (1.3110)	grad_norm 35.2848 (inf)	mem 4879MB
[2022-05-31 09:35:50 MetaFG_0] (main.py 265): INFO Train: [79/300][1420/1562]	eta 0:00:43 lr 0.000005	time 0.2952 (0.3057)	loss 1.5903 (1.3117)	grad_norm 19.6948 (inf)	mem 4879MB
[2022-05-31 09:35:53 MetaFG_0] (main.py 265): INFO Train: [79/300][1430/1562]	eta 0:00:40 lr 0.000005	time 0.2950 (0.3057)	loss 1.3972 (1.3114)	grad_norm 32.0883 (inf)	mem 4879MB
[2022-05-31 09:35:56 MetaFG_0] (main.py 265): INFO Train: [79/300][1440/1562]	eta 0:00:37 lr 0.000005	time 0.2937 (0.3057)	loss 1.4964 (1.3121)	grad_norm 21.8994 (inf)	mem 4879MB
[2022-05-31 09:35:59 MetaFG_0] (main.py 265): INFO Train: [79/300][1450/1562]	eta 0:00:34 lr 0.000005	time 0.2997 (0.3057)	loss 1.3028 (1.3133)	grad_norm 21.5332 (inf)	mem 4879MB
[2022-05-31 09:36:02 MetaFG_0] (main.py 265): INFO Train: [79/300][1460/1562]	eta 0:00:31 lr 0.000005	time 0.2956 (0.3057)	loss 1.2629 (1.3132)	grad_norm 17.7294 (inf)	mem 4879MB
[2022-05-31 09:36:05 MetaFG_0] (main.py 265): INFO Train: [79/300][1470/1562]	eta 0:00:28 lr 0.000005	time 0.2959 (0.3057)	loss 1.4288 (1.3135)	grad_norm 45.6873 (inf)	mem 4879MB
[2022-05-31 09:36:08 MetaFG_0] (main.py 265): INFO Train: [79/300][1480/1562]	eta 0:00:25 lr 0.000005	time 0.2974 (0.3057)	loss 1.5033 (1.3139)	grad_norm 21.0016 (inf)	mem 4879MB
[2022-05-31 09:36:11 MetaFG_0] (main.py 265): INFO Train: [79/300][1490/1562]	eta 0:00:22 lr 0.000005	time 0.2944 (0.3057)	loss 1.2823 (1.3140)	grad_norm 29.6635 (inf)	mem 4879MB
[2022-05-31 09:36:14 MetaFG_0] (main.py 265): INFO Train: [79/300][1500/1562]	eta 0:00:18 lr 0.000005	time 0.2936 (0.3057)	loss 1.3899 (1.3133)	grad_norm 19.6362 (inf)	mem 4879MB
[2022-05-31 09:36:18 MetaFG_0] (main.py 265): INFO Train: [79/300][1510/1562]	eta 0:00:15 lr 0.000005	time 0.2927 (0.3057)	loss 1.4246 (1.3131)	grad_norm 16.1464 (inf)	mem 4879MB
[2022-05-31 09:36:21 MetaFG_0] (main.py 265): INFO Train: [79/300][1520/1562]	eta 0:00:12 lr 0.000005	time 0.2943 (0.3057)	loss 1.2454 (1.3131)	grad_norm 26.0371 (inf)	mem 4879MB
[2022-05-31 09:36:24 MetaFG_0] (main.py 265): INFO Train: [79/300][1530/1562]	eta 0:00:09 lr 0.000005	time 0.2921 (0.3057)	loss 1.2105 (1.3133)	grad_norm 37.1351 (inf)	mem 4879MB
[2022-05-31 09:36:27 MetaFG_0] (main.py 265): INFO Train: [79/300][1540/1562]	eta 0:00:06 lr 0.000005	time 0.2941 (0.3057)	loss 1.2220 (1.3140)	grad_norm 33.2400 (inf)	mem 4879MB
[2022-05-31 09:36:30 MetaFG_0] (main.py 265): INFO Train: [79/300][1550/1562]	eta 0:00:03 lr 0.000005	time 0.3012 (0.3057)	loss 0.7119 (1.3138)	grad_norm 13.8663 (inf)	mem 4879MB
[2022-05-31 09:36:33 MetaFG_0] (main.py 265): INFO Train: [79/300][1560/1562]	eta 0:00:00 lr 0.000005	time 0.2922 (0.3057)	loss 1.5875 (1.3133)	grad_norm 33.2778 (inf)	mem 4879MB
[2022-05-31 09:36:33 MetaFG_0] (main.py 272): INFO EPOCH 79 training takes 0:07:57
[2022-05-31 09:36:33 MetaFG_0] (utils.py 121): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_79.pth saving......
[2022-05-31 09:36:34 MetaFG_0] (utils.py 123): INFO output/MetaFG_0/cub-200_v1/ckpt_epoch_79.pth saved !!!
[2022-05-31 09:36:34 MetaFG_0] (utils.py 127): INFO output/MetaFG_0/cub-200_v1/latest.pth saving......
[2022-05-31 09:36:36 MetaFG_0] (utils.py 129): INFO output/MetaFG_0/cub-200_v1/latest.pth saved !!!
[2022-05-31 09:36:36 MetaFG_0] (main.py 167): INFO **********normal test***********
[2022-05-31 09:36:36 MetaFG_0] (main.py 324): INFO Test: [0/313]	Time 0.669 (0.669)	Loss 0.3157 (0.3157)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)	Mem 4879MB
[2022-05-31 09:36:37 MetaFG_0] (main.py 324): INFO Test: [10/313]	Time 0.091 (0.149)	Loss 0.6747 (0.5376)	Acc@1 84.375 (90.625)	Acc@5 96.875 (98.580)	Mem 4879MB
[2022-05-31 09:36:38 MetaFG_0] (main.py 324): INFO Test: [20/313]	Time 0.094 (0.124)	Loss 0.2888 (0.5204)	Acc@1 96.875 (90.179)	Acc@5 100.000 (99.256)	Mem 4879MB
[2022-05-31 09:36:39 MetaFG_0] (main.py 324): INFO Test: [30/313]	Time 0.102 (0.115)	Loss 0.6452 (0.5436)	Acc@1 87.500 (89.214)	Acc@5 100.000 (99.093)	Mem 4879MB
[2022-05-31 09:36:40 MetaFG_0] (main.py 324): INFO Test: [40/313]	Time 0.096 (0.111)	Loss 0.5947 (0.5513)	Acc@1 84.375 (88.415)	Acc@5 100.000 (99.238)	Mem 4879MB
[2022-05-31 09:36:41 MetaFG_0] (main.py 324): INFO Test: [50/313]	Time 0.097 (0.107)	Loss 0.7267 (0.5750)	Acc@1 87.500 (87.684)	Acc@5 93.750 (99.020)	Mem 4879MB
[2022-05-31 09:36:42 MetaFG_0] (main.py 324): INFO Test: [60/313]	Time 0.093 (0.105)	Loss 0.7048 (0.5792)	Acc@1 81.250 (87.654)	Acc@5 100.000 (99.027)	Mem 4879MB
[2022-05-31 09:36:43 MetaFG_0] (main.py 324): INFO Test: [70/313]	Time 0.097 (0.104)	Loss 0.6918 (0.5803)	Acc@1 84.375 (87.852)	Acc@5 100.000 (98.988)	Mem 4879MB
[2022-05-31 09:36:44 MetaFG_0] (main.py 324): INFO Test: [80/313]	Time 0.104 (0.103)	Loss 0.5989 (0.5784)	Acc@1 87.500 (87.924)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 09:36:45 MetaFG_0] (main.py 324): INFO Test: [90/313]	Time 0.096 (0.102)	Loss 0.6007 (0.5757)	Acc@1 87.500 (87.946)	Acc@5 100.000 (98.970)	Mem 4879MB
[2022-05-31 09:36:46 MetaFG_0] (main.py 324): INFO Test: [100/313]	Time 0.099 (0.101)	Loss 0.3990 (0.5764)	Acc@1 93.750 (87.871)	Acc@5 100.000 (98.917)	Mem 4879MB
[2022-05-31 09:36:47 MetaFG_0] (main.py 324): INFO Test: [110/313]	Time 0.088 (0.100)	Loss 0.6648 (0.5755)	Acc@1 84.375 (87.922)	Acc@5 100.000 (98.958)	Mem 4879MB
[2022-05-31 09:36:48 MetaFG_0] (main.py 324): INFO Test: [120/313]	Time 0.094 (0.100)	Loss 0.4707 (0.5740)	Acc@1 96.875 (88.068)	Acc@5 100.000 (98.915)	Mem 4879MB
[2022-05-31 09:36:49 MetaFG_0] (main.py 324): INFO Test: [130/313]	Time 0.094 (0.099)	Loss 0.4709 (0.5777)	Acc@1 93.750 (87.786)	Acc@5 100.000 (98.927)	Mem 4879MB
[2022-05-31 09:36:49 MetaFG_0] (main.py 324): INFO Test: [140/313]	Time 0.099 (0.099)	Loss 0.5942 (0.5780)	Acc@1 87.500 (87.744)	Acc@5 100.000 (98.914)	Mem 4879MB
[2022-05-31 09:36:50 MetaFG_0] (main.py 324): INFO Test: [150/313]	Time 0.093 (0.099)	Loss 0.8552 (0.5814)	Acc@1 75.000 (87.583)	Acc@5 96.875 (98.882)	Mem 4879MB
[2022-05-31 09:36:51 MetaFG_0] (main.py 324): INFO Test: [160/313]	Time 0.097 (0.098)	Loss 0.6949 (0.5814)	Acc@1 84.375 (87.636)	Acc@5 100.000 (98.894)	Mem 4879MB
[2022-05-31 09:36:52 MetaFG_0] (main.py 324): INFO Test: [170/313]	Time 0.094 (0.098)	Loss 0.6819 (0.5826)	Acc@1 87.500 (87.500)	Acc@5 96.875 (98.904)	Mem 4879MB
[2022-05-31 09:36:53 MetaFG_0] (main.py 324): INFO Test: [180/313]	Time 0.091 (0.098)	Loss 0.4691 (0.5790)	Acc@1 93.750 (87.707)	Acc@5 96.875 (98.912)	Mem 4879MB
[2022-05-31 09:36:54 MetaFG_0] (main.py 324): INFO Test: [190/313]	Time 0.094 (0.098)	Loss 0.4556 (0.5756)	Acc@1 90.625 (87.795)	Acc@5 100.000 (98.953)	Mem 4879MB
[2022-05-31 09:36:55 MetaFG_0] (main.py 324): INFO Test: [200/313]	Time 0.087 (0.097)	Loss 0.7521 (0.5773)	Acc@1 87.500 (87.702)	Acc@5 96.875 (98.927)	Mem 4879MB
[2022-05-31 09:36:56 MetaFG_0] (main.py 324): INFO Test: [210/313]	Time 0.087 (0.097)	Loss 1.0099 (0.5786)	Acc@1 75.000 (87.693)	Acc@5 90.625 (98.874)	Mem 4879MB
[2022-05-31 09:36:57 MetaFG_0] (main.py 324): INFO Test: [220/313]	Time 0.098 (0.097)	Loss 0.8048 (0.5797)	Acc@1 81.250 (87.726)	Acc@5 100.000 (98.840)	Mem 4879MB
[2022-05-31 09:36:58 MetaFG_0] (main.py 324): INFO Test: [230/313]	Time 0.099 (0.097)	Loss 0.6582 (0.5791)	Acc@1 90.625 (87.730)	Acc@5 100.000 (98.891)	Mem 4879MB
[2022-05-31 09:36:59 MetaFG_0] (main.py 324): INFO Test: [240/313]	Time 0.096 (0.097)	Loss 0.7518 (0.5809)	Acc@1 81.250 (87.656)	Acc@5 96.875 (98.885)	Mem 4879MB
[2022-05-31 09:37:00 MetaFG_0] (main.py 324): INFO Test: [250/313]	Time 0.093 (0.097)	Loss 0.7358 (0.5785)	Acc@1 81.250 (87.786)	Acc@5 96.875 (98.892)	Mem 4879MB
[2022-05-31 09:37:01 MetaFG_0] (main.py 324): INFO Test: [260/313]	Time 0.097 (0.097)	Loss 0.5005 (0.5779)	Acc@1 84.375 (87.763)	Acc@5 100.000 (98.886)	Mem 4879MB
[2022-05-31 09:37:02 MetaFG_0] (main.py 324): INFO Test: [270/313]	Time 0.097 (0.097)	Loss 0.5163 (0.5790)	Acc@1 87.500 (87.742)	Acc@5 100.000 (98.881)	Mem 4879MB
[2022-05-31 09:37:03 MetaFG_0] (main.py 324): INFO Test: [280/313]	Time 0.098 (0.097)	Loss 0.4493 (0.5793)	Acc@1 93.750 (87.778)	Acc@5 96.875 (98.832)	Mem 4879MB
[2022-05-31 09:37:04 MetaFG_0] (main.py 324): INFO Test: [290/313]	Time 0.094 (0.097)	Loss 1.0559 (0.5800)	Acc@1 65.625 (87.715)	Acc@5 93.750 (98.840)	Mem 4879MB
[2022-05-31 09:37:05 MetaFG_0] (main.py 324): INFO Test: [300/313]	Time 0.097 (0.097)	Loss 0.5697 (0.5816)	Acc@1 93.750 (87.656)	Acc@5 100.000 (98.837)	Mem 4879MB
[2022-05-31 09:37:06 MetaFG_0] (main.py 324): INFO Test: [310/313]	Time 0.087 (0.097)	Loss 0.5985 (0.5798)	Acc@1 87.500 (87.701)	Acc@5 100.000 (98.875)	Mem 4879MB
[2022-05-31 09:37:06 MetaFG_0] (main.py 330): INFO  * Acc@1 87.680 Acc@5 98.870
[2022-05-31 09:37:06 MetaFG_0] (main.py 169): INFO Accuracy of the network on the 10000 test images: 87.7%
[2022-05-31 09:37:06 MetaFG_0] (main.py 171): INFO Max accuracy: 87.68%
[2022-05-31 09:37:07 MetaFG_0] (main.py 265): INFO Train: [80/300][0/1562]	eta 0:27:17 lr 0.000005	time 1.0483 (1.0483)	loss 1.3720 (1.3720)	grad_norm 34.1914 (34.1914)	mem 4879MB
[2022-05-31 09:37:10 MetaFG_0] (main.py 265): INFO Train: [80/300][10/1562]	eta 0:09:45 lr 0.000005	time 0.2926 (0.3771)	loss 1.7402 (1.3648)	grad_norm 22.6827 (32.2069)	mem 4879MB
[2022-05-31 09:37:13 MetaFG_0] (main.py 265): INFO Train: [80/300][20/1562]	eta 0:08:48 lr 0.000005	time 0.2932 (0.3425)	loss 1.3189 (1.3847)	grad_norm 14.1370 (31.6152)	mem 4879MB
[2022-05-31 09:37:16 MetaFG_0] (main.py 265): INFO Train: [80/300][30/1562]	eta 0:08:25 lr 0.000005	time 0.2945 (0.3300)	loss 1.4408 (1.3960)	grad_norm 29.1477 (29.9461)	mem 4879MB
[2022-05-31 09:37:19 MetaFG_0] (main.py 265): INFO Train: [80/300][40/1562]	eta 0:08:13 lr 0.000005	time 0.3001 (0.3242)	loss 1.5019 (1.3807)	grad_norm 42.7715 (31.4461)	mem 4879MB
[2022-05-31 09:37:22 MetaFG_0] (main.py 265): INFO Train: [80/300][50/1562]	eta 0:08:05 lr 0.000005	time 0.2961 (0.3209)	loss 1.1472 (1.3594)	grad_norm 27.5544 (33.6379)	mem 4879MB
[2022-05-31 09:37:25 MetaFG_0] (main.py 265): INFO Train: [80/300][60/1562]	eta 0:07:58 lr 0.000005	time 0.3046 (0.3184)	loss 1.6325 (1.3561)	grad_norm 27.0612 (34.1803)	mem 4879MB
[2022-05-31 09:37:28 MetaFG_0] (main.py 265): INFO Train: [80/300][70/1562]	eta 0:07:51 lr 0.000005	time 0.2995 (0.3163)	loss 1.6245 (1.3542)	grad_norm 20.1925 (33.4184)	mem 4879MB
[2022-05-31 09:37:31 MetaFG_0] (main.py 265): INFO Train: [80/300][80/1562]	eta 0:07:46 lr 0.000005	time 0.2930 (0.3149)	loss 1.4353 (1.3496)	grad_norm 24.4695 (33.2981)	mem 4879MB
[2022-05-31 09:37:34 MetaFG_0] (main.py 265): INFO Train: [80/300][90/1562]	eta 0:07:41 lr 0.000005	time 0.2990 (0.3138)	loss 1.4785 (1.3581)	grad_norm 16.8956 (32.9015)	mem 4879MB
[2022-05-31 09:37:37 MetaFG_0] (main.py 265): INFO Train: [80/300][100/1562]	eta 0:07:37 lr 0.000005	time 0.2987 (0.3131)	loss 1.1981 (1.3559)	grad_norm 19.0897 (32.1847)	mem 4879MB
[2022-05-31 09:37:41 MetaFG_0] (main.py 265): INFO Train: [80/300][110/1562]	eta 0:07:33 lr 0.000005	time 0.2930 (0.3124)	loss 1.4555 (1.3522)	grad_norm 15.3468 (31.8124)	mem 4879MB
[2022-05-31 09:37:44 MetaFG_0] (main.py 265): INFO Train: [80/300][120/1562]	eta 0:07:29 lr 0.000005	time 0.2943 (0.3116)	loss 1.3887 (1.3545)	grad_norm 23.0121 (31.5574)	mem 4879MB
[2022-05-31 09:37:47 MetaFG_0] (main.py 265): INFO Train: [80/300][130/1562]	eta 0:07:25 lr 0.000005	time 0.2925 (0.3112)	loss 1.3483 (1.3480)	grad_norm 26.1180 (31.1428)	mem 4879MB
[2022-05-31 09:37:50 MetaFG_0] (main.py 265): INFO Train: [80/300][140/1562]	eta 0:07:22 lr 0.000005	time 0.2987 (0.3110)	loss 0.8954 (1.3381)	grad_norm 33.2749 (31.0053)	mem 4879MB
[2022-05-31 09:37:53 MetaFG_0] (main.py 265): INFO Train: [80/300][150/1562]	eta 0:07:18 lr 0.000005	time 0.2947 (0.3105)	loss 1.5784 (1.3364)	grad_norm 32.5340 (30.8075)	mem 4879MB
[2022-05-31 09:37:56 MetaFG_0] (main.py 265): INFO Train: [80/300][160/1562]	eta 0:07:14 lr 0.000005	time 0.3000 (0.3102)	loss 1.3103 (1.3349)	grad_norm 20.7645 (31.0971)	mem 4879MB
[2022-05-31 09:37:59 MetaFG_0] (main.py 265): INFO Train: [80/300][170/1562]	eta 0:07:11 lr 0.000005	time 0.2935 (0.3099)	loss 1.5744 (1.3373)	grad_norm 36.8656 (31.2569)	mem 4879MB
[2022-05-31 09:38:02 MetaFG_0] (main.py 265): INFO Train: [80/300][180/1562]	eta 0:07:08 lr 0.000005	time 0.2952 (0.3097)	loss 1.3806 (1.3337)	grad_norm 28.0537 (31.0861)	mem 4879MB
[2022-05-31 09:38:05 MetaFG_0] (main.py 265): INFO Train: [80/300][190/1562]	eta 0:07:04 lr 0.000005	time 0.2939 (0.3095)	loss 1.4706 (1.3361)	grad_norm 49.4586 (31.0410)	mem 4879MB
[2022-05-31 09:38:08 MetaFG_0] (main.py 265): INFO Train: [80/300][200/1562]	eta 0:07:01 lr 0.000005	time 0.2932 (0.3092)	loss 1.3297 (1.3308)	grad_norm 18.3712 (30.9824)	mem 4879MB
[2022-05-31 09:38:11 MetaFG_0] (main.py 265): INFO Train: [80/300][210/1562]	eta 0:06:57 lr 0.000005	time 0.2942 (0.3090)	loss 1.1459 (1.3313)	grad_norm 43.4860 (30.9698)	mem 4879MB
