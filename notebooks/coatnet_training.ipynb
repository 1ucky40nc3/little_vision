{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coatnet_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "BPyAiHM1scWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUWOF4qt8jpl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dqshuai/MetaFormer.git\n",
        "%cd MetaFormer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm==0.4.5\n",
        "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install yacs\n",
        "!pip install -U PyYAML"
      ],
      "metadata": {
        "id": "EmYiDYzZAk7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "pHOxM_F68uGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the local training"
      ],
      "metadata": {
        "id": "Xz0J2Tc9sgLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/u/0/uc?id=1BYbe3mrKioN-Ara6hhJiaiEgJLl_thSH&export=download\n",
        "\n",
        "!mkdir pretrained_model\n",
        "!mv metafg_0_1k_224.pth ./pretrained_model/metafg_0_1k_224.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg6vIf5Q8zXR",
        "outputId": "2f4aca23-94eb-49ce-c1e0-02f8ff96bf5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BYbe3mrKioN-Ara6hhJiaiEgJLl_thSH\n",
            "To: /content/MetaFormer/metafg_0_1k_224.pth\n",
            "100% 117M/117M [00:02<00:00, 40.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Add the CIFAR-100 dataset\n",
        "\n",
        "%%writefile /content/MetaFormer/data/build.py\n",
        "# --------------------------------------------------------\n",
        "# Swin Transformer\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Ze Liu\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.distributed as dist\n",
        "from torchvision import datasets, transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.data import Mixup\n",
        "from timm.data import create_transform\n",
        "from timm.data.transforms import _pil_interp\n",
        "\n",
        "from .cached_image_folder import CachedImageFolder\n",
        "from .samplers import SubsetRandomSampler\n",
        "from .dataset_fg import DatasetMeta\n",
        "def build_loader(config):\n",
        "    config.defrost()\n",
        "    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(is_train=True, config=config)\n",
        "    config.freeze()\n",
        "    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build train dataset\")\n",
        "    dataset_val, _ = build_dataset(is_train=False, config=config)\n",
        "    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build val dataset\")\n",
        "\n",
        "    num_tasks = dist.get_world_size()\n",
        "    global_rank = dist.get_rank()\n",
        "    if config.DATA.ZIP_MODE and config.DATA.CACHE_MODE == 'part':\n",
        "        indices = np.arange(dist.get_rank(), len(dataset_train), dist.get_world_size())\n",
        "        sampler_train = SubsetRandomSampler(indices)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.DistributedSampler(\n",
        "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
        "        )\n",
        "\n",
        "    indices = np.arange(dist.get_rank(), len(dataset_val), dist.get_world_size())\n",
        "    sampler_val = SubsetRandomSampler(indices)\n",
        "\n",
        "    data_loader_train = torch.utils.data.DataLoader(\n",
        "        dataset_train, sampler=sampler_train,\n",
        "        batch_size=config.DATA.BATCH_SIZE,\n",
        "        num_workers=config.DATA.NUM_WORKERS,\n",
        "        pin_memory=config.DATA.PIN_MEMORY,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    data_loader_val = torch.utils.data.DataLoader(\n",
        "        dataset_val, sampler=sampler_val,\n",
        "        batch_size=config.DATA.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=config.DATA.NUM_WORKERS,\n",
        "        pin_memory=config.DATA.PIN_MEMORY,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # setup mixup / cutmix\n",
        "    mixup_fn = None\n",
        "    mixup_active = config.AUG.MIXUP > 0 or config.AUG.CUTMIX > 0. or config.AUG.CUTMIX_MINMAX is not None\n",
        "    if mixup_active:\n",
        "        mixup_fn = Mixup(\n",
        "            mixup_alpha=config.AUG.MIXUP, cutmix_alpha=config.AUG.CUTMIX, cutmix_minmax=config.AUG.CUTMIX_MINMAX,\n",
        "            prob=config.AUG.MIXUP_PROB, switch_prob=config.AUG.MIXUP_SWITCH_PROB, mode=config.AUG.MIXUP_MODE,\n",
        "            label_smoothing=config.MODEL.LABEL_SMOOTHING, num_classes=config.MODEL.NUM_CLASSES)\n",
        "\n",
        "    return dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn\n",
        "\n",
        "\n",
        "def build_dataset(is_train, config):\n",
        "    transform = build_transform(is_train, config)\n",
        "    if config.DATA.DATASET == 'imagenet':\n",
        "        prefix = 'train' if is_train else 'val'\n",
        "        if config.DATA.ZIP_MODE:\n",
        "            ann_file = prefix + \"_map.txt\"\n",
        "            prefix = prefix + \".zip@/\"\n",
        "            dataset = CachedImageFolder(config.DATA.DATA_PATH, ann_file, prefix, transform,\n",
        "                                        cache_mode=config.DATA.CACHE_MODE if is_train else 'part')\n",
        "        else:\n",
        "#             root = os.path.join(config.DATA.DATA_PATH, prefix)\n",
        "            root = './datasets/imagenet'\n",
        "            dataset = datasets.ImageFolder(root, transform=transform)\n",
        "        nb_classes = 1000\n",
        "    elif config.DATA.DATASET == 'inaturelist2021':\n",
        "        root = './datasets/inaturelist2021'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET,\n",
        "                             class_ratio=config.DATA.CLASS_RATIO,per_sample=config.DATA.PER_SAMPLE)\n",
        "        nb_classes = 10000\n",
        "    elif config.DATA.DATASET == 'inaturelist2021_mini':\n",
        "        root = './datasets/inaturelist2021_mini'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 10000\n",
        "    elif config.DATA.DATASET == 'inaturelist2017':\n",
        "        root = './datasets/inaturelist2017'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 5089\n",
        "    elif config.DATA.DATASET == 'inaturelist2018':\n",
        "        root = './datasets/inaturelist2018'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 8142\n",
        "    elif config.DATA.DATASET == 'cub-200':\n",
        "        root = './datasets/cub-200'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 200\n",
        "    elif config.DATA.DATASET == 'stanfordcars':\n",
        "        root = './datasets/stanfordcars'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 196\n",
        "    elif config.DATA.DATASET == 'oxfordflower':\n",
        "        root = './datasets/oxfordflower'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 102\n",
        "    elif config.DATA.DATASET == 'stanforddogs':\n",
        "        root = './datasets/stanforddogs'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 120\n",
        "    elif config.DATA.DATASET == 'nabirds':\n",
        "        root = './datasets/nabirds'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 555\n",
        "    elif config.DATA.DATASET == 'aircraft':\n",
        "        root = './datasets/aircraft'\n",
        "        dataset = DatasetMeta(root=root,transform=transform,train=is_train,aux_info=config.DATA.ADD_META,dataset=config.DATA.DATASET)\n",
        "        nb_classes = 100\n",
        "    elif config.DATA.DATASET == 'cifar100':\n",
        "        root = './datasets/cifar100'\n",
        "        dataset = datasets.CIFAR100(root=root,transform=transform,train=is_train, download=True)\n",
        "        nb_classes = 100\n",
        "    else:\n",
        "        raise NotImplementedError(\"We only support ImageNet and inaturelist.\")\n",
        "\n",
        "    return dataset, nb_classes\n",
        "\n",
        "\n",
        "def build_transform(is_train, config):\n",
        "    resize_im = True #config.DATA.IMG_SIZE > 32\n",
        "    if is_train:\n",
        "        # this should always dispatch to transforms_imagenet_train\n",
        "        transform = create_transform(\n",
        "            input_size=config.DATA.IMG_SIZE,\n",
        "            is_training=True,\n",
        "            color_jitter=config.AUG.COLOR_JITTER if config.AUG.COLOR_JITTER > 0 else None,\n",
        "            auto_augment=config.AUG.AUTO_AUGMENT if config.AUG.AUTO_AUGMENT != 'none' else None,\n",
        "            re_prob=config.AUG.REPROB,\n",
        "            re_mode=config.AUG.REMODE,\n",
        "            re_count=config.AUG.RECOUNT,\n",
        "            interpolation=config.DATA.TRAIN_INTERPOLATION,\n",
        "        )\n",
        "        if not resize_im:\n",
        "            # replace RandomResizedCropAndInterpolation with\n",
        "            # RandomCrop\n",
        "            transform.transforms[0] = transforms.RandomCrop(config.DATA.IMG_SIZE, padding=4)\n",
        "        return transform\n",
        "\n",
        "    t = []\n",
        "    if resize_im:\n",
        "        if config.TEST.CROP:\n",
        "            size = int((256 / 224) * config.DATA.IMG_SIZE)\n",
        "            t.append(\n",
        "                transforms.Resize(size, interpolation=_pil_interp(config.DATA.INTERPOLATION)),\n",
        "                # to maintain same ratio w.r.t. 224 images\n",
        "            )\n",
        "            t.append(transforms.CenterCrop(config.DATA.IMG_SIZE))\n",
        "        else:\n",
        "            t.append(\n",
        "                transforms.Resize((config.DATA.IMG_SIZE, config.DATA.IMG_SIZE),\n",
        "                                  interpolation=_pil_interp(config.DATA.INTERPOLATION))\n",
        "            )\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "    return transforms.Compose(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "48pFNt5K-RCM",
        "outputId": "b9326f4c-4723-498f-868d-f1a3dae75769"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MetaFormer/data/build.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Change the default config\n",
        "\n",
        "%%writefile /content/MetaFormer/config.py\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Swin Transformer\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Ze Liu\n",
        "# --------------------------------------------------------'\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "from yacs.config import CfgNode as CN\n",
        "\n",
        "_C = CN()\n",
        "\n",
        "# Base config files\n",
        "_C.BASE = ['']\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data settings\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.DATA = CN()\n",
        "# Batch size for a single GPU, could be overwritten by command line argument\n",
        "_C.DATA.BATCH_SIZE = 32\n",
        "# Path to dataset, could be overwritten by command line argument\n",
        "_C.DATA.DATA_PATH = ''\n",
        "# Dataset name\n",
        "_C.DATA.DATASET = 'imagenet'\n",
        "# Input image size\n",
        "_C.DATA.IMG_SIZE = 224\n",
        "# Interpolation to resize image (random, bilinear, bicubic)\n",
        "_C.DATA.INTERPOLATION = 'bicubic'\n",
        "_C.DATA.TRAIN_INTERPOLATION = 'bicubic'\n",
        "# Use zipped dataset instead of folder dataset\n",
        "# could be overwritten by command line argument\n",
        "_C.DATA.ZIP_MODE = False\n",
        "# Cache Data in Memory, could be overwritten by command line argument\n",
        "_C.DATA.CACHE_MODE = 'part'\n",
        "# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n",
        "_C.DATA.PIN_MEMORY = True\n",
        "# Number of data loading threads\n",
        "_C.DATA.NUM_WORKERS = 4\n",
        "# hdfs data dir\n",
        "_C.DATA.TRAIN_PATH = None\n",
        "_C.DATA.VAL_PATH = None\n",
        "# arnold dataset parallel\n",
        "_C.DATA.NUM_READERS = 4\n",
        "\n",
        "\n",
        "#meta info\n",
        "_C.DATA.ADD_META = False\n",
        "_C.DATA.FUSION = 'early'\n",
        "_C.DATA.MASK_PROB = 0.0\n",
        "_C.DATA.MASK_TYPE = 'constant'\n",
        "_C.DATA.LATE_FUSION_LAYER = -1\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Model settings\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.MODEL = CN()\n",
        "# Model type\n",
        "_C.MODEL.TYPE = ''\n",
        "# Model name\n",
        "_C.MODEL.NAME = ''\n",
        "# Checkpoint to resume, could be overwritten by command line argument\n",
        "_C.MODEL.RESUME = False\n",
        "# Number of classes, overwritten in data preparation\n",
        "_C.MODEL.NUM_CLASSES = 1000\n",
        "# Dropout rate\n",
        "_C.MODEL.DROP_RATE = 0.0\n",
        "# Drop path rate\n",
        "_C.MODEL.DROP_PATH_RATE = 0.1\n",
        "# Label Smoothing\n",
        "_C.MODEL.LABEL_SMOOTHING = 0.1\n",
        "#pretrain\n",
        "_C.MODEL.PRETRAINED = \"/content/MetaFormer/pretrained_model/metafg_0_1k_224.pth\"\n",
        "_C.MODEL.DORP_HEAD = True\n",
        "_C.MODEL.DORP_META = True\n",
        "\n",
        "_C.MODEL.ONLY_LAST_CLS = False\n",
        "_C.MODEL.EXTRA_TOKEN_NUM = 1\n",
        "_C.MODEL.META_DIMS = []\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training settings\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.TRAIN = CN()\n",
        "_C.TRAIN.START_EPOCH = 0\n",
        "_C.TRAIN.EPOCHS = 300\n",
        "_C.TRAIN.WARMUP_EPOCHS = 20\n",
        "_C.TRAIN.WEIGHT_DECAY = 0.05\n",
        "_C.TRAIN.BASE_LR = 1e-4 # 5e-4\n",
        "_C.TRAIN.WARMUP_LR = 5e-7\n",
        "_C.TRAIN.MIN_LR = 1e-5 # 5e-6\n",
        "# Clip gradient norm\n",
        "_C.TRAIN.CLIP_GRAD = 5.0\n",
        "# Auto resume from latest checkpoint\n",
        "_C.TRAIN.AUTO_RESUME = False\n",
        "# Gradient accumulation steps\n",
        "# could be overwritten by command line argument\n",
        "_C.TRAIN.ACCUMULATION_STEPS = 0\n",
        "# Whether to use gradient checkpointing to save memory\n",
        "# could be overwritten by command line argument\n",
        "_C.TRAIN.USE_CHECKPOINT = False\n",
        "\n",
        "# LR scheduler\n",
        "_C.TRAIN.LR_SCHEDULER = CN()\n",
        "_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n",
        "# Epoch interval to decay LR, used in StepLRScheduler\n",
        "_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n",
        "# LR decay rate, used in StepLRScheduler\n",
        "_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n",
        "\n",
        "# Optimizer\n",
        "_C.TRAIN.OPTIMIZER = CN()\n",
        "_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n",
        "# Optimizer Epsilon\n",
        "_C.TRAIN.OPTIMIZER.EPS = 1e-8\n",
        "# Optimizer Betas\n",
        "_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n",
        "# SGD momentum\n",
        "_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Augmentation settings\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.AUG = CN()\n",
        "# Color jitter factor\n",
        "_C.AUG.COLOR_JITTER = 0.4\n",
        "# Use AutoAugment policy. \"v0\" or \"original\"\n",
        "_C.AUG.AUTO_AUGMENT = 'rand-m9-mstd0.5-inc1'\n",
        "# Random erase prob\n",
        "_C.AUG.REPROB = 0.25\n",
        "# Random erase mode\n",
        "_C.AUG.REMODE = 'pixel'\n",
        "# Random erase count\n",
        "_C.AUG.RECOUNT = 1\n",
        "# Mixup alpha, mixup enabled if > 0\n",
        "_C.AUG.MIXUP = 0.8\n",
        "# Cutmix alpha, cutmix enabled if > 0\n",
        "_C.AUG.CUTMIX = 1.0\n",
        "# Cutmix min/max ratio, overrides alpha and enables cutmix if set\n",
        "_C.AUG.CUTMIX_MINMAX = None\n",
        "# Probability of performing mixup or cutmix when either/both is enabled\n",
        "_C.AUG.MIXUP_PROB = 1.0\n",
        "# Probability of switching to cutmix when both mixup and cutmix enabled\n",
        "_C.AUG.MIXUP_SWITCH_PROB = 0.5\n",
        "# How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"\n",
        "_C.AUG.MIXUP_MODE = 'batch'\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Testing settings\n",
        "# -----------------------------------------------------------------------------\n",
        "_C.TEST = CN()\n",
        "# Whether to use center crop when testing\n",
        "_C.TEST.CROP = True\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Misc\n",
        "# -----------------------------------------------------------------------------\n",
        "# Mixed precision opt level, if O0, no amp is used ('O0', 'O1', 'O2')\n",
        "# overwritten by command line argument\n",
        "_C.AMP_OPT_LEVEL = ''\n",
        "# Path to output folder, overwritten by command line argument\n",
        "_C.OUTPUT = ''\n",
        "# Tag of experiment, overwritten by command line argument\n",
        "_C.TAG = 'default'\n",
        "# Frequency to save checkpoint\n",
        "_C.SAVE_FREQ = 1\n",
        "# Frequency to logging info\n",
        "_C.PRINT_FREQ = 10\n",
        "# Fixed random seed\n",
        "_C.SEED = 0\n",
        "# Perform evaluation only, overwritten by command line argument\n",
        "_C.EVAL_MODE = False\n",
        "# Test throughput only, overwritten by command line argument\n",
        "_C.THROUGHPUT_MODE = False\n",
        "# local rank for DistributedDataParallel, given by command line argument\n",
        "_C.LOCAL_RANK = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _update_config_from_file(config, cfg_file):\n",
        "    config.defrost()\n",
        "    with open(cfg_file, 'r') as f:\n",
        "        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "    for cfg in yaml_cfg.setdefault('BASE', ['']):\n",
        "        if cfg:\n",
        "            _update_config_from_file(\n",
        "                config, os.path.join(os.path.dirname(cfg_file), cfg)\n",
        "            )\n",
        "    print('=> merge config from {}'.format(cfg_file))\n",
        "    config.merge_from_file(cfg_file)\n",
        "    config.freeze()\n",
        "\n",
        "\n",
        "def update_config(config, args):\n",
        "    _update_config_from_file(config, args.cfg)\n",
        "\n",
        "    config.defrost()\n",
        "    if args.opts:\n",
        "        config.merge_from_list(args.opts)\n",
        "\n",
        "    # merge from specific arguments\n",
        "    if args.batch_size:\n",
        "        config.DATA.BATCH_SIZE = args.batch_size\n",
        "    if args.data_path:\n",
        "        config.DATA.DATA_PATH = args.data_path\n",
        "    if args.zip:\n",
        "        config.DATA.ZIP_MODE = True\n",
        "    if args.cache_mode:\n",
        "        config.DATA.CACHE_MODE = args.cache_mode\n",
        "    if args.resume:\n",
        "        config.MODEL.RESUME = args.resume\n",
        "    if args.accumulation_steps:\n",
        "        config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n",
        "    if args.use_checkpoint:\n",
        "        config.TRAIN.USE_CHECKPOINT = True\n",
        "    if args.amp_opt_level:\n",
        "        config.AMP_OPT_LEVEL = args.amp_opt_level\n",
        "    if args.output:\n",
        "        config.OUTPUT = args.output\n",
        "    if args.tag:\n",
        "        config.TAG = args.tag\n",
        "    if args.eval:\n",
        "        config.EVAL_MODE = True\n",
        "    if args.throughput:\n",
        "        config.THROUGHPUT_MODE = True\n",
        "\n",
        "        \n",
        "    if args.num_workers is not None:\n",
        "        config.DATA.NUM_WORKERS = args.num_workers\n",
        "        \n",
        "    #set lr and weight decay\n",
        "    if args.lr is not None:\n",
        "        config.TRAIN.BASE_LR = args.lr\n",
        "    if args.min_lr is not None:\n",
        "        config.TRAIN.MIN_LR = args.min_lr\n",
        "    if args.warmup_lr is not None:\n",
        "        config.TRAIN.WARMUP_LR = args.warmup_lr\n",
        "    if args.warmup_epochs is not None:\n",
        "        config.TRAIN.WARMUP_EPOCHS = args.warmup_epochs\n",
        "    if args.weight_decay is not None:\n",
        "        config.TRAIN.WEIGHT_DECAY = args.weight_decay\n",
        "\n",
        "    if args.epochs is not None:\n",
        "        config.TRAIN.EPOCHS = args.epochs\n",
        "    if args.dataset is not None:\n",
        "        config.DATA.DATASET = args.dataset\n",
        "    if args.lr_scheduler_name is not None:\n",
        "        config.TRAIN.LR_SCHEDULER.NAME = args.lr_scheduler_name\n",
        "    if args.pretrain is not None:\n",
        "        config.MODEL.PRETRAINED = args.pretrain\n",
        "\n",
        "    # set local rank for distributed training\n",
        "    config.LOCAL_RANK = args.local_rank\n",
        "\n",
        "    # output folder\n",
        "    config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME, config.TAG)\n",
        "\n",
        "    config.freeze()\n",
        "\n",
        "\n",
        "def get_config(args):\n",
        "    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n",
        "    # Return a clone so that the defaults will not be altered\n",
        "    # This is for the \"local variable\" use pattern\n",
        "    config = _C.clone()\n",
        "    update_config(config, args)\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "################### For Inferencing ####################\n",
        "def update_inference_config(config, args):\n",
        "    _update_config_from_file(config, args.cfg)\n",
        "\n",
        "    config.defrost()\n",
        "\n",
        "    config.freeze()\n",
        "\n",
        "\n",
        "def get_inference_config(cfg_path):\n",
        "    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n",
        "    # Return a clone so that the defaults will not be altered\n",
        "    # This is for the \"local variable\" use pattern\n",
        "    config = _C.clone()\n",
        "    update_inference_config(config, cfg_path)\n",
        "\n",
        "    return config\n",
        "\n",
        "################### For Inferencing ####################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "8f4_vstDB5o-",
        "outputId": "544cb75a-9132-4729-cfe0-5424eeb2329c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MetaFormer/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ],
      "metadata": {
        "id": "11yx1_geskkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MetaFormer\n",
        "!python -m torch.distributed.launch \\\n",
        "  --nproc_per_node 1 \\\n",
        "  --master_port 12345  \\\n",
        "  main.py \\\n",
        "  --cfg ./configs/MetaFG_0_224.yaml \\\n",
        "  --batch-size 32 \\\n",
        "  --lr 5e-5 \\\n",
        "  --min-lr 5e-7 \\\n",
        "  --warmup-lr 5e-8 \\\n",
        "  --epochs 300 \\\n",
        "  --warmup-epochs 20 \\\n",
        "  --dataset cifar100 \\\n",
        "  --pretrain ./pretrained_model/metafg_0_1k_224.pth \\\n",
        "  --accumulation-steps 2 \\\n",
        "  --opts DATA.IMG_SIZE 224  "
      ],
      "metadata": {
        "id": "WYMaPORI9NUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}